,id,title,author,advisor,year,abstract,university,degree,URI,department,discipline,language,schooltype,oadsclassifier,borndigital
0,1436,Plasma Diagnostics and Plasma-Surface Interactions in Inductively Coupled Plasmas,"Titus, Monica Joy","Graves, David B;",2010,"The semiconductor industry's continued trend of manufacturing device features on the nanometer scale requires increased plasma processing control and improved understanding of plasma characteristics and plasma-surface interactions. This dissertation presents a series of experimental results for focus studies conducted in an inductively coupled plasma (ICP) system. First novel ""on-wafer"" diagnostic tools are characterized and related to plasma characteristics. Second, plasma-polymer interactions are characterized as a function of plasma species and processing parameters. Complimentary simulations accompany each focus study to supplement experimental findings.Wafer heating mechanisms in inductively coupled molecular gas plasmas are explored with PlasmaTempTM, a novel ""on-wafer"" diagnostic tool. Experimental wafer measurements are obtained with the PlasmaTempTM wafer processed in argon (Ar) and argon-oxygen (Ar/O2) mixed plasmas. Wafer heating mechanisms were determined by combining the experimental measurements with a 3-dimensional heat transfer model of the wafer.  Comparisons between pure Ar and Ar/O2 plasmas demonstrate that two additional wafer heating mechanisms can be important in molecular gas plasmas compared to atomic gas discharges.  Thermal heat conduction from the neutral gas and O-atom recombination on wafer surface can contribute as much as 60 % to wafer heating under conditions of low-energy ion bombardment in molecular plasmas.Measurements of a second novel ""on-wafer"" diagnostic sensor, the PlasmaVoltTM, were tested and validated in the ICP system for Ar plasmas varying in power and pressure. Sensor measurements were interpreted with a numerical sheath simulation and comparison to scaling laws derived from the inhomogeneous sheath model. The study demonstrates sensor measurements are proportional to the RF-current through the sheath and the scaling is a function of sheath impedance. PlasmaVoltTM sensor measurements are proportional to the square root of the plasma density at the plasma-sheath interface, one-fourth root of the electron temperature, and one-fourth root of the RF bias voltage under conditions where the sheath is predominantly capacitive. When the sheath impedance becomes increasingly resistive, the sensor measurements deviate from the scaling law and tend to be directly proportional to the plasma density. Vacuum ultraviolet (VUV) emissions in Ar ICPs are characterized and the chemical and physical modifications to 193 nm photoresist (PR) polymer materials processed in Ar ICPs are investigated. Fourier transform infrared (FTIR) transmission measurements as a function of VUV photon fluence demonstrate that VUV-induced bond breaking occurs over a period of time.  A numerical model demonstrates that VUV photons deplete near-surface O-containing bonds, leading to deeper, subsequent penetration and more bond losses, while the remaining near-surface C-C bonds absorb the incident radiation and slow VUV photon penetration. The roughening mechanism of blanket and patterned 193 nm PR samples are explored in a well characterized Ar ICP.  FTIR and atomic force microscopy (AFM) analysis of plasma processed 193 nm PR suggests that ion-induced generation of a graphitized layer at high energies, combined with VUV bulk modification of 193 nm PR may initiate PR roughening. The roughness of blanket samples increases as a function of VUV fluence, ion energy, and substrate temperature.  Line width roughness (LWR) measurements of patterned samples demonstrate a similar trend suggesting that LWR may correlate with surface roughness of patterns. The results are compared to PR studies previously conducted in an ultra-high vacuum beam system demonstrating that the vacuum beam system is a useful tool that can deconvolute and simplify complex plasma systems.",ucb,,https://escholarship.org/uc/item/0hn5z4f1,,,eng,REGULAR,0,0
1,1437,Declarative Systems,"Condie, Tyson","Hellerstein, Joseph M;",2011,"Building system software is a notoriously complex and arduous endeavor.Developing tools and methodologies for practical system software engineeringhas long been an active area of research.  This thesis explores system softwaredevelopment through the lens of a declarative, data-centric programminglanguage that can succinctly express high-level system specifications and bedirectly compiled to executable code.  By unifying specification andimplementation, our approach avoids the common problem of implementationsdiverging from specifications over time.  In addition, we show that using adeclarative language often results in drastic reductions in code size (100× andmore) relative to procedural languages like Java and C++.  We demonstrate theseadvantages by implementing a host of functionalities at various levels of thesystem hierarchy, including network protocols, query optimizers, and schedulingpolicies.  In addition to providing a compact and optimized implementation, wedemonstrate that our declarative implementations often map very naturally totraditional specifications: in many cases they are line-by-line translations ofpublished pseudcode.We started this work with the hypothesis that declarative languages --originally developed for the purposes of data management and querying -- couldbe fruitfully adapted to the specification and implementation of core systeminfrastructure.  A similar argument had been made for networking protocols afew years earlier [61].  However, our goals were quite different: we wanted toexplore a broader range of algorithms and functionalities (dynamic programming,scheduling, program rewriting, and system auditing) that were part of complex,real-world software systems.  We identified two existing system components --query optimizers in a DBMS and task schedulers in a cloud computing system --that we felt would be better specified via a declarative language.  Given ourinterest in delivering real-world software, a key challenge was identifying theright system boundary that would permit meaningful declarative implementationsto coexist within existing imperative system architectures.  We found thatrelations were a natural boundary for maintaining the ongoing system state onwhich the imperative and declarative code was based, and provided an elegantway to model system architectures.This thesis explores the boundaries of declarative systems via two projects.We begin with Evita Raced; an extensible compiler for the Overlog language usedin our declarative networking system, P2.  Evita Raced is a metacompiler -- anOverlog compiler written in Overlog -- that integrates seamlessly with the P2dataflow architecture.  We first describe the minimalist design of Evita Raced,including its extensibility interfaces and its reuse of the P2 data model andruntime engine.  We then demonstrate that a declarative language like Overlogis well-suited to expressing traditional and novel query optimizations as wellas other program manipulations, in a compact and natural fashion.  FollowingEvita Raced, we describe the initial work in BOOM Analytics, which began as alarge-scale experiment at building ""cloud"" software in a declarative language.Specifically, we used the Overlog language to implement a ""Big Data"" analyticsstack that is API-compatible with the Hadoop MapReduce architecture andprovides comparable performance.  We extended our declarative version of Hadoopwith complex distributed features that remain absent in the stock Hadoop Javaimplementation, including alternative scheduling policies, online aggregation,continuous queries, and unique monitoring and debugging facilities.  We presentquantitative and anecdotal results from our experience, providing concreteevidence that both data-centric design and declarative languages cansubstantially simplify systems programming.",ucb,,https://escholarship.org/uc/item/0sn1r9st,,,eng,REGULAR,0,0
2,1438,Portrait of the Rugged Individualist: The Nonverbal Pride Display Communicates Support for Meritocracy,"Horberg, Elizabeth Jane","Keltner, Dacher;",2010,"Emotions profoundly influence beliefs about morality and justice (Haidt, 2001) and emerging research suggests that expressions of emotion communicate an individual's moral attributes to others (e.g., Brown, Palameta, & Moore, 2002). The present research examines the moral beliefs signaled by the nonverbal pride display. Pride is triggered by appraisals that the self merits high status and greater access to resources (Tracy & Robins, 2004) and pride's nonverbal expression has been shown to convey these appraisals to observers (Shariff & Tracy, 2009). Guided by appraisal-tendency frameworks of the association between distinct emotions and moral beliefs (Horberg, Oveis, & Keltner, 2010), I predicted that the nonverbal expression of pride would communicate greater support for meritocracy--the belief that social and material resources ought to be distributed according to merit--relative to egalitarianism, or beliefs that resources ought to be distributed in ways that promote equality of outcomes. Study 1 demonstrated these effects using unfamiliar male and female targets posing pride or joy in photographs. Study 2 found that individuals previously shown a photo of Barack Obama expressing pride, relative to a neutral expression, subsequently rated Obama as more likely to endorse meritocracy. Finally, Study 3 tests the validity of pride-based inferences of support for meritocracy. This study demonstrated that individuals who spontaneously expressed pride to a greater degree were more likely to advocate dividing a resource between the self and another on the basis of merit rather than equally. Moreover, consistent with Studies 1 and 2, observers rated the high-pride expressers as more likely to support meritocracy and less likely to support egalitarianism.",ucb,,https://escholarship.org/uc/item/0v37d9g2,,,eng,REGULAR,0,0
3,1439,Essays in Empirical Macroeconomics,"Nelson Mondragon, John Alexander","Gorodnichenko, Yuriy;",2015,"This dissertation provides evidence on the eﬀects of changes in the supply of credit to households during the 2000s on employment and other outcomes of interest during the 2000s. In the ﬁrst chapter I study the eﬀects of contractions in household credit supply during the ﬁnancial crisis of 2007-2009. I exploit a county’s exposure to the collapse of a large and previously healthy lender as a natural experiment. I show that exposure to this shock appears to be uncorrelated with other important shocks at the time. Reduced form estimates suggest that this shock had large eﬀects on the ﬂow of credit, housing and non-housing expenditures, and employment. Using exposure to this shock as an instrument gives an estimated elasticity of employment with respect to household credit of about 0.3, caused by declines in both housing and non-housing demand. In the second chapter I study the size of the credit supply shock using non-parametric methods. I identify lender-speciﬁc supply-side shocks, which I then aggregate into a simple  measure of credit supply shocks to counties. I provide conditions under which this measured shock can be used to quantify the importance of supply shocks to credit in both the cross-section and, in a partial equilibrium sense, the aggregate. Combining this measure with various estimates of the elasticity of employment with respect to the measure, I calculate that shocks to household credit can be responsible for 30 to 60% of the decline in employment from 2007 to 2010.",ucb,,https://escholarship.org/uc/item/0wh0h5bj,,,eng,REGULAR,0,0
4,1440,Control and Trajectory Generation of a Wearable Mobility Exoskeleton for Spinal Cord Injury Patients,"Swift, Timothy Alan","Kazerooni, Homayoon;",2011,"There are currently nearly 1.3 million people in the United States who have some form of lower extremity paralysis due to a spinal cord injury (SCI).  For many of these subjects, a wheelchair is their primary means of mobility which brings along with it a collection of health complications stemming from extended periods of time in a seated position.  To provide these SCI patients with a viable upright mobility option, this work presents the control structure and trajectory generation method for a mobility exoskeleton that allows them to ambulate while reliably generating a gait similar to that of an unimpaired subject.  Also included are theoretical extensions to the trajectory generation structure and results from initial rounds of subject testing.",ucb,,https://escholarship.org/uc/item/0xc9q3b6,,,eng,REGULAR,0,0
5,1441,Errors as a Productive Context for Classroom Discussions: A Longitudinal Analysis of Norms in a Classroom Community,"Leveille Buchanan, Nicole Therese","Saxe, Geoffrey;",2016,"How do teachers and students create classroom environments in which mathematical errors are regarded as important opportunities for learning?  What norms support students in learning from their errors, and how to these norms develop in a classroom community?  This dissertation addresses these questions through a longitudinal case study investigating the emergence of classroom norms related to the treatment of errors.  Classroom norms are here understood to be taken-as-shared expectations for behavior in a classroom. The fifth-grade case study classroom was selected because in prior research studies the experienced and highly-regarded teacher had engaged students in rich discussions of their mathematical errors and the opportunities presented by these errors for learning.  To answer the questions of (a) what norms related to mathematical errors were taken up, and (b) how these norms developed over the course of the school year, Saxe’s (2012) framework describing the relation of micro-, onto-, and sociogenetic processes was used as a guide for determining methods.  Sociogenetic processes were the focus of this investigation, and Saxe’s framework points to microgenetic constructions, when studied collectively over time, as likely to illuminate otherwise difficult to observe sociogenetic processes, such as norm development.  To provide information about microgenetic constructions related to errors, several types of evidence were collected throughout the school year during three data collection periods lasting two weeks each: at the beginning (September), middle (January), and end (April) of the 2014 to 2015 school year.  During all three time periods, the teacher was interviewed, five “focal” students were interviewed, classroom mathematics lessons were video-recorded daily, and all students in the classroom answered a paper-and-pencil multiple choice survey about their expectations related to errors. Interviews were analyzed using grounded analysis methods, and video-recordings were analyzed using a focused coding procedure and StudioCode software.  Sources of evidence were used in a triangulating fashion to identify norms in the classroom.Through this analysis, seven norms were identified as having been taken up by the majority of members of the class by the end-of-year data collection period. Two of these norms were selected for in-depth description. The norm everyone has some mathematical understandings to which you should pay attention provides a good example of a norm that was closely tied to a specific collective practice, the “coaching” practice that was used frequently in the case-study classroom.  The norm there are different types of errors, only some of which are acceptable provides an example of a norm that emerged part-way through the school year in response to a problem with the way errors were being treated.  Classroom interactions and teacher and student interview statements exemplifying these norms are described. The process through which these two norms emerged in the case study classroom over the course of the school year is detailed, using evidence collected throughout the school year.  In general, the teacher strongly promoted these norms by frequently and persistently modeling, describing, and praising behaviors consistent with these norms and by correcting inconsistent behaviors. Implications for how Saxe’s framework may be productively applied in future investigations of classrooms norms are discussed.  In particular, attention to ontogenetic processes – that is, individuals’ shifting expectations over time – was found to be useful as an access point for identifying the norms of a classroom community, and the teacher’s actions and expectations were found to be especially important indicators of classroom norms.  Examination of collective practices related to errors was also useful for the identification of norms because some norms were strongly associated with collective practices, such as the “coaching” practice.  The results of this study also have implications for teaching practice.  The findings indicate that children are capable of taking up challenging practices related to the study of errors, and teachers who promote these practices in their classrooms may be successful if they are persistent in modeling, explaining, and praising the desired practices.",ucb,,https://escholarship.org/uc/item/0zz775v7,,,eng,REGULAR,0,0
6,1442,Interrogating the Role of Spatial Organization in Receptor Function: Eph-Ephrin Signaling in Breast Cancer,"Nair, Pradeep","Groves, Jay T.;",2010,"Cells in living tissue integrate multiple signals from their environment to govern numerous aspects of both healthy and diseased behavior.  The cell membrane serves as an exquisite functional filter that regulates information transmission between a cell and its surrounding environment.  This viscoelastic plasma membrane, which allows lateral diffusion while restricting the orientation of signaling molecules within the plane of a phospholipid bilayer, is uniquely well suited to make sense of the myriad biochemical, mechanical, and spatial cues that constantly stimulate receptors on the cell surface.  The chemical basis for the cell membrane, a fluid phospholipid bilayer, can be used to create a supported membrane that retains these properties while allowing precise control over the physical and chemical aspects of signaling molecules on the supported membrane surface.Cell communication is critical for proper maintenance of multicellular organisms, and tumorigenesis can occur when communication is not properly controlled.  Cancerous cells often display a vastly altered array of cell surface receptors compared to normal cells, and the abnormal signaling that these receptors trigger has grave consequences for the fate of the cell and the organism as a whole.  The dynamics by which these receptors bind to ligands within the environment are not well understood because the cell membrane is a chemically heterogeneous and physically irregular surface that is difficult to study in vivo.  Here we recapitulate signaling events that occur in live cancer cells using the supported membrane to present laterally mobile ligands to receptor-expressing human breast cancer cells.This platform allows for precise control of the spatial organization of signaling molecules on the supported membrane surface, as well as a detailed examination of subsequent changes in signaling events within living cells.  Using this approach we observe receptor reorganization responses that are strongly linked to tissue invasion and our observations reveal a mechanism by which cells respond to the spatial and mechanical aspects of their environment.",ucb,,https://escholarship.org/uc/item/1117n3zf,,,eng,REGULAR,0,0
7,1443,Hydrodynamic Exchange in Estuarine Perimeter Habitats,"HSU, KEVIN KAI-WIN","Stacey, Mark T.;",2013,"Hydrodynamic exchange in estuaries is forced by tides, freshwater input, density forcing, and winds, and controls transport of important quantities such as salinity, sediment, nutrients, and pollutants.  Previous work has characterized many aspects of estuarine transport and contributed to our understanding of transport mechanisms such as gravitational exchange, tidal dispersion processes, and residual flows due to tidal asymmetries.  In general, studies of estuarine transport have focused on large-scale transport processes in the along-channel direction of the estuary, which determine the overall salinity and flow structure in estuarine environments.  However, study of hydrodynamic exchange at the perimeter of estuaries has also been recognized to be important, as exchange at the perimeter is relevant for understanding questions related to environmental restoration and management and ecological habitat quality.In this work, hydrodynamic exchange in estuaries and perimeter habitats is studied using numerical modeling and field observations of South San Francisco Bay.  First, the exchange between the estuary and a small perimeter slough is measured using salinity and temperature as tracers to calculate hydrodynamic flushing of the slough through tidal exchange, using a modified tidal prism method.  This method applies quasi-Lagrangian analysis to Eulerian measurements of exchange, and the results are compared to previous results from larger-scale estuarine systems, where tidal flushing is found to be significantly affected by the scale of mixing volumes in the system.  Next, Lagrangian methods of particle-tracking and Lagrangian coherent structure (LCS) analysis, developed from dynamical systems theory in order to analyze complex, chaotic flows, are applied to analyze tidal transport.  The results reveal the significant effects of tidal interactions with perimeter estuarine features on Lagrangian tidal transport over the tidal cycle, where perimeter interactions are found to significantly contribute to longitudinal estuarine dispersion.  Finally, the effect of wind forcing on estuarine transport is examined, using Lagrangian analysis methods applied to cases of constant wind forcing with varying wind direction relative to the main axis of the estuary.  Wind forcing is found to have a significant effect on hydrodynamic exchange and connectivity between the estuary and perimeter habitats, where wind in all directions increases perimeter exchange and connectivity, with the greatest effect for winds aligned with the along-axis direction of the estuary.  The results of these studies are relevant to a wide range of applications requiring analysis of connectivity near the estuarine perimeter, including sediment exchange and transport and seagrass population colonization in the context of wetland habitat restoration.",ucb,,https://escholarship.org/uc/item/11r7t98v,,,eng,REGULAR,0,0
8,1444,An Environmental and Economic Trade-off Analysis of Manufacturing Process Chains to Inform Decision Making for Sustainability,"Robinson, Stefanie","Dornfeld, David A;",2013,"Increasing costs, consumer awareness, and environmental legislation have driven industry to reduce its resource consumption and the impact from that consumption.  So, both traditional economic objectives (e.g., cost, time, and quality) and environmental objectives (e.g., CO2 emissions) have become strategically relevant for the manufacturing sector. For many manufacturing companies, production systems have a major influence on the environmental footprint of a product and therefore represent a major opportunity to minimize the company's overall environmental impact. Currently within industry, there is not an accurate, effective, or widely accepted method to assess the resource consumption of process chains used to manufacture a product.  As a result, this information is often not considered when making decisions about what processes to use. A considerable part of the energy and resource demand in manufacturing is determined during production planning.  An important component of this planning is determining the process chains to be used.  Process chains are a combined sequence of specifically arranged, single processes used to manufacture a product. As manufacturing processes are very resource intensive, it is now necessary to assess the resource consumption as well as the economics of these process chains.  Because of this, additional information must be considered when selecting the process chains used to manufacture a product.     Many life cycle assessment (LCA) tools focus on the materials and final disposition of a product, but do not include detailed information or data on the manufacturing required to fabricate the product. Sustainability impacts of discrete manufacturing processes and product value streams are needed to develop more complete LCAs. The development of a methodology and user tool to quantify sustainability impacts, leading to the identification of gaps and opportunities, is essential to facilitate decision making to support sustainability in manufacturing facilities.     To address these issues, this dissertation proposes an approach to evaluate and quantify the resource use in addition to the environmental and economic impacts associated with discrete manufacturing processes as part of a complex process chain. A methodology to evaluate multiple process chain configurations will be presented.       First, a database of industrial assessment metrics was compiled.  This database allows users to sort and select from a list of key metrics in order to choose the metrics that are relevant for the performance that they want to measure.  Next, an industrial assessment methodology was developed.  This methodology gives users an overview of the key areas to address when conducting an industrial assessment.  This methodology, which was applied to three case studies, can be used in combination with the key metrics.      In the second part of the dissertation, a resource consumption assessment and mapping methodology for complex manufacturing process chains was developed. This systematic methodology was developed to identify and quantify the resource consumption (energy, water, materials) for discrete manufacturing processes.  The processes mapped include: welding (manual and robotic), cutting (plasma arc and laser), rework (air carbon arc cutting and hand grinding), and machining (milling).      Next, a model consisting of database modules for each process was developed. This model quantifies the sustainability impacts (energy, water, and material consumption, waste generation, emissions, and resource consumption cost) of manufacturing process chains.  The model was validated using a case study with Caterpillar Inc. for a process chain including welding, plasma arc cutting, laser cutting, and milling.       Next, a process chain assessment tool was created.  This tool enables manufacturers to assess the resource consumption and associated impacts of multiple fabrication process chain configurations. This enables a more comprehensive assessment compared to other software tools.  Finally, a methodology modeled after the Six Sigma DMAIC (Define, Measure, Analyze, Improve, Control) process was presented to show how to translate the results from the model and tool to an Environmental Value Stream Map and to translate those results into improvements in manufacturing systems. This methodology was validated on a machining operation in a Caterpillar facility.         This research has developed and evaluated an effective approach for the analysis of energy, water, and other resource use in multiple processes in a manufacturing process chain.  This allows manufacturers to better understand the resource consumption and environmental and economic impacts of fabrication process chains used to make a product. This dissertation helps to provide the technical understanding and tools to enable designers and manufacturing engineers to create manufacturing systems that are truly more sustainable.  The implementation of this work can be directly applied to assessing and optimizing manufacturing process chains and the work presented in this dissertation directly contributes to the realization of a sustainable and prosperous manufacturing sector.",ucb,,https://escholarship.org/uc/item/1513r3t9,,,eng,REGULAR,0,0
9,1445,Essays on Markets and Institutions in Emerging Economies,"GHANI, TAREK FOUAD","Tadelis, Steven;Dal Bo, Ernesto;",2015,"Market frictions pervade emerging economies and constrain private sector development. In such settings, formal institutions to help address contract enforcement, property rights and information asymmetries are typically weak or absent. Instead, market participants must rely on informal practices and institutions to mitigate uncertainty, instability and opportunism. For example, personalized exchange relationships are useful when contract enforcement is weak, and cash holdings can be attractive when financial institutions are unreliable. In three specific emerging economy settings, I explore how informal practices and institutions interact with formal market development, and in particular the role that market frictions play in determining outcomes for firms, technologies and employees. The first chapter of this dissertation explores how changes in formal upstream market structure affect the economics of downstream relationships using original data from the ice industry in Sierra Leone. In this setting, a monopoly ice manufacturer sells through independent retailers to fishermen buyers. I demonstrate that a shock that increases upstream competition among manufacturers improves the contractual terms offered by retailers to buyers.  Under the monopoly manufacturer, late deliveries are common due to outside demand shocks. To help mitigate this uncertainty, retailers prioritize loyal customers when faced with shortages, and buyers respond by rarely switching retailers. When manufacturers compete, prices fall, quantities increase and services improve with fewer late deliveries. Entry upstream also disrupts collusion among retailers by increasing the value of competing for buyer relationships. Competing retailers expand trade credit provision as a new basis for loyalty, and stable buyer relationships reemerge after a period of intense switching. The findings suggest that market structure shapes informal contractual institutions, and that competition can reconstitute the nature of relationships.  The second chapter addresses the relationship between violence and financial decisions in Afghanistan. In particular, I investigate how violence affects the tradeoff between informal cash holdings - which are liquid but insecure - and usage of a more secure but less liquid formal financial account. Using three separate data sources, I find that individuals experiencing violence retain more cash and are less likely to adopt and use mobile money, a new financial technology. First, combining detailed information on the entire universe of mobile money transactions in Afghanistan with administrative records for all violent incidents recorded by international forces, I find a negative relationship between violence and mobile money use. Second, in the context of a randomized control trial, violence is associated with decreased mobile money use and greater cash balances. Third, in financial survey data from nineteen of Afghanistan's 34 provinces, I find that individuals experiencing violence hold more cash. Collectively, the evidence indicates that individuals experiencing violence prefer cash to mobile money. More speculatively, it appears that this is principally because of concerns about future violence. These results emphasize the difficulty of creating robust financial networks in conflict settings.Finally, in the third chapter, I study how informal behaviors interact with incentives to affect employees' decisions to formally save in the context of a large firm in Afghanistan. I analyze a mobile phone-based account that allows savings to be automatically deducted from salaries. Employees who are automatically enrolled in this defined-contribution account are 40 percentage points more likely to contribute than individuals with a default contribution of zero. Analyzing randomly assigned employer matching contributions, I find that the effect of automatic enrollment on participation is approximately equivalent to providing financial incentives equal to a 50 percent match. To understand why default enrollment increases participation, some employees are randomly offered an immediate financial consultation, and others a financial consultation in one week. Employees are more likely to discuss changing their savings contributions in one week, suggesting that defaults raise contributions because of the perceived complexity of financial decisions, and because employees procrastinate in developing a financial plan for the future.",ucb,,https://escholarship.org/uc/item/15g1g7dv,,,eng,REGULAR,0,0
10,1446,A Nexus Between Two Disruptions: A Multiscale Analysis of Transportation Electrification to Forecast the Impacts of Vehicle Grid Integration,"Sheppard, Colin John Ritter","Walker, Joan;",2019,"In this dissertation, I present a body of work that advances our understanding of the technical and economic potential for vehicle grid integration based on a variety of methodological approaches that quantify the opportunity at multiple scales, across multiple geographies, and that cover scenarios with both personally owned plug-in electric vehicles (PEVs) and shared autonomous electric vehicles (SAEVs). The key research questions addressed in this dissertation include:* How can charging infrastructure be cost-effectively deployed to maximize utilization and value to PEV drivers?* How much flexibility exists in the charging demand from PEVs? * What is the economic opportunity to manage the charging of PEVs to occur at lower cost time periods?* How will fleets of electrified autonomous vehicles serving mobility on-demand differ in how they are managed to minimize the cost of charging or to serve as a source of electricity for buildings?These questions are motivated by the fact that transportation electrification and emerging forms of mobility are dramatically changing how the transportation system is planned, operated, and analyzed. PEVs present new challenges and constraints around the siting and operation of refueling infrastructure. Electric load from PEVs can exacerbate grid congestion at either transmission or distribution scales if left unmanaged. Sharing and autonomy are changing mobility which will have unique implications for the grid integration of PEVs. Meanwhile, there are strong social and environmental forces compelling planners, regulators, and private industry to electrify transportation as soon as possible. The transportation sector is the largest emitter of greenhouse gases in the United States. With the exception of the great recession, emissions in the transportation sector have been growing for the last three decades, in contrast to the electric power and industrial sectors which have been on a downward trend in emissions. Transportation, therefore, represents one of the primary challenges to achieving deep decarbonization of the U.S. economy.In the electric power sector, policy and economic forces are upending incumbent generation technologies (coal and natural gas) in favor of lower cost and lower carbon alternatives, particularly wind and solar power. As these intermittent renewable resources increase in capacity, the incidence of renewable energy (RE) curtailment increases due to time periods when supply is greater than demand and generators are turned down or shut off from the level that they would otherwise be producing. Curtailment raises the overall system cost of supplying electricity. In addition, some utilities must meet an energy production standard to satisfy state mandates for renewable production. Renewable curtailment forces utilities to either acquire more RE or introduce sources of grid flexibility to relieve the curtailment. One low cost strategy to mitigate these challenges is to manage the temporal profile of electricity demand to make use of the renewable resources when they are available.PEVs are generally analyzed through modeling using one of two approaches, statistical modeling and activity-based modeling. Statistical models typically summarize or infer travel patterns from travel survey data and use them to characterize the need for PEV charging and the temporal opportunities to charge. The key disadvantage of such approaches is that they cannot account for the individual mobility constraints of travelers and they typically require an assumption that charging infrastructure is unlimited. Another common approach is to develop Markov Chain models of mobility and PEV charging. In these models, transitions between states are treated as random events. Because they lack a representation of the causal mechanism for these transitions, these models are difficult to generalize and their utility is degraded if applied in prospective contexts assuming a transportation system with dramatically different characteristics than present. Activity-based models make use of travel diaries from surveys or GPS data logging which are then provided as input to mobility and charging simulations. Agent-based models are a subset of activity-based models, in so far as they treat travelers individually and require a representation of each individual's activity schedule in order to model the travel necessary to engage in those activities. What distinguishes agent-based models are two key features: 1) wrapping the individuals in a virtual environment (e.g. the transportation system) with detailed representation of transportation supply and 2) dynamically simulating the agents' interactions with the virtual environment and with each other. These interactions open the opportunity to model the choices of the agents based on empirical studies of human behavior as well as to make agent behavior contingent on the time-evolving state of their environment and other agents.In the electric power and grid modeling domain, load from PEVs are typically represented as static or derived from very rudimentary estimation techniques. Studies either ignore flexibility entirely or they make simplistic assumptions about the timing and degree to which PEV load can be shaped. The inaccuracy in these modeling choices have had a relatively low impact in the recent past due to the still relatively low penetration of PEVs in the national vehicle fleet. But within a decade it will no longer suffice to ignore or simplify PEV load, which could eventually make up more than 20% of U.S. electricity demand.This dissertation addresses these gaps by coupling models of electric mobility and the grid at multiple scales. Each paper presented in this dissertation was produced in collaboration with co-authors across multiple projects and contexts. I employ reduced-form models in the context of optimization to solve the charger scheduling and vehicle mobility problems, as well as detailed agent-based models that simulate context-specific traveler behaviors and the dynamics of resource-constrained charging infrastructure.To address the infrastructure siting problem, I develop a spatially explicit agent-based simulation model that represents charging infrastructure, charging behavior, competition for scarce chargers, and driver adaptation. A differential evolution and a heuristic optimization scheme are employed to find a cost-effective distribution of charging infrastructure. I then address the question of flexibility in two ways. First I develop a scheme for optimizing the charging profiles of individual PEV drivers based an objective that simultaneously considers the costs of charging and the benefits associated with providing ancillary services to the grid. Then I employ a much higher fidelity approach to simulate both the electrified mobility system as well as the power sector. I develop the BEAM modeling framework (Behavior, Energy, Mobility, and Autonomy), which is an agent-based model of PEV mobility and charging behavior designed as an extension to MATSim (the Multi-Agent Transportation Simulation model). I apply BEAM to the San Francisco Bay Area and conduct a preliminary calibration and validation of its prediction of charging load based on observed charging infrastructure utilization for the region in 2016.  I link the BEAM model with PLEXOS, an industry standard production cost model that accurately characterizes grid dispatch constraints.Finally, I consider the impact of grid-integrated fleets of SAEVs providing mobility on-demand. In two separate studies I develop models to consider how such fleets could be used to serve building energy demand during power outages as well as a more general analysis of the battery and charging infrastructure requirements to serve nationwide mobility.The key findings across all of this work are the following:* In today's energy markets, PEV flexibility can reach values of $155/year/vehicle for NYISO and $98/year/vehicle for CAISO. The annual cost savings due to optimizing dispatch come more from savings on the price of energy (74% in CAISO and 61% in NYISO) but providing ancillary services (in the form of regulation) also contributes value to the solution (26% in CAISO and 39% in NYISO).* When we project the energy market of California to a future year when renewables make up 50% of the annual energy produced, PEV flexibility is even more beneficial to the power sector, primarily in lowering grid operating cost and the amount of RE that must be curtailed to avoid over-generation when supply and demand are mismatched. For example, if treated as flexible loads, 2.5 million smart charging PEVs avoid 50\% of incremental system operating costs annually and reduce renewable energy curtailment by 27% annually relative to when the same number of unmanaged charging PEVs are added to the grid. * When SAEVs serve power to buildings during an extreme outage, the fleet can generate 32%-40% more revenue than is earned serving mobility alone. While the overall value of providing on-demand power depends on the frequency and severity of power outages, the results show that serving power demand during large-scale outages can provide a substantial value stream, comparable to what can be earned providing grid services. * All mobility in the United States currently served by 276 million personally owned vehicles could be served by 12.5 million SAEVs at a cost of $0.27/vehicle-mile or $0.18/passenger-mile. The energy requirements for this fleet would be 1142 GWh/day (8.5% of 2017 U.S. electricity demand) and the peak charging load 76.7 GW (11% of U.S. power peak). In total, this body of work contributes new insights into the opportunity for electric mobility to reduce the cost of operating the electric grid, enabling deeper and faster adoption of renewable power in the electric sector, and providing reliable mobility to travelers in the transportation sector. The domain of vehicle grid integration is still relatively new, there are many areas of research that require additional attention, such as increased research on traveler preferences around PEV charging, the intersection between electric mobility and the distribution grid, electrification of medium and heavy duty vehicles, as well as properly incentivizing electric vehicles to ride hail drivers in the gig economy.",ucb,,https://escholarship.org/uc/item/1663f91r,,,eng,REGULAR,0,0
11,1447,Tracing Patterns of Textiles in Ancient Java (8th–15th century),"Sardjono, Sandra","Williams, Joanna;Klokke, Marijke;",2017,"Few attempts have been made to study the numerous textile depictions in Java from the eighth to fifteenth centuries, also known as the Hindu-Buddhist or the Ancient Javanese Period. This thesis seeks for the textiles that inspired these depictions and considers their techniques. It also traces the evolution of particular patterns in Java over time. To do so, I employ close art-historical analysis of works of art and draw supportive materials from archaeology, epigraphy and literature, as well as ethnography. After the introductory chapter, Chapters One and Two focus each on a different textile pattern: the connected circles and the overlapping circles patterns. These chapters follow the evolution of the patterns with particular interest to search for connections to current textile tradition in Indonesia. A similar approach of inquiry is applied in Chapter Three to a type of short sleeve jacket. Chapter Four investigates the depiction of weavers in Ancient Javanese textual and visual sources. This study of textile depictions will underscore the global connection between Java and the outside world, particularly China and India, from where many prototypes of the textile images originated. The study will also reveal that these images, in addition to being historical records, were also ornamentations, which the Javanese artists were adept at translating, decontextualizing, and re-contextualizing—as a whole or in part—into the local aesthetic and usage.",ucb,,https://escholarship.org/uc/item/16f914tm,,,eng,REGULAR,0,0
12,1448,"Microtopographical control of cell adhesion, organization, and proliferation in a cardiac tissue engineering scaffold","Patel, Anuj Ashwin","Kumar, Sanjay;",2011,"Myocardial infarction, commonly known as a heart attack, is caused by the blockage of blood flow to heart, resulting in the death of cardiomyocytes, or heart muscle cells. Scar tissue formation occurs in the area of the damage due to the heart's inability to regenerate myocardial tissue. Therefore, regeneration of myocardial tissue through the use of synthetic scaffolds requires strategies to promote cardiomyocyte attachment while minimizing proliferation of the fibroblast cells that contribute to scar tissue. Previous studies have demonstrated that a synthetic platform consisting of an array of microscale polydimethylsiloxane (PDMS)-based pillars (""micropegs"") can accomplish both of these goals, but the mechanism through which this occurs has remained a mystery.   In this work the interaction between microtopographical cues and both fibroblasts and cardiomyocytes is further explored. It is shown that a fibroblast that is attached to a micropeg is less likely to proliferate than ones on a flat surface, but this difference can be partially abrogated in the presence of drugs that inhibit cell contractility. The cells also show increased adhesion to the micropegs as opposed to flat surfaces, as demonstrated by measurements of the dynamics of deadhesion from the surface and changes in expression of specific mechanotransductive genes. Together, these data support a model in which microtopographical cues alter the local mechanical microenvironment of cells by modulating adhesion and adhesion-dependent mechanotransductive signaling, thereby leading to a reduction in proliferation capability.   The research focus then shifts to the use of microtopographical cues to control cardiomyocyte adhesion and organization. Cardiomyocytes cluster around and interact with the full length of the micropegs, exhibiting three-dimensional organization on a two-dimensional surface. By controlling the diameter and spatial arrangement of the micropegs, the degree of clustering can be regulated. The expression of functional markers N-cadherin and connexin 43 also exhibit a dependence on the spatial arrangement of the micropegs. The preference of cardiomyocytes for three-dimensional adhesion is further investigated in the final part of the thesis. By isolating cardiomyocytes in PDMS microwells, the cells are presented with the option of attaching to a vertical wall or a flat space. The cells demonstrate a preferential attachment to the side walls and corners of the microwell. Introduction of the myosin inhibitor blebbistatin reduces the percentage of cells attached to these side walls. Cells attached to a side wall also are less likely to proliferate, similar to the behavior of fibroblasts attached to micropegs. Taken together, these data indicate that incorporation of microtopographical features into cardiac tissue engineering scaffolds can be used to control the adhesion and organization of cardiomyocytes while simultaneously limiting the formation of scar tissue.",ucb,,https://escholarship.org/uc/item/1c78p3zh,,,eng,REGULAR,0,0
13,1449,"Grocery Stores: Neighborhood Retail or Urban Panacea? Exploring the Intersections of Federal Policy, Community Health, and Revitalization in Bayview Hunters Point and West Oakland, California","Elias, Renee Roy","Corburn, Jason;",2013,"Throughout the nation, grocery retailers are reentering underserved communities amidst growing public awareness of food deserts and the rise of federal, state, and local programs incentivizing urban grocery stores. And yet, even with expanding research on food deserts and their public health impacts, there is still a lack of consensus on whether grocery stores truly offer the best solution. Furthermore, scholars and policymakers alike have limited understandings of the broader neighborhood implications of grocery stores newly introduced into underserved urban communities.This dissertation analyzes how local organizations and agencies pursue grocery development in order to understand the conditions for success implementation. To do this, I examine the historical drivers, planning processes, and outcomes of two extreme cases of urban grocery development: a Fresh and Easy Neighborhood Market (a chain value store) in San Francisco's Bayview Hunters Point and the Mandela Foods Cooperative (a worker-owned cooperative) in Oakland's West Oakland districts. Through a comparative institutional analysis, I find that both Fresh and Easy and Mandela Foods reflect distinctive neighborhood revitalization legacies, critical moments of institutional capacity building, localized versions of national policy narratives, and the role of charismatic leaders in grocery store implementation. While national narratives shape the rhetoric of urban grocery development, ultimately local context dictates how food access issues are defined, who addresses them, and how. These findings suggest that federal grocery incentive programs should: 1) maintain a broad framework that enables local communities to define food access problems and their solutions on a case-by-case basis, 2) encourage diverse solutions not limited to grocery stores and supermarkets, and 3) emphasize community reinvestment goals.",ucb,,https://escholarship.org/uc/item/1d45296j,,,eng,REGULAR,0,0
14,1450,Wavefront metrology for high resolution optical systems,"Miyakawa, Ryan","Attwood, David;",2011,"Next generation extreme ultraviolet (EUV) optical systems are moving to higher resolution optics to accommodate smaller length scales targeted by the semiconductor industry.  As the numerical apertures (NA) of the optics become larger, it becomes increasingly difficult to characterize aberrations due to experimental challenges associated with high-resolution spatial filters and geometrical effects caused by large incident angles of the test wavefront.  This dissertation focuses on two methods of wavefront metrology for high resolution optical systems.  The first method, lateral shearing interferometry (LSI), is a self-referencing interferometry where the test wavefront is incident on a low spatial frequency grating, and the resulting interference between the diffracted orders is used to reconstruct the wavefront aberrations.  LSI has many advantages over other interferometric tests such as phase-shifting point diffraction interferometry (PS/PDI) due to its experimental simplicity, stability, relaxed coherence requirements, and its ability to scale to high numerical apertures. While LSI has historically been a qualitative test, this dissertation presents a novel quantitative investigation of the LSI interferogram.   The analysis reveals the existence of systematic aberrations due to the nonlinear angular response from the diffraction grating that compromises the accuracy of LSI at medium to high NAs. In the medium NA regime (0.15 < NA < 0.35), a holographic model is presented that derives the systematic aberrations in closed form, which demonstrates an astigmatism term that scales as the square of the grating defocus.  In the high NA regime (0.35 < NA), a geometrical model is introduced that describes the aberrations as a system of transcendental equations that can be solved numerically.  The characterization and removal of these systematic errors is a necessary step that unlocks LSI as a viable candidate for high NA EUV optical testing.The second method is a novel image-based reconstruction  that characterizes the aberrations of an optical system with arbitrary numerical aperture and illumination coherence.   In this method a known pattern is imaged by the test optic at several planes through focus.  A computer model is created that iterates through possible sets of wavefront aberrations until the through-focus series of aerial images matches the aerial images from the experiment.  Although the sample space of Zernike coefficients is non-convex, a hybrid algorithm consisting of pattern search and simulated annealing methods is used to search for the global minimum.The computation of aerial images from a partially coherent optical system is expedited with a novel decomposition of the Hopkins equation known as the Reduced Optical Coherent Sum (ROCS).  In this method, the Hopkins integral is described by an operator S which maps the space of pupil aberrations directly to the space of aerial images.  This operator is shown to be semipositive definite and well-approximated by a truncated sum of its spectral components.  The ROCS decomposition has a customizable error bound allowing one to tradeoff aerial image fidelity for significant speed improvements.  For aerial image errors of 1-3%, the ROCS algorithm can compute aerial images up to 15 times faster than the Hopkins integration.  The ROCS-based wavefront test is extremely versatile since it is applicable in nearly all optical systems that measure aerial image intensity regardless of numerical aperture or illumination coherence and requires little or no experimental modifications.  This test is used to characterize the field-dependent aberrations of the SEMATECH Berkeley Actinic Inspection Tool (AIT), and the results match an independent analysis of the astigmatism aberrations to within lambda/20 rms.",ucb,,https://escholarship.org/uc/item/1dd5j7ss,,,eng,REGULAR,0,0
15,1451,Investigating Innovation Practice: Cross-Disciplinary Studies in International Development,"Gordon, Pierce Edward Cornelius","Agogino, Alice M;",2018,"Innovation practice is a transdisciplinary field that aims to create a better world out of an existing one by pooling methods and mindsets of inquiry and creation. The field observes design contexts, assimilates the collected knowledge into problems to be addressed, ideates solutions to those problems, and iteratively tests those solutions in real environments to determine how they address these problems. Over the past decade, the field has become more accessible to a much broader collection of amateur designers. They utilize the field to understand more diverse contexts, to include and adapt more disciplines, and to address a wide variety of complex and seemingly intractable issues. Due to the evolution of the fields’ popularity, debates began to arise about the fields’ utility and place in society. Development professionals treated design thinking and related fields as a silver bullet that could easily address issues of global poverty. Critics asked if the field was different from existing disciplines, whether the field delivers demonstrable impact, and if the democratization of design practice to ‘amateur’ designers is even worthwhile. However, these debates revealed how little knowledge is collected about how practitioners conduct innovation practice in the first place. To learn about the activities, benefits, methods, and obstacles of beneficial development-focused design practice, I detail three studies that apply lenses of analysis to innovation narratives to see how various collectives of self-determined innovators actually practice their craft. The first study outlines a systematic literature review of human-centered design for development. By applying design principles to a population of researcher-designers and their narratives, we learn if these designers actually practice innovation with these principles of human-centeredness in mind. I outline three previously conducted studies about the nature of this field, which describe the population, location, history, and methods these projects use across various contexts. and detail an analysis of the participatory nature of human-centered design for development. In so doing, I describe statistics about the prevalence of participatory design practice, reveal how the studies report the complexities of participation, and collect insights about the stakeholders who are allowed to design. The study then sums up the importance of investigative analysis methods across populations of design narratives, so that researchers can learn more about how ‘good practice’ is perceived. The second study describes an ethnographic evaluation study of notable actors in the Botswana innovation community. This study begins with a reflection on epistemological frictions between the popular fields of innovation practice and impact evaluation. After revealing the theoretical and practical gaps in how innovators evaluate, I introduce the Botswana history, policies, and institutions that support innovation practice on the national level, while describing their activities and how innovation actors perceive them. I then detail the creation of a grassroots innovation community that practices participatory co-design of locally beneficial technologies by outlining the history of its indigenous stakeholders and describing an ethnographic narrative of two formative innovation workshops. I then describe the methods, approaches, purpose, and stakeholders involved in the evaluation of innovation in the local and national institutions. This analysis reveals evaluation tools applicable to many innovation contexts, and insights about how these evaluation approaches are aligned and misaligned with each other. Finally, I describe insights on the practice and facilitation of innovation in the country, to clarify cultural, institutional, and practical barriers and qualities that hinder the potential benefit of innovation. The final study is a reflection on the inadequacies of ethics systems in Botswana to support beneficial innovation practice. While investigating the previous chapter, I happened upon narratives with no simple solutions, and few resources for development-centric designers to effectively navigate this ethics space. Moreover, while facing the country’s institutional review board system, I gained first-hand experience with the goals, dynamics, and limitations of the Botswana research system of ethics. This chapter unpacks how the ethical system fails to align with the needs of beneficial innovation practice and suggests theoretical alternatives to draw upon for future use. This dissertation describes the complex possibilities of participatory design practice, the various goals, activities, and perceptions of the evolving Botswana innovation ecosystem, and details the frictions between the understudied field of ethics in design for development and existing institutions. These studies reveal how ‘good’ innovation practice is wholly based on the context it is applied: on its practitioners, their tools, their goals, the environment where it is used, and the stakeholders with whom the designers interact. Though these studies outline how the methods and mindsets of innovation practice are accessible to more communities than ever, it does not mean that innovation practice itself becomes simpler. These lenses of cross-contextual analysis, participation, evaluation, and ethics reveal how the amorphous, evolving field requires innovators who are responsive and respectful of the contexts in which they are situated. These studies outline a few of many approaches that reveal the unique dynamics in development-centered innovation practice but are essential for any designers’ toolbox to ensure we collectively create a better world.",ucb,,https://escholarship.org/uc/item/1f20709j,,,eng,REGULAR,0,0
16,1452,Optimal Reconstruction of Cosmological Density Fields,"Horowitz, Benjamin Aaron","Seljak, Uros;",2019,"A key objective of modern cosmology is to determine the composition and distribution of matter in the universe. While current observations seem to match the standard cosmological model with remarkable precision, there remains tensions between observations as well as mysteries relating to the true nature of dark matter and dark energy. Despite the recent increased availability of cosmological data across a wide redshift, these tensions have remained or been further worsened. With the explosion of astronomical data in the coming decade, it has become increasingly critical to extract the maximum possible amount of information available across all available scales. As the available volume for analysis increases, we are no longer sample variance limited and existing summary statistics (as well as related estimators) need to be re-examined. Fortunately, parallel with the construction of these surveys there is significant development in the computational techniques used to analyze that data. Algorithmic developments over the past decade and expansion of computational resources allow large cosmological simulations to be run with relative simplicity and parallel theoretical developments motivate increased interest in recovering the underlying large scale structure of the universe beyond the power spectra.The detailed study of this large scale structure has the potential to shed light on various unanswered questions and under-constrained physical models for the dark sector and the nature of gravity. As we reach higher redshifts with statistically significant samples, the large scale structure can serve as a link between local observations and the cosmic microwave background. These surveys rely on a variety of biased probes, including the lensing and distribution of galaxies, imprints of large scale structure in secondary anisotropies of the CMB, and absorption lines in the spectra of high redshift quasars. These observations are complementary; they probe different scales, have different sources of astrophysical and observational uncertainties, have unique degenercies in parameter space, and require their own methods to extract cosmological parameters from.In this thesis, I discuss a number of new developments in the analysis of these diverse cosmological datasets. After introductory material, I discuss work re-examining the lensing of the Cosmic Microwave Background by cluster-sized objects and implement techniques for accurate mass estimation. I demonstrate that this analysis is optimal in the low noise, small scale limit. In the second part, I develop a maximum likelihood formalism for linear density fields, applicable for reconstructing underlying signal from a variety of cosmological probes including projected galaxy fields and cosmic shear, showing that effects of anisotropic noise and masking can be mitigated. Finally, I extend this work to nonlinear observables by using a forward modeling approach for Lyman Alpha forest tomography, finding more accurate cosmic web reconstruction verses existing techniques. The unifying theme of all these works is revisiting existing matter density reconstruction techniques with a critical eye and using new statistical and computational techniques to efficiently perform an unbiased, lower variance, estimate. Included is discussion of the possible impacts of these methods to improve constraints of cosmological parameters and/or astrophysical processes.",ucb,,https://escholarship.org/uc/item/1fk125d0,,,eng,REGULAR,0,0
17,1453,Effects of shape and surfaces on fluid-dynamic performance of organisms at intermediate Re,"Dolinajec, Trevor Hendry","Koehl, Mimi A.R.;",2015,"An organism's performance in relation to the fluid it lives and operates in is importantacross size and time scales, but the effects on performance of body shape andproximity to a surface become particularly nuanced at intermediate Re. This physicalregime in which both viscosity and inertia play important roles has not been studiedas extensively as that of macroscopic animals in which inertia dominates or that ofmicroscopic animals in which viscosity dominates. However, many ecologically importantanimals such as the copepod occupy these intermediate flow conditions, as doboth airborne and aquatic propagules such as the sporocarps of fungi and the larvae ofbenthic animals. Through recorded observation and modeling this dissertation arrivesat biological implication regarding these organisms' habitats and life cycles. This workalso creates a fuller understanding of general principles that govern intermediate Re.Zooplankton contain a range of morphologies, and life cycles that bring them in contactwith surfaces that act as crucibles. The purpose of this study was to determinehow the morphology and orientation of a variety of ecologically-important microscopicmarine animals (adult copepod, snail veliger larva, barnacle nauplius and cyprid larvae)affect the forces they experience while swimming in the water column, and whileon surfaces (e.g. prey captured on tentacles of benthic predators, larvae settled ontobenthic substrata). Drag, lift, and side forces as well as moments were measuredabout three axes for dynamically-scaled physical models of each animal. These forcesand moments can transport and reorient swimming animals, and can push, lift, peel,or shear animals o surfaces, and thus affect important ecological processes such asdispersal, predation, and larval settlement. The Reynolds numbers (Re, the ratio ofinertial to viscous forces) for the zooplankton and the models was in the range of 10^2to 10^3. Body shape and orientation of small animals were found to have significant effects on the magnitudes of fluid dynamic forces and moments at Reynolds numbersof order 10^3, but were less important at lower Re's. The magnitude and direction ofthe net force on an organism was found to change drastically as an organism nears,and then lands on a surface. The shear stress on the attachment of a small animal toa surface that is caused by drag pushing the animal downstream is greater than theshear stress due to rotation of the organism by flow-induced spinning, thus zooplanktonon surfaces are more likely to be pushed than twisted o the surfaces by water currents.For phytopathogenic fungi in the order Erysiphales, the cause of the diseases calledpowdery mildew, reinfection or dispersal to a new host plant is contingent on sporocarpsescaping a fluttering leaf, but the mechanisms that allow for this liberation arelargely unknown and unquantied. The genus Phyllactinia, unlike other members ofthe order, has specialized and upwardly bent radial appendages that allow the body ofthe sporocarp to extend down from the bottom of the host leaf. This causes the tipsof the appendages to be the only physical connection between the sporocarp and theleaf with a gap of up to 300 microns, thus creating an arrangement where fluid flow maycontribute to liberation. To test the importance of ambient fluid flow on sporocarp liberationforces and moments were measured and fluid flow around dynamically-scaledphysical models was observed at Re of 60 - 360. Flow velocities, boundary layer heights,and sporocarp morphologies were varied to match unsteady flow conditions and sporocarpmaturation. To test the importance of aeroelastically induced inertial forces the kinematics of fluttering leaves in a wind tunnel were recorded at a range of wind speeds, and samples of sporocarps were weighed. Physically modeled aerodynamic forces and moments alongside recorded inertial forces were compared to measured adhesive forces. The comparative forces strongly suggest that steady wind flow and realistic turbulent wind flow do not exert force necessary for liberation in magnitude or direction, but that unsteady flow can lead to significant pitching moments. The accelerations of fluttering leaves and the resulting inertial forces on sporocarps varied greatly among leaves, with forces large enough to liberate sporocarps occurring in a small subset of leaves with a characteristic flutter frequency of 25 Hz. Pitch-induced overturning of sporocarps can explain the removal of sporocarps observed on wind-exposed leaves, with more sporocarps liberated at greater wind speeds and towards the tips of leaves.Terminal velocity is an important parameter in the wind dispersal of propagules (seeds,pollen grains, spores). Aerial righting and aerodynamic stability is common amongvertebrate and invertebrate animals, and some propagules. Fungal sporocarps of thepowdery mildew Phyllactinia have shapes that aect their terminal velocity and aerodynamicstability while operating at Re 1.0 - 3.3, thus Phyllactinia represents a modelorganism for aerodynamic performance at near-unity Re. The reproductive success ofthese mildew species is dependent on stability during aerial transport so that a particularorientation is achieved upon deposition. High speed videography was used tomeasure terminal velocity, angular velocities, and angular accelerations of free-fallingsporocarps during aerial righting. Physical models allowed for quantification of forcesand moments acting on sporocarps falling at terminal velocity, as well as providing fine-scaleflow visualization. The morphology of sporocarps is dependent on their maturity,and experiments carried out with collected sporocarps showed that terminal velocityis partially a function of morphological parameters. Terminal velocities of sporocarpsranged from 8 to 28 cm/s. Flow visualizations showed that both the width and lengthof the wake formed around a falling sporocarp were dependent on the spread of thecharacteristic radial appendages of the genus. Sporocarps were recorded rotating whilefalling prior to reaching stability, and angular velocity and angular accelerations decreasedas sporocarps approached zero angle of attack. Models conrmed that a stablexed point existed at an angle of attack of zero for all tested morphologies of Phyllactiniasporocarps. However, naturally occurring morphologies that were the most likelyto have smaller terminal velocities also displayed smaller aerial-righting moments, andsporocarps most likely to have larger terminal velocities displayed larger aerial-rightingmoments. This suggests a potential trade-o between sporocarps that are more stable(larger aerial-righting moments) and those that can disperse longer horizontal distances(smaller terminal velocity).",ucb,,https://escholarship.org/uc/item/0jd1746s,,,eng,REGULAR,0,0
18,1454,Fabrication and Optimization of Nano-Structured Composites for Energy Storage,"Carrington, Kenneth Russell","Mao, Samuel S;Carey, Van P;",2009,"This dissertation is focused on the development and characterization of a novel class of solid-state nano-structured composites for hydrogen storage based on silica aerogel. It is organized sequentially around experiments conducted to fabricate, optimize and characterize silica aerogel and the composites for hydrogen storage. First, the basics of nano-structured media, silica aerogel technology and solid-state hydrogen storage are introduced. Next, the fabrication and optimization of silica aerogel for hydrogen storage is described in detail. The key result is that varying fabrication parameters can improve the physical properties of the resultant silica aerogel in the context of hydrogen storage. The fabrication of solid-state nano-structured composites using chemical vapor infiltration is then discussed. A series of experiments is used to parameterize the fabrication process, which results in a collection of parameters that minimize variation and structural damage in the composites. Silica aerogel and the composites are then physically characterized using transmission electron microscopy, X-ray diffraction and porosimetry in order to investigate their nano-structuring.An overview of hydrogen storage characterization and two innovations that improve the accuracy and efficiency of hydrogen storage characterization of low-bulk density media like silica aerogel and the composites are then presented. Finally, the innovations are applied to silica aerogel and the composites to characterize their hydrogen storage performance. Silica aerogel and the composites are found to outperform the most common benchmark in physisorption media, and one composite in particular shows unique hydrogen storage performance.",ucb,,https://escholarship.org/uc/item/0nc0p6fm,,,eng,REGULAR,0,0
19,1455,Worlds of Desire: Gender and Sexuality in Classical Tamil Poetry,"Segran, Elizabeth Rani","Hart, George L;",2011,"This dissertation contributes to the nascent study of the Tamil Cankam corpus, a collection of poetic anthologies produced in the first three centuries CE. The Cankam poems are constructed around the two complementary themes of the ""inner world"" relating to emotions, romance and family life, and the ""outer world"" relating to kingship, warfare and public life. This dissertation argues that the thematic division within the corpus is gendered, as the ""inner world"" is associated with the feminine while the ""outer world"" is associated with the masculine. Each chapter explores the way that the poets establish the boundaries of femininity and masculinity through both the form and content of their verses. This dissertation focuses closely on the moments of rupture in the poets' system of gender construction, for these moments suggest that the poets acknowledged that gender is more fluid and complex than it initially appears. To better understand the workings of gender and sexuality in these poems, this study juxtaposes recent theoretical frameworks with these poems from the distant past. Methodologically, this dissertation collapses traditional historical time, bringing the ancient Cankam anthologies into conversation with ideas that are circulating now. In doing so, it seeks to elucidate both the poems and the theory, while also opening up new questions in both fields.",ucb,,https://escholarship.org/uc/item/0nk260ck,,,eng,REGULAR,0,0
20,1456,Cellodextrin Transporters of Neurospora crassa and their Utility in Saccharomyces cerevisiae During a Biofuel Production Process,"Galazka, Jonathan Matthew","Cate, Jamie H. D.;",2011,"The filamentous fungus, Neurospora crassa, is capable of depolymerizing and metabolizing plant cell walls. When grown on plant cell walls or pure cellulose, N. crassa upregulates an overlapping set of 114 genes. Amongst this set are 10 major facilitator superfamily transporters. I have shown that two of these, CDT-1 and CDT-2, transport cellodextrins, which are the major degradative product of fungal cellulases. Deletion of cdt-2 affects the growth of N. crassa on crystalline cellulose. Furthermore, diverse fungi transcriptionally upregulate orthologs of cdt-1 and cdt-2 when in contact with plant cell walls, suggesting that cellodextrin transporters are important to fungal interactions with plants. Engineering the cellodextrin transport pathway into Saccharomyces cerevisiae allows this yeast to ferment cellodextrins to ethanol with high yields, and facilitates the simultaneous saccharification and fermentation of cellulose to ethanol. Cellodextrin transport can be coupled to downstream hydrolysis or phosphorolysis of cellodextrins by a cellodextrin hydrolase or cellobiose phosphorylase, respectively. Cellodextrin transport circumvents a major limitation of yeast in fuel production: the inability to simultaneously transport and ferment pentose sugars and glucose to ethanol. S. cerevisiae did not evolve to co-ferment cellobiose and xylose and unintended consequences are likely.  For example, we speculate that S. cerevisiae may not sense cellobiose as a fermentable carbon source.",ucb,,https://escholarship.org/uc/item/0nr9n5zw,,,eng,REGULAR,0,0
21,1457,Dynamics of Viral Packaging: Single-Molecule Observations in Multiple Dimensions,"Hetherington, Craig Lee","Bustamante, Carlos J;",2011,"During the self-assembly of bacteriophage phi29, the viral genome is packaged into a pre-formed capsid by a molecular motor. The packaging motor is a complex of several oligomers including a dodecameric connector ring and a pentameric ATPase ring. These rings coordinate with each other, generating high forces in order to compact the dsDNA genome into the capsid at high pressures.The connector was proposed to engage and directly perform work on the DNA during packaging. Consideration of the symmetry of the connector and the capsid predicts that the connector must rotate relative to the capsid as part of the mechanism for translocating the DNA. An experiment designed to directly measure rotation of the connector is discussed in Chapter 2 of this dissertation. A combination magnetic tweezers and total internal reflection microscope is used to track the polarization of single fluorescent dyes attached to the connector. No evidence of polarization changes were found, indicating that the connector does not rotate at the expected rates. This further suggests that the connector does not directly perform mechanical work on the DNA during packaging.Viral packaging can be observed in optical tweezers by monitoring the length of the DNA as it is drawn into the capsid. Past studies have revealed many details of the packaging mechanism by following the dependence of the packaging velocity on factors like ATP concentration and applied load. In Chapter 3, I propose an experimental design intended to measure the effect of the packaging motor on the angle of the DNA in addition to its length and thus recover the full three-dimensional trajectory of the DNA as it passes into the capsid. In addition, this scheme can be used to apply torque and thus provides an additional tool with which to probe the packaging mechanism.In Chapter 4, we find that during packaging the downstream DNA is twisted in an underwinding direction, with a magnitude that depends on the extent to which the capsid is filled. The change in twisting can be attributed to cumulative looping of the DNA within the capsid, and the data predicts that the loops formed in the last kilobasepair of packaging are as small as 4 nm in radius. In addition, a non-lethal method of rupturing the viral capsids prior to packaging was discovered. Observations of DNA twisting by those packaging complexes revealed that, in the absence of internal pressure, the DNA is twisted by -1.2 °/bp. This number suggests that one of the packaging motor's five subunits makes contact with the DNA every ten basepairs, and that the cycle repeats with that subunit performing the same function every time. Such a strict functional segregation, in addition to the catalytic segregation revealed in previous high-resolution optical tweezers experiments, is an important part of the mechanism by which the motor packages DNA against high forces.",ucb,,https://escholarship.org/uc/item/0pw8703c,,,eng,REGULAR,0,0
22,1458,Viral Politics: Sex Worker Activism and HIV/AIDS Programs from Bangalore to Nairobi,"Vijayakumar, Srigowri","Ray, Raka;",2016,"This dissertation studies the international success story of India’s HIV/AIDS response and the activism of sex workers and sexual minorities that produced it. A number of recent ethnographies have turned their attention to the workings of state programs in middle-income countries (e.g. Baiocchi 2005; Sharma 2008; A. Gupta 2012; Auyero 2012), demonstrating both the micro-effects of state strategies for managing poverty on poor people and the ways in which state programs are produced outside the visible boundaries of “the state”—through NGOs and social movement organizations as well as transnational donors and research institutes.  Yet, even as state programs are constituted through struggles over resources and representations within and outside the official agencies of the state, states also derive legitimacy from projecting themselves as cohesive rather than disaggregated, and as autonomous from society rather than anchored within it (Abrams 1988; Mitchell 1991b; Mitchell 1999; A. Gupta 2012). The representation of state programs as cohesive, pre-constituted, exportable “models” serves as a new way of consolidating state legitimacy within a global, hierarchical order of development “success.” However, this dissertation argues that the traveling policies disseminated through transnational expert communities are a selective codification of hard-fought struggles among institutions within the state, between the state and organizations, among organizations, and among groups within organizations over the aims and strategies of social policies and programs.  These struggles shape what travels in traveling policies and what is left out.  Drawing on over 150 in-depth interviews and a year of participant observation with sex workers involved in implementing policy in community-based organizations, NGOs, and activist groups, I show how the material and social conditions of men, women, and transgender women in sex work, mediated through community-based organizations, constituted the successful approaches to HIV prevention that were later, sometimes selectively, translated around the world.",ucb,,https://escholarship.org/uc/item/0qs1n4fh,,,eng,REGULAR,0,0
23,1459,Optodynamical Measurement and Coupling of Atomic Motion and Spin,"Kohler, Jonathan","Stamper-Kurn, Dan M;",2018,"The quantum nature of light makes it a basic component for models of quantum measurement and information exchange between disparate quantum modes, pioneered in the field of cavity quantum electrodynamics. The interaction of atomic ensembles with the mode of an optical cavity provides a flexible platform for exploring the coherent interaction of light with diverse macroscopic dynamics, such as collective motion and spin. This dissertation presents experimental results and theoretical models for continuous measurement and control of the center of mass motion and collective spin precession of an atomic ensemble, mediated by coupling to a high-finesse optical cavity. First, the theory of dispersive coupling between the cavity mode and the collective motion and spin of an atomic ensemble is derived, and then a general time-domain formalism is developed for theoretical analysis of multi-mode optodynamical systems. Single-mode optodynamical effects are introduced through experimental demonstrations of measurement and control of the collective atomic spin, providing a close analogy to cavity optomechanics.Next, multiple collective atomic modes are considered within a single cavity, in order to assemble optically mediated interactions within multi-mode optodynamical systems. A demonstration of optodynamical interactions between the center of mass motion of two atomic ensembles is presented, coupled through an optical spring mediated by the cavity mode. Then simultaneous coupling of the center of mass motion and total spin precession of a single ensemble of atoms is described, yielding an experimental realization of a negative-mass instability, facilitated by the novel resource of the spin ensembles inverted state. A theoretical analysis of the negative-mass instability is presented, which indicates the possibility of generating two-mode squeezed states in the absence of excess incoherent noise. Finally, linear state retrodiction from the optodynamical signals is discussed, providing background and supplemental material for a forthcoming manuscript.",ucb,,https://escholarship.org/uc/item/0hn835xt,,,eng,REGULAR,0,0
24,1460,"Searching for Sustainable Utopia: Art, Political Culture, and Historical Practice in Germany, 1980-2000","Allen, Jennifer Leigh","Jay, Martin E;",2015,"At the end of the twentieth century, the gradual triumph of liberal democracy and capitalism over “really existing socialism” brought to many West Germans not relief but melancholy. Facing what they interpreted as the dissipation of radical social and political alternatives, academics and public intellectuals pronounced the death of ideology, of history, and of utopian ambitions. This dissertation asks how West Germans nevertheless found ways to challenge this resignation by giving voice to new, radical hopes for Germany’s future. For their broad popularity and sustained impact, this study traces the grassroots efforts of three groups. First, it follows the Berlin History Workshop, a collection of amateur and professional historians, as they attempted to liberate the process of researching and writing history from the rigid confines German universities. This group sought, instead, to bring the historian’s craft into Berlin’s local neighborhoods in order to enable ordinary Germans to narrate their own histories. Second, this dissertation analyzes the Green Party, which practiced localized plebiscitary policy making in an effort to endow German citizens with greater political agency. The Greens brought this political practice to numerous cultural projects in an effort to democratize German society by democratizing the cultural encounters of its citizens. Third, this project investigates a loosely-connected group of artists who echoed the investments of the historians and politicians by creating art installations with ordinary objects in ordinary spaces that prompted passersby to reevaluate their relationships to the topography of their daily lives.This dissertation argues that, through these groups, everyday Germans adopted a set of cultural practices in the 1980s and ‘90s that not only critiqued established institutions but also crafted new institutions in their place. Their critical practices followed three conventions. First, they championed radical grassroots democracy by giving citizens opportunities to create socially-significant cultural products like museums and monuments. Second, they decentralized the creative process, locating it in the spaces of everyday life in order to make it widely accessible. Finally, they borrowed from the environmental movement the concept of sustainability, which demanded that any alternative to existing society be both enduring and adaptable. These practices put culture to work in realizing a more democratic, more socially-integrated Germany. In doing so, they permitted their practitioners to reclaim utopian hope from the dustbin of historical ideas.These three case studies span Germany’s academic, political, and aesthetic terrain. As such, together they offer evidence that efforts to battle twentieth-century apathy signaled a broad shift in German cultural sensibilities, not an isolated phenomenon. The first three chapters of the dissertation treat each of these groups individually as they began to advocate for new, more democratic geographies of cultural engagement, or “alternative public spheres,” in the early 1980s. Their focus on Germany’s cultural environment made them particularly receptive to the idea of sustainability popularized after the convening of the World Commission on Environment and Development in the middle of the decade. The next three chapters trace their pursuit of sustainability in culture. A sustainable culture, they came to realize, had to regard its projects as part of an ongoing process rather than as static goals: theirs was a renewable, future-oriented cultural movement in the present, or a “sustainable utopia.” Faced with radical changes to the international political landscape and the rapid expansion of their constituencies to include sixteen million East Germans alongside more pedestrian concerns like funding difficulties and interpersonal conflicts, these groups weathered the last decade of the twentieth century with varying success. The study concludes by underscoring the irony that the most durable component of their cultural programs in the wake of German political reunification was their push for cultural decentralization.",ucb,,https://escholarship.org/uc/item/0j67m8xp,,,eng,REGULAR,0,0
25,1461,"Conservation, Economics, and Management of Hunting on Private Land: An International, National and California Analysis","Macaulay, Luke Thomas","Huntsinger, Lynn;Barrett, Reginald;",2015,"Privately owned land accounts for significant areas of land internationally, nationally, and in California. In the U.S. and elsewhere, private land tends to support high levels of biodiversity because land with more productive natural resources was settled and privatized first. These lands, which are integral to conservation goals, are often the most vulnerable to habitat degradation and loss through changes in land-use and fragmentation. In 1930, Aldo Leopold encouraged the development of an incentive scheme to better conserve private lands in the U.S. where hunters would pay landowners for access to conserved wildlife habitat and game populations that could be sustainably harvested. Although a wide body of literature has discussed this approach, much of the research is either theoretical or limited to particular regions and these studies have rarely tested for an explicit connection to whether conservation is ultimately improved as a result of paid hunting. The goal of this dissertation is to evaluate the economic, conservation and management aspects of hunting on private land internationally, nationally, and in California. The first chapter uses a case study approach to explore the environmental and economic issues surrounding hunting in the context of Spain and California. The study found that increased game management rights in Spain appears to yield improved economic return, but at an environmental cost. The second chapter evaluates the scale, distribution and conservation aspects of spending to access private land for fishing, hunting, and wildlife-watching in the United States. This study found that that approximately 440 million acres of private land, an estimated 22% of the contiguous land area of the U.S. and 33% of all private land in the U.S., are either leased or owned for wildlife-associated recreation. Much of these lands are private rangelands and forestlands.  Land utilized for hunting accounted for 81% of that total, while land utilized for fishing and wildlife-watching, although comparatively small, likely includes riparian zones and areas with high environmental or amenity values. Hunters own or lease properties of larger size classes than anglers or wildlife-watchers, providing a possible economic incentive for maintaining large unfragmented properties that provide a variety of conservation benefits. Results show that Americans annually spend an estimated $814 million in day-use fees, $1.48 billion for long-term leases, and $14.8 billion to own private land primarily for wildlife-associated recreation. Hunting, in particular big game hunting, comprises some of the largest contributions to payments for wildlife-associated recreational use on private land. This finding suggests that hunting may be an important market-based mechanism to maintain large unfragmented parcels of wildlife habitat.  Chapter 3 utilizes interviews with a random sample of landowners in California to evaluate conservation practices associated with hunting enterprises. This study found relatively low adoption of hunting enterprises among landowners, and that there were mixed conservation outcomes associated with hunting. Landowners who enrolled in the California Department of Fish & Wildlife’s Private Land Management program, which provides enhanced game management rights to landowners in exchange for habitat improvement practices, performed the most comprehensive habitat improvement practices, including riparian zone restoration and adjustment of grazing practices to enhance cover and forage resources for wildlife. Many other landowners, however, earned some income from hunting, but either did not implement additional conservation practices to enhance wildlife habitat or performed practices that could cause some ecological problems, such as planting of feed crops that can create openings for invasive noxious weeds to be established on a property. This study found significant opportunities in California to not only increase adoption of hunting enterprises, but to engage in educational efforts to encourage ecologically-friendly wildlife management practices as a way to enhance both revenue from hunting enterprises and conservation outcomes. The final chapter focuses on the development of methods to better understand the population characteristics of the Columbian black-tailed deer (Odocoileus hemionus columbianus) in order to improve harvest recommendations. Using camera-traps on a 2,500 acre private ranch in San Benito County, California, the study estimates the density and sex ratios of deer by using a Bayesian spatial mark-resight model. It also evaluates the effect of using bait in developing population estimates. The study found that deer densities on the property are estimated to be 9.9 (SE 0.91) individuals/km2, and that antlered bucks make up only 11% (SE 1%) of the population. Bait increased encounter rates of deer by a factor of 3.7, showing that the use of bait can help reduce the length of time that cameras must be operational and may create more precise population estimates due to increased detectability of deer.In conclusion, this dissertation found the game management rights for hunting were important for economic return from hunting on private land, but without regulation may result in negative environmental impacts. Across the United States, hunting contributes significantly to landowner income, especially to properties of larger size classes in rangeland and forestry habitats, which suggests that hunting provides an economic incentive to maintain large unfragmented properties. In the context of California, programs that give landowners greater game management rights in exchange for habitat improvement practices resulted in benefits for landowners and the environment. Finally, this dissertation has developed a statistical model that can be utilized to evaluate population parameters for one of the most economically important game species in California. In sum, recreational hunting can provide income to the private landowner and with the appropriate regulations, education and management, can incentivize the enhancement and maintenance of wildlife habitat on private land.",ucb,,https://escholarship.org/uc/item/0jm6t2jx,,,eng,REGULAR,0,0
26,1462,Quantum Trajectories of a Superconducting Qubit,"Weber, Steven Joseph","Siddiqi, Irfan;",2014,"In quantum mechanics, the process of measurement is intrinsically probabilistic.  As a result, continuously monitoring a quantum system will randomly perturb its natural unitary evolution.  An accurate measurement record documents this stochastic evolution and can be used to reconstruct the quantum trajectory of the system state in a single experimental iteration.   We use weak measurements to track the individual quantum trajectories of a superconducting qubit that evolves under the competing influences of continuous weak measurement and Rabi drive. We analyze large ensembles of such trajectories to examine their characteristics and determine their statistical properties.   For example, by considering only the subset of trajectories that evolve between any chosen initial and final states, we can deduce the most probable path through quantum state space.  Our investigation reveals the rich interplay between measurement dynamics, typically associated with wavefunction collapse, and unitary evolution.  Our results provide insight into the dynamics of open quantum systems and may enable new methods of quantum state tomography, quantum state steering through measurement, and active quantum control.",ucb,,https://escholarship.org/uc/item/0k0687ns,,,eng,REGULAR,0,0
27,1463,Project Planning Algorithms: Lowering Cost and Improving Delivery Time in Capital Projects,"Jabbari, Arman","Kaminsky, Philip;",2020,"With the goal of developing models and approaches leading to better operation of large-scale project delivery supply chains, we interviewed a variety of consultants and project and supply chain managers (with a particular emphasis on oil and gas major capital project delivery) and asked them a set of questions so that we could better understand current capital project delivery views of supply chain management, inventory, risk management tools, and related topics (Appendix A). Our interviewees expressed surprisingly diverse opinions, particularly regarding the future of mega-project delivery and the need for more closely coordinating supplier deliveries with onsite needs.The work in the dissertation is particularly motivated by mega-projects in the oil and gas industry, and our goal is to build models that lead to better operation of capital project delivery supply chains. The characteristics of this industry, and of these projects, place specific requirements on project scheduling models, and many of the existing models in the literature do not meet these requirements. Our focus in this dissertation is to formulate models and develop approaches that are particularly useful for mega-projects in the oil and gas industry, and that enable the concurrent determination of the project schedule and inventory delivery times in order to efficiently manage the project supply chain, and to effectively control project delivery time and cost.We consider the Stochastic Resource-Constrained Project Scheduling Problem with inventory, where the objective is to minimize a weighted combination of the expected project makespan and the expected inventory holding costs. Motivated by the requirements of major oil and gas industry projects, we introduce a class of proactive policies for the problem. We develop several effective heuristics for this problem, as well as deterministic and probabilistic lower bounds on the optimal solution. In computational testing, we demonstrate the effectiveness of these heuristics and develop insights into the value of explicitly considering inventory in this setting.A related problem that arises is the scheduling of oil field drilling operations, where the goal is to maximize the expected revenue generated by oil extraction. For this problem, we build a model and propose a heuristic approach. We confirm the effectiveness of our heuristic approach by analyzing its performance compared to the current practice in a real-world case study. Our results demonstrate the potential to increase the efficiency and productivity of drilling operations significantly and to boost profitability by decreasing the time until wells start the extraction.Through our interviews, and through analysis in subsequent models, it is clear that suppliers, and the timely delivery of supplies, plays a critical role in the successful implementation of large-scale projects. In the context of oil and gas projects, our focus is on the suppliers that provide customized materials. While the bulk of this dissertation focuses on projects from the project planner’s point of view, we were also motivated to consider these problems from the perspective of the supplier, and hence, to consider scheduling models with due dates. We present a stylized model, where we consider sequencing decisions on a single processor, here representing a supplier, in an online setting where no data about the future incoming opportunities is available. With the goal of minimizing total weighted (modified) earliness and tardiness cost, we introduce a new scheduling policy, which we refer to as the list-based delayed shortest processing time policy, and develop lower and upper bounds on the performance of this policy.Finally, we consider an alternative view of managing construction in projects, a location-based method known as the Work Density Method for takt planning. Given a work space and the number of zones in which to divide that space, the so-called WoLZo problem is to identify the shape and dimensions of each zone while minimizing the peak in the trades’ workloads per zone. We model this problem and develop an optimization algorithm to divide a work space into zones while leveling work densities across trades in a process.The tools presented in this dissertation are useful for managing different elements of mega-projects and significantly advance the state-of-the-art in those areas. We confirm the effectiveness of these tools by analyzing their performance compared to current practice in real-world case studies as well as their performance over the benchmark test problems that are available in the literature.",ucb,,https://escholarship.org/uc/item/0xt4s766,,,eng,REGULAR,0,0
28,1464,Risk Management and Combinatorial Optimization for Large-Scale Demand Response and Renewable Energy Integration,"Yang, Insoon","Tomlin, Claire J;",2015,"To decarbonize the electric power grid, there have been increased efforts to utilize clean renewable energy sources, as well as demand-side resources such as electric loads. This utilization is challenging because of uncertain renewable generation and inelastic demand. Furthermore, the interdependencies between system states of power networks or interconnected loads complicate several decision-making problems. Growing interactions between power and energy systems and human agents with advances in sensing, computing and communication technologies also increase the need for personalized operations.In this dissertation, we present three control and optimization tools to help to overcome these challenges and improve the sustainability of electric power systems. The first tool is a new dynamic contract approach for direct load control that can manage the financial risks of utilities and customers, where the risks are generated by uncertain renewable generation. The key feature of the proposed contract method is its risk-limiting capability, which is achieved by formulating the contract design problem as mean-variance constrained risk-sensitive control. To design a globally optimal contract, we develop a dynamic programming solution method based on a novel dynamical system approach to track and limit risks. The performance of the proposed contract framework is demonstrated using data from the Electricity Reliability Council of Texas. The second tool is developed for combinatorial decision-making under system interdependencies, which are inherent in interconnected loads and power networks. For such decision-making problems, which can be formulated as optimization of combinatorial dynamical systems, we develop a linear approximation method that is scalable and has a provable suboptimality bound. The performance of the approximation algorithm is illustrated in ON/OFF control of interconnected supermarket refrigeration systems. The last tool seeks to provide a personalized control mechanism for electric loads, which can play an important role in demand-side management. We integrate Gaussian progress regression into a model predictive control framework to learn the customer's preference online and automatically customize the controller of electric loads that directly affect the customer's comfort. Finally, we discuss several future research directions in the operation of sustainable cyber-physical systems, including a unified risk management framework for electricity markets, a selective optimal control mechanism for resilient power grids, and contract-based modular management of cyber-physical infrastructure networks.",ucb,,https://escholarship.org/uc/item/0zh6b007,,,eng,REGULAR,0,0
29,1465,"Ecological Restoration for Community Benefit: People and Landscapes in Northern California, 1840-2010","Diekmann, Lucy Ontario","Huntsinger, Lynn;",2011,"Restoration has important ecological work to do, particularly maintaining biological diversity and repairing impaired ecological functions. In addition, many people anticipate and hope that restoration will also produce changes in and provide benefits to human communities. Although these expectations are widespread, relatively little is known about how well restoration projects achieve their goals generally, and even less about the social and cultural consequences of restoration work.This dissertation draws on the experiences of two communities in northwestern California--the American Indians and non-Indians who are part of the United Indian Health Services (UIHS) and the resource managers, scientists, and landowners who work together to implement restoration projects throughout Humboldt County--to explore the impact of ecological restoration on human communities that undertake, use, or are home to restoration projects. I used qualitative interviews along with a review of historical and contemporary documents to develop an understanding of restoration goals and outcomes that is grounded in the experiences of UIHS community members and members of the broader Humboldt County restoration community.UIHS community members share a vision of restoration that is rooted in cultural understandings of the relationship between people and the environment and in historical changes to the local landscape and American Indian communities that have affected their ability to enact this relationship and to apply key cultural values. In the contemporary cultural landscapes of northwestern California, UIHS community members' access to culturally significant places and natural resources is restricted. Restoration offers one way to restore a role for American Indians in the landscape through active management, traditional activities, and applications of cultural knowledge. I find that the process of restoring and using the Ku' wah-dah-wilth Restoration Area has had at least six outcomes that contribute to community wellbeing. These are: encouraging healthy behaviors; offering opportunities for cultural and environmental education; serving as a source of inspiration; facilitating community interaction; providing a culturally meaningful place that produces a range of positive emotional responses; and acting as positive symbol of living American Indian cultures. However, the Restoration Area's potential for meaningful change is constrained at present by the limited number of people who access the site or receive information about it and the relatively small number of opportunities to actively engage with the site.Members of the Humboldt County restoration community are also motivated by the hope that restoration will benefit communities culturally and economically. Although restoration contributes significantly to the county's economy and has led to relationship building and improved knowledge about local ecosystems, general uncertainty about restoration's community impacts suggests that restoration goals are not necessarily reflected in restoration outcomes. Taken together the experiences of these two communities indicate that restoration has a range of social and cultural outcomes. They also suggest that more effectively realizing cultural and social goals will take active planning, engagement with the broader political and social forces that have contributed to current conditions, ongoing involvement with restored sites to create opportunities for education and use, monitoring and evaluation of social outcomes, and attention to who is and who is not benefitting from restoration.",ucb,,https://escholarship.org/uc/item/1365d8cr,,,eng,REGULAR,0,0
30,1466,Development and Application of Oxidative Coupling Bioconjugation Reactions with ortho-Aminophenols,"Obermeyer, Allie","Francis, Matthew B;",2013,"The synthetic modification of proteins plays an important role in the fields of chemical biology and biomaterials science. As applications of protein-based materials continue to become more complex, improved methods for the covalent modification of proteins are needed. Although many methods for the modification of native and artificial amino acids exist, they often require long reaction times or lengthy syntheses of reactive substrates. This work describes the development and application of a suite of bioconjugation reactions that utilize ortho-aminophenols. The oxidative coupling of aniline residues with o-aminophenol substrates was optimized. Potassium ferricyanide was identified as an alternative, mild oxidant for this coupling. These new conditions enabled the use of the oxidative coupling reaction in the presence of free cysteines and glycoslated substrates. Aminophenols were also discovered to react with native residues on protein substrates in addition to artificial aniline moieties. Cysteine and the N-terminus were identified as the reactive residues. The oxidative coupling of o-aminophenols with the N-terminus was optimized to achieve high levels of modification on peptide and protein substrates. The oxidative coupling of anilines and o-aminophenols was applied to the synthesis of a targeted, virus-like particle and to the detection of protein tyrosine-nitration. Overall, these updated and novel oxidative coupling methods expand the utility ortho-aminophenols for the modification of proteins.",ucb,,https://escholarship.org/uc/item/15p1m5m0,,,eng,REGULAR,0,0
31,1467,Effects of Market Approaches to Green Technologies for the Poor: The Case of Improved Cookstoves,"Booker, Kayje Merrea","Huntsinger, Lynn;",2011,"""Sustainable"" or ""green"" technologies for the global poor have been proposed as solutions to the difficult problem of how to improve the lives of the world's poorest without contributing to climate change or other environmental catastrophes. While such technologies were once the domain of non-profit and government funded initiatives, theyare now increasingly developed and deployed through market mechanisms. Using improved biomass cookstoves as a representative technology, this dissertation seeks to assess the social and technological effects of this shift to market-based approaches for development and dissemination of sustainable technologies for the poor.Chapter 2 uses a Science and Technology Studies theoretical framework to follow the coproduction of the material form of improved biomass cookstoves and the cookstove movement from the 1960s to the present. The chapter shows that during the 1980s, particular conceptions and articulations of the problem that cookstoves were meant to solve led to a definition of technological ""improvement"" that included fuel efficiency, consistency of performance, and ability to scale quickly. This particular type of cookstove was much more compatible with mass-production than traditional artisanal production, creating social organizations that could mass-produce cookstoves, which then encouraged commercial approaches in order to recover costs. The move to a market-based approach was in part driven by and in part the cause of a particular kind of technology, demonstrating the mutual coproduction of the social and technological.Chapter 3 takes one market-based tool, intellectual property, and analyses the effect of deploying it in the realm of green technologies for the poor. Using the contrasting cases of UV Waterworks and the Berkeley-Darfur Stove the chapter identifies some of the salient social and technical characteristics that determine whether such effect is positive. The complex social arrangements involved in developing technologies for the poor mean that tools such as intellectual property can be useful but must be compatible with the organizations involved at the level at which the tool is targeted, each of which may have different orientations and incentives. The type of funding at each level, donor versus investor, appears to be a particularly important variable in predicting positive or negative outcomes.Chapter 4 examines one specific environmental policy market mechanism, the carbon market, and its role in stimulating technological change, invention, innovation, and dissemination (Schumpeter, 1942) in biomass cookstoves. It shows that carbon credits are thus far improving diffusion of current cookstoves but failing to stimulate innovation in cookstoves with stronger health and environmental impacts. Additionally, the chapter shows that the carbon market is influencing the selection of cookstoves for dissemination. The characteristics selected for are most compatible with centralized, mass production, which is likely to strengthen the shift towards these approaches.",ucb,,https://escholarship.org/uc/item/16v148bg,,,eng,REGULAR,0,0
32,1468,Seiberg-Witten and Gromov invariants for self-dual harmonic 2-forms,"Gerig, Chris","Hutchings, Michael;",2018,"For a closed oriented smooth 4-manifold X with $b^2_+(X)>0$, the Seiberg-Witten invariants are well-defined. Taubes' ""SW=Gr"" theorem asserts that if X carries a symplectic form then these invariants are equal to well-defined counts of pseudoholomorphic curves, Taubes' Gromov invariants. In the absence of a symplectic form there are still nontrivial closed self-dual 2-forms which vanish along a disjoint union of circles and are symplectic elsewhere. This thesis describes well-defined counts of pseudoholomorphic curves in the complement of the zero set of such near-symplectic 2-forms, and it is shown that they recover the Seiberg-Witten invariants (modulo 2). This is an extension of Taubes' ""SW=Gr"" theorem to non-symplectic 4-manifolds.The main results are the following. Given a suitable near-symplectic form w and tubular neighborhood N of its zero set, there are well-defined counts of pseudoholomorphic curves in a completion of the symplectic cobordism (X-N, w) which are asymptotic to certain Reeb orbits on the ends. They can be packaged together to form ""near-symplectic"" Gromov invariants as a map on the set of spin-c structures of X. They are furthermore equal to the Seiberg-Witten invariants with mod 2 coefficients, where w determines the ""chamber"" for defining the latter invariants when $b^2_+(X)=1$.In the final chapter, as a non sequitur, a new proof of the Fredholm index formula for punctured pseudoholomorphic curves is sketched. This generalizes Taubes' proof of the Riemann-Roch theorem for compact Riemann surfaces.",ucb,,https://escholarship.org/uc/item/1730608x,,,eng,REGULAR,0,0
33,1469,Pathway and organelle engineering for production of useful chemicals in yeast,"Grewal, Parbir","Dueber, John E;Clark, Douglas S;",2020,"Researchers in the field of metabolic engineering aim to develop processes to produceuseful chemicals, sustainably and responsibly, using biotechnology. These processes areoften designed to replace products derived from fossil fuels, which are unsustainable andcontribute to climate change, or plant-based products, which compete with foodproduction for scarce land and are subject to supply uncertainty due to weather, cropdisease, and climate change. Here, we present two research projects in metabolicengineering. First, we demonstrate microbial production of the red food dye betanin byengineering the betalain biosynthesis pathway into yeast. Betanin is currentlymanufactured through extraction from red beets specifically grown for dye production.We achieved betanin production levels of 17 mg/L, which is equivalent to the amount ofbetanin found in 10 g/L of beet extract. With further production improvements, thisbioprocess may become cost-competitive with agricultural production and is likely tolead to a purer product. We also demonstrate the synthesis of a suite of non-naturalbetalain dyes achieved through feeding of diverse amines to a yeast production host,including several which have never been reported. In the second research project, wediscover that an enzyme that limits production levels of a drug family is toxic to the yeastproduction host. This enzyme, norcoclaurine synthase, is critical to the production ofbenzylisoquinoline alkaloids, an important family of medicines that are extracted fromplants like the opium poppy. We devised a novel subcellular compartmentalizationstrategy, sequestering norcoclaurine synthase in the peroxisome to alleviate cytotoxicitywhile maintaining access to the enzyme’s substrates. By targeting norcoclaurine synthasefor organellar compartmentalization, we achieved improved cell growth, final titer, andculture productivity. These projects highlight the potential of engineering complex plantpathways into microbial hosts for economical and sustainable chemical production.",ucb,,https://escholarship.org/uc/item/17p341hd,,,eng,REGULAR,0,0
34,1470,Slow Photoelectron Velocity-Map Imaging of Transient Species and Infrared Multiple Photon Dissociation of Atmospherically Relevant Anion Clusters,"Yacovitch, Tara Irene","Neumark, Daniel M;",2012,"Two different types of vibrationally resolved spectroscopies are used in the experimental study of reactive species: slow-electron velocity map imaging (SEVI) and infrared multiple photon dissociation (IRMPD).SEVI spectroscopy is used to study the series of vinoxy and substituted vinoxy radicals: vinoxy (H2C=CH-O), i-methylvinoxy (H2C=C(-O)-CH3) and n-methylvinoxy (H3C-HC=CH-O). Vibrational resolution of their ground and first excited electronic states is achieved, leading to accurate measurement of electron affinities, term energies and vibrational frequencies. Radical geometries are deduced and conformational isomers for the larger species are identified. The i-methylvinoxy radical is found to be most stable when the methyl substituent is eclipsed in the ground-state radical and staggered in the excited state radical and ground state anion. Both cis and trans isomers of the n-methylvinoxy radical are observed, with the lower-energy cis isomer contributing to most of the spectral peaks. The SEVI experiment is also used to study the transition state region of the F + H2 and F + CH4 reactions. The F + H2 results improve on previous spectra, resolving narrow features and suggesting that additional theoretical treatment is necessary to fully describe and assign the experimental results. The entrance valley of the F + CH4 reaction coordinate is measured, showing extended structure attributed to bending or hindered rotation of the methane moiety. The significance of these results in terms of reactive resonances is discussed.The final SEVI experiments involve a series of alkoxy radicals and their sulfur-substituted analogs: methoxy (CH3O), thiomethoxy (CH3S), ethoxy (CH3CH2O),  thioethoxy (CH3CH2S), i-propoxy ((CH3)2CHO) and n-propoxy (CH3CH2CH2O). The two lowest electronic states are close in energy (or formally degenerate) leading to a slew of nonadiabatic effects such as vibronic coupling from the Jahn-Teller or pseudo-Jahn-Teller effect and spin-orbit splitting. Precise determinations for the electron affinities and splittings between the electronic states are made. Variation of the size, symmetry and O/S atoms significantly affects the potential energy landscape of these radicals, leading to drastically altered spectra governed by differing contributions of the various nonadiabatic effects. IRMPD spectra of negatively charged cluster species containing inorganic acids and water are studied, revealing structural information and size-dependent trends. The small bisulfate-water clusters, HSO4-(H2O)n, show lengthening of the acidic bond in the bisulfate anion, H-OSO3-. This is observed through the characteristic SOH bending vibration. The small mixed clusters of sulfuric and nitric acid, HSO4-(HNO3), NO3-(H2SO4)(HNO3) and HSO4-(H2SO4)(HNO3), show charge localization effects that in some cases counter the structural assumptions made based on the gas phase acidities of the molecular acids. Finally, the clusters containing bisulfate, sulfuric acid and water, HSO4-(H2SO4)m(H2O)n show the recurrence of the triply hydrogen-bound HSO4-(H2SO4) configuration for n = 0, while incorporation of water disrupts this stable motif for clusters with m > 1.",ucb,,https://escholarship.org/uc/item/1862c66d,,,eng,REGULAR,0,0
35,1471,Variability Modeling and Statistical Parameter Extraction for CMOS Devices,"Qian, Kun","Spanos, Costas J;",2015,"Semiconductor technology has been scaling down at an exponential rate for many decades, yielding dramatic improvements in power, performance and cost, year after year. Today’s advanced CMOS transistors have critical dimensions well below 24nm. This means that controlling the manufacturing process is increasingly difficult. Process and material fluctuations cause device and circuit characteristics to deviate from design goals, and introduce significant device-to-device variability due to spatial variations across silicon wafers. Accurate modeling of these spatial process variations has become critical to both foundries and circuit designers that seek optimal power/speed/area balance. To understand the nature of spatial process variations, we first carried out a comprehensive variability analysis of data measured from thousands of variability-sensitized test structures, including ring oscillators, SRAM bit cells and their internal transistors. We manufactured these test chips using early stage 90nm and 45nm commercial semiconductor processes. We proposed a hierarchical variability model to capture the systematic and random components of device parameter variations across silicon wafers, and across chips. The detailed decomposition of the process variation profile reveals significant across-wafer systematic component for the delay and leakage of ring oscillators, and across-chip systematic component for the read/write margins of SRAM bit cells, as well as their internal transistors. The proper modeling of each hierarchical component proved to be crucial for the accurate estimation of the statistics of device performance distribution and its parametric yield.The knowledge gained about process variation from carefully designed test structures was leveraged into estimating the variation and parametric yield of new devices and circuits. This was accomplished by improved the statistical compact model parameter extraction methodology, and by proposing a stepwise parameter selection method. We used a normalized notional confidence interval and, and the sum of squares of fitting residuals as extraction and fitting quality criteria. This allowed us to determine the essential model parameters for accurate fitting over a large number of transistors. We applied this methodology to EKV and PSP with both simulated and experimental data, demonstrating its effectiveness. Finally, we combined the results from statistical parameter extraction with the hierarchical spatial variability model. This, compared to traditional methods, produced much-improved estimates of device performance and manufacturing yield.",ucb,,https://escholarship.org/uc/item/19x656kn,,,eng,REGULAR,0,0
36,1472,"Gravure-printed electronics: Devices, technology development and design","Grau, Gerd Fritz Milan Nino","Subramanian, Vivek;",2016,"Printed electronics is a novel microfabrication paradigm that is particularly well suited for fabrication of low-cost, large-area electronics on flexible substrates. Applications include flexible displays, solar cells, RFID tags or sensor networks. Gravure printing is a particularly promising printing technique because it combines high print speed with high resolution patterning. In this thesis, gravure printing for printed electronics is advanced on multiple levels. The gravure process is advanced in terms of tooling and understanding of printing physics as well as its application to substrate preparation and device fabrication.Gravure printing is applied to transform paper into a viable substrate for printed electronics. Paper is very attractive for printed electronics because it is low-cost, biodegradable, lightweight and ubiquitous. However, printing of high-performance electronic devices onto paper has been limited by the large surface roughness and ink absorption of paper. This is overcome here by gravure printing a local smoothing layer and printed organic thin-film transistors (OTFTs) are demonstrated to exhibit performance on-par with device on plastic substrates.If highly-scaled features are to be printed by gravure, traditional gravure roll making techniques are limited in terms of pattern definition and surface finish. Here, a novel fabrication process for gravure rolls is demonstrated utilizing silicon microfabrication. Sub-3μm features are printed at 1m/s. Proximity effects are demonstrated for more complex highly-scaled features. The fluid mechanics of this effect is studied and it is suggested how it can be used to enhance feature quality by employing assist features.Finally, advancements are made to printed organic thin-film transistors as an important technology driver and demonstrator for printed electronics. First, a novel scanned thermal annealing technique is presented that significantly improves the crystallization of an organic semiconductor and electrical performance. Second, transistors are fully gravure printed at a high print speed of 1m/s. By scaling both lateral and thickness dimensions and optimizing the printing processes, good electrical performance, low-voltage operation and low variability is demonstrated.",ucb,,https://escholarship.org/uc/item/1bn3t372,,,eng,REGULAR,0,0
37,1473,Actomyosin mediated tension orchestrates thermogenic programs in adipocytes,"Tharp, Kevin Menard","Stahl, Andreas;",2017,"Innovative approaches to shift energy balance are urgently needed to combat metabolic disorders such as obesity and diabetes. One promising approach has been the expansion or activation of thermogenic adipose tissues to improve metabolic homeostasis. My doctoral studies presented in the following text have identified novel approaches to translate adipose based metabolic therapeutics and the underlying mechanisms by which thermogenic adipocytes establish their therapeutically applicable metabolic capacity.In chapter I, I present a novel biomaterial technology optimized to expand metabolically beneficial thermogenic adipose depots in vivo. This system enabled me to determine the degree of metabolic enhancement possible with the exogenous expansion of thermogenic adipose depots. To generate therapeutic adipose implants I modified hyaluronic acid-based hydrogels to support the differentiation of white fat derived multipotent stem cells (ADMSCs) into lipid accumulating, uncoupling protein 1 (UCP1) expressing thermogenic adipocytes. Subcutaneous implantation of the synthetic tissues successfully attracted host vasculature and persisted for several weeks and the implant recipients demonstrated elevated core body temperature during cold challenges, enhanced respiration rates, improved glucose homeostasis, and reduced weight gain demonstrating the therapeutic merit of this highly translatable approach.In chapter II, I outline the experimentation leading to the discoveries presented in chapter III as well as thoroughly review pertinent tissue engineering strategies. Specifically, I sought to define the mechanism by which synthetic ECM components identified in chapter I could alter differentiation outcomes of preadipocytes to yield greater thermogenic capacity. In chapter III, I demonstrate that actomyosin-based mechanical responses provide a critical differentiation cue for the development of thermogenic adipocytes. Since I had determined that the hydrogel optimization techniques described in chapter I were likely acting through cytoskeletal-mediated processes I examined the role of cytoskeletal structure and tension in thermogenic adipose development. I identified that the muscle-like gene expression patterns of UCP1+ adipocytes are critical for the acute induction of oxidative metabolism and uncoupled respiration and regulate mechanosensitive transcriptional co-activators, YAP/TAZ, that control thermogenic gene expression.This dissertation establishes the role of physical mechanics in the development and function of thermogenic adipocytes which may engender future metabolic therapeutics.",ucb,,https://escholarship.org/uc/item/1g26d365,,,eng,REGULAR,0,0
38,1474,"""Other Lovings"": Abjection, Love Bonds, and the Queering of Race","Lee, Seulghee","JanMohamed, Abdul;",2014,"This dissertation discusses the intersection of racial abjection and love bonds in late 20th-century and 21st-century African-American and Asian-American literature and culture. The manuscript deploys affect studies and queer theory to discuss works by Audre Lorde, Amiri Baraka, David Henry Hwang, Adrian Tomine, and Gayl Jones, in addition to the cultural phenomena of ""Linsanity"" and ""afro-pessimism."" Whereas most critical readings of failed love in minority literature have emphasized the tragic interpersonal consequences of internalized racism, this dissertation argues that these writers narrate love's apparent failure in order to explore the positive content emergent in the felt rupture of breakups. Through readings of dissolved love relationships in these authors' works, I inquire into love's operation as an affect that always desires more and better sociality. The appearance of love's failure is precisely what illuminates the ineluctably positive content of love, and I situate this content in the context of recent theoretical discussions of love as narcissistic, not-yet-here, oppressive, or antisocial. The project ultimately argues that blackness, yellowness, and queerness share a privileged access to and familiarity with love's affective positivity.",ucb,,https://escholarship.org/uc/item/0mw1p6xm,,,eng,REGULAR,0,0
39,1475,Phonological Encoding and Phonetic Duration,"Fricke, Melinda",,2013,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0q71t6xk,,,eng,REGULAR,0,0
40,1476,A Convenient Partnership: The Ribosome and the Nascent Chain Interact to Modulate Protein Synthesis and Folding,"Goldman, Daniel Hershel","Bustamante, Carlos J;",2015,"During translation, the ribosome reads the genetic code of the messenger RNA, adding one amino acid at a time to the nascent protein. The sequence of the polypeptide determines the three dimensional structure of the natively folded protein, and thus encodes its biological activity. Because folding rates are often fast compared to translation, many proteins likely undergo folding transitions during synthesis, with folding potentially modulated by the sequential appearance of the polypeptide and the chemical environment of the ribosome. Traditional experimental approaches to study protein folding employ chemical, temperature or pH-induced denaturation and would irreversibly destroy the ribosome; thus, such techniques cannot be used to probe folding on the ribosome. In this work, we implement a novel single-molecule optical tweezers assay to probe folding transitions of the nascent polypeptide as it emerges from the ribosome. We demonstrate that the ribosome can modulate the kinetics of folding through interactions between the nascent chain and the charged ribosomal surface. Additionally, the ribosome can prevent misfolding of incompletely synthesized protein fragments. These observations point to a chaperone-like role for the ribosome in guiding the nascent protein to its native state.In addition to interacting with the exterior of the ribosome, some nascent chain sequences can form specific contacts with the ribosome exit tunnel. These contacts lead to conformational changes of the ribosome, and reduced translation rates. The Secretion Monitor protein stalls the ribosome upon translation of a 17 amino acid motif. Arrest release requires targeting of the stalled ribosome-nascent chain complex to the translocon; thus, it has been hypothesized that arrest is released by a mechanical pulling force generated as the polypeptide is translocated across the membrane. By applying force to the nascent polypeptide of stalled ribosomes, we demonstrate that translation arrest at SecM is released by mechanical force. Additionally, we show that the force needed to release stalling can be generated by a protein folding in close proximity to the ribosome tunnel exit. Our results demonstrate the feasibility of a feedback mechanism, whereby a folding protein can modulate its synthesis through the generation of force. More generally, since the nascent polypeptide in the cell can undergo a number of potentially force-generating events–chaperone binding, protein or ligand binding, translocation and membrane insertion–force applied to the nascent chain may be an important modulator of protein synthesis.",ucb,,https://escholarship.org/uc/item/0rh988bb,,,eng,REGULAR,0,0
41,1477,"An Athenian Commentary on Plato's Republic: Poetry, science and textual engagement in Proclus' In Rem.","Pass, David Blair","Long, Anthony;Ferrari, G.R.F.;",2013,"Proclus' Commentary on Plato's Republic is the only extant ancient Greek commentary on Plato's Republic.  Despite the fact that it includes discussions of most of the major parts of the book, it has received very little scholarly attention.  This dissertation introduces the work in its entirety and tries to identify some of the most important contributions it can make to philosophical and philological scholarship on the Republic.  I am particularly attentive to ways in which Proclus' concerns--such as responding to Epicurean critiques of Platonic myth or defending Homer--may help us see Plato's work in its cultural context.The first chapter focuses on introducing the work and answering basic questions about the place of the Republic in late antique Platonism, the extent of Proclus' sources and what portions of the Republic Proclus discusses.  I consider the form of the commentary, arranged as various essays, in comparison with Proclus' other commentaries which proceed in a line by line manner.  I respond to arguments that have claimed that the commentary is not a unified work by considering the form and extent of the essays relative to the content of the Republic.The second chapter argues that Proclus' commentary is not trimming the Platonic tradition to fit into the religious orthodoxy of late antiquity but rather stressing arguments and interpretative approaches that became most influential in the Renaissance.  I consider several examples such as Proclus' interest in the Orphic and Pythagorean tradition, his emphasis on gender equality and the scientific aspects of his approach to natural philosophy.The third chapter considers some important aspects of Proclus' hermeneutics.  I consider how and why Proclus sometimes disagrees with Plato.  In particular, I focus on some portions of the commentary that demonstrate Proclus' approach to the dramatic aspects of the dialogues and discuss why Proclus' defence of Homer includes some observations about his Platonic hermeneutics.  I consider also his responses to Aristotle's idea of catharsis and his approach to Glaucon's role in the Republic.The fourth chapter translates and discusses a particular portion of the sixth essay in which Proclus argues, contrary to the view Socrates expresses in the Republic, that Homer is a text which teaches the political virtue of sophrosune.  I consider the historical origins of allegorizing interpretations and then distinguish between Proclus' use of allegory and his use of other interpretative methodologies.  I consider in particular Proclus' defence of the idea of euphrosune and compare his approach with earlier philosophical discussions which responded to the same passage of Homer (Odyssey 9.6-10) and interrogated the passage along the lines suggested in the Republic.",ucb,,https://escholarship.org/uc/item/0sn280zv,,,eng,REGULAR,0,0
42,1478,Impacts of Cloud Microphysics on Extreme Precipitation and Lightning,"Charn, Alexander Benedict","Collins, William D;",2020,"The microphysical processes involving water droplets and ice crystals in clouds are too small to be explicitly simulated by climate and weather models. Nevertheless, they play a critical role in the large-scale energy balance of the Earth and its atmosphere, as well as smaller-scale phenomena such as storms. This dissertation examines the impact of microphysics on the latter, specifically extreme precipitation and lightning. Climate change threatens to exacerbate such events, making the understanding of such extremes crucial.We focus primarily on the effects of microphysical processes as they are simulated in a superparameterized climate model, which is better suited to studying clouds and the associated extreme weather events than conventional models. We find statistically significant differences in extreme precipitation rates via two separate mechanisms when replacing one commonly used microphysics parameterization with another. We also find that the sign of changes in lightning flash rates with global warming depends on the microphysics representation used. Finally, we employ observations to address a longstanding question about the necessity of ice as a precursor of lightning. With the data available it is concluded that there is insufficient evidence to suggest that thunderstorm electrification can occur in the absence of ice.",ucb,,https://escholarship.org/uc/item/0sw3x7qx,,,eng,REGULAR,0,0
43,1479,"Cities on the Periphery: Urbanization in Bithynia, Pontus, and Paphlagonia under the Roman Empire","PITT, ERIN MIKAEL","Noreña, Carlos;",2016,"This dissertation, entitled “Cities on the Periphery: Urbanization in Bithynia, Pontus, and Paphlagonia under the Roman Empire,” seeks to provide the first comprehensive urban history of the region during the period of Roman rule. Modern scholarship on this region has focused on cultural and political topics, including Greek reactions to Roman rule; provincial elites and euergetism; and urban life. This scholarship has ignored dramatic increases in the number of new settlements in north central Anatolia, urban and rural, as well as consistent vitality and even growth during the turbulent 3rd century CE. I address these lacunae and investigate the factors behind this growth and stability. I analyze the complexities of this development across four frameworks: the construction and finance of civic monuments, shifting settlement patterns, the extent of bulk and prestige goods networks, and integration into networks of administration, military affairs, and imperial ideology.The introductory first chapter documents the dramatic increases in the number of urban and rural settlements in the region and poses a set of key questions regarding urbanization, imperial intervention, and local stability. I then set out the methodology of my dissertation. I briefly review and critique previous scholarship on this region, which has focused mainly on cultural and political topics of urban and imperial life. I then indicate the advantages of shifting the focus to consider the diachronic nature of urbanization over the long term, the archaeological record, integration and connectivity, and interpretive questions that address the uniqueness of the region. My approach is highly interdisciplinary, making heavy use of evidence from archaeological surveys, epigraphic finds, and network theory, as well as ancient literary and historical accounts.The second chapter examines how local preferences and financial resources influenced the construction and use of civic monuments. The emphasis on Graeco-Roman cities as lived environments, not synchronic monumental landscapes, plays a critical role in this analysis. My discussion qualifies recent assertions that cities in the eastern empire expressed their Greek identity by building democratic monuments with public money. Monuments such as theaters and temples are clearly prioritized, yet cities also enthusiastically adopted monuments marked as Roman, such as baths, or used democratic structures for Roman entertainment. Though civic funds remained a consistent resource, the patronage of local elites and the emperor were essential in the 1st and later 3rd and 4th centuries, respectively. The third chapter synthesizes five decades of archaeological survey. I identify broad trends in expansion, size, and continuity from the Iron Age to the Late Roman period and assess the extent of Roman influence behind these fluctuations. Administrative, economic, and military priorities guided the efficient management of this region. This was achieved by the creation of a few new cites and by an extensive road network. Both constituted unique developments and indirectly encouraged the proliferation of small towns and villages, which benefitted from the demands of regional capitals and access to roads. This produced a balanced urban system that fashioned a robust administrative hierarchy, but that was relatively moderate in overall urban density. The fourth and fifth chapters discuss connectivity across a range of landscapes: city and hinterland, the Black Sea area, and the Mediterranean basin as a whole. The third chapter focuses on the circulation of staple goods and luxury items. This area was remarkably well integrated and even self-sufficient at the local and regional levels. Its position on the periphery of the Roman empire limited intensive contact with the broader Mediterranean, but encouraged intensive commercial relationships with the Black Sea, Armenia, and Syria. The fourth chapter also examines connectivity, but in the context of imperial administration, communication, and military activity.This project ultimately seeks to provide the first comprehensive synthesis of the urban history of north central Anatolia in the Roman period. Roman intervention and traditional urban ideals were early stimuli; as I argue, however, regional preferences, a geographical position on the Mediterranean periphery, and heightened imperial interests in the 3rd century were the most prominent influences on urban development and stability in north central Anatolia. The region occupied a unique geographical, political, and economic position within the Roman empire and it represents a compelling contrast to the urban character of other Roman provinces. I conclude by stressing the complexity of the urban development of this region as well as the strong role that local traditions and geographical position played in negotiating imperial interaction.",ucb,,https://escholarship.org/uc/item/0t88x6h9,,,eng,REGULAR,0,0
44,1480,Syntactic Agreement in Bilingual Corpora,"Burkett, David","Klein, Dan;",2012,"The task of automatic machine translation (MT) is the focus of a huge variety of active research efforts, both because of the intrinsic utility of this difficult task, and the theoretical and linguistic insights that arise from modeling relationships between natural languages. However, MT systems that leverage syntactic information are only recently becoming practical, and in a typical system of this sort, syntactic information is generated by monolingual parsers; the task of explicitly modeling syntactic relationships between target and source languages is yet to be fully explored.This thesis investigates the problem of finding syntactic parse trees of target and/or source sentences that are more appropriate for use in a syntactic MT system. Two basic methodologies are explored.First, we present a sequence of two statistical models that leverage bilingual information to improve the linguistic quality of syntactic parses, as measured by their ability to replicate human-generated gold-standard annotations. The first model uses word to word alignments as an external source of information, while the second models the alignments jointly. These models are both quite effective at improving the intrinsic quality of the parse trees, and the second model additionally improves word alignment performance. However, while the two models achieve similar parsing improvements, we find that improving parses in conjunction with word alignments is much more helpful for the downstream machine translation task.In the next part of the thesis, we explore this finding further by investigating the effects on MT performance of agreement between parse trees and word alignments. We present a simple method for transforming input trees in a way that ignores gold-standard annotations, concentrating instead on improving syntactic agreement directly. In experiments, we find that though we obviously lose fidelity to more linguistically informed treebank annotation guidelines, this transformation-based approach yields the strongest improvements in syntactic machine translation.",ucb,,https://escholarship.org/uc/item/0k07m9zb,,,eng,REGULAR,0,0
45,1481,Wetland water flows and interfacial gas exchange,"Poindexter, Cristina Maria","Variano, Evan A;",2014,"The flow of water in wetlands may exert significant influence on wetland biogeochemistry, and specifically interfacial gas exchange.  Measuring currents in wetlands requires caution.  The acoustic Doppler velocimeter (ADV) is widely used for the characterization of water flow and turbulence.  However, deployment of ADVs in low-ﬂow environments is hampered by a unique source of bias related to the ADV's mode of operation.  The extent of this bias is revealed by Particle image velocimetry (PIV) measurements of an ADV operating in quiescent fluid.   Image-based flow measurement techniques such as PIV may provide improved accuracy in low-flow environments like wetlands.  Such techniques were applied to observe wind-driven flows in a wetland with emergent vegetation and investigate the effects of the wind shear on gas transfer across the air-water interface.  Wind speed is the parameter most often used to model interfacial gas exchange in other aquatic environments.  In wetlands with emergent vegetation, the emergent vegetation will attenuate wind speed above the water surface, modify fluid shear at the water surface, and influence stirring beneath the water surface.  Direct measurements of gas transfer in a model wetland in the laboratory indicated that unless wind speeds are extreme, interfacial gas transfer in wetlands is typically dominated by another physical force: surface cooling-induced thermal convection.  In an application of these lab results, gas transfer across the air-water interface due to thermal convection in the water column is shown to account for a sizable portion of total methane fluxes from a restored marsh in California's Sacramento-San Joaquin Delta.",ucb,,https://escholarship.org/uc/item/1191m1hx,,,eng,REGULAR,0,0
46,1482,"Nanoscale Optical Devices: Force, Torque and Modulator","Liu, Ming","Zhang, Xiang;",2010,"Manipulating and utilizing light in nanoscale are becoming tasks of not only scientific interest, but also industrial importance. My research includes two major topics in nanoscale optics: 1. Nanoscale optical motor. 2. Optical modulator based on novel materials.Light carries both linear and angular momentum, and therefore generating force or torque with light is feasible. The ability to provide torque in nano-meter scale opens up a new realm of applications in physics, biology and chemistry, ranging from DNA unfolding and sequencing, to active Nano-Electro-Mechanical Systems (NEMS). In the first part of this dissertation, I demonstrate a nano-scale plasmonic structure generating a significantly large rotational force when illuminated with linearly-polarized light. I show that a metallic particle with size of 1/10 of the wavelength is capable of rotating a silica microdisk, 4,000 times larger in volume. Furthermore, the rotation velocity and direction can be controlled by merely varying the wavelength of the incident light, thereby inducing different plasmonic modes which possess different torque directions. The tiny dimensions along with the tremendous torque may have a profound impact over a broad range of applications such as energy conversion, and in-vivo biological manipulation and detection.Compared with the interaction between particles and photons, the optical force between particles is of fundamentally important as well. In the end of the first part, I propose a new technique to measure the optical binding forces between two plasmonic particles. By using localization technique on a build-in cantilever, I prove the potential to measure the force with accuracy up to sub-pico-Newton.The second part of this dissertation is about the modulation of light, an optical modulator. Integrated optical modulator with high modulation speed, small footprint and large optical bandwidth is poised to be the enabling device for on-chip optical interconnects. However, present devices suffer from intrinsic narrow-bandwidth aside from their sophisticated optical design, stringent fabrication, temperature tolerances and large foot print. By using graphene, a monolayer of carbon atoms, I experimentally demonstrated a broadband, high-speed, waveguide-integrated electroabsorption modulator. The extremely strong interaction between light and relativistic electrons in graphene allows us to integrate an optical modulator within an ultra-small footprint while operating at a high speed with broad bandwidth under ambient conditions. Even monolayer of being less than 1 nm in thickness, its modulation effectiveness is comparable with the best materials like Ge and SiGe with tens nanometers. In addition, the athermal optoelectronic properties of graphene make the device immune to harsh operation environments, in sharp contrast to all existing semiconductor approaches.",ucb,,https://escholarship.org/uc/item/12f6b8rg,,,eng,REGULAR,0,0
47,1483,"Total Synthesis of (+)-Spectinabilin, Taiwaniaquinoids, Synthetic Progress toward Aspergillin PZ, and Synthesis of a Photoswitchable Agonist of Glutamate Receptor-dMAG","Xu, Yue","Trauner, Dirk;",2010,"A new and highly enantioselective synthetic route to γ-methoxypyranone addressed the long unsolved racemization problem in literature. A concise total synthesis of (+)-Spectinabilin was achieved with this method in 10 linear steps. Concept of kinetic resolution with temporary stereocenter was used to improve the enantiomeric excess.A new variant of Nazarov reaction, aromatic Nazarov triflation was discovered which allowed rapid access of polycyclic ring skeleton. The triflation product, indene triflate, was further elaborated with modern palladium cross coupling methods in the total syntheses of many taiwaniaquinoid natural products. Also, the triflation method worked well with electron rich and neutral substrates and was not compatible with electron deficient substrates.Effort toward total synthesis of Aspergillin PZ was described. The biomimetic synthetic hypothesis was pursued. Synthesis of all components was achieved. Future work would be focused on mild reaction condition to bring all components together to for the key intermediate for Aspergillin PZ.Finally, based on the similar principle of previous work in our group, a photoswitchable agonist for metabotropic glutamate receptors was designed and synthesized. Preliminary results confirmed the agonist activity and reversible isomerization with radiation of light of different wavelengths. Further studies are under investigation under collaboration.",ucb,,https://escholarship.org/uc/item/1365h97h,,,eng,REGULAR,0,0
48,1484,"Low Power, Crystal-Free Design for Monolithic Receivers","Wheeler, Bradley","Pister, Kristofer S.J.;",2019,"Predictions of the proliferation of hundreds of billions of connected wireless devices have yet to come true.The economics of such deployments becoming feasible require that current wireless modules become smaller, cheaper, and use less power.A typical wireless device combines a RF System-on-Chip with multiple frequency references, passive components, an antenna, and a battery on a printed circuit board.The Single Chip Mote project aims to reduce the size, weight, power, and cost of these devices by eliminating the off-chip frequency references and passives.The ultimate goal being to form a 2.4 GHz wireless node by attaching only an antenna and energy source to a single CMOS die.Of particular interest is the range of applications this could enable where the size and weight of current wireless devices has prohibited their use.This work implements a crystal-free IEEE 802.15.4 receiver that covers the data path from RF to bits.The receiver utilizes a passive front-end to reduce power and quadrature down-conversion followed by on-chip filtering and digitization.Integrated digital baseband is included for demodulation and clock recovery as well as built-in estimation of the errors in the RF channel frequency and data rate.Initial frequency calibration is performed simultaneously with bootloading using contact-less optical programming.Operation across the 0 - 70 C commercial temperature range has been demonstrated while inter-operating with commercial off the shelf IEEE 802.15.4 devices.The analog portion of the receiver, including the free-running LO, consumes 1.03 mW from a 1.5 V battery while achieving a sensitivity of -83 dBm.",ucb,,https://escholarship.org/uc/item/13f3h6hd,,,eng,REGULAR,0,0
49,1485,"Living Taiwanese Opera: Improvisation, Performance of Gender, and Selection of Tradition","Hsu, Pattie","Wade, Bonnie C.;",2010,"This dissertation investigates the culture and cultural production of itinerant, professional Taiwanese opera performers in Taipei's temple circuit.  I argue that the community of actors and musicians and their occupational and lifestyle practices constitute a subculture that is central to both maintaining and transforming Taiwanese opera.  Drawing on ethnographic research, I characterize the opera subculture's idiosyncratic and fluid features, examine the major ways in which they are manifested--namely in improvisation, performance of gender, and selection of tradition--and discuss the cultural work they perform.Full-time, for-profit troupes--the focus of my research--primarily work for temple patrons in privately contracted performances and occasionally in government-sponsored events.  Performances in the former venue are improvised or, as the performers describe it, ""alive,"" whereas the latter type privileges written practices and marginalizes oral conventions.  I assert that improvisation, a distinctive and crucial attribute in the temple-contracted context, is an imperative performance skill for producing unscripted stories and a professional strategy for adapting to new circumstances.  My analyses of improvisation as a performance skill highlight actor-musician interactions in song performance that shows spontaneous musical processes in opera production.  Improvisation, or the ability to be flexible, is a professional strategy with which performers operate enabling them to maintain the appeal of a traditional art in a rapidly changing cosmopolitan society.  In particular, I argue that the socioeconomic situation in recent decades and the developing hybrid opera style in the temple context opened a space for an alternative model of gender performance, one that expresses female masculinity.  Moreover, improvisation as a professional strategy enables performers to adapt to the demands of recently developed government-sponsored events and participate in a hegemonically-constructed process for selecting a dominant version of the Taiwanese opera tradition.  Through three case studies, I posit that the performers' flexible approach in this process constructs multiple versions of the opera tradition, thereby disrupting authoritative attempts at claiming a singular mode of production.Through these analyses, I suggest that Taiwanese opera is a living tradition with continually shifting conventions and cultural meanings.  The performers rapidly adjust to different and new ways of performance in order to capitalize on opportunities, ensure the cultural relevancy of their creative production, and secure their livelihood.",ucb,,https://escholarship.org/uc/item/1471d868,,,eng,REGULAR,0,0
50,1486,A role for host activation-induced cytidine deaminase in innate immune defense against herpesviruses,"Bekerman, Elena","Coscoy, Laurent;",2013,"Activation-induced cytidine deaminase (AID) is specifically induced in germinal center B cells to carry out somatic hypermutation and class-switch recombination, two processes responsible for antibody diversification. Because of its mutagenic potential, AID expression and activity are tightly regulated to minimize unwanted DNA damage. Surprisingly, AID expression has been observed ectopically during pathogenic infections. However, the function of AID outside of the germinal centers remains largely uncharacterized. This dissertation demonstrates that infection of human primary naive B cells with Kaposi's sarcoma-associated herpesvirus (KSHV) rapidly induces AID expression in a cell intrinsic manner. We find that infected cells are marked for elimination by Natural Killer cells through upregulation of NKG2D ligands via the DNA-damage pathway, a pathway triggered by AID. Moreover, AID impinges directly on the viral fitness by inhibiting lytic reactivation without having a measurable effect on KSHV latency. We extend this analysis to the murine homologue of KSHV, MHV68 and find that AID mutates the viral genome at a rate that exceeds normal somatic mutation by several orders of magnitude. The tremendous mutational load accumulated by sequential passaging of MHV68 through AID-expressing cells leads to the eventual inactivation of the virus. Importantly, we uncover two KSHV-encoded microRNAs that directly regulate AID abundance, further reinforcing the value of AID in the antiviral response. Together our findings reveal an additional role for AID in innate immune defense against herpesviruses with implications for a broader role in innate immunity to other pathogens.",ucb,,https://escholarship.org/uc/item/15150708,,,eng,REGULAR,0,0
51,1487,A Study of Electrolytic Processes in Micro-Electroporation and Electroporation,"Meir, Arie","Rubinsky, Boris;",2015,to be completed,ucb,,https://escholarship.org/uc/item/15p9j6dx,,,eng,REGULAR,0,0
52,1488,Genetically Tuning Cellular Mechanobiology,"MacKay, Joanna Lynn","Kumar, Sanjay;",2013,"The recognition that cells can sense physical cues has inspired numerous investigations into the roles that mechanical forces play in both healthy and diseased cells.  This rapidly growing area of research, often called cellular mechanobiology, has shown that physical interactions between cells and the surrounding extracellular matrix can regulate a number of fundamental cell behaviors.  This insight has prompted the development of new methods to systematically engineer the mechanical properties of extracellular matrices (e.g., rigidity and geometry), both as a way to study the mechanisms behind cellular mechanosensing and as a way to directly control cell behavior in tissue engineering applications.  In contrast, an equally powerful approach for manipulating cell-matrix interactions could be to directly engineer the mechanical properties of the cells themselves by modifying the intracellular signaling pathways that regulate how cells sense and respond to physical cues.  This type of ""inside-out"" strategy would be useful for controlling cell behavior independently from extracellular matrix properties and would allow investigation into how changes in cellular mechanics such as cell shape, stiffness, and contractility can directly alter cell behavior.In these dissertation studies, a genetic strategy was developed to precisely control the mechanical properties and motility of cells by manipulating the activity of cytoskeletal signaling proteins.  Genetic mutants of the signaling proteins RhoA GTPase, Rac1 GTPase, or myosin light chain kinase (MLCK) were introduced into human glioblastoma cells under the control of conditional promoters, thereby enabling graded and dynamic control over their expression through addition and withdrawal of the transcriptional inducers.  Increasing the activity of RhoA or MLCK by inducibly expressing constitutively active (CA) mutants increased both the stiffness and the contractility of cells in a graded manner, which had an inhibitory effect on cell migration.  Interestingly, decreasing RhoA activity through expression of a dominant negative (DN) mutant also produced a graded decrease in cell migration speed, indicating that cell motility varies biphasically with RhoA activity levels.  A similar biphasic dependence was discovered upon varying the activity of Rac1.  These results demonstrate the importance of using quantitative methods to reveal potentially nonlinear relationships between protein activity and cell behavior.Expanding upon this strategy, two orthogonal promoter systems were combined to provide simultaneous control over the activity of two proteins in the same cell.  RhoA and Rac1 are known to suppress each other through crosstalk between their signaling pathways, suggesting that cells can normally have high activity of only one protein or the other.  To investigate the effects of forcing high activation of both proteins, CA RhoA and CA Rac1 were introduced into the same cells under different conditional promoters (either doxycycline-inducible or cumate-inducible).  Expression of CA RhoA did not alter Rac1 activity and vice versa, demonstrating that the activity levels of RhoA and Rac1 can be independently varied with this strategy.  Notably, expressing both CA mutants had a greater inhibitory effect on cell migration than either mutant alone, indicating that the effects of these mutants were additive rather than suppressive.Finally, these orthogonal promoters were used to dynamically control the motility of multiple cell populations in a three-dimensional matrix, providing a new way to spatially pattern cells.  When cells were cultured as multicellular spheroids within a collagen matrix, CA Rac1 expression stimulated cell migration, while DN Rac1 expression strongly inhibited it.  Thus the ability to switch these two phenotypes on and off by adding and withdrawing the transcriptional inducers provided a way to control both the timing and the extent of cell migration.  To exploit this as a patterning method, cells expressing DN Rac1 from either the doxycycline-inducible promoter or the cumate-inducible promoter were mixed together as multicellular spheroids and then subjected to alternating administration of the two inducers.  When the inducer was switched (e.g., doxycycline removed and replaced with cumate), one population was stimulated to migrate while the other was inhibited, and this created radially symmetric patterns of cells over time.The strategies developed in these dissertation studies represent a novel method for tuning cellular mechanical properties and behaviors, which we expect will be useful in a number of tissue engineering applications.  In addition, by enabling graded control over the activity of multiple proteins, these methods provide a unique opportunity to investigate the quantitative relationships describing how protein activity levels influence cell behavior.  Given that cell and tissue mechanics have been discovered to play critical roles in a number of human diseases, a better understanding of these relationships may lead to new therapeutic targets for disease treatments.",ucb,,https://escholarship.org/uc/item/16m4157x,,,eng,REGULAR,0,0
53,1489,Topics in Evidence Synthesis,"Pozzi, Luca","Jewell, Nicholas P.;Hubbard, Alan E.;",2014,"This dissertation considers three different topics related to extracting and merging evidence from heterogeneous sources. This problem is addressed from different angles, from the field of design of experiment to machine learning.Within this dissertation, we add to the existing literature in each area by developing novel methodology and software. Adaptive trial designs can considerably improve upon traditional designs,by modifying design aspects of the ongoing trial, like early stopping,adding or dropping doses, or changing the sample size. We propose a two-stage Bayesian adaptive design for a Phase IIb study aimed at selecting the lowest effective dose for Phase III. In this setting, efficacy has been proved for a high dose in a Phase IIa proof-of-concept study, but the existence of alower but still effective dose is investigated before the scheduled Phase III starts.In the first stage patients are randomized to placebo, maximaltolerated dose, and one or more additional doses within the doserange. Based on an interim analysis, the study is either stopped forfutility or success, or enters the second stage, where newly recruitedpatients are allocated to placebo, some fairly high dose, and oneadditional dose chosen based on interim data. At the interim analysiscriteria based on the predictive probability of success are used todecide on whether to stop or to continue the trial, and, in the lattercase, which dose to select for the second stage.Finally, a dose will be selected as lowest effective dose for Phase IIIeither at the end of the first or at the end of the second stage. The operating characteristics of the procedure are evaluated viasimulations and results are presented for several scenarios comparingthe performance of the proposed procedure to those of the non adaptivedesign.The development of novel therapies in multiple sclerosis (MS) is one area where a range of surrogateoutcomes are used in various stages of clinical research. While the aim of treatments in MS is to preventdisability, a clinical trial for evaluating a drugs effect on disability progression would require a largesample of patients with many years of follow-up. The early stage of MS is characterized by relapses. Toreduce study size and duration, clinical relapses are accepted as primary endpoints in phase III trials. Forphase II studies, the primary outcomes are typically lesion counts based on Magnetic Resonance Imaging(MRI), as these are considerably more sensitive than clinical measures for detecting MS activity.Recently, Sormani and colleagues \cite{sormani2010surrogate} provided a systematic review, andused weighted regression analyses to examine the role of either MRI lesions or relapses as trial levelsurrogate outcomes for disability. We build on this work by developing a Bayesian three-level model,accommodating the two surrogates and the disability endpoint, and properly taking into account thattreatment effects are estimated with errors. Specifically, a combination of treatment effects based onMRI lesion count outcomes and clinical relapse, both expressed on the log risk ratio scale, were used todevelop a study level surrogate outcome model for the corresponding treatment effects based ondisability progression. While the primary aim for developing this model was to support decision makingin drug development, the proposed model may also be considered for future validation.In Genomics and Epidemiology we deal with a high number of features for each observation. Many well known approaches to drawing inferences in this kind of settings use the topology of the feature space, induced by an appropriate metric, to group observations and summarize their main characteristics to get rid of the noise and to predict an outcome of interest. In the present work we generalize this approach in the context of Loss-Based Estimation. We propose an alternative method for constructing a nonparametric multidimensional regression function. This approach is based on the simple idea of clustering data points in the feature space and then fitting a constant to the outcome. HOPACH-PAM is used for partition. This approach results in the choice of a small number of distinct regions easy to interpret. This is specifically illustrated by simulations from which we can see immediately the superiority of this method on CART. Pre-screening and feature selections methods are also developed to improve the performances and reduce the noise. Software is also available in the R package HOPSLAM (HOpach-Pam Supervised Learning AlgorithM) to make this methodology easily accessible.",ucb,,https://escholarship.org/uc/item/1b61431k,,,eng,REGULAR,0,0
54,1490,Evaluating a Telenovela: The Safety of Latino Construction Workers,"Castaneda, Diego Emiliano","Syme, S. Leonard;",2011,"Latino-Hispanic construction workers in the United States are at significantly higher risks for injuries and fatalities at construction worksites than their White and African-American counterparts. Currently the main mode of dissemination of workplace safety information is through direct translation of work safety material delivered at the worksite. Current research, however, suggests that even when translated into Spanish, many of these materials are not culturally or linguistically effective modes of preventable risk education and persuasion. One promising method for far-reaching, cost-effective, and culturally relevant education may be found in the Entertainment-Education (EE) health communication strategy. EE leverages popular entertainment media - such as movies, television shows, music, theater and/or radio - by embedding specific health messages within a storyline and using the power of narrative to stimulate positive health choices. Spanish-language soap operas (telenovelas) are an entertainment media format culturally embraced by Latino Spanish-speaking audiences and have been effectively utilized by health educators and public health officials to promote changes in knowledge, attitudes, and behaviors for a variety of health issues. The Centers for Disease Control/National Institute of Occupational Safety and Health (CDC/NIOSH) worked with two public health partners and the Spanish language TV chain Telemundo to develop and implement an entertainment education intervention that utilized a telenovela embedded with construction worksite safety information. A statistical analysis of audience survey data collected both before and after the airing of the workplace storyline showed improvements in knowledge outcomes but not in changes in perceptions or behavioral intention outcomes. Detailed analyses revealed that survey respondents who reported recognition of the telenovela workplace storyline were more likely to identify key safety messages embedded within the storyline than respondents who did not recognize the storyline. In addition to the quantitative data, semi-structured key informant interviews were conducted with eight (8) individuals associated with the intervention project. The objective of these interviews was to explore how the partnership between public health institutions and media organizations affected the development, implementation, and evaluation of this project. Project stakeholders voiced challenges which stemmed from the chaotic nature of network television, tensions between developing entertaining vs. accurate educational messaging, and difficulty in communicating actionable messages that would be effective in changing workplace behaviors. Despite these challenges partners felt confident that future endeavors using an EE strategy should be made in communicating other workplace safety issues to Latino and other vulnerable populations. Improved collaboration between entertainment media writers/producers and public health experts is needed to create interventions with the power to change viewers behaviors over time. In addition, more refined research methods are needed to examine EE intervention development and outcomes.",ucb,,https://escholarship.org/uc/item/1cj6d90r,,,eng,REGULAR,0,0
55,1491,Statistical Problems in DNA Microarray Data Analysis,"Wang, Nancy Naichao","Speed, Terence P;",2009,"DNA microarrays are powerful tools for functional genomics studies.  Each array contains thousands of microscopic spots of DNA oligonucleotides with specific sequences, which can hybridize with their complementary DNA sequences.  Thus each microarray experiment consists of parallel assays about thousands of genomic fragments.  This thesis concerns some statistical issues in the analysis of DNA microarray data.One common usage of DNA microarrays is to monitor the dynamic levels of gene expression in response to a stimulus.  This is often achieved through a time course experiment, in which RNA samples are extracted at various time points after exposing the organism to the stimulus.  A particularly interesting type of time course experiments involve replicated series of longitudinal samples.  In 2006, Tai and Speed proposed a multivariate empirical Bayes model for analyzing this type of data.  The MB-statistic derived from this model was shown useful for ranking the genes according to changes in their temporal expression profiles.  In the first part of this thesis, we propose an empirical Bayes false discovery rate (FDR)-controlling procedure for multiple hypothesis testing using the MB-statistic.  A null distribution is obtained using the parametric bootstrap.  Critical values are determined according to the empirical Bayes FDR procedure.  This method was compared, through simulations, to the frequentist FDR procedure, which requires a theoretical null distribution for calculating the nominal p-values.  Although our method is slightly anti-conservative, it is more robust to the variability in the estimates of the hyperparameters, when the degree of moderation is small.Another common usage of DNA microarrays is to detect genomic locations that are associated with DNA-binding proteins.  This is often achieved through ChIP-chip experiments that combine chromatin immunoprecipitation with the microarray technology.  Traditional DNA microarrays designed for gene expression studies contain only a few probes for each gene.  A special type of DNA microarrays, called tiling arrays, are often used in ChIP-chip experiments.  They typically contain probes that are placed densely along the chromosomes to cover either the entire genome or contigs of the genome.  A couple of challenges in the analysis of ChIP-chip tiling array data have not been met satisfactorily in the literature.  When large scale genomic studies are carried over a long period of time, tiling arrays with different probe designs are often used for practical reasons.  The first challenge is the integration of replicate experiments performed using different tiling array designs.  When the biological process of interest involves a large protein complex, the investigators often perform ChIP-chip experiments on each component DNA-binding protein individually.  DNA targets that are shared by the individual proteins are thought to be the localization sites of the protein complex.  The second challenge is the joint analysis of multiple DNA-binding proteins, aimed at identifying their shared targets.  In the second part of this thesis, we propose a nonhomogeneous hidden Markov model (HMM) for addressing these two challenges.  The nonhomogeneous time axis represents the genomic positions of the probes.  The hidden states represent the binding statuses of the proteins.  The state-conditional emission distributions of the tiling array data are protein-specific and design-specific.  We derived a modified Baum-Welch algorithm for fitting the model parameters.  We also developed a procedure that converts the probe level summaries into peaks, which represent the putative binding sites, based on both signal strength and peak shape.  To compare our method with existing methods, we curated a set of positive and negative genomic regions from a C. elegans dataset, and performed some receiver operating characteristics (ROC) analyses.  When applied to each experiment separately, our method performs similarly as the three best existing methods.  When applied to the combined data set, which consists of tiling arrays with different probe designs, our method shows a drastic improvement in performance.  A generalization of the nonhomogeneous HMM enables the joint analysis of the ChIP-chip data of multiple proteins.  We present an application of this method to identify the shared localization sites of two DNA-binding proteins, under two different conditions.",ucb,,https://escholarship.org/uc/item/1dm0z29w,,,eng,REGULAR,0,0
56,1492,Employer Preparedness for Pandemic Influenza: Shifting the Conversation from Insurance to Investment,"Lachance, Jennifer Alice","Reingold, Arthur;",2010,"Pandemic influenza is currently one of the most visible public health threats of concern to the general public, and private businesses are an important part of pandemic preparedness. The health of communities is affected by the local economy, which is driven by the businesses in that economy. To date, public health authorities' efforts to engage businesses in pandemic influenza preparedness efforts have justified preparedness based on potential losses due to future, uncertain threats. However, this approach has not successfully engaged businesses on a broad scale. This dissertation proposes that a more effective way to engage the private sector may be to shift the conversation away from justifying preparedness only as a long-term insurance strategy and toward justifying it as an investment strategy with short-term benefits such as improved employee health during interpandemic cold and influenza seasons. The viability and acceptability of this new approach are explored here via three distinct but complementary studies using both quantitative and qualitative methods.       The first study, a prospective observational cohort study, examined the individual characteristics and situations that predicted changes in hand and respiratory hygiene and social distancing behaviors among university students during an interpandemic cold and influenza season. This analysis reveals that individuals have higher adherence to behaviors in situations such as when they are ill. Additionally, some individual characteristics predict higher behavior adherence. In particular, individuals who perceive peer expectations concerning adherence to hygiene behaviors tend to have better adherence to those behaviors over the course of a cold and influenza season.       The second study, a cost-effectiveness analysis of a hand and respiratory hygiene intervention among university students, assessed whether an intervention could be cost-effective in reducing influenza-like illness and associated time lost from productive activities during an interpandemic cold and influenza season. This analysis finds that hand and respiratory hygiene interventions can be cost-effective and may even become cost-saving during a severe cold and influenza season, especially using group-level interventions that may create peer expectations to influence behaviors.      Finally, the third study, an exploratory analysis based on key informant interviews with private sector business continuity managers, consultants, and public sector planners, examined private sector preparedness for pandemic influenza. This analysis assessed the key components of employer pandemic influenza preparedness plans, including whether short-term benefits are a consideration in business planning. The results indicate that the most important components of private sector pandemic influenza plans before and during the 2009 H1N1 influenza pandemic included communications and employee education around hygiene behaviors. Participants further identified that implementation of these initiatives during interpandemic cold and influenza seasons is of interest to organizations due to potential short-term and long-term benefits.      These results together provide evidence that education and provision of materials for hygiene behaviors at a group level can be cost-effective in reducing influenza-like illnesses during interpandemic cold and influenza seasons and are an acceptable strategy to the private sector. This provides a basis for the hypothesis that employer preparedness for public health events such as pandemic influenza can be justified as a short-term business investment strategy rather than only as a long-term insurance strategy.",ucb,,https://escholarship.org/uc/item/0p05b5qx,,,eng,REGULAR,0,0
57,1493,The Role of Distribution Infrastructure and Equipment in the Life-cycle Air Emissions of Liquid Transportation Fuels,"Strogen, Bret","Horvath, Arpad;",2012,"Production of fuel ethanol in the United States has increased ten-fold since 1993, largely as a result of government programs motivated by goals to improve domestic energy security, economic development, and environmental impacts.  Over the next decade, the growth of and eventually the total production of second generation cellulosic biofuels is projected to exceed first generation (e.g., corn-based) biofuels, which will require continued expansion of infrastructure for producing and distributing ethanol and perhaps other biofuels.  In addition to identifying potential differences in tailpipe emissions from vehicles operating with ethanol-blended or ethanol-free gasoline, environmental comparison of ethanol to petroleum fuels requires a comprehensive accounting of life-cycle environmental effects.  Hundreds of published studies evaluate the life-cycle emissions from biofuels and petroleum, but the operation and maintenance of storage, handling, and distribution infrastructure and equipment for fuels and fuel feedstocks had not been adequately addressed.  Little attention has been paid to estimating and minimizing emissions from these complex systems, presumably because they are believed to contribute a small fraction of total emissions for petroleum and first generation biofuels.  This research aims to quantify the environmental impacts associated with the major components of fuel distribution infrastructure, and the impacts that will be introduced by expanding the parallel infrastructure needed to accommodate more biofuels in our existing systems.  First, the components used in handling, storing, and transporting feedstocks and fuels are physically characterized by typical operating throughput, utilization, and lifespan.  US-specific life-cycle GHG emission and water withdrawal factors are developed for each major distribution chain activity by applying a hybrid life-cycle assessment methodology to the manufacturing, construction, maintenance and operation of each component.  Emissions from activities at the end of life of equipment and infrastructure are not included, as these activities have previously been shown to contribute negligibly to life-cycle emissions.  Life-cycle transportation mode GHG emission factors per tonne-kilometer (t-km) are presented for long distance pipelines (5-20 g CO2-e/t-km), ocean tankers (5-17 g/t-km), fuel-carrying barges (31 g/t-km), fuel-carrying unit trains (25 g/t-km), tanker trucks (140-180 g/t-km), and bale-transporting flatbed trucks (200 g/t-km).  Life-cycle emission factors are also presented per tonne of material throughput for several types of agricultural equipment (600-19,000 g CO2-e/t handled), fuel conversion facilities (9,000-98,000 g/t), fuel storage and dispensing facilities (2,000-12,000 g/t), and the portion of passenger vehicle operations dedicated to refueling errands (2,000-200,000 g/t).  The emissions intensity ranges reported for specific transportation modes are largely due to the greater energy efficiency of larger vehicles and pipelines, and the emissions intensity ranges within stationary storage and handling equipment is often due to differences in utilization of capital equipment and/or material losses during storage and handling activities.  Consistent with existing literature, the contribution of non-operation stages to life-cycle GHG emissions ranges from 20% to 40% for most of the components modeled.  Criteria air pollutant (NOx, PM2.5, SOx, VOC, CO) emission factors are also presented for the operation stage (e.g., tailpipe only) of each transportation mode.  In order to apply the new emission factors to policy-relevant scenarios, a projection is made for the fleet inventory of infrastructure components necessary to distribute 21 billion gallons of ethanol (the 2022 federal mandate for advanced biofuels under the Energy Independence and Security Act of 2007) derived entirely from Miscanthus grass, for comparison to the baseline petroleum system.  Due to geographic, physical and chemical properties of biomass and alcohols, the distribution system for Miscanthus-based ethanol is more capital- and energy-intensive than petroleum per unit of fuel energy delivered.  Assuming steady-state annual turnover, operation, and maintenance of infrastructure to supply the projected quantities of ethanol and petroleum fuels, ethanol is estimated to be approximately five times more GHG and water intensive than petroleum (i.e., GHG emissions of more than 17 g CO2-e/MJ versus 3 g/MJ, and water withdrawals of 380 L/MJ vs. 77 L/MJ of consumed fuel, neglecting feedstock production and conversion).  Embodied GHG emissions from manufacturing and maintaining infrastructure, equipment, and vehicles make up less than half of these emissions, at approximately 1 g CO2-e/MJ of petroleum fuel and 8 g CO2-e/MJ of ethanol.  Although petroleum fuels are projected to supply twenty times the energy content of ethanol in 2022, the annual GHG and water withdrawal footprint of petroleum's liquid fuel infrastructure and distribution system is slightly less than four times that of ethanol (i.e., 110 vs. 30 million tonnes of CO2-e and 2,500 vs. 640 billion liters of water).  Opportunities to significantly reduce emissions include shifting transportation to more efficient modes, consuming products closer to producers, and converting biorefineries to produce fuel with higher energy density than ethanol.  Minimizing fuel transportation distance is believed to be the most feasible and cost-effective opportunity to reduce emissions in the near term.The transportation of biofuels away from producer regions poses environmental, health, and economic trade-offs that are herein evaluated using a simplified national distribution network model.  In just the last ten years, ethanol transportation within the contiguous United States is estimated to have increased more than ten-fold in total t-km as ethanol has increasingly been transported away from Midwest producers due to air quality regulations pertaining to gasoline, renewable fuel mandates, and the 10% blending limit (i.e., the E10 blend wall).  From 2004 to 2009, approximately 10 billion t-km of ethanol transportation are estimated to have taken place annually for reasons other than the E10 blend wall, leading to annual freight costs greater than $240 million and more than 300,000 tonnes of CO2-e emissions and significant emissions of criteria air pollutants from the combustion of more than 90 million liters of diesel.  Although emissions from distribution activities are small when normalized to each unit of fuel, they are large in scale.Archetypal fuel distribution routes by rail and by truck are created to evaluate the significance of mode choice and route location on the severity of public health impacts from locomotive and truck emissions, by calculating the average PM2.5 pollution intake fraction along each route.  Exposure to pollution resulting from trucking is found to be approximately twice as harmful as rail (while trucking is five times more energy intensive).  Transporting fuel from the Midwest to California would result in slightly lower human health impacts than transportation to New Jersey, even though California is more than 50% farther from the Midwest than most coastal Northeast states.In summary, this dissertation integrated concepts from infrastructure management, climate and renewable fuel policy, fuel chemistry and combustion science, air pollution modeling, public health impact assessment, network optimization and geospatial analysis.  In identifying and quantifying opportunities to minimize damage to the global climate and regional air quality from fuel distribution, results in this dissertation provide credence to the urgency of harmonizing policies and programs that address national and global energy and environmental goals.  Under optimal future policy and economic conditions, infrastructure will be highly utilized and transportation minimized in order to reduce total economic, health, and environmental burdens associated with the entire supply and distribution chain for transportation fuels.",ucb,,https://escholarship.org/uc/item/0r60z01k,,,eng,REGULAR,0,0
58,1494,Prediction Methods for Astronomical Data Observed with Measurement Error,"Long, James Patrick","Rice, John A;El Karoui, Noureddine;",2013,"We study prediction when features are observed with measurement error. The research is motivated by classification challenges in astronomy.In Chapter 1 we introduce the periodic variable star classification problem. Periodic variable stars are periodic functions which belong to a particular physical class. These functions are often sparsely sampled, which introduces measurement error when attempting to estimate period, amplitude, and other function features. We discuss how measurement error can impact performance of periodic variable star classifiers. We introduce two general strategies, noisification and denoisification, for addressing measurement error in prediction problems.In Chapter 2 we study density estimation with Berkson error. In this problem, one observes a sample from the density $f_X$ and seeks to estimate $f_Y$, the convolution of $f_X$ with a known error distribution. We derive asymptotic results for the behavior of the mean integrated squared error for kernel density estimates of $f_Y$. The presence of error generally increases convergence rates of estimators and optimal smoothing parameters. We briefly discuss some potential applications for this work, including classification tasks involving measurement error.In Chapter 3 we study prediction of a continuous response for an observation with measurement error in its features. Using Nadaraya Watson type estimators we derive limit theorems for convergence of the mean squared error as a function of the smoothing parameters.In Chapter 4 we study the effects of measurement error on classifier performance using data from the Optical Gravitational Lensing Experiment (OGLE) and the Hipparcos satellite. We illustrate some challenges in constructing statistical classifiers when the training data is collected by one astronomical survey and the unlabeled data is collected by a different survey. We use noisification to construct classifiers that are robust to some sources of measurement error and training--unlabeled data set differences.",ucb,,https://escholarship.org/uc/item/0s79z3hk,,,eng,REGULAR,0,0
59,1495,"Social Work Delivered Intervention for Persons with Mild Traumatic Brain Injury: Implementation and Evaluation in an Urban, Public, Trauma Center Emergency Department","Moore, Megan","Segal, Steven P;",2012,"Mild traumatic brain injury (mTBI) is a prevalent and costly public health problem with potentially disabling consequences.  Interventions aimed at alleviating cognitive, emotional and behavioral sequelae are underdeveloped.  This prospective, quasi-experimental cohort study evaluated a brief social work delivered intervention (SWDI) for adults with mTBI discharged from the emergency department.  The SWDI included education, reassurance, coping strategies and community resource information.  Participants were recruited from consecutive admissions to the emergency department.  A total of 64 persons with confirmed mTBI diagnoses were assessed 3 months post-injury.  Participants in the Usual Care group (N=32) were identified via medical record; confirmation of mTBI was based on World Health Organization definition.  Participants in the SWDI group (N=32) were identified and mTBI diagnosis confirmed by emergency department medical staff.  Both groups completed standardized assessments of post-concussion symptoms, depression, anxiety, Posttraumatic Stress Disorder, alcohol use, and community functioning three months after injury.  To assess change in alcohol use and community functioning, participants were asked to recall pre-injury drinking levels and functioning and then asked about current status three months post injury.  The SWDI group also completed an open-ended Patient Experience Survey following their ED service.    The paired sample t test was used to assess community functioning outcomes.  For all other standardized measures, non-parametric Mann Whitney or Wilcoxon Signed Rank tests were used to compare groups.  Qualitative themes from the Patient Experience Survey were identified through systematic review of all survey responses.Three months post injury, both groups reported pre-injury drinking in the ""hazardous"" range.  The SWDI group reported significantly reduced alcohol use from pre-injury to post-intervention (p < 0.05).  The Usual Care group maintained their pre-injury level of drinking.  Analysis of the community functioning measure revealed the SWDI group maintained pre-injury levels of community functioning, while the Usual Care group reported significant decline in functioning (p = 0.05).  All other analyses of standardized measures (anxiety, depression, PTSD, post-concussive symptoms) trended in favor of the intervention group, but were not statistically significant.  Results from the SWDI Patient Experience Survey indicate that 96% of participants who remembered receiving the intervention (N=25) found it helpful.  In response to an open ended question about the most helpful aspects of the intervention, 60% reported it was most helpful to learn about symptoms to expect because this decreased anxiety about symptoms, 28% reported that the recovery tips were most helpful and 24% reported that education about ceasing alcohol use was most helpful. The study provides support for the use of the SWDI in the emergency department.  Decrease in alcohol use and maintenance of community functioning are clinically and functionally significant outcomes.  Alcohol use is a risk factor for re-injury and poor outcome, and the measure of community functioning includes probes about work, school and social activity attendance as well as ability to complete household and daily living activities.  In addition, the SWDI group overwhelmingly found the intervention helpful.  Education about symptoms to expect and decreasing alcohol use was particularly salient for participants.  Future studies should consider survey themes and ways to enhance the intervention in order to increase the impact on additional outcomes of interest.",ucb,,https://escholarship.org/uc/item/0sw9w8t7,,,eng,REGULAR,0,0
60,1496,Understanding and Engineering Cellulase Binding to Biomass Components,"Strobel, Kathryn Lynn","Clark, Douglas S;",2015,"Lignocellulosic biomass is an abundant, low-cost resource for the renewable production of fuels and chemicals. To unlock the potential of lignocellulosic biomass, the cellulose must be broken down into sugars before fermentation to produce ethanol, butanol, or other bio-based products. Unfortunately, lignocellulose is highly resistant to enzymatic degradation, necessitating high enzyme loadings that increase the cost of biofuels. The recalcitrance of biomass stems in part from the presence of lignin, a major component of lignocellulosic biomass. Lignin impedes enzymatic hydrolysis by non-productively binding cellulases and contributing to cellulase denaturation. Despite numerous studies documenting cellulase adsorption to lignin, the structural basis has not been fully elucidated and few attempts have been made to engineer enzymes for reduced lignin affinity. In this work, we investigate and engineer cellulase adsorption to lignin and the resulting effect on hydrolysis of cellulose. The lignin inhibition of two homologous cellulases, T. reesei Cel7A and T. emersonii Cel7A, was found to differ significantly. In Chapter 2, we propose that differences in surface charge, stability, and glycosylation patterns may be the driving force/s behind the observed differences in lignin inhibition and we suggest engineering strategies for improving lignin tolerance of Cel7A catalytic domains.  Chapters 3 and 4 detail our efforts to investigate the mechanisms of cellulase lignin adsorption and engineer an enzyme with reduced lignin affinity using site directed mutagenesis of the T. reesei Cel7A carbohydrate binding module (CBM) and linker. Mutation of aromatic and polar residues on the planar face of the CBM greatly decreased binding to both cellulose and lignin, supporting the hypothesis that the cellulose-binding face is also responsible for the majority of lignin affinity. Cellulose and lignin affinity of the alanine mutants were highly correlated, indicating similar binding mechanisms for cellulose and lignin. CBM mutations that added hydrophobic or positively charged residues decreased the selectivity toward cellulose, while mutations that added negatively charged residues increased the selectivity.  Mutating the linker to alter predicted glycosylation patterns greatly impacted lignin affinity but did not affect cellulose affinity. Beneficial mutations were combined to generate a mutant with 2.5 fold less lignin affinity and fully retained cellulose affinity. This mutant was not inhibited by added lignin during hydrolysis of Avicel and generated 40% more glucose than the wild type enzyme from dilute acid-pretreated Miscanthus. The mutations studied here inform engineering efforts of other homologous CBMs and will hopefully contribute to reducing the cost of biofuels. The final chapter details the development of a high-throughput selection platform for engineering protease enzymes with new sequence specificity. Proteases are commonly used in research, industry, and medicine, and there is considerable promise for new proteases that could cleave at a user-specified sequence. Positive selection and counter-selection were combined to select a tobacco etch virus protease mutant with new substrate compatibility.",ucb,,https://escholarship.org/uc/item/0tw1f2tz,,,eng,REGULAR,0,0
61,1497,"The Therapeutic Turn in International Humanitarian Law: War Crimes Tribunals as Sites of ""Healing""?","Anders, Diana Elizabeth","Butler, Judith;Cohen, David;",2012,"AbstractThe Therapeutic Turn in International Humanitarian Law: War Crimes Tribunals as Sites of ""Healing""?  by  Diana Elizabeth Anders  Doctor of Philosophy in Rhetoric Designate Designated Emphasis in Women, Gender, and SexualityUniversity of California, Berkeley  Professor Judith Butler, Co-Chair  Professor David Cohen, Co-Chair This dissertation examines the growing tendency to figure international war crimes tribunals in terms of their therapeutic value for their victims. My project documents and questions how the discourse of juridical healing emerged from what I term ""the therapeutic turn"" in international humanitarian law (hereafter, IHL). I analyze this phenomenon in terms of its key features, conditions of possibility, modes of legitimization, and effects, focusing on legal institutions designed to adjudicate crimes such as genocide, mass rape, and torture. My central argument is that the rhetoric of juridical healing, despite its commendable achievements, comes at an important cost, in that the appeal to law can invite new forms of regulation and domination. In short, this novel form of justice produces and authorizes its own forms of violence. It does so in part by obscuring the political effects of the law's promise to heal. To bring this uncomfortable fact into relief is but a first step towards countering such ill effects.  This project focuses on the first two international ad hoc tribunals -- the International Criminal Tribunals for the former Yugoslavia and the International Tribunal for Rwanda -- as well as the International Criminal Court (hereafter, the ICTY, the ICTR, and the ICC). All were established in the 1990s in the beginning of what has been called the ""tribunal era,""  which has ushered in an unprecedented emphasis on victims of atrocity. Primarily by means of discourse analysis, I examine court documents and trial transcripts, as well as relevant statements made by diplomats, politicians, court officials, scholars, and non-governmental-organizations. Such analysis aims to chart the expansion of a new norm of justice as healing that has so far largely gone unrecognized.   Chapter One outlines the general problem of the dissertation, introducing the phenomenon of juridical healing and situating it historically. Although such healing has become a powerful, even normative trope in humanitarian discourse, it has not been well defined. The chapter raises questions concerning what juridical healing can realistically achieve, and how it might constitute a new mode of power that paternalistically regulates the very subjects it pledges to heal. It also examines how the special status of healing discourse as ""above reproach"" has shielded it from critical scrutiny.     Chapter Two surveys the growing scholarly discourse on juridical healing, arguing that such inquiries tend to uncritically accept the core terms of the therapeutic turn. This work can thus serve to reify the problematic notion of healing promulgated elsewhere. Such thinking holds that tribunals can occasion forms of ""catharsis"" and ""closure,"" both for individual victim-witnesses and more broadly. I argue that this belief in ""disclosure for closure"" forecloses critical reflection on the effects of juridical healing, or on alternatives to this conception.  In Chapter Three, I develop a genealogy of juridical healing in relation to new legal institutions. I analyze how the promise of healing has served as a means of legitimization, even as it has led courts into uncharted legal territory. Even as the tribunals of the 1990s derived their credibility from the Nuremberg and Tokyo tribunals that followed World War II, they also had to distance themselves from the accusation that the latter had only dispensed ""victors' justice."" In an uncanny echo, recent tribunals can be said to have produced forms of ""victims' justice."" I examine how such rhetoric threatens to undermine the same credibility that it otherwise means to establish, even at the cost of the victims it purportedly champions.  Chapter Four considers the tribunals' adjudication of sexual violence as a war crime. Here I use individual case studies to show the unforeseen costs of such procedures. I examine how the female victim of sexual violence is effectively condemned to victimhood by the very discourse that promises to heal her, but denies her meaningful agency. At the same time, ""other"" victims of wartime sexual violence--such as men, boys, or women from the ""enemy camp""--are marginalized. My analyses of these cases explore how therapeutic-juridical interventions can undermine their avowed aims, while concealing the power relations on which they rely and which they perpetuate.   The final chapter is based on fieldwork that I carried out in 2009 in The Hague, Netherlands, and examines the depoliticizing effects of juridical healing.  Drawing on interviews with ICTY and ICC officials, the chapter outlines the temporal and spatial coordinates of the rhetoric of healing. I focus on the ways in which such rhetoric enacts movements of deferral and displacement, and thus neutralizes potential forms of political activity. As an alternative, I examine Hannah Arendt's account of politics, which is centered on collective, participatory action and antagonistic debate. Such a view allows us to imagine a more capable subject of politics, one with the potential to recover, resist, and revolt.  The Epilogue evaluates the current and future implications of the rhetoric of healing, exploring alternative responses to extreme violence. I claim that juridical healing can be understood as the latest ""last utopia""  or the least ""lesser evil""  in a time when ""human rights"" and ""humanitarianism"" have become increasingly wed to military interventions. I proceed to trace additional contradictions in the discourse of juridical healing, in that contemporary IHL also identifies with the ideology of militarized humanitarianism in its endorsement of the UN doctrine of ""Responsibility to Protect."" I close by suggesting that juridical healing presents the international community with an aporia that might ultimately be generative, insofar as it produces conditions under which the very politics it stifles might also be aroused. By rethinking and reframing this rhetoric, I hope to indicate avenues for differently imagining and producing the future--a future not destined to repeat or be dictated by the violence, injustice, and pain of the past.",ucb,,https://escholarship.org/uc/item/0vc1f4gc,,,eng,REGULAR,0,0
62,1498,Biodiversity and Ecosystem Services in Agriculture: Evaluating the Influence of Floral Resource Provisioning on Biological Control of Erythroneura Leafhoppers (Hemiptera: Cicadellidae) and Planococcus Mealy Bugs (Hemiptera: Pseudococcidae) in California Vineyards,"Miles, Albie Felix","Altieri, Miguel A;",2013,"The research tested the natural enemies hypothesis in an attempt to explain why lower pest densities are observed in some diversified farming systems. The research evaluated the influence of floral resource provisioning (FRP) and chemical ecology strategies on biological control of Erythroneura leafhoppers (Hemiptera: Cicadellidae) and Planococcus mealybug (Hemiptera: Pseudococcidae) in California vineyards. Field and laboratory studies quantified the impacts on crop damage, pest and natural enemy abundance, and natural enemies fitness theorized to be enhanced through floral resource provisioning in agroecosystems. Multiple two-year studies measured the impact of intercropping three flowering ground covers, lacy phacelia (Phacelia tanacetifolia), bishop's weed (Ammi majus), and common carrot (Daucus carota) on biological control of leafhoppers and vine mealybug by the parasitoids Anagrus spp. (Hymenoptera: Mymaridae) and Anagyrus pseudococci (Hymenoptera: Encyrtidae). Using identical intercropping treatments, the research included three large scale and fully replicated research designs located in the central San Joaquin, the northern San Joaquin, and the Napa Valley of California. Laboratory studies quantified the impacts of FRP on the fitness of Anagyrus pseudococci, a key parasitoid natural enemy of vine mealybug. The central San Joaquin Valley field study measured the impact of FRP and pheromone based mating disruption on biological control of vine mealybug. The northern San Joaquin Valley field study measured the impact of FRP and methyl salicylate on biological control of Erythroneura leafhoppers. The Napa Valley field study measured the effect of methyl salicylate alone on biological control of Erythroneura leafhoppers.",ucb,,https://escholarship.org/uc/item/0vt4z0fd,,,eng,REGULAR,0,0
63,1499,Seeing in the Dark: Weak Lensing from the Sloan Digital Sky Survey,"Huff, Eric Michael","Schlegel, David J;Selak, Uros;",2012,"Statistical weak lensing by large-scale structure { cosmic shear { is a promising cosmological tool, which has motivated the design of several large upcoming astronomical surveys. This Thesis presents a measurement of cosmic shear using coadded Sloan Digital Sky Survey (SDSS) imaging in 168 square degrees of the equatorial region, with r < 23:5 and i < 22:5, a source number density of 2.2 per arcmin2 and median redshift of zmed = 0.52. These coadds were generated using a new rounding kernel method that was intended to minimize systematic errors in the lensing measurement due to coherent PSF anisotropies that are otherwise prevalent in the SDSS imaging data. Measurements of cosmic shear out to angular separations of 2 degrees are presented, along with systematics tests of the catalog generation and shear measurement steps that demonstrate that these results are dominated by statistical rather than systematic errors. Assuming a cosmological model corresponding to WMAP7(Komatsu et al., 2011) and allowing only the amplitude of matter fluctuations &sigma8 to vary, the best-t value of the amplitude of matter fluctuations is &sigma8=0.636+0.109-0.154 (1&sigma); without systematic errors this would be  &sigma8=0.636+0.099-0.137 (1&sigma). Assuming a flat &LambdaCDM model, the combined constraints with WMAP7 are &sigma8=0.784super>+0.028-0.026  (1&sigma). The 2&sigma error range is 14 percent smaller than WMAP7 alone. Aside from the intrinsic value of such cosmological constraints from the growth of structure, some important lessons are identied for upcoming surveys that may face similar issues when combining multi-epoch data to measure cosmic shear. Motivated by the challenges faced in the cosmic shear measurement, two new lensing probes are suggested for increasing the available weak lensing signal. Both use galaxy scaling relations to control for scatter in lensing observables.The first employs a version of the well-known fundamental plane relation for early type galaxies. This modified ""photometric fundamental plane"" replaces velocity dispersions with photometric galaxy properties, thus obviating the need for spectroscopic data. We present the first detection of magnication using this method by applying it to photometric catalogs from the Sloan Digital Sky Survey. This analysis shows that the derived magnication signalis comparable to that available from conventional methods using gravitational shear. We suppress the dominant sources of systematic error and discuss modest improvements that may allow this method to equal or even surpass the signal-to-noise achievable with shear. Moreover, some of the dominant sources of systematic error are substantially different from those of shear-based techniques. The second outlines an idea for using the optical Tully-Fisher relation to dramatically improve the signal-to-noise and systematic error control for shear measurements. The expectederror properties and potential advantages of such a measurement are proposed, and a pilot study is suggested in order to test the viability of Tully-Fisher weak lensing in the context of the forthcoming generation of large spectroscopic surveys.",ucb,,https://escholarship.org/uc/item/0z22n08t,,,eng,REGULAR,0,0
64,1500,Exploring Landscapes of Naturalness with Lifshitz Field Theories,"Grosvenor, Kevin Torres","Horava, Petr;",2015,"In this thesis, we examine the question of technical naturalness from the point of view of nonrelativistic quantum field theories of Lifshitz type. Lifshitz field theories are distinguished from standard relativistic quantum field theories by the spacetime scaling symmetries that they enjoy in the vicinity of their renormalization group fixed points. These scaling symmetries are parametrized by the dynamical critical exponent, which measures the degree of scaling of time relative to spatial coordinates. Whereas in relativistic theories, space and time scale equally with each other, in Lifshitz field theories, the dynamical critical exponent may differ from unity. Furthermore, Lifshitz field theories live in spacetimes that possess a foliation structure by spatial leaves of constant time. Therefore, in contrast to relativistic theories, Lifshitz field theories are not invariant under the full diffeomorphism group of spacetime, but rather only those diffeomorphisms that preserve the built-in foliation structure. In the flat spacetime, this would simply exclude the usual spacetime boost symmetries. Since time is treated on a fundamentally different footing as is space, these theories are often referred to as anisotropic. In addition, these theories often require the tuning of multiple parameters in order to approach their fixed points under renormalization group flow, and are consequently called multicritical.We will explore some new and interesting lessons that nonrelativistic theories have to teach us about technical naturalness. We begin this study at the modest level of Lifshitz scalar field theories. We examine the nature of Nambu-Goldstone (NG) bosons that arise from spontaneous symmetry breaking in multricritical systems described by Lifshitz scalar field theories. The NG modes in such nonrelativistic theories were previously classified into two types: (1) Type-A, which disperses linearly, and which derives its kinetic energy from a term which is quadratic in time derivatives; and (2) Type-B, which disperses quadratically, and which is described by a pair of fields with kinetic terms linear in time derivatives. In principle, Type-A modes dispersing by a power different from unity, and Type-B modes dispersing by a power different from two, can exist. However, the naive expectation from relativistic quantum field theory is that these would require fine tuning and are therefore technically unnatural. We discover that this is not the case. Instead of fine-tuning, all one needs is a new type of symmetry by which the fields transform by a polynomial function of some appropriate degree in the spatial coordinates. This polynomial shift symmetry protects the naturalness of the corresponding NG bosons. This leads to a refinement of the classification of technically natural NG modes in nonrelativistic theories.Having discovered these polynomial shift symmetries, we turn our attention to the classification of Lagrangians that are invariant (up to total derivatives) with respect to these symmetries. We develop a novel graph-theoretical technique in order to address this problem. In this language, the invariants display beautiful patterns that otherwise remain obscured. For example, linear-shift invariants are presented as equal-weight sums over all labeled trees with some fixed number of vertices. Furthermore, we develop a graph-theoretical method for constructing invariants under polynomial shifts of high degree from invariants under polynomial shifts of lower degree. In this way, one no longer needs to repeat the entire classification process for each degree of the polynomial shift symmetry.The third part of the thesis uses some of the theories, which are built out of invariants constructed in the second part of the thesis, to study a novel feature that these Lifshitz theories possess as one changes the energy scale at which the systems are examined. We find that these systems can flow from one fixed point described by one value of the dynamical exponent, to another fixed point described by a different value of the dynamical exponent. Furthermore, the system can explore any number of fixed points between the extreme high energy regime and the extreme low energy regime. We refer to this behavior as a cascade. Not only can Type-A modes cascade into other Type-A modes (or similarly for Type-B modes), but Type-A modes can flow towards Type-B at low energies as well. Both mechanisms are protected by symmetries. The purely Type-A or Type-B cascade is protected by the polynomial shift symmetries in space. The Type-A to Type-B cascade can be protected by various symmetries, including a linear shift symmetry in time. Furthermore, we re-examine the Coleman-Hohenberg-Mermin-Wagner (CHMW) theorem, which prohibits the spontaneous breaking of global internal continuous symmetries in relativistic theories in two spacetime dimensions. Naively, this theorem would prevent the existence of a Type-A mode unless its dynamical exponent is strictly less than the spatial dimension, which is when the theory is in its lower critical dimension. The cascade represents a mechanism by which this result can be circumvented.Next, we examine the renormalization group flow of one particular Lifshitz scalar field theory. We perform the analysis explicitly using three different standard techniques of renormalization and show that they are all mutually consistent. Furthermore, we demonstrate that the RG flow of Lifshitz theories can be interpreted physically in many different, but consistent, ways due to the additional freedom of renormalizing the dynamical exponent.The lessons in Lifshitz field theories discussed in this thesis are most readily applied in the area of condensed matter physics, where systems often display a richer spectrum of behavior than is described by relativistic physics. We perform a preliminary study of the effects of coupling these Lifshitz theories with other systems. In particular, we study the naturalness problem of the linear dependence on temperature of the resistivity of so-called strange metals, which are high-temperature superconductors above their critical temperature. We show that this behavior is reproduced by the standard electron-phonon interaction picture of superconductivity, if the phonons are allowed to be multicritical and at their lower critical dimension. We also examine the impact that this model has on the heat capacity of the system.",ucb,,https://escholarship.org/uc/item/0073j1dx,,,eng,REGULAR,0,0
65,1501,Global Innovation Bridges: A new policy instrument to support global entrepreneurship in peripheral regions,"Martinez de Velasco Aguirre, Emilio","Chapple, Karen;",2012,"This dissertation analyzes a new set of policy instruments that several national and regional governments have recently implemented to help their home-grown innovative companies gain access to global technology markets. These initiatives, which in this dissertation are referred to as Global Innovation Bridges (GIBs), introduce a novel spatial approach to supporting global entrepreneurship in peripheral regions. Establishing a physical presence in the most dynamic regions of technological innovation around the world, and having deep ties with organizations in their home country, GIBs have effectively instituted a cross-national business support structure with the capacity to mobilize knowledge, talent, technology and capital across borders. These initiatives are based on the premise that facilitating innovative companies' access to global markets will accelerate their growth at home, generating new jobs and income. But in addition to a quantitative increase in economic activity, governments are implementing GIBs in an attempt to foster a transition towards high-growth, high value-added economic activities. Despite their potential to stimulate economic development and to foster a qualitative transformation in the economic structure of countries and regions, the literature on entrepreneurship and global entrepreneurship policies remains completely silent about GIBs. This dissertation is the first academic contribution to reveal the workings of this emerging economic development tool. The research achieves two main objectives. First, it provides an initial characterization of GIBs, describing their main features and the factors that are driving national and regional governments to implement them. Based on a multiple case-study of six GIBs with operations in Silicon Valley, California, this characterization also introduces a taxonomy that clearly differentiates GIBs from similar organizations supporting entrepreneurship. Second, it develops an in-depth analysis of the Mexican GIB, the Technology Business Accelerator (TechBA) program, in order to explain how GIBs work. This in-depth study reveals the diversity of actors supporting the mission of the TechBA program as well as the learning processes involved in turning a local company into a global player.Applying the concept of `communities of practice'  (Lave 1991; Brown and Duguid 1991; Wenger 1998; Brown and Duguid 2001) to the analysis of the TechBA program, this dissertation advances the following arguments:* The TechBA program articulates a community of practice that involves individuals in various organizations linked together by shared experience, expertise, and commitment to a joint enterprise: supporting the global expansion of Mexican companies. These are individuals whose work is related to the many technological, commercial, financial, and legal aspects of launching a new global venture. While all these individuals work for organizations that have their own agendas and goals, they all contribute in one way or another to advancing the mission of the TechBA program.* TechBA sustains a `distributed' community of practice (Hildreth et al., 2000) that transcends national borders. Through formal partnerships but primarily through informal collaborations with actors in both Mexico and in foreign markets, TechBA articulates a community of practice that operates across distant regions in different countries. The staff and individuals more closely involved in the operation of the TechBA program serve as a `brokers,' mediating among various technical and business communities in distant regions.* Supporting the global expansion of innovative companies involves a transformation in the views and practices of the entrepreneurs leading the global expansion effort as much as it involves adaptations in the strategy, structure, and organization of a firm. Parallel to the activities to support firm-level adaptations, TechBA facilitates a process of enculturation in which Mexican entrepreneurs develop the values and practices of a foreign business community. Through formal training, but primarily through numerous experience-based learning opportunities, Mexican entrepreneurs develop a new language and codes of communication, new know-how in the form of foreign business practices, new know-who or the knowledge to participate in professional networks in foreign markets, as well as new values and views in line with those of a foreign business community. * Rather than simply bridging the geographical distance to markets, the cross-national community of practice built around the TechBA program provides the social context for developing the knowledge, skills, practices, and views that are time- and context-specific and difficult to transmit over long distances. The TechBA community of practice serves as a ""living curriculum"" (Wenger 2006) in which Mexican entrepreneurs can develop a new identity and learn how to be a global entrepreneur.",ucb,,https://escholarship.org/uc/item/1f29r1dw,,,eng,REGULAR,0,0
66,1502,Generalized Arrows,"Joseph, Adam Megacz","Wawrzynek, John;",2014,"Multi-level languages and arrows both facilitate metaprogramming, the act of writing a program which generates a program. The arr function required of all arrows turns arbitrary metalanguage expressions into object language expressions; because of this, arrows may be used for metaprogramming only when the object language is a superset of the metalanguage.This thesis introduces generalized arrows, which are less restrictive than arrows in that they impose no containment relationship between the object language and metalanguage; this allows generalized arrows to be used for heterogeneous metaprogramming. This thesis also establishes a correspondence between two-level programs and one-level programs which take a generalized arrow instance as a distinguished parameter. A translation across this correspondence is possible, and is called a flattening transformation.The flattening translation is not specific to any particular object language; this means that it needs to be implemented only once for a given metalanguage compiler. Support for various object languages can then be added by implementing instances of the generalized arrow type class; this does not require knowledge of compiler internals. Because of the flattening transformation the users of these object languages are able to program using convenient multi-level types and syntax; the conversion to one-level terms manipulating generalized arrow instances is handled by the flattening transformation.A modified version of the Glasgow Haskell Compiler (GHC) with multi-level types and expressions has been produced as a proof of concept. The Haskell extraction of the Coq formalization in this thesis have been compiled into this modified GHC as a new flattening pass.",ucb,,https://escholarship.org/uc/item/1gg3m11q,,,eng,REGULAR,0,0
67,1503,Essays in China's Anti-corruption Campaign,"Lu, Xi","Wright, Brian;",2017,"China's unique system of hiring and promoting talented people within the state, under the supervision of the Communist Party, has been held up as an important institutional factor supporting its remarkably rapid and sustained economic growth. Jointly with Professor Peter L. Lorentzen, we explore this meritocracy argument in the context of Chinese leader Xi Jinping's ongoing anti-corruption campaign. Some question the sincerity of the campaign, arguing that it is nothing but a cover for intra-elite struggle and a purge of Xi's opponents. In the first chapter of my thesis, we use a dataset I have created to identify accused officials and map their connections. Our evidence supports the Party's claim that the crackdown is primarily a sincere effort to cut down on the widespread corruption that was undermining its efforts to develop an effective meritocratic governing system. First, we visualize the ""patron-client'' network of all probed officials announced by the central government and identify the core targets of the anti-corruption campaign. Second, we use a recursive selection model to analyze who the campaign has targeted, providing evidence that even personal ties to top leaders have provided little protection. Finally, we show that, in the years leading up to the crackdown, the provinces later targeted had departed from the growth-oriented meritocratic selection procedures evident in other provinces. 	In addition to its motivation, I also discuss the campaign's effects on economic efficiency. The second chapter of my thesis tests the ""greasing-the-wheels'' hypothesis in the context of China's residential land market. We show that China's anti-corruption campaign, aimed at removing corruption in China's monopoly land market, caused a decrease in land transaction volumes. Furthermore, not removing any form of corruption would also lead to a similar decrease. It is only necessary to remove corruption that enables real estate developers to circumvent red tape and reduce trading costs. Our findings support the ""greasing-the-wheels'' hypothesis hypothesis: when an economy has a low outcome owing to some preexisting distortions, corruption could be a positive factor in that it offers a ""second-best world.''",ucb,,https://escholarship.org/uc/item/16g527r2,,,eng,REGULAR,0,0
68,1504,Efficient Multi-Level Modeling and Monitoring of End-use Energy Profile in Commercial Buildings,"Kang, Zhaoyi","Spanos, Costas J;",2015,"In this work, modeling and monitoring of end-use power consumption in commercial buildings are investigated through both Top-Down and Bottom-Up approaches. In the Top-Down approach, an adaptive support vector regression (ASVR) model is developed to accommodate the nonlinearity and nonstationarity of the macro-level time series, thus providing a framework for the modeling and diagnosis of end-use power consumption. In the Bottom-Up approach, an appliance-data-driven stochastic model is built to predict each end-use sector of a commercial building. Power disaggregation is studied as a technique to facilitate Bottom-Up prediction. In Bottom-Up monitoring and diagnostic detection, a new dimensionality reduction technique is explored to facilitate the analysis of multivariate binary behavioral signals in building end-uses.",ucb,,https://escholarship.org/uc/item/1867c6vm,,,eng,REGULAR,0,0
69,1505,Nations of Retailers: The Comparative Political Economy of Retail Trade,"Watson, Bartholomew Clark","Levy, Jonah;",2011,"This dissertation analyzes the development of the retail sector in the United States and Western Europe.  The predominant literature on the service sector in both economics and political science has argued that the only way to create jobs in services such as retailing is through the low-wage, high-inequality route epitomized by the United States.  Nevertheless, in the retail trade sector, a number of European countries have matched or exceeded American productivity and employment growth, despite considerably higher wage levels.  How do we explain the puzzling combination of rapid job growth and high wages in European retail?This dissertation resolves this puzzle through a cross-national comparison of the retailing strategy in three countries: the United States, Denmark, and France.  It identifies three modes of competition in retail, labeled ""lean retailing"" (US), ""relational contracting"" (Denmark), and ""vertical integration"" (France).  It shows that each approach has strengths and limitations, with none inherently more successful than the others.The first model, American lean retailing, is a cost-squeezing strategy built around scale, turnover, and low margins.  It uses dominating relationships with suppliers and workers to strip costs and retailer control over logistics to improve efficiency.  The second model, Danish relational contracting, illustrates that more collaborative relationships can produce equally efficient retailing outcomes.  Danish retailers work with workers and suppliers, finding ways to share and reduce long-term costs through worker training, improved productivity, and reduced costs from confrontation.  Finally, a third model, French vertical integration, seeks to add and capture as much value as possible throughout the distribution supply chain.  French retailers have built their own partners and brands, developing services and private label products that allow them to add value.Most explanations of retail business strategies emphasize either the imperatives of the technology available to retailers or the vicissitudes of consumer preferences or markets.  This dissertation argues, by contrast, that contemporary retailer strategies are rooted in a series of political battles fought in the 1960s.  The key to this political explanation is the embedded nature of the retail sector.  Retailers are amongst the most connected actors in political economy, with ties to numerous economic and political players, including consumers, suppliers, workers, and both local and national governments.  These connections were activated in the 1960s as a new crop of large-scale retail entrants began shifting the retail sector from shopkeepers to supermarkets.The political coalitions that emerged were a function of national structural factors, notably the power resources of the retail sector, electoral institutions, avenues of interest aggregation, and the economic organization of small shops.  These coalitions set in motion longer economic trajectories of firm management and policymaking.  Where retailers could defend their interests unilaterally, without coalition partners (US), the lean retail model took hold; where retailers forged multiple coalitions with other stakeholders (Denmark), the relational contracting model developed; and where coalitions were partial and unstable (France), vertical integration predominated.In the United States, the fragmented and decentralized political environment meant that the opposition to retailers was local and diffuse.  Consequently retailers were able to counter with unilateral political action.  This unilateral political approach set the stage for a confrontational, and ultimately dominating   lean retail strategy that uses market power and digital information as a club against suppliers and workers.In Denmark, weak retailers confronted powerful small shops, producers, and workers.  Needing support, retailers reached a broad compromise involving all of these groups that divided the gains from consumer distribution in a more stable, long-term and equitable fashion.  Under this relational contracting approach, retailers were forced to work with partners, but eventually found numerous benefits from these alliances, and have continued their cooperation long after the initial political crisis has past.  Policymaking has also continued its history of concertation, and new challenges are still tackled jointly by a broad coalition of groups.Finally, in France, retailers faced vocal and national, but disjointed opposition movements of shopkeepers and workers.  Retailers responded by forging uneasy, partial alliances, but these were constantly contested.  In this unstable political environment, retailers pursued a vertical integration strategy, seeking to maximize value by controlling and internalizing much of the production process.Analysts of the retail sector suggest that new developments, such as trans-national retailing and e-commerce (sales over the Internet) are undermining national models of retailing.  This dissertation shows, however, that retailers are largely integrating these new developments into their existing, nationally distinctive business strategies.  In the United States, retailers are using market power and fragmented politics to dominate where possible.  In Denmark, numerous partners are working collaboratively to share the gains of new opportunities.  Finally, in France, national political fights over digital commerce are reopening unresolved policy questions from the pre-digital era.  Like the previous set of political challenges facing retailers, therefore, national institutions and politics continue to mediate economic transformations and drive divergence in firm strategy, competitive advantage, and national variation in secondary outcomes such as wage equality, price levels, and patterns of technology implementation.",ucb,,https://escholarship.org/uc/item/18z1138t,,,eng,REGULAR,0,0
70,1506,"Inheritance and Inflectional Morphology: Old High German, Latin, Early New High German, and Koine Greek","LeBlanc, MaryEllen","Rauch, Irmengard;",2014,"The inheritance framework originates in the field of artificial intelligence.  It was incorporated first into theories of computational linguistics, and in the last two decades, it has been applied to theoretical linguistics.  Inheritance refers to the sharing of properties: when a group of items have a common property, each item is said to inherit this property.  The properties may be mapped in tree format with nodes arranged vertically.  The most general (i.e. the most widely shared, unmarked) properties are found at the highest nodes, and the most specific (marked) information is found at the lowest nodes.Inheritance is particularly useful when applied to inflectional morphology due to its focus on the generalizations within and across paradigms.  As such, it serves as an alternative to traditional paradigms, which may simplify the translation process; and provides a visual representation of the structure of the language's morphology.  Such a mapping also enables cross-linguistic morphological comparison. In this dissertation, I apply the inheritance framework to the nominal inflectional morphology of Old High German, Latin, Early New High German, and Koine Greek.  The corpus consists of parallel biblical passages in each language which will serve as the basis for comparison.  The trees may be used as a translation aid to those reading these texts as an accompaniment to or substitute for traditional paradigms.  Moreover, I aim to shed light on the structural similarities and differences between the four languages by means of the inheritance trees.",ucb,,https://escholarship.org/uc/item/19n1x8hk,,,eng,REGULAR,0,0
71,1507,An Integrative Approach to Data-Driven Monitoring and Control of Electric Distribution Networks,"Dobbe, Roel Ignatius Jacobus","Tomlin, Claire J.;Callaway, Duncan;",2018,"The commodification of computing, sensors, actuators, data storage and algorithms has unleashed a new wave of automation throughout society. Motivated by the promise of new capabilities, quality improvements, or efficiency gains, data-driven technologies have captured the attention and imagination of the public and many domain experts. Though opportunities are ample, the rapid introduction of data-driven functionality also triggers well-founded concerns about safeguarding critical values, such as safety, privacy and justice.In the context of operating electric distribution networks, the need for data-driven monitoring and control is explained by the irreversible transition from fossil to renewable generation and the accompanied electrification of our economy in areas like transportation and heating.The traditional fit-and-forget paradigm of designing networks conservatively for the projected peak loads assumed unidirectional power flow, predictable future demand and monotonic voltage drops, and allowed for operating at near-100\% reliability with minimal requirement for sensing and actuation. The intermittent nature of Distributed Generation (DG), its ability to feed power back to the grid and cause bidirectional power flow, and the diversifying and nonlinear behavior of electric loads are all eating away at the robustness of this approach, causing Distribution System Operators (DSOs) to put caps on the allowable DG and revisit their design and operating practice. Rather than making traditional expensive network reinforcements in often aging physical infrastructures, DSOs are trying to increase the observability and controllability of their networks by leveraging new sensing and actuation technologies and exploring the ability to use data-driven algorithms to help with the integration of more DG in a more distributed (in space and time) and cost-effective way. This dissertation works towards this vision by formulating a systematic control-theoretic approach for integrating data-driven monitoring and control in the operation of electric distribution networks. Firstly, a Bayesian approach to state estimation overcomes the constraint of limited available real-time sensors by integrating voltage forecasting. A second class of tools discussed is the use of machine learning to decentralize Optimal Power Flow (OPF) methods, by utilizing inverter-interfaced Distributed Energy Resources (DERs). The Decentralized OPF method lets each DER learn a policy that contributes to network objectives from its local historical data and measurements alone. This approach is formulated as a compression and reconstruction problem through an information-theoretic lens, providing fundamental limits of reconstruction and a strategy for optimal communication to improve learning-based reconstruction of optimal policies throughout a network.Lastly, the ambition to control networks in a distributed fashion triggers concerns about privacy-sensitive information that may be inferred from an agent's shared data. For a general class of algorithms, a new notion of local differential privacy is integrated that allows each agent to customize the protection of local information captured in constraints and objective functions.The ultimate goal of the work presented in this dissertation is to contribute to a framework for the integral and value-sensitive design and implementation of data-driven methodologies in critical infrastructure. To address the inherent cross-disciplinary nature of this larger goal, the final chapter explains how each automated decision-making tool reflects and affects values important to its stakeholders. The chapter argues that in order to enable beneficial integration of such tools, practitioners need to reflect on their epistemology and situate the design of automated decision-making in its inherently dynamic and human context.",ucb,,https://escholarship.org/uc/item/1bw112wx,,,eng,REGULAR,0,0
72,1508,"Fine-Grained Soil Liquefaction Effects in Christchurch, New Zealand","Beyzaei, Christine Zahra","Bray, Jonathan D;",2017,"Liquefaction damage from the 2010-2011 Canterbury earthquake sequence devastated parts of Christchurch, New Zealand. There were many sites where state-of-practice liquefaction assessment procedures indicated liquefaction would be expected to occur, and surface manifestations of liquefaction were observed. However, there were also numerous sites, which were predominantly silty soil sites, where state-of-practice liquefaction assessment procedures indicated that liquefaction would be expected to occur, but no surface manifestations of liquefaction were observed. This discrepancy between state-of-practice liquefaction assessments and post-earthquake liquefaction observations led to the development of the research program presented in this dissertation. Several silty soil sites were selected for investigation to further our understanding of fine-grained soil liquefaction response and to evaluate potential limitations in the current state-of-practice liquefaction assessment procedures, which are based primarily on case histories and laboratory testing of sands. This dissertation investigates the liquefaction response of silty soil sites through no-liquefaction case histories from the Canterbury earthquake sequence, evaluating depositional environment effects on observed liquefaction performance, site characterization of silty soil deposits, and laboratory testing to characterize element-scale cyclic response.       Depositional environment effects are evaluated through regional CPT-based analyses and site-specific comparisons. Stratified silty soil swamp deposits are shown to have mitigating effects on the manifestation of liquefaction beyond what can be captured by simplified liquefaction assessment procedures in Christchurch. Differing surficial geology and depositional environments are found through examining historical documents to explain in part the limitations of current liquefaction assessment procedures in the swamps of southwest Christchurch, which contain stratified silt/sand deposits or thick silt layers. Consideration of depositional environment distinguishes between liquefaction performances that could not be differentiated through the CPT-based assessment alone. CPT resolution is not sufficient to capture the thin layering at these stratified sites, and the simplified liquefaction assessment methods do not take into account the effects of the stratification on pore water pressure movement within a soil profile. Continuous sampling and careful logging of high-quality samples provides important insights on in-situ stratification at these silty soil swamp sites, discerning differences in stratigraphy resulting from differences in depositional environment.       Site investigation techniques are evaluated at the silty soil case history sites to discern their capability to characterize thin layers and groundwater table fluctuation, two potential causes for the discrepancies between state-of-practice liquefaction assessments and post-earthquake liquefaction observations. CPT, mini-CPT, sonic borings, and high-quality sampling are critiqued in terms of their ability to capture thin layer stratigraphy, which is of importance for liquefaction assessment. Piezometers, sonic borings, high-quality sampling, crosshole testing, and regional groundwater maps are evaluated to assess their ability to capture groundwater table fluctuation. CPT, mini-CPT, and conventional sonic borings offer important information for site characterization, but they do not capture full details of thin layering at silty soil sites. Detailed logging of high-quality samples captures the actual in-situ layering that helps explain limitations of simplified liquefaction assessment procedures. Use of multiple groundwater measurement methods more fully illuminate fluctuating groundwater conditions. Subsurface investigation programs should utilize tools that characterize features impacting liquefaction potential in adequate detail for the intended engineering purpose. Use of multiple, complementary investigation techniques provides the most robust assessment.       A field investigation and advanced laboratory testing program was conducted in Christchurch. High-quality samples were obtained using a Dames & Moore hydraulic fixed-piston thin-walled sampler for cyclic triaxial testing to characterize the liquefaction response of silty soils at the no-liquefaction sites in southwest Christchurch. These natural silty soil specimens contained heterogeneity and variability that should be considered and is difficult, if not impossible, to replicated with laboratory-prepared specimens. Test results for stress-strain response and axial strain accumulation indicate a nuanced range of transitional responses for these intermediate soils. Post-liquefaction reconsolidation testing shows clear differences in specimen response, ranging from ""sand-like"" immediate reconsolidation to time-dependent reconsolidation. Simplified liquefaction assessment procedures estimate significant liquefaction at these case history sites and yet no liquefaction manifestations were observed during the Canterbury earthquake sequence. Laboratory estimates of cyclic resistance (CRR) are consistent with estimates from the simplified procedures, and both estimates of CRR are well below simplified procedure estimates of seismic demand (CSR). Depositional characteristics such as thin-layering of fine sand and silt may be why manifestations of liquefaction were not observed at these sites. Post-liquefaction reconsolidation testing provides insight that water and ejecta may not accumulate in these stratified silty soils as they would accumulate in thick deposits of liquefiable clean sands. Additional mitigating factors may also contribute to the discrepancy between simplified procedure estimates of liquefaction and the lack of liquefaction observed at these sites. The interaction of several factors contributing to observed liquefaction response at these silty soil sites indicates that in-situ “system” response should be considered and that further research on silty soils is warranted.",ucb,,https://escholarship.org/uc/item/0s06z6gh,,,eng,REGULAR,0,0
73,1509,Multiexponential T2 and Diffusion Magnetic Resonance Measurements of Glioma Cells,"Jackson, Pamela","McKnight, Tracy R;",2012,"Using monoexponential models, magnetic resonance (MR) parameters transverse relaxation time (T2) and apparent diffusion coefficient (ADC) have been used to non–invasively assess cell density, which characteristically increases with brain tumor malignancy. Multiexponential models of T2 and ADC may allow for a more accurate evaluation of cell density than the traditionally used monoexponential model. To better understand how cell density affects multicomponent T2 and ADC, tumor models with a range of cell densities were created by suspending astrocytoma cells in agarose at different densities. T2 was measured using a Carr–Purcell–Meiboom–Gill (CPMG) sequence with 64 echo times. ADC was measured using a diffusion–weighted sequence with 32 b–values. Three models were used to fit the data and determine the T2 values, ADC values, and associated fractions: monoexponential, biexponential, non–negative least squares (NNLS). Spearman Rank was used to test the correlation with cell density. Both the monoexponential T2 and ADC were significantly negatively correlated with cell density. The biexponential model identified two T2 components, short and long. Both T2 components’ values and fractions were significantly correlated to cell density. The biexponential model identified two ADC components, slow and fast. Both ADC components’ values were significantly correlated with cell density, but not the fractions. The NNLS model identified up to three T2 and three ADC components. For the NNLS identified T2s, the components were labeled short, medium, and long. Both the NNLS short and medium T2 values and fractions were significantly correlated with cell density. For the NNLS identified ADCs, the components were labeled slow, intermediate, and fast. Only the fast fraction was significantly correlated with cell density.The NNLS components obtained from samples of packed cells were further evaluated by washing the samples with a gadolinium contrast agent (Gd–DTPA) to shorten the T2 associated with the extracellular space. Gd–DTPA did not affect the short T2, which was considered to be associated with the intracellular space. The medium T2 component was no longer identified in samples with Gd–DTPA and was considered to be associated with the extracellular space. Overall, the results suggest that separately measurable components exist that could be utilized for compartment specific information.",ucb,,https://escholarship.org/uc/item/00g6d7qg,,,eng,REGULAR,0,0
74,1510,Microfluidic Microbial Fuel Cells for Microstructure Interrogations,"Parra, Erika Andrea","Lin, Liwei;",2010,"The breakdown of organic substances to retrieve energy is a naturally occurring process in nature.  Catabolic microorganisms contain enzymes capable of accelerating the disintegration of simple sugars and alcohols to produce separated charge in the form of electrons and protons as byproducts that can be harvested extracellularly through an electrochemical cell to produce electrical energy directly.  Bioelectrochemical energy is then an appealing green alternative to other power sources. However, a number of fundamental questions must be addressed if the technology is to become economically feasible. Power densities are low, hence the electron flow through the system: bacteria-electrode connectivity, the volumetric limit of catalyst loading, and the rate-limiting step in the system must be understood and optimized.  This project investigated the miniaturization of microbial fuel cells to explore the scaling of the biocatalysis and generate a platform to study fundamental microstructure effects.  Ultra-micro-electrodes for single cell studies were developed within a microfluidic configuration to quantify these issues and provide insight on the output capacity of microbial fuel cells as well as commercial feasibility as power sources for electronic devices.Several devices were investigated in this work. The first prototype consisted of a gold array anode on a silicon dioxide passivation layer that intended to imitate yet simplify the complexity of a 3D carbon structure on a 2D plane. Using Geobacter sulfurreducens, an organism believed to utilize direct electron transfer to electrodes, the 1 mm2 electrode demonstrated a maximum current density of 1.4 μA and 120 nW of power after 10 days. In addition, the transient current-voltage responses were analyzed over the bacterial colonization period. The results indicated that over a 6-day period, the bacteria increased the capacitance of the cell 5-orders-of-magnitude and decreased the resistance by 3X over the bare electrode. Furthermore, over short experimental scales (hours), the RC constant was maintained but capacitance and resistance were inversely related.  As the capacitance result coincides with expected biomass increase over the incubation period, it may be possible for an electrical spectroscopy (impedance) non-invasive technique to be developed to estimate biomass on the electrode. Similarly, the R and C relationship over short experimental scales could be explored further to provide insight on biolm morphology. Lastly, fluorescence and SEM microscopy were used to observe the biofilm development and demonstrated that, rather than growing at even density, the bacteria nucleated at points on the electrode, and dendritically divided, until joining to form the ""dense"" biofilm. In addition, viable microorganisms undergoing cell division were found dozens of microns from electrode surfaces without visible pili connections.To investigate single-cell catalysis or microstructure effects, a sub-micro-liter microfluidic single-channel MFC with an embedded reference electrode and solid-state nal electron acceptor was developed. The system allowed for parallel (16) working ultra-micro-electrodes and was microscopy compatible. With Geobacter sulfurreducens, the semiconducting ITO electrodes demonstrated forward bias behavior and suitability for anodic characterization.  The first prototype demonstrated, with 179 cells on the electrode, a per cell contribution of 223 fA at +400 mV (vs. SHE). The second prototype with a 7 μm diameter electrode produced a current density of 3.9 pA/μm2 (3.9 A/m2) at +200 mV (vs. SHE) and a signal-to-noise ratio (SNR) of 4.9 when inoculated at a seeding density of 109 cells/mL. However, diluting the sample by 10x produced an SNR of 0.5, suggesting that obtaining single cell electron transfer rates to an electrode over short experimental time scales may not be possible with the system as tested.  Nevertheless, the platform allows microstructure characterization and multiplexing within a single microfluidic chamber.",ucb,,https://escholarship.org/uc/item/04r1g2xs,,,eng,REGULAR,0,0
75,1511,Essays in Behavioral Economics and Environmental Policy,"Sexton, Steven E.","Zilberman, David;",2012,"Social planners have long relied upon non-coercive interventions in order to achieve social welfare improvements that are not obtained by markets or direct policy. Such policies are perhaps nowhere more relevant and common than in environmental economics. Environmental goods and services are typically not traded in markets because of the difficulties of property rights assignment. And yet efforts to create markets or correct market failures by coercive policy are fraught with controversy. Thus, in addition to coercive mechanisms, social planners use information provision campaigns, appeals for cooperation, and ""nudges"" to improve the efficiency of environmental resource allocations. Non-coercive interventions have grown in popularity among social planners as behavioral economics has gained acceptance within the mainstream of the field. Indeed, such policies typically affect market outcomes and achieve environmental goals only insofar as they can exploit or correct decision making that deviates from standard theory.In this dissertation, agent behavior is analyzed to assess the potential of non-coercive interventions to achieve socially preferred environmental outcomes. In a first essay, the concept of conspicuous conservation is introduced as a modern variant of conspicuous consumption that affords status for displays of austerity meant to signal environmental preferences rather than displays of ostentation meant to signal wealth. I identify conspicuous conservation in the automobile market and estimate a willingness to pay up to several thousand dollars for the ""green"" signal transmitted by ownership of the Toyota Prius. In a second essay, I demonstrate how automatic bill payment programs can induce excessive consumption of goods and services by boundedly rational consumers who exhibit inattention to prices. As automatic payment programs have spread throughout industries characterized by recurring payments, from utility and telecommunication services to insurance and loan markets, this essay is the first to consider their implications for consumer demand and welfare. It is also the first to test empirically whether enrollment in such programs increases demand, as price salience theory suggests. It is shown that residential electricity consumption increases on average 2-4.5% due to enrollment in automatic payment programs, while commercial electricity consumption grows much as 6%. Moreover, bill-smoothing programs that utilities offer to low-income households are shown to induce an 8-9% increase in electricity consumption.A final essay examines the extent to which free transit fares and appeals for car-trip avoidance reduce car pollution on smoggy days. With data on freeway traffic volumes and transit ridership, public appeals for cooperation are shown to have no significant effect on car trip demand. Free transit fares, however, do have a significant effect on car trip demand. But the effect is perverse in that it generates an increase in car trips and related pollution. Free fares also increase transit ridership. These results suggest that free transit rides do not induce motorists to substitute to transit, but instead subsidize regular transit rides and additional trips. Appeals for cooperation also have no affect on carpooling behavior.Viewed in their totality, these essays communicate the importance of behavioral theories in formulating environmental policies and predicting agents' responses to such policies. Policies formulated without due regard for agents' bounded rationality and multifaceted motivations are doomed to unintended consequeces. However, recognition of these behavioral responses and their incorporation in policy design can result in improved environmental outcomes and efficient policies.",ucb,,https://escholarship.org/uc/item/04x0f3cc,,,eng,REGULAR,0,0
76,1512,The Organizational Weapon: Ruling Parties in Authoritarian Regimes,"Meng, Anne","Arriola, Leonardo;Powell, Robert;",2016,"This project examines party building in authoritarian regimes. The overarching puzzle I seek to address is: why are some autocratic ruling parties stronger organizations than others? What explains variation in the institutional capacity of autocratic rule? The collection of three essays in this dissertation outline the strategic logic of party institutionalization, in addition to providing new and original ways in which to measure this key concept of authoritarian party strength. It tests previously untested hypotheses about the origins of strong autocratic parties and provides insights on the conditions under which leaders will be incentivized to rule through binding institutions. The first paper conceptualizes autocratic party strength as institutionalization and provides new ways of measuring this variable. The second paper describes the strategic logic of party institutionalization in autocracies and explains why and when some autocrats choose to tie their own hands. The third paper evaluates the thesis that parties emerging out of revolutions and independence wars tend to be more durable by examining parties that emerged out of independence struggles in Africa.",ucb,,https://escholarship.org/uc/item/04x1698k,,,eng,REGULAR,0,0
77,1513,Exploring Competing Orders in the High-Tc Cuprate Phase Diagram Using Angle Resolved Photoemission Spectroscopy,"Garcia, Daniel Robert","Lanzara, Alessandra;",2010,"With more than a quarter century of study, the high temperature superconducting cuprates still represent one of the most active areas of research in condensed matter physics. Its complex phase diagram  continues to present challenges to our understanding, stemming from its correlated electronic nature. Being able to tease out the effect of different lattice orderings and their effects on electronic states may be crucial to understanding the physics of the cuprates where such  orderings may be crucial to the phase diagram. Thus, because of its ability to directly probe electronic band structure, Angle Resolved  Photoemission Spectroscopy (ARPES) is an ideal probe to study the effects of competing orders on electronicstates near EF.  This thesis will be organized in the following way. Chapter 1 provides a broad introduction to the physics central to our work including concepts of band structure and Fermi liquid theory, as well as more exotic phenomena explored throughout the thesis. Chapter 2 introduces the ARPES technique, how it is physically understood via concepts like Green's functions, and traditional methods of data analysis. Chapter 3 explores magnetic ordering and its effect on both core level and valance band states in the iron oxypnictides. From the near-EF electronic states, we find that the magnetic physics of the parent compound may still be present even at superconducting dopings. Chapter 4 explores charge density wave (CDW) ordering by looking at the rare earth ditellurides. This ARPES work establishes LaTe2 as the first quasi-2D CDW system to behave like a true Peierls transition, with both Fermi surface nesting tied to a metal - to - insulating transition. Chapter 5 explores the effectof lattice strain on electronic states by studying the single layered Bi2201 cuprates with lanthanide substitution. The effect of this substitution competes with superconductivity and appears to enhance bosonic modes acting on the nodal point states which are otherwise unaffected. Chapter 6 takes the specific case of Nd-Bi2201 and finds evidence of a distinct crossover point in the electronic states near EF segregating the nodal point states. Finally, Chapter 7 provides a summary of our work and its conclusions.",ucb,,https://escholarship.org/uc/item/04x3m7kc,,,eng,REGULAR,0,0
78,1514,Ion Transport and Structure in Polymer Electrolytes with Applications in Lithium Batteries,"Chintapalli, Mahati","Balsara, Nitash P;Minor, Andrew M;",2016,"When mixed with lithium salts, polymers that contain more than one chemical group, such as block copolymers and endgroup-functionalized polymers, are promising electrolyte materials for next-generation lithium batteries.  One chemical group can provide good ion solvation and transport properties, while the other chemical group can provide secondary properties that improve the performance characteristics of the battery.  Secondary properties of interest include non-flammability for safer lithium ion batteries and high mechanical modulus for dendrite resistance in high energy density lithium metal batteries.  Block copolymers and other materials with multiple chemical groups tend to exhibit nanoscale heterogeneity and can undergo microphase separation, which impacts the ion transport properties.  In block copolymers that microphase separate, ordered self-assembled structures occur on longer length scales.  Understanding the interplay between structure at different length scales, salt concentration, and ion transport is important for improving the performance of multifunctional polymer electrolytes.  In this dissertation, two electrolyte materials are characterized: mixtures of endgroup-functionalized, short chain perfluoropolyethers (PFPEs) and lithium bis(trifluoromethanesulfonyl) imide (LiTFSI) salt, and mixtures of polystyrene-block-poly(ethylene oxide) (PS-b-PEO; SEO) and LiTFSI.  The PFPE/LiTFSI electrolytes are liquids in which the PFPE backbone provides non-flammability, and the endgroups resemble small molecules that solvate ions.  In these electrolytes, the ion transport properties and nanoscale heterogeneity (length scale ~1 nm) are characterized as a function of endgroup using electrochemical techniques, nuclear magnetic resonance spectroscopy, and wide angle X-ray scattering.  Endgroups, especially those containing PEO segments, have a large impact on ionic conductivity, in part because the salt distribution is not homogenous; we find that salt partitions preferentially into the endgroup-rich regions.  On the other hand, the SEO/LiTFSI electrolytes are fully microphase-separated, solid, lamellar materials in which the PS block provides mechanical rigidity and the PEO block solvates the ions.  In these electrolytes longer length scale structure (~10 nm – 1 μm) influences ion transport.  We study the relationships between the lamellar grain size, salt concentration, and ionic conductivity using ac impedance spectroscopy, small angle X-ray scattering, electron microscopy, and finite element simulations.  In experiments, decreasing grain size is found to correlate with increasing salt concentration and increasing ionic conductivity.   Studies on both of these polymer electrolytes illustrate that structure and ion transport are closely linked.",ucb,,https://escholarship.org/uc/item/04z7w363,,,eng,REGULAR,0,0
79,1515,Essays in Applied Microeconomics,"Severnini, Edson Roberto","Card, David;",2013,"This dissertation consists of three studies analyzing causes and consequences of location decisions by economic agents in the U.S. In Chapter 1, I address the longstanding question of the extent to which the geographic clustering of economic activity may be attributable to agglomeration spillovers as opposed to natural advantages. I present evidence on this question using data on the long-run effects of large scale hydroelectric dams built in the U.S. over the 20th century, obtained through a unique comparison between counties with or without dams but with similar hydropower potential. Until mid-century, the availability of cheap local power from hydroelectric dams conveyed an important advantage that attracted industry and population. By the 1950s, however, these advantages were attenuated by improvements in the efficiency of thermal power generation and the advent of high tension transmission lines. Using a novel combination of synthetic control methods and event-study techniques, I show that, on average, dams built before 1950 had substantial short run effects on local population and employment growth, whereas those built after 1950 had no such effects. Moreover, the impact of pre-1950 dams persisted and continued to grow after the advantages of cheap local hydroelectricity were attenuated, suggesting the presence of important agglomeration spillovers. Over a 50 year horizon, I estimate that at least one half of the long run effect of pre-1950 dams is due to spillovers. The estimated short and long run effects are highly robust to alternative procedures for selecting synthetic controls, to controls for confounding factors such as proximity to transportation networks, and to alternative sample restrictions, such as dropping dams built by the Tennessee Valley Authority or removing control counties with environmental regulations. I also find small local agglomeration effects from smaller dam projects, and small spillovers to nearby locations from large dams. Lastly, I find relatively small costs of environmental regulations associated with hydroelectric licensing rules. In Chapter 2, I study the joint choice of spouse and location made by individuals at the start of their adult lives. I assume that potential spouses meet in a marriage market and decide who to marry and where they will live, taking account of varying economic opportunities in different locations and inherent preferences for living near the families of both spouses. I develop a theoretical framework that incorporates a collective model of household allocation, conditional on the choice of spouse and location, with a forward-looking model of the marriage market that allows for the potential inability of spouses to commit to a particular intra-household sharing rule. I address the issue of unobserved heterogeneity in the tastes of husbands and wives using a control-function approach that assumes there is a one-to-one mapping between unobserved preferences of the two spouses and their labor supply choices. Estimation results for young dual-career households in the 2000 Census lead to three main findings. First, I find excess sensitivity of the sharing rule that governs the allocation of resources among couples to the conditions in the location they actually choose, implying that spouses cannot fully commit to a sharing rule. Second, I show that the lack of commitment has a relatively larger effect on the share of family resources received by women. Third, I find that the failure of full commitment can explain nearly all of the gap in the interstate migration rates of single and married people in the U.S. Finally, in Chapter 3, I examine unintended consequences of environmental regulations affecting the location of power plants. I present evidence that while hydroelectric licensing rules do conserve the wilderness and the wildlife by restricting the development of hydro projects in some counties, they lead to more greenhouse gas emissions in those same locations. Such environmental regulations aimed to preserve natural ecosystems do not seem to really protect nature. Basically, land conservation regulations give rise to a replacement of hydropower, which is a renewable, non-emitting source of energy, with conventional fossil-fuel power, which is highly pollutant. Restrictions imposed by hydroelectric licensing rules might be used as leverage by electric utilities to get permits to expand thermal power generation. Each megawatt of hydropower potential that is not developed because of those regulations induces the production of the average emissions of carbon dioxide per megawatt of U.S. coal-fired power plants. Environmental regulations focusing only on the preservation of ecosystems appears to stimulate dirty substitutions within electric utilities regarding electricity generation.",ucb,,https://escholarship.org/uc/item/0554c2gv,,,eng,REGULAR,0,0
80,1516,The Development and Evolution of Floral Symmetry in the Zingiberales and Interactive Tools for Teaching Evolution (ArborEd),"Bruenn, Riva Anne","Specht, Chelsea D;",2017,"AbstractThe Development and Evolution of Floral Symmetry in the Zingiberales and Interactive Tools for Teaching Evolution (ArborEd)byRiva Anne BruennDoctor of Philosophy in Plant BiologyUniversity of California, BerkeleyProfessor Chelsea D. Specht, ChairFloral symmetry is a key innovation in the evolution of flowering plants. Zygomorphy, or single-planed symmetry, is associated with the diversification of many flowering plant lineages. The model system for floral symmetry is the snapdragon (Antirrhinum majus). In A. majus flowers, a set of TCP and MYB-related transcription factors form a core gene regulatory network necessary for zygomorphy. The genes involved in this network have been implicated in several independent transitions to zygomorphy from actinomorphy (many-planed symmetry). Although the TCP components of the symmetry network have been investigated across flowering plants, MYB-related transcription factors remain largely unstudied outside of the Asterid group containing A. majus and close relatives. Here we investigate the evolution of MYB-related genes DIVARICATA-like (DIV-like), RADIALIS-like (RAD-like), and DIVARICATA and RADIALIS INTERACTING FACTOR-like (DRIF-like) across flowering plants, and their expression patterns in the developing flowers of two zygomorphic species of the monocot order Zingiberales. We found that RAD-like and DIV-like are sister MYB-related genes which diverged before the diversification of flowering plants. Each gene contains one MYB-like domain that has been closely conserved throughout flowering plant evolution. Furthermore, we identified candidate homologs to A. majus RAD and DIV in several monocot taxa, with at least three copies of each in the Zingiberales. In the Zingiberales, RAD-like and DIV-like genes are expressed in Costus spicatus (Costaceae) and Musa basjoo (Musaceae) in patterns consistent with roles in floral symmetry. Using Reverse Transcription PCR and in situ hybridization we recovered asymmetric expression patterns for some RAD-like genes across the dorsal/ventral plane of developing flowers, and universal expression of DIV-like genes, consistent with the model known from Antirrhinum majus. We identified DRIF-like genes across flowering plants, recovering a previously undescribed duplication in eudicot DRIF Group 1 genes. Furthermore, we recovered candidate DRIF-like genes in Musa basjoo (Musaceae: Zingiberales) with expression patterns similar to those described in A. majus DRIF1 and DRIF2. Finally, we developed a tutorial for high school and college students to investigate a coevolutionary hypothesis in sharpshooters and their bacterial endosymbionts. This tool will help students understand how comparative evolutionary research is performed, and give them hands-on experience performing common analyses.",ucb,,https://escholarship.org/uc/item/01x4k6vh,,,eng,REGULAR,0,0
81,1517,"From Subjects to Citizens: American Colonial Education and Philippine Nation-Making, 1900-1934","Francisco, Adrianne","Candida Smith, Richard;",2015,"This dissertation examines the U.S. colonial state's efforts to promote Filipino national sentiment and patriotism through the public school system between 1900 and 1934. During the early years of American rule, U.S. colonial officials argued that Filipinos lacked a sense of nationality due to their linguistic and religious diversity, cultural heterogeneity, and regionalism. This perception shaped U.S. educational policy in the Philippines, leading to the creation of a curriculum that would attempt to homogenize and foster national affiliation among Filipinos. Using administrator files, Bureau of Education records, textbooks, and curricular materials collected in both the United States and the Philippines, this study reconstructs the colonial curriculum, paying special attention to English language instruction, history and civics, and vocational education. It shows that colonial education aimed to quell Filipino anti-colonial nationalism and facilitate obedience to the colonial state by casting good citizenship and “proper” patriotism in terms of economic self-sufficiency and non-violence, and by defining national allegiance as loyalty to both the Philippines and the U.S. Its central contention is that American colonial education created a form of Philippine nationalism that would become the dominant strain of official nationalism among Filipino leaders and educators. Bringing local actors the fore, this study enlists Filipino students’ and educators’ writings, vernacular novels, newspapers, and Philippine education journals to examine how Filipinos, both in the colony and metropole, responded to colonial education. It finds that Filipinos reformulated colonial lessons to fit in with older strains of Filipino nationalism even as they saw their American education as a path to economic opportunity and Philippine independence. By looking at the U.S. colonial state’s promotion of a native national identity, this study contributes to and complicates current narratives of U.S. colonial education and Philippine nationalism.",ucb,,https://escholarship.org/uc/item/01x8n57g,,,eng,REGULAR,0,0
82,1518,"Mobiles, Media, and the Agency of Indian Youth","Kumar, Neha","Parikh, Tapan S;",2013,"Technologies have been and are being designed to address varied human needs. Of these, the need for physical and economic well-being is typically considered to trump the need for culture, leisure, fun, and entertainment. Research initiatives in the field of Information and Communication Technology and Development (ICTD) have been in motion to address agricultural, educational, and health care needs, among others. The need for entertainment is central even in the lives of the `have-less', my dissertation affirms. Affordable new media technologies play a critical role towards the procurement of entertainment content and the resulting production of culture. Individuals quickly learn to navigate their way around technology, also paving the way for development-friendly outcomes. It is this phenomenon that my dissertation analyzes, as it studies individual agency in the intertwining of culture (society) and new media (technology) within the larger discourse of development.I use ethnographic methods to investigate the leisure-driven appropriation of the mobile phone by youth from socioeconomically disadvantaged backgrounds in rural, small-town, and urban India. I first analyze the influx of new media and its resulting impact on folk music practices in rural Madhya Pradesh and Rajasthan. Shifting focus to the motivations that drive youth towards mobile consumption of folk and popular media, I examine the unique material affordances of new media technologies and their influence on emerging practices. I use the Actor-Network Theory (ANT) lens to draw particular attention to the notion of agency, both human and material, as I investigate the pirate media actor-network responsible for the widespread dissemination of digital media and technical skills. I then focus on the agency of urban Indian youth that leads them to build further on these skills as they negotiate various linguistic, social, and technological hurdles for engagement with social media towards a new, improved identity for themselves.",ucb,,https://escholarship.org/uc/item/01x9b6bb,,,eng,REGULAR,0,0
83,1519,A Fractured Society: The Socio-Legal Environment of Fracking in the United States,"Kluttz, Daniel N","Fligstein, Neil D;Haveman, Heather A;",2017,"This dissertation examines the relationships between law and society when encountering disruptive, risky economic activities. In doing so, it assesses how culture, politics, civil society, and powerful industry interests influence the laws and legal instruments intended to protect or benefit citizens exposed to such activities. My case is the domestic shale-energy boom brought about by “fracking,” which, over the past decade, has revolutionized the US energy economy and sparked controversy for its potentially detrimental effects on local communities and the environment. By examining sociological influences on states’ fracking regulations and revealing inequalities reinforced in individual mineral-rights leasing contracts, I span time and space to analyze the social forces that shape who wins and who loses when fracking comes to an area.The dissertation is organized around three empirical studies. In the first, I draw from theories of social movements, organizations, politics, and markets to examine how social movements, economic industries, and state institutional environments influence the decisions of states to issue new regulations governing fracking or ban it altogether. Analyzing a longitudinal dataset of 34 states at risk of fracking from 2009 to 2016, and consistent with findings from political sociology and social-movement literatures, I find that increased economic security and increased environmental movement organizational capacity in a state boost the likelihood that a state will regulate the fracking industry or even ban fracking entirely. I also find that higher potential profitability (and accordingly, potential environmental risk) for fracking in a state moderates the effects of state government ideology and resource dependence on industry. These findings support my argument that the effects of non-state actors and institutional context on the regulation of disruptive industrial activity in new markets depend, in part, on the extent of potential economic benefits and societal risks posed by the economic activity.In the second empirical chapter, I examine the political, economic, and cultural factors influencing how stringently states regulate fracking. Analyzing state fracking chemical disclosure requirements from 2009 through 2016, I find that a state’s expected chemical disclosure stringency is most positively influenced by how stringently its geographically proximate peer states regulate. Interacting economic hardship with fossil-fuel industry political influence is associated with less stringent regulation. I argue for a field theory-based approach to state-level regulation, which conceives of states as both constitutive of their own regulatory fields and embedded within broader fields, taking similarly situated states into account but susceptible to industry capture during particularly difficult economic times.Finally, in the last empirical chapter, I move from the state to the local level and investigate how social inequalities become reinforced in legal instruments. Specifically, I analyze economic disparities in a ubiquitous but understudied aspect of the fracking boom: mineral-rights lease contracts. Lease contracts represent an alternative, but no less important, way that socio-legal processes determine who stands to gain, and who stands to lose, when fracking comes to town. I analyze a unique proprietary dataset of nearly 90,000 leases in Texas’s Barnett shale. I find that 1) local-community embeddedness yields expected higher payments to mineral-rights owners when compared to those who reside outside of the local community, and 2) people of color, in particular those of Hispanic/Latino ethnicity, receive significantly lower royalty terms when compared to whites, all else equal. The results hold when extended to a national-level analysis. These findings suggest that local ties can open pathways to locally sourced information and confer social capital, which can be beneficial during contract negotiations. They also support sociological theories of how social biases and categories affect economic transactions, resulting in patterned inequalities and discriminatory effects for socially disadvantaged groups. This chapter opens a new empirical domain—subsurface property rights—for socio-legal studies of contracts, and it offers new theoretical directions into how social inequalities become reinforced in legal instruments.",ucb,,https://escholarship.org/uc/item/0264r1mc,,,eng,REGULAR,0,0
84,1520,Global Architects Meet the Place - Bridging the Gap through Information and Communication Technology,"Perez, Yael Valerie","Kalay, Yehuda E;Agogino, Alice M;",2013,"In this study, I examine the ability of Information and Communication Technologies (ICTs) to narrow the gap between architects, aspiring to meet the place, and local users that are part of the place. The overarching goal is to identify tools necessary for successful place-driven design, particularly in the extreme design conditions in marginalized places. International architects are often invited to design in difficult to access, marginalized places through aid-organizations or through international developers invested in these places. This scenario propagates the gap between the architect's conceptions of place and the local users' conceptions of place. The design literature provides a range of recommendations for comprehending place. Yet, as expressed by several of the architects interviewed, these commonly used design methods appear to be ineffective in marginalized places, too often leading to designs that are inappropriate. Addressing the gap with marginalized places is especially valuable given their limited resources and the impact that design projects have on human development, which I refer to as `design freedom'.In search for tools to comprehend place I take on Canter's 1977 definition of place as the overlap between physical attributes, activities, and conceptions. Through interviews with architects, designing in marginalized places, both within the non-profit and for-profit realms, I found that while Internet-based ICTs are currently used for capturing physical attributes of place they are underutilized in communicating the subjective conceptions of place. By compiling the recommended methods in the literature together with those used by architects I interviewed I identify five levels of depth of the experiences available for comprehending place: egocentric, passive, active, interactive, and immersive. My hypothesis is therefore that when designing in marginalized places, a set of technologies that communicates the breadth of place through deep experiences will equip designers with comprehensive information about the place, enabling more place-appropriate design.Participatory Action Research (PAR) methodologies were used in two case-studies of design with the Pinoleville Pomo Nation (PPN), a Native American nation located near Ukiah, California. The first case study is a reflection on action through which I evaluated both face-to-face and mediated techniques for meeting the PPN. Through this reflection I identify the most-appropriate ICTs, and assembled them to communicate the PPN's place. In the second study I assess and measure how these technologies are used in an actual design project through ParticiPlace, an international design competition that attracted 17 design teams from around the world, to work on the PPN's Living Culture Center.Through these studies I found that technologies which communicate all three elements of place - physical-attributes, activities, and conceptions - can bridge the gap between designers and place. More specifically, architects who visited the site produced, on average, the same levels of place-appropriate designs compared to those who were too far to visit it and relied solely on ICTs to experience place. I have identified social networks as a technology that enables immersion in the conceptions of place. Nevertheless, while social networks can immerse users in conceptions, several limitations, including privacy setting still hinder its professional design use in marginalized communities. Moreover, integration of social network with technologies to allow interaction with physical attributes and with activities of place is still required to make these more effective place-driven design tools. I conclude with recommendations for ICT attributes to support place-driven design with a focus on marginalized communities.",ucb,,https://escholarship.org/uc/item/02c496hg,,,eng,REGULAR,0,0
85,1521,The Impact of Sleep Deprivation on Anxiety and Affective Brain Function,"Goldstein, Andrea Nicole","Walker, Matthew P;",2014,"Recent evidence suggests there is an intimate and causal relationship between sleep and anxiety. Despite such progress, several unknowns remain. For example which factors predispose some individuals to be vulnerable to the anxiogenic impact of sleep loss while others appear to be resilient have yet to be identified. Furthermore, whether trait-anxiety, in turn, predicts the sensitivity of individuals to amplified emotional brain reactivity associated with a lack of sleep is similarly unknown. Moreover, little is currently known about the embodied interplay between peripheral and central nervous system mechanisms leading to such abnormalities of affective processing caused by sleep deprivation. Characterizing these mechanisms is necessary not only to gain a deeper understanding of the pathophysiological pathways underlying the condition of anxiety, but also for developing effective treatments for the amelioration of anxiety as well as guiding public health policy aimed at anxiety disorder prevention. Targeting these unanswered questions, this thesis combines functional and structural MRI techniques, together with high-density EEG recordings, to test the overarching hypothesis that sleep deprivation leads to dysregulation of the extended-limbic system contributing to, and interacting with, the state of anxiety. Three specific predictions emerged from this overarching hypothesis and were tested in separate experiments. First, experiment 1 confirms the hypothesis that structural brain morphology in a network of limbic brain regions predicts vulnerability to the anxiogenic impact of sleep deprivation, and that sex moderates this interaction. Second, experiment 2 provides evidence supporting the hypothesis that trait anxiety determines the degree to which sleep deprivation amplifies limbic brain reactivity during the anticipation of potentially aversive emotional experiences. Finally, results from experiment 3 affirms the prediction that, beyond central limbic brain changes, sleep deprivation additionally dysregulates peripheral, autonomic cardiac signaling in response to affective stimuli as well as decouples the normally inter-related association between central brain and peripheral nervous systems. Collectively, these results help characterize a framework in which sleep deprivation contributes to anxiety symptomology through impairments in affective processing by both the central (limbic) and peripheral (autonomic) nervous system functioning. Moreover, these data suggest that the co-morbid features of sleep disruption and altered limbic as well as autonomic function commonly reported across anxiety disorders may be causally related. Considering the continued decreases in sleep time across society and the high prevalence of anxiety disorders, these findings have significant therapeutic, clinical and public health ramifications.",ucb,,https://escholarship.org/uc/item/0569x2sd,,,eng,REGULAR,0,0
86,1522,Essays on Environmental Policy in Energy Markets,"Boomhower, Judson Paul","Borenstein, Severin;",2015,"Producing and consuming energy involves costly environmental externalities, which are addressed through a wide range of public policy interventions.  This dissertation examines three economic questions that are important to environmental regulation in energy.  The first chapter measures the effect of bankruptcy protection on industry structure and environmental outcomes in oil and gas extraction.  The second chapter measures additionality in an appliance replacement rebate program.  Finally, the third chapter focuses on the environmental impacts of subsidizing electricity production from forest-derived biomass fuels.   The first chapter measures the incentive effect of limited liability.  When liability is limited by bankruptcy, theory says that firms will take excessive environmental and public health risks.  In the long run, this ``judgment-proof problem'' may increase the share of small producers, even when there are economies of scale.  I use quasi-experimental variation in liability exposure to measure the effects of bankruptcy protection on industry structure and environmental outcomes in oil and gas extraction.  Using firm-level data on the universe of Texas oil and gas producers, I examine the introduction of an insurance mandate that reduced firms' ability to avoid liability through bankruptcy.  The policy was introduced via a quasi-randomized rollout, which allows me to cleanly identify its effects on industry structure. The insurance requirement pushed about 6% of producers out of the market immediately.  The exiting firms were primarily small and were more likely to have poor environmental records.  Among firms that remained in business, the bond requirement reduced oil production among the smallest 80% of firms by about 4% on average, which is consistent with increased internalization of environmental costs. Production by the largest 20% of firms, which account for the majority of total production, was unaffected.  Finally, environmental outcomes, including those related to groundwater contamination, also improved sharply.  These results suggest that incomplete internalization of environmental and safety costs due to bankruptcy protection is an important determinant of industry structure and safety effort in hazardous industries, with significant welfare consequences.The second chapter focuses on the importance of a regulator's inability to distinguish between households responding to a subsidy, and households doing what they would also have done in the absence of policy.  Economists have long argued that many recipients of energy-efficiency subsidies may be ``non-additional,'' getting paid to do what they would have done anyway.  Demonstrating this empirically has been difficult, however, because of endogeneity concerns and other challenges. In this paper we use a regression discontinuity analysis to examine participation in a large-scale residential energy-efficiency program. Comparing behavior just on either side of several eligibility thresholds, we find that program participation increases with larger subsidy amounts, but that most households would have participated even with much lower subsidy amounts. The large fraction of inframarginal participants means that the larger subsidy amounts are almost certainly not cost-effective. Moreover, the results imply that about half of all participants would have adopted the energy-efficient technology even with no subsidy whatsoever.Finally, the third chapter addresses consequences of renewable energy subsidies in other markets.  Electricity generated from logging residues provides a large and growing share of US renewable electricity generation.  Much of the low-value wood used by biomass power plants might otherwise be left in the field.  This increased harvest can negatively affect forest health. I investigate the supply of woody biomass fuel in Maine using a 15-year panel of prices and quantities for whole tree wood chips.  I find that doubling the price of woody biomass increases harvest by about 64%.  I also find that coal prices are a major determinant of woody biomass harvest.  This suggests that environmental policies that raise the price of coal will affect forest health.",ucb,,https://escholarship.org/uc/item/05f4r67b,,,eng,REGULAR,0,0
87,1523,"Machine Learning Techniques in Nuclear Material Detection, Drug Ranking and Video Tracking","Yang, Yan","Hochbaum, Dorit S.;",2013,"The main focus of this thesis is using machine learning and data mining techniques to solve challenging problems.  Three problems from different subject areas are discussed: nuclear material detection, drug ranking and target tracking in video sequences. The techniques of the three problems described are all based on an efficiently solvable variant of normalized cut, Normalized Cut Prime (NC').The first problem concerns detecting concealed illicit nuclear material, an important part of strategies preventing and deterring nuclear terrorism.  What makes this an extremely difficult task are physical limitations of nuclear radiation detectors (arising from energy resolutions and efficiency) and shielding materials terrorists would presumably use to surround the radioactive nuclear material and absorb some of the radiation, thereby reducing the strength of the detected signal. This means the central data analysis problem is identifying a potentially very weak signal, and distinguishing it from both background noise arising from the detector characteristics and naturally occurring environmental radiation.  We aim at enhancing the capabilities of detection with algorithmic methods specifically tailored to nuclear data. A novel graph-theory-based methodology based on NC' is used, called Supervised Normalized Cut (SNC).  This data mining method classifies measurements obtained from very low resolution plastic scintillation detectors.  The accompanying computational study, comparing SNC method with several alternative classification methods shows that in terms of accuracy, the SNC method is on par with alternative approaches, yet SNC is computationally more efficient.The second subject area is in the field of drug ranking.  This problem refers to placing in rank order, according to their effectiveness, several drugs treating the same disease, using data derived from cell images.   Current technologies use the recently developed high-throughput drug profiling (high content screening or HCS).  Despite the potential of HCS for accurate descriptions of drug profiles,   it produces a deluge of data of quantitative and multidimensional nature, posing analytical challenges in the data mining process.  Our new framework is designed to alleviate these difficulties, in the way of producing graph theoretic descriptors and automatically ordering the performance of drugs, called fractional adjusted bi-partitional score (FABS), a way of converting classification to scores.   We experimented with the FABS framework by implementing different algorithms and assessing the accuracy of results by a comparative study, which includes other four baseline methods.  The conclusion is encouraging: FABS implemented with NC' consistently outperforms other implementations of FABS and alternative methods currently used for ranking that are unrelated to FABS.   The third problem is target tracking in video sequences -- it can be framed as an unsupervised learning problem:  the goal is to delineate a target of interest in a video from background.  The tracking task is cast as a graph-cut, incorporating intensity and motion data into the formulation.  Tests on real-life benchmark videos show that the developed technique, NC-track, based on NC', is more efficient than many existing techniques, and that it delivers good quality results.",ucb,,https://escholarship.org/uc/item/05f838rg,,,eng,REGULAR,0,0
88,1524,ARTELIA GREEN’S & OLIVIA WILLIAMS’ LEGACY: A STUDY ON THE PEDAGOGICAL PRACTICES THAT IMPROVE HEALTH FOR BLACK CHILDREN,"Johnson, Tiffani Marie","Perlstein, Daniel;",2019,"This dissertation examines the relationship between caring teaching practices and greater health outcomes for black children. Public health theory suggests that Black youth generally experienced greater levels of adversity compared to non-black youth (Schilling et al., 2007; Marie, 2016). Exposure to these frequent and/or sustained stressors without the buffering care of a supportive adult can change children’s brains and bodies, including disrupting learning, behavior, immune systems, and even the way DNA is read and transcribed. My research examines the efficacy of critical classroom pedagogy (Duncan-Andrade & Morrell, 2008) and social design-based research (Gutierrez, 2016) as a framework to address and attenuate the impacts of toxic stressors that black youth embody.This study honors research principles grounded in care (Angelou, 1979; Noddings, 1988; Duncan-Andrade, 2006), to generate grounded theory for social transformation. This dissertation anchors data (field notes, classroom video, in-depth interviews) in order to integrate the fields of education and public health to produce ecologically valid findings that: 1) highlight and reproduce that types of teaching practices and conditions that mediate healthier children and 2) reframe our understandings of the possibilities of education.",ucb,,https://escholarship.org/uc/item/05g5p795,,,eng,REGULAR,0,0
89,1525,"The Grammaticalization of Grammatical Relations: A Typological and Historical Study Involving Kashaya Pomo, Old English, and Modern English","Gamon, David",,1997,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/05g8s71b,,,eng,REGULAR,0,0
90,1526,Applying riboswitches for novel sensing and chemistry,"Truong, Johnny","Hammond, Ming C;Francis, Matthew B;",2019,"Riboswitches are cis-regulatory structured RNA elements capable of controlling expression of downstream genes by binding to small molecule ligands. These naturally evolved RNA elements possess remarkable affinity and selectivity for their small molecule ligands, high folding efficiencies, and thermostability for functioning in cellular environments.  Due to these properties, a number of riboswitch-based technologies have emerged such as riboswitch reporters, aptazymes, and RNA-based fluorescent (RBF) biosensors which all have wide applications for detection, imaging, and regulatory circuits. While riboswitch reporters and aptazymes have been robustly studied to better understand how to improve their function, there are fewer studies that expand on RBF biosensor development.  Here, novel approaches towards expanding the functional repertoire of RBF biosensors and systematically probing their properties are described.First, we show that engineering circular permutations of the riboswitch aptamer domain yields functional biosensors for S-adenosyl-L-methionine (SAM), using the SAM-I riboswitch as our model. We reveal that this design can enhance fluorescence turn-on and ligand binding affinity compared to the non-permuted topology. Expanding upon these established design principles, novel biosensors for the ligand guanidine was developed. Two novel designs were added to our existing repertoire that generated functional RBF biosensors using the architecture of the guanidine-I riboswitch. A new base-pair mutation strategy was applied on these guanidine biosensors which, resulting in modest changes to biosensor activation speeds just from single base-pair mutations. Lastly, riboswitches were explored as potent scaffolds to generate a self-labeling ribozyme.  Various natural or engineered riboswitches for the electrophilic ligand, SAM, were screened for reactivity with an analog, Hey-SAM, as a proxy to measure ribozyme activity. In collaboration with Agilent Labs, a high-throughput method was developed for probing and screening latent ribozyme activity using a microarray platform. The efforts and strategies put forth here use riboswitches outside their native context for applications in detection and catalysis further showcasing the broad utility of riboswitch-based tools.",ucb,,https://escholarship.org/uc/item/05n8k3nq,,,eng,REGULAR,0,0
91,1527,Writing the Storyteller: Folklore and Literature from Nineteenth-Century France to the Francophone World,"Gipson, Jennifer","Paige, Nicholas;",2011,"Nineteenth-century modernity, according to Walter Benjamin and other critics, kills storytelling.  Instead of treating this as a real disappearance, I consider how writers continually reinvent this death to work through historically specific questions about tradition, memory, and cultural transmission.  In nineteenth-century France, for example, the recurrent belief in the end of tradition prompted movements for folklore collection--like Napoléon III's decree for the preservation of poésies populaires--as well as broad reflections about the future of cultural expressivity.  Nodier, Nerval, Mérimée, and Champfleury, all combined literary creation with folklore study for the eclipse of oral tradition was, paradoxically, the very foundation of modern literature as it was coming to be defined.  Thus, Nerval appends to his Sylvie a folklore collection so as to mark the distance between the written and the oral, reaffirming literary modernity while mourning tradition.  Though far-removed from folklore, Barbey d'Aurevilly's short stories and fragmented frames that impede narrative transmission also question the very possibility of storytelling.  In addition to such formal innovations, writers also revisited earlier storytelling topoi.  In the early eighteenth century, when Antoine Galland introduced Les Mille et une nuits to the West, Schéhérazade's life-or-death storytelling stood as commentary on the tyranny of audience demands as the patronage system was breaking down.  But nineteenth-century writers returned to this ancient storytelling sultana to think about the demands of newspaper editors, growing readerships, and the transformation of literature into mass market entertainment.  Finally, interest in folklore fades on the mainland, but debates about preservation of traditions and ownership of cultural goods--debates once linked to France's colonial projects--become central to post-colonial Francophone literature.  Patrick Chamoiseau and other proponents of créolité spotlight the paradox of preserving Creole storytellers' legacies in writing--or in French.  Assia Djebar even intersperses her bloody tale of a modern Algerian Schéhérazade with fragments from Galland's eighteenth-century text, foregrounding the question of what happens to the voice and to stories after a storyteller dies.  In short, folklore has a long history of being a reference point for thinking about the very notions of literature or modernity that supposedly spell its demise.  Literary depictions of storytelling tell a story less about oral culture than about literature itself. And concern about who is no longer telling stories often reveals a deeper cultural anxiety about who is now writing or reading stories.",ucb,,https://escholarship.org/uc/item/05p5h8zd,,,eng,REGULAR,0,0
92,1528,Quasi-Variational Inequalities for Source-Expanding Hele-Shaw Problems,"DeIonno, John Adrian","Evans, Lawrence C.;",2013,"We study nonlinear partial differential equations describing Hele-Shaw flows for which the source of the fluid, which classically is at a single point, instead expands as the moving boundary uncovers new sources. This problem is reformulated as a type of quasi- variational inequality, for which we show there exists a unique weak solution. Our procedure utilizes a Baiocchi integral transform. However, unlike in traditional applications, the inequality solved by the transformed variable continues to depend on the free boundary, and thus standard theory of variational inequalities cannot be immediately applied. Instead, we develop convergent time-stepping scheme. As a further application, we generalize and study a related problem with a growing density function obeying a hard constraint on the maximum density.",ucb,,https://escholarship.org/uc/item/05z2t7br,,,eng,REGULAR,0,0
93,1529,Modern Low-Complexity Capacity-Achieving Codes For Network Communication,"Goela, Naveen","Gastpar, Michael;",2013,"Communication over unreliable, interfering networks is one of the current challenges inengineering. For point-to-point channels, Shannon established capacity results in 1948, and it took more than forty years to find coded systems approaching the capacity limit with feasible complexity. Significant research efforts have gone into extending Shannon's capacity results to networks with many partial successes. By contrast, the development of low-complexity codes for networks has received limited attention to date. The focus of this thesis is the design of capacity-achieving network codes realizable by modern signal processing circuits. For classes of networks, the following codes have been invented on the foundation of algebraic structure and probability theory: i ) Broadcast codes which achieve multi-user rates on the capacity boundary of several types of broadcast channels. The codes utilize Arýkan's polarization theory of random variables, providing insight into information-theoretic concepts such as random binning, superposition coding, and Marton's construction. Reproducible experiments over block lengths n = 512, 1024, 2048 corroborate the theory; ii ) A network code which achieves the computing capacities of a countably infinite class of simple noiseless interfering networks. The code separates a network into irreducible parallel sub-networks and applies a new vector-space function alignment scheme inspired by the concept of interference alignment for channel communications. New bounds are developed to tighten the standardcut-set bound for multi-casting functions. As an additional example of low-complexity codes, reduced-dimension linear transforms and convex optimization methods are proposed for the lossy transmission of correlated sources across noisy networks. Surprisingly, simple un-coded or one-shot strategies achieve a performance which is exactly optimal in certain networks, or close to optimal in the low signal-to-noise regime relevant for sensor networks.",ucb,,https://escholarship.org/uc/item/05z6v4zg,,,eng,REGULAR,0,0
94,1530,SQUID-Detected MRI in the Limit of Zero Static Field,"Kelso, Nathan","Clarke, John;",2009,"The magnetic gradient fields used in magnetic resonance imaging (MRI) have a component which is parallel to the uniform field B0 = B0z, as well as a component perpendicular to B0.  The component parallel to B0 is used in spatial encoding.  The component perpendicular to B0, called the ``concomitant gradient,"" causes image distortions (by altering the magnitude and direction of the total field) if its magnitude approaches B0 at any point in the field of view (FOV).  In a conventional imaging sequence, the presence of the concomitant gradients limits the maximum gradient that can be used with a given B0 field or, conversely, limits the minimum B0 field that can be used with a given gradient field.This thesis describes an implementation of the so-called ``zero-field MRI"" (ZFMRI) pulse sequence, which allows for imaging in an arbitrarily low B0 field.  The ZFMRI sequence created an effective unidirectional gradient field by using a train of π pulses to average out the concomitant gradient components during encoding.  The signals were acquired using a low-transition temperature dc Superconducting QUantum Interference Device (low-Tc dc SQUID) coupled to a first-order axial gradiometer.  The experiments were carried out in a liquid helium dewar which was magnetically shielded with a single-layer mu-metal can around the outside and a superconducting Pb can contained within the helium space.  We increased the filling factor of the custom-made, double-walled Pyrex insert by placing the liquid alcohol sample, at a temperature of approximately -50°C, at the center of one loop of the superconducting gradiometer, which was immersed in the helium bath.Using the aforementioned sequence and apparatus, images were acquired in the limit of zero static field, using gradients of up to 100 μT/m over a 23 mm FOV.  The change in field magnitude over the FOV due to gradients was up to 10 times larger than the magnitude of any static field present in the dewar (static fields arose from residual magnetic fields and were 1 μT or less).  These images were free of concomitant gradient distortions.  Images encoded using aconventional imaging sequence under similar conditions were also acquired; the conventional images were irreparably distorted.The limitations of the present ZFMRI sequence implementation are considered, as well as how the procedure could be made more practical with regard to imaging time.  The extension of the technique to unshielded operation in a uniform ambient field is discussed, as are other methods of mitigating or eliminating concomitant gradient distortions.",ucb,,https://escholarship.org/uc/item/02k9c28f,,,eng,REGULAR,0,0
95,1531,From Another Psyche: The Other Consciousness of A Speculative American Mystic (The Life and Work of Jane Roberts),"Skafish, Peter William","Pandolfo, Stefania;",2011,"This dissertation attempts to develop the beginnings of a new approach to understanding the significance of modes of thought marginal and/or external to those of the modern West. I call this approach an ""anthropology of concepts"" because it examines concepts and themes belonging to scriptural, ""philosophical,"" and poetic traditions as concepts rather than, as normally happens in anthropology, in the context of social practices, historical events, or everyday life. I also call it this because it accordingly involves the close reading and interpretation of the written or oral texts in which concepts are articulated. Concepts, when treated this way, retain their capacity to bring about novel understandings of the real, and to engender thereby theoretical perspectives not attainable through more conventional interpretive means. Such an approach may be necessary if the humanities and social sciences are to continue to hold a critical perspective on a world so enclosed that gaining any distance from its basic schemes of thought has become extremely difficult.The present dissertation undertakes such an ""anthropology of concepts"" in order to elaborate what I intend to be a new theory of the psyche and consciousness. Popularly regarded as one of the founders of the New Age spirituality of the United States, Jane Roberts (1925-1984) was a ""channel"" (a kind of spirit medium) and visionary mystic who published in the 1960's and 1970's over twenty books that she understood to have been dictated or written through her by different spiritual beings, including one she called ""Seth."" Although these texts were crucial to the popularization of Western occult ideas about reincarnation, magic, and health that were at the heart of the New Age, Roberts's intellectual curiosity and background as an author of science fiction give her writings a speculative, intellectually reflexive, and even manifestly ontological tone that is reminiscent of certain mystical thinkers and that sets them apart from popular religious discourse. My engagement with Roberts' writings focuses, first of all, on the concepts she and her cohort of personalities articulated in the course of addressing what was for her the most pressing question raised by the decades she spent channeling: how could her experience during her trances of being herself and another self in the same instant of time be possible? Her answer was that such an experience--what she called ""other-consciousness""--occurs not through language but when the subject sees itself in the non-sensory, mental images of dreams and the imagination. She was right in the sense that such images, as Jean-Paul Sartre makes clear in Psychology of the Imagination, allow two aesthetic figures or persons to appear as one. My argument is that her claim is significant for showing, surprisingly enough, that contrary to what French philosophy claimed for decades, the other can be brought into and made part of consciousness without being appropriated and consciousness therefore takes a radically altered form. The baseline consciousness of oneself, that is, changes from apperception to a consciousness of oneself as both oneself and another--and even of oneself as a plurality of selves. To make this point, I read concurrently with Jane Roberts' texts the work of Deleuze, showing that she raised in her own fashion some of the same questions about being, time, and the subject as he did, but that the strange context in which she thought led her to furnish significantly different--and now for us, novel--responses to them. Given that a subject that would be at once itself and another would also be both what it actually is and what it otherwise only could have been, I furthermore show how Roberts' work allows one to rethink the Deleuzean (and by implication deconstructive) understandings of the categories of actuality and possibility and another concept--time--to which they are integrally tied. The fact that her writings provide a basis for recasting the thought of such a comprehensive philosopher on matters this fundamental is an indication, I think, of the broad value an anthropology of concepts could hold for humanistic research.",ucb,,https://escholarship.org/uc/item/02m143dp,,,eng,REGULAR,0,0
96,1532,Characterization of Complex Genetic Component Contributing to the Susceptibility for Multiple Sclerosis and Rheumatoid Arthritis,"Briggs, Farren Basil Shaw","Barcellos, Lisa F;",2010,"Autoimmune diseases (ADs) are a major public health concern, as the third most common category of disease in the US, after cancer and heart disease. As a result, ADs has become one of the most active genetic and epidemiologic research areas, however, unraveling the etiological mechanisms of ADs has proven difficult. There is strong evidence suggesting a complex genetic component contributing to all ADs. For most ADs, the prominent genetic risk locus is within the major histocompatibility complex (MHC) on chromosome 6p21.3. Unfortunately, identifying non-MHC susceptibility loci has proven difficult in these complex ADs with multigenic patterns of inheritance. Recently, through concerted international efforts, several genome-wide association (GWA) studies and subsequent replication analyses have confirmed several other AD susceptibility loci of modest effects; much of the remaining genetic variants contributing to AD susceptibility are unknown. It is clear that current approaches will be limited to identify all the complex genetic component ADs, therefore this dissertation focuses using strong epidemiological approaches and robust analytical frameworks to identify additional non-MHC genetic risk factors in two complex ADs: multiple sclerosis (MS) and rheumatoid arthritis (RA).In Chapter 1, the relationship between variation in DNA repair pathways genes and risk for MS was investigated. Univariate association testing, epistatic tests of interactions, logistic regression modeling and non-parametric Random Forests analyses were performed using genotypes from 1,343 MS cases and 1,379 healthy controls of European ancestry. A total of 485 single nucleotide polymorphisms (SNPs) within 72 genes related to DNA repair pathways, including base excision repair, nucleotide excision repair, and double strand breaks repair, were investigated. A SNP variant within GTF2H4 on 6p21.33 was significantly associated with MS (odds ratio=0.7, p=3.5 x 10-5) after accounting for multiple testing, and was not due to linkage disequilibrium with HLA-DRB1*1501. Despite clear evidence for an association between GTF2H4and MS, collectively, these results, derived from a well-powered study, do not support a strong role for variation within DNA repair pathway genes in MS.In Chapter 2, the relationship between variation within 8 candidate hypothalamic-pituitary-adrenal (HPA) axis genes and susceptibility to MS were comprehensively investigated. A total of 326 SNPs were investigated in 1,343 MS cases and 1,379 healthy controls of European ancestry using a multi-analytical strategy. Random Forests identified 8 SNPs within the corticotropin releasing hormone receptor 1 or CRHR1 locus on 17q21.31 as important predictors of MS. Based on univariate analyses: five CRHR1 variants were associated with decreased risk for disease following a conservative correction for multiple tests. Independent replication was observed in a large meta-analysis comprised of 2,624 MS cases and 7,220 healthy controls of European ancestry. The results provide strong evidence for the involvement of CRHR1 (rs242936: p=9.7 x 10-5) in MS.In Chapter 3, epistatic interactions with a well-established genetic factor (PTPN22 1858T) in a RA was investigated. The analysis consisted of four principal stages: Stage I (data reduction) - identifying candidate chromosomal regions in 292 affected sibling pairs, by predicting PTPN22 concordance using multipoint identity-by-descent probabilities and Random Forests; Stage II (extension analysis) - testing detailed genetic data within candidate chromosomal regions for epistasis with PTPN22 1858T in 677 cases and 750 controls using logistic regression; Stage III (replication analysis) - confirmation of epistatic interactions in 947 cases and 1,756 controls; Stage IV (combined analysis) - a pooled analysis including all 1,624 RA cases and 2,506 control subjects for final estimates of effect size. A total of 7 replicating epistatic interactions were identified. A SNP variant (rs7200573) within CDH13 demonstrated significant evidence for interaction (p=1.5 x 10-4) with PTPN22. There was also evidence for epistasis between PTPN22 and SNP variants within MYO3A, CEP72 and near WFDC1.The research conducted in Chapters 1 through 3 describe analytical approaches that were based on strong hypotheses, multi-stage analyses, and the use of robust non-parametric methods in tandem with conventional association testing. These chapters are scientifically important, as they contribute to our understanding of the underlying genetic architecture in two debilitating ADs (MS and RA) and provide strong methodological frameworks for investigating other chronic diseases with a complex genetic component.",ucb,,https://escholarship.org/uc/item/02m2x4jz,,,eng,REGULAR,0,0
97,1533,Context in Constructions,"Lee-Goldman, Russell",,2011,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/02m6b3hx,,,eng,REGULAR,0,0
98,1534,Coping with Text Complexity in the Disciplines: Vulnerable Readers' Close Reading Practices,"Buffen, Leslie","Pearson, P. David;",2019,"Early reactions suggest that secondary teachers need support implementing the Common Core State Standards, specifically when teaching close reading strategies with complex disciplinary texts to vulnerable readers. This mixed-methods study conducted in a formative experiment paradigm (Reinking & Bradley, 2008) aimed to provide explanatory theories generated by applying a grounded theory approach to data analysis. An ethnographic case study design framed the grounded theory (Glaser & Strauss, 1967) and the formative experiment. I wanted to understand how two teachers adjusted their curriculum and instruction to create a more rigorous experience for vulnerable readers. I also sought to explore how such an approach affected the academic identities of these students. Teacher surveys, teacher interviews, student interviews, student surveys, student work, reading assessments, classroom observations, and teacher planning sessions provided evidence about how readers in two classrooms make sense of text and the strategies that support their comprehension and engagement during two curricular units that involve close reading of disciplinary texts. The units were used by students in two Special Day English classrooms at two different school sites and were co-designed by each high school teacher and myself. Reflections upon the first curricular unit informed the planning for the second curricular unit in each class. Also, I followed three students into at least one of their other disciplinary classes to understand their experiences with discipline-specific literacy instruction, both in the language arts and their disciplinary classes. Finally, I interviewed students about their in-school and out-of-school literacy practices, again to look for evidence of transfer from the units to their everyday repertoire of practices. Results indicate that both teachers, as they implemented the collaboratively planned lessons, asked predominantly open-ended questions that expected students to include textual evidence in their responses and did not have predetermined answers. This behavior was contrary to what I expected at the beginning of my study because I expected teachers might have predetermined answers. When responding to each teacher’s instruction, students referenced the text in supporting the claims they developed on their own. In both classrooms, some of the students’ academic identities improved during the study. All students reported rich relationships with text outside of school; however, only some students experienced success with reading complex disciplinary text in their English class. Overall findings suggest a positive effect of open-ended tasks that require students reference both the text and the knowledge that they bring to the task in forming arguments and applying understandings gained while reading. Future studies including a greater number of teachers from a range of disciplines who implement instruction with more students over a longer period of time would be beneficial in developing a more robust database about the power and influence of close reading practices.",ucb,,https://escholarship.org/uc/item/02w5k412,,,eng,REGULAR,0,0
99,1535,"Translating Sweetness: Type 2 Diabetes, Race, Research, and Outreach","Battle, James","Hayden, Cori;",2012,"Through the lens of Type 2 diabetes this dissertation considers race and problems of difference and risk with developments in treatment, genomic science, and the conduct of research and research priorities. Based primarily on fieldwork in New York and California, I interrogate public health notions of outreach with biotechnology and clinical research concepts of biomedical translation as synonymous practices. Institutional relationships and marketing drivers, I argue, reflect relatedness back onto the Type 2 diabetes patient through causal narratives of risk and inevitability. In effect, kinship--genetic, familial, racial, ethnic, and environmental--becomes the driver of both risk and emergent forms of bioliterary discipline.  I present a narrative of how diabetic risk became racialized over time and how African Americans became seen as a desirable research population. Arguing against biological race, I present an ethnographic example of how one such African American population, or community, emerged from particular social and political histories. Fieldwork uncovered lingering memories of the Tuskegee Experiment combined with cultural incompetency in both public health and biotechnology sectors. Further, I consider the bioethical challenges of African (American) participation in new genomic research aimed toward reducing health disparities. However, as a racial category, genetics researchers debate the precise genetic location and definition of ""Africa"" in the human genome. I suggest that the search for a pathological Africa in the human genome may deepen racial stigmatization as well as author new narratives of difference. I submit that social disparity, not biological disparity, presents the ultimate challenge to successful effective clinical translation and public health outreach.",ucb,,https://escholarship.org/uc/item/06f5s847,,,eng,REGULAR,0,0
100,1536,GB Virus Type C (HGV) and Human Immunodeficiency Virus (HIV) Co-Infection: Incidence and Impacts on Survival in a Cohort of HIV-Infected Transfusion Recipients,"Vahidnia, Farnaz","Reingold, Arthus L.;",2011,"GB virus C (GBV-C), an RNA virus closely related to hepatitis C virus (HCV), is transmitted through sexual, parenteral, and vertical routes. GBV-C is highly prevalent among patients receiving blood products and those at high risk of sexual or parenteral exposure. Unlike HCV, GBV-C replicates mainly in lymphocytes; many attempts to find an association between GBV-C infection and human disease have been unsuccessful. Therefore, donated blood is not routinely screened for GBV-C infection. In vitro and clinical studies have suggested that GBV-C co-infection may inhibit human immunodeficiency virus (HIV) replication by several different biological mechanisms. Some previous studies, but not all, have shown an association between GBV-C infection and both lower HIV viral load (VL) and better survival among HIV-infected patients.  Few studies describe predictors of acute GBV-C infection following transfusion in HIV-infected patients. Reports on survival benefits associated with co-infection after advent of highly active retroviral therapy (HAART) are inconclusive. An open question in many previous reports is the temporal relationship between GBV-C infection and HIV disease markers.  To address some of the currently unanswered questions concerning GBV-C and HIV co-infection, we used a limited access database obtained from the National Heart, Lung, and Blood Institute. The Viral Activation Transfusion Study (VATS) was a randomized controlled trial comparing leukoreduced (LR) vs. non-LR transfusions given to anemic HIV-infected transfusion-naïve patients. Pre- and post-transfusion samples from 489 subjects were tested for GBV-C markers. We used the VATS dataset and the results of GBV-C testing to examine two hypotheses. First, we tested the hypothesis that GBV-C is transmitted to HIV-infected VATS subjects (n=294) via transfusion. We estimated the risk of acquiring GBV-C RNA per unit of blood transfused and examined the predictors of GBV-C acquisition. We found an incidence of 39 GBV-C infections per 100 person-years during follow-up in this population and an 8% increased risk of acquiring GBV-C associated with each additional unit of blood transfused, controlling for HAART status and baseline HIV VL. A lower HIV VL, use of HAART and white race were associated with an increased risk of subsequent GBV-C acquisition. Second, we examined the hypothesis that GBV-C co-infection is associated with lower mortality and lower HIV VL in 489 HIV-infected VATS subjects and in two VATS sub-cohorts. GBV-C viremia was associated with significantly lower mortality and HIV VL in unadjusted analyses. We found a non-significant trend towards lower mortality and lower HIV VL among HIV-infected VATS subjects, after adjusting for HIV risk behavior and time-updated E2 antibody, HAART status, HIV VL, and CD4 cell count. Acquisition of GBV-C was associated with lower mortality in the sub-cohort of individuals who were GBV-C RNA and antibody negative at baseline (n=294), adjusting for time-updated covariates (HR= 0.31, 95% CI 0.11, 0.86). Our results suggest high rates of GBV-C transmission by transfusion among HIV-infected subjects and an increased hazard of GBV-C acquisition with lower pre-transfusion HIV VL and current use of HAART. Our results also indicate that GBV-C viremia is associated with a trend towards lower mortality and lower HIV VL, and GBV-C acquisition via transfusion is associated with a significant reduction in mortality in HIV-infected individuals, after adjusting for HIV disease markers. These findings confirm previous reports that GBV-C infection inhibits HIV replication in vitro and in vivo.",ucb,,https://escholarship.org/uc/item/00p0412d,,,eng,REGULAR,0,0
101,1537,Introns and alternative splicing in choanoflagellates,"WESTBROOK, MARJORIE WRIGHT","King, Nicole;",2011,"The first organisms to evolve were unicellular, and the vast majority of life has remained so for billions of years. Complex forms of multicellularity, requiring increased levels of cell adhesion, cell signaling and gene regulation, have evolved in only a few eukaryotic lineages. The comparison of genomes from choanoflagellates, the closest relatives of metazoans, with genomes from metazoans may reveal genomic changes underlying metazoan origins. I used this approach to investigate the evolution of introns during the origin of metazoans.By analyzing the genome of the first choanoflagellate to be sequenced, Monosiga brevicollis, I found that its intron density rivals that of genes in intron-rich metazoans. Many intron positions are conserved between choanoflagellates and metazoans, implying that their shared unicellular ancestor was also intron-rich. In my analysis of the M. brevicollis genome, I made the unexpected discovery that, unlike most choanoflagellate genes, the longest genes contain relatively few introns. Indeed, one M. brevicollis gene contains the longest stretch of intron-free coding sequence known to date. I also found a similar trend in the genome of a basal metazoan, the sponge A. queenslandica. However, most long genes in other metazoans are not depleted of introns, revealing a difference in gene structure between eumetazoans and their closest relatives that may have implications for how these genes are regulated. The results of these analyses led me to investigate the evolution of alternative splicing during the emergence of metazoans. Intron-rich metazoan genes undergo complex patterns of developmentally regulated alternative splicing. My analysis of intron evolution revealed that the unicellular ancestor of metazoans was also intron-rich, raising the possibility that alternative splicing was common before the transition to multicellularity. To test this, I used transcriptome sequencing to detect alternative splicing in choanoflagellates and the early branching metazoan, Hydra magnipapillata. I found that alternative splicing, especially the skipping of entire exons, occurs less frequently in choanoflagellates than in H. magnipapillata. Increased alternative splicing of already intron-rich genes may thus represent an augmentation of gene regulation that evolved during the origin of metazoans. My analyses suggest that metazoans evolved from an intron-rich unicellular ancestor, setting the stage for complex patterns of alternative splicing to evolve during the transition to multicellularity. The connection between gene structure and alternative splicing provides an example of how non-coding features of eukaryotic genomes can impact the evolution of regulatory and morphological complexity.",ucb,,https://escholarship.org/uc/item/00w3t04k,,,eng,REGULAR,0,0
102,1538,"Bimetallic Nanoparticles: Synthesis, Characterization, and Applications in Catalysis","Landry, Alexandra Marcela","Iglesia, Enrique;",2016,"Bimetallic nanoparticles can lead to catalysts with improved turnover rates and selectivities, but many synthetic protocols, such as impregnation or precipitation, typically form particles of non-uniform size and composition. Colloidal methods may be able to improve their uniformity, but often require reagents that poison catalytic surfaces (ex. S, B, P). Such compositional non-uniformity and ubiquitous impurities have prevented rigorous conclusions about the consequences of alloying on reactivity and selectivity. Herein, we describe a sequential galvanic displacement-reduction (GDR) colloidal synthesis method using precursors containing only C, H, O, and N, that leads to bimetallic AuPt and PtPd nanoparticles narrowly distributed in size and composition. Au3+ or Pt4+ precursors were added to monometallic Pt or Pd clusters, respectively, whose surface atoms are thermodynamically driven to reduce and deposit the solvated cations onto cluster surfaces due to the lower reduction potentials (E0) of the seed metal relative to the precursor cations (E0Au > E0Pt > E0Pd); oxidized Pt and Pd surface atoms subsequently return to cluster surfaces upon reduction by the solvent, a reductant (ethanol or ethylene glycol, respectively). Such methods have been previously used to synthesize AuPd clusters from Pd seed clusters. TEM micrographs confirm that initial seed cluster sizes increase monotonically with increasing Au3+ or Pt4+ content, with final bimetallic cluster dispersity values near unity indicating a narrow size distribution. UV visible spectroscopy of AuPt cluster suspensions show no plasmon resonance features characteristic of Au nano-sized surfaces, indicating the presence of Pt atoms at bimetallic surfaces, as expected for GDR processes. Elemental analysis by EDS confirmed the formation of strictly bimetallic particles with the mean composition of the synthesis mixture.The GDR model requires that bimetallic growth be proportional to the initial seed surface area, with the number of precursor atoms deposited per surface metal atom of the seed constant and independent of seed metal size. Elemental analysis using EDS supports this hypothesis for thermodynamically favorable alloys such as PtPd and AuPd, but not for AuPt, an unfavorable alloy. These differences appear to reflect the segregation of metals within AuPt clusters during synthesis, placing the metal with the lower surface energy, Au, at cluster surfaces, and decreasing the availability of Pt0 surface atoms for GDR. Consequently, autocatalytic Au3+ reduction on Au0 sites becomes a competitive Au3+ reduction pathway during the synthesis of AuPt clusters.Polymers such as polyvinylpyrrolidone (PVP)—which bind to metal surfaces during synthesis via charge-transfer interactions—were required in colloidal suspensions to prevent particle agglomeration in solution, but must be removed prior to catalysis. We show that after depositing clusters on SiO2, PVP can be removed from particle surfaces by post-synthetic treatments at mild temperatures (≤ 423 K) in reductants such as H2 and/or EtOH without significant particle agglomerations. Reductants compete with the polymer at the metal surface, thus breaking the polymer-metal bond. The absence of surface residues was confirmed by the similar cluster sizes derived from O2 chemisorption and TEM measurements. Larger cluster sizes and surfaces that chemisorb oxygen more weakly—such as Pt relative to Pd—were found to facilitate the removal of PVP from metal particles due to weaker metal-polymer bonds. The model catalytic materials prepared in this study are of both fundamental and practical interest to probe the effects of alloying. Using AuPd and AuPt, we investigate the consequences of alloying with Au on the reactivity of catalyst surfaces saturated with either chemisorbed CO* (CO oxidation) or O* (H2 oxidation) that bind strongly to Pt and Pd surfaces and inhibit rates. Singleton Pt-CO* bond energies, reflected in vibrational CO* stretches, were decoupled from dipole-dipole coupling effects using isotopic dilution methods, and were shown to decrease with increasing catalyst Au content. Despite lower CO* binding energies, CO oxidation turnover rates (normalized per metal surface atom) on AuPt catalysts decreased with increasing Au content. These results show that CO oxidation rates depend weakly on CO* binding energy—consistent with the reported structed insensivity of this reaction—and that Au acts primarily as an inert diluent of the active Pt ensembles required for catalysis. In contrast, H2 oxidation turnover rates (normalized per metal surface atom) on AuPt and AuPd catalysts increase with increasing Au content (up to 11 % at. Au content on AuPt and up to 67 % at. Au content on AuPd), indicating that the reactivity of O* saturated surfaces is more sensitive to changes in adsorbate binding energy than surfaces saturated in CO*, consisted with the reported structure sensitivity of reactions on O* saturated surfaces. Reconstruction of CO* adlayers is facile due to highly mobile CO* molecules, thus allowing CO* adlayers to access configurations that help mitigate strong CO* binding and introduce vacancies. O* adlayers, meanwhile, are more strongly bound to Pt and Pd metal surfaces and less mobile. H2 oxidation rates thus depend more strongly on adsorbate binding energy than CO oxidation rates.",ucb,,https://escholarship.org/uc/item/00w6r4tn,,,eng,REGULAR,0,0
103,1539,Regulation of isoprenoid precursor pathways in Listeria monocytogenes,"Lee, Eric David","Portnoy, Daniel A;",2019,"Listeria monocytogenes is a facultative, Gram-positive intracellular pathogen that is also a model organism for studying bacterial pathogenesis. Much of the appeal of using L. monocytogenes as a model organism derives from the fact that much of the knowledge gained from studying L. monocytogenes pathogenesis or metabolism can be applied to other pathogens that are more difficult to work with or manipulate. However, the unique aspects of L. monocytogenes biology are equally fascinating and still shed light on the host cell processes that help protect against infections.Isoprenoids are a diverse class of compounds produced by all forms of life and are synthesized from essential precursors derived from either the mevalonate pathway or the nonmevalonate pathway. Most organisms have one pathway or the other, but L. monocytogenes is unique because it encodes all of the genes from both pathways. Other scientists reported that the mevalonate pathway was essential for growth, but here we report that the nonmevalonate pathway was sufficient for growth anaerobically. Deleting the mevalonate pathway gene hmgR (∆hmgR) impaired bacterial growth in all L. monocytogenes strains studied, but the laboratory strain 10403S grew significantly slower than two lineage I strains, FSL N1-017 and HPB2262 Aureli 1997. The faster anaerobic growth of the lineage I strains was initially traced to a difference in the nonmevalonate pathway enzyme GcpE, and then chimeric proteins were constructed to precisely identify the molecular basis of this phenotype. Three amino acid residues, K291T, E293K, and V294A were shown to be necessary and sufficient to increase the anaerobic growth rate of 10403S ∆hmgR. However, these mutations did not map to GcpE in a way that provided any obvious mechanistic insights to how the enzyme function was altered. Even though the nonmevalonate pathway was found to function anaerobically, we were unable to identify conditions where deleting the nonmevalonate pathway impaired L. monocytogenes growth of any strain. Given the fitness cost of maintaining all seven genes in the nonmevalonate pathway, it is likely that there are yet unidentified anaerobic growth conditions that require the nonmevalonate pathway. This work showed that, contrary to previous reports, the mevalonate pathway is not essential for L. monocytogenes growth anaerobically, although it is still essential for growth aerobically.After finding specific mutations in GcpE that altered growth, we were also interested in understanding the broader networks regulating the nonmevalonate pathway in L. monocytogenes. The difference in anaerobic growth rates between 10403S ∆hmgR and lineage I ∆hmgR mutants indicated that it could be possible to find suppressor mutations that increased 10403S ∆hmgR growth rate. 10403S ∆hmgR cultures were passaged anaerobically, and fast-growing suppressor mutants began arising after three passages, which was noted by the observation that cultures initially required four or five days to reach stationary phase, but later only required two days to reach stationary phase. Whole genome sequencing of these cultures identified mutations in several genes, but the most common mutations were in the histidine kinase of a two-component system LisRK. We demonstrated that the suppressor mutations altered rather than disrupted LisRK signaling, but were unable to identify specific genes in the regulon that were responsible for the increased rate of anaerobic growth. This work showed that multiple mutations can be made that alter the rate of L. monocytogenes growth using the nonmevalonate pathway. The previously identified mutations in GcpE likely directly change enzyme kinetics, allowing for increased flux through the nonmevalonate pathway, but the additional mutations identified here likely change gene expression levels or alter metabolite concentrations at points upstream or downstream of the nonmevalonate pathway itself.",ucb,,https://escholarship.org/uc/item/00w7f1kg,,,eng,REGULAR,0,0
104,1540,The Complexity of Optimal Auction Design,"Pierrakos, Georgios","Papadimitriou, Christos H;",2013,"This dissertation provides a complexity-theoretic critique of Myerson's theorem, one of Mechanism Design's crown jewels, for which Myerson was awarded the 2007 Nobel Memorial Prize in Economic Sciences. This theorem gives a remarkably crisp solution to the problem faced by a monopolist wishing to sell a single item to a number of interested, rational bidders, whose valuations for the item are distributed independently according to some given distributions; the monopolist's goal is to design an auction that will maximize her expected revenue, while at the same time incentivizing the bidders to bid their true value for the item. Myerson solves this problem of designing the revenue-maximizing auction, through an elegant transformation of the valuation space, and a reduction to the problem of designing the social welfare-maximizing auction (i.e. allocating the item to the bidder who values it the most). This latter problem is well understood, and it admits a deterministic (i.e. the auctioneer does not have to flip any coins) and simple solution: the Vickrey (or second-price) auction. In the present dissertation we explore the trade-offs between the plausibility of this result and its tractability:First, we consider what happens as we shift away from the simple setting of Myerson to more complex settings, and, in particular, to the case of bidders with arbitrarily correlated valuations. Is a characterization as crisp and elegant as Myerson's still possible? In Chapter 2 we provide a negative answer: we show that, for three or more bidders, the problem of computing a deterministic, ex-post incentive compatible and individually rational auction that maximizes revenue is NP-complete --in fact, inapproximable. Even for the case of two bidders, where, as we show, the revenue-maximizing auction is easy to compute, it admits nonetheless no obvious natural interpretation a-la Myerson.Then, motivated by the subtle interplay between social welfare- and revenue-maximizing auctions implied by Myerson's theorem, we study the trade-off between those two objectives for various types of auctions. We show that, as one moves from the least plausible auction format to the most plausible one, the problem of reconciling revenue and welfare becomes less and less tractable. Indeed, if one is willing to settle for randomized solutions, then auctions that fare well with respect to both objectives simultaneously are possible, as shown by Myerson and Satterthwaite. For deterministic auctions on the other hand, we show in Chapter 3 that it is NP-hard to exactly compute the optimal trade-off (Pareto) curve between those two objectives. On the positive side, we show how this curve can be approximated within arbitrary precision for some settings of interest. Finally, when one is only allowed to use variants of the simple Vickrey auction, we show in Chapter 4 that there exist auctions that  achieve constant factor approximations of the optimal revenue and social welfare simultaneously.",ucb,,https://escholarship.org/uc/item/0132p6nt,,,eng,REGULAR,0,0
105,1541,Mercury Sensing with Optically Responsive Gold Nanoparticles,"James, Jay Zachary","Koshland, Catherine P;Fernandez-Pello, Carlos;",2012,"Mercury, a potent neurotoxin, is a global environmental problem. New mercury sensing technologies are needed to meet the expanding demands on the mercury observation network. We demonstrate the utility of gold nanoparticles as a stand-alone, inexpensive, and sensitive mercury monitor. Gold nanoparticles display a peak in the visible range of their UV-vis absorbance spectra due to localized surface plasmon resonance (LSPR). The energy of this resonance is affected by adsorption of mercury. Isolated individual gold nanorods with an average length and diameter of 60 and 20 nm saturate after adsorption of 4 attograms of elemental mercury, and produce a 3 nm blue shift in their LSPR, with the shift dependent on the surface-area-to-volume ratio and aspect ratio. A modified Gans theory model predicts the shift at saturation given the particle dimensions if saturation occurs with 45% monolayer coverage. Nanoparticle films on a transparent substrate show potential as a practical and robust method for sensing low levels of mercury vapor. The adsorption of 15 atoms of Hg causes a 1 nm shift in the LSPR wavelength of 5 nm gold spheres. The rate of shift in the peak absorbance is linear with mercury concentrations from 1 to 825 µgHg/mair3. The speed of the sensor response is limited by diffusive mass transfer and can be enhanced by optimizing the sample flow characteristics. We modeled the diffusive mass transfer and optimized the sample delivery accordingly. Regeneration of the sensing films, done by heating to 160°C, allows for repeatable measurements on the same film.",ucb,,https://escholarship.org/uc/item/0327r0g6,,,eng,REGULAR,0,0
106,1542,C0Rigidity in Hofer Geometry and Floer Theory,"Seyfaddini, Sobhan","Weinstein, Alan D;",2012,"This dissertation explores two instances of C0 rigidity in symplectic geometry: First, we prove that continuous Hamiltonian flows as defined by Oh and M\""uller have unique  generators.  Second, we study the behavior of certain Floer theoretic invariants of Hamiltonian flows, called spectral invariants, under  C0 perturbations of Hamiltonian flows.",ucb,,https://escholarship.org/uc/item/0385r0bk,,,eng,REGULAR,0,0
107,1543,Essays in International and Development Economics,"Li, Nicholas","Gourinchas, Pierre-Olivier;",2011,"Standard analysis of consumer behavior uses theory and evidence on quantities and prices to measure consumer welfare, particularly through the use of price indexes that capture the cost-of-living. In three essays I go beyond this standard analysis to analyze the impact of household variety, store variety, and energy requirements on food demand, consumption patterns and consumer welfare, with implications for several important topics in development and international economics. In the first essay I develop theoretical and empirical tools for analyzing variety-seeking behavior by households in a non-homothetic setting, showing how changes in the slope and intercept of ``variety Engel curves'' map into the level and distribution of consumer welfare in a way that is not captured by standard analysis. The ``cost'' of variety is introduced as an important concept for analysis that is distinct from price, e.g. the ``cost'' of quantity. The application to Indian food consumption indicates that increases in food variety over time and greater urban food variety are not attributable solely to income growth, with significant implications for both the size and distribution of welfare gains from variety. In the second essay, co-authored with Shari Eli, we continue to explore empirical patterns in Indian food consumption and draw attention to the decline in calorie intake at all expenditure levels. Once again the Engel curve is used as a guiding framework for the empirical analysis and once again there are changes in consumption patterns that are not explained by movements in prices and income alone. In this case the ``cost'' of calories is represented by the role of calories (as opposed to other food attributes) in satisfying household caloric requirements. Using data on time-use to impute caloric requirements and combining this with data on ownership of labor-saving durables, we find support for the hypothesis that a decline in energy requirements has had an important impact on variation in calorie intake over time and space, explaining most of the rural-urban discrepancy in calorie intake but only a modest share of the decline in calorie intake over time. In the third essay I examine product variety from the point of view of retailers rather than consumers, showing that differences in variety across different stores of a North American grocery retailer have large impacts on consumer welfare. I argue that these differences in variety can be largely attributed to differences in store size and international borders, and provide a stylized partial equilibrium model to analyze what factors influence the retailer's choice of store size in different locations. I find empirical support for the predictions of the model, with larger markets (higher income per capita, higher population density) leading to larger store size and variety while greater competition reduces store size and variety, with significant impacts on consumer welfare across different geographic areas.",ucb,,https://escholarship.org/uc/item/03j674h7,,,eng,REGULAR,0,0
108,1544,Spatial Frustration: Using Pharmacological and Mechanical Disruption of Membrane Receptor Transport to Probe Cell Signaling Dynamics on Supported Lipid Bilayers,"Petit, Rebecca","Groves, John T;",2013,"Micron-scale spatial reorganization of cell membrane receptor protein clusters is emerging as an important early modulator of inter- and intracellular signaling input. Detailed within is an in-vitro investigative technique utilizing cultured cells, nanolithograpically-patterned supports for fluid biomolecule-functionalized phospholipid bilayer membranes and inverted optical microscopy. This method allows the user to visualize and, if desired, restrict the formation and transport of these transient macroclusters as they coalesce on the membrane of a stimulated cell. Multi-color fluorescence imaging allows the simultaneous tracking of multiple molecules, permitting real-time analysis of coupling between surface receptors and intracellular signaling and structural proteins. The two systems examined are the T cell immunological synapse and ephrinA1-EphA2 interactions in highly invasive breast cancer cells. In both cases, it is demonstrated that induced restrictions of receptor-mediated cytoskeletal reorganization is sufficient to alter downstream signaling outcomes, and the molecular mechanisms of feedback coupling are investigated.",ucb,,https://escholarship.org/uc/item/06w9d9n5,,,eng,REGULAR,0,0
109,1545,Fragment-Based Identification of Phosphatase Inhibitors,"Rawls, Katherine Anne","Ellman, Jonathan A;",2010,"AbstractFragment-Based Identification of Phosphatase InhibitorsbyKatherine Anne RawlsDoctor of Philosophy in ChemistryUniversity of California, BerkeleyProfessor Jonathan A. Ellman, ChairChapter 1.  A new fragment-based method for the identification of phosphatase inhibitors, Substrate Activity Screening, is described.  Application of the method to Mycobacterium tuberculosis protein tyrosine phosphatase PtpB resulted in the identification of novel, nonpeptidic substrate scaffolds that were optimized by rapid analog synthesis and evaluation.  These substrate scaffolds were then converted to low molecular weight inhibitors for PtpB by incorporation of a variety of established phosphate mimetics, resulting in nanomolar affinity inhibitors that were highly selective for PtpB over mycobacterial and human phosphatases.Chapter 2.  The design and synthesis of new inhibitor analogs based on the scaffold identified in Chapter 1 is described.  The synthesis of more challenging inhibitor scaffolds was achieved, resulting in a panel of low molecular weight, nanomolar to micromolar affinity inhibitors for the Mycobacterium tuberculosis protein tyrosine phosphatase PtpB.  These compounds represent chemical tools for further dissection of the biochemical role of PtpB in tuberculosis infection.Chapter 3.  Application of the method described in Chapter 1 to Mycobacterium tuberculosis protein tyrosine phosphatase PtpA is described.  Inhibitors incorporating a well established phosphate mimetic were explored, resulting in compounds with low micromolar affinity for PtpA.  Modeling studies provided a rationale for the observed structure-activity relationships and guided further compound optimization.  The most potent compound was additionally shown to be selective for PtpA over a variety of human enzymes as well as the other Mycobacterium tuberculosis phosphatase PtpB.  This inhibitor represents a chemical tool that can be used in conjunction with the inhibitors described in Chapters 1-2 to further probe the role of PtpA and PtpB in tuberculosis infection, and to examine potential synergistic effects.Chapter 4.  The application of inhibitors developed in Chapters 1-3 to the pathogenic target Staphylococcus aureus, the causative agent of Staph infection, is described.  Several of the inhibitors described in Chapter 1 were found to have activity versus Staphylococcus aureus bacteria, prompting further probing of structure-activity relationships.  The synthesis of new analogs was realized by developing a new synthetic strategy to allow for rapid analog synthesis and evaluation.  The cellular target is postulated to be Staphylococcus aureus protein tyrosine phosphatases SaPtpA and SaPtpB, newly discovered enzymes which may play a role in pathogenesis.  Compounds were evaluated directly in cell assays, and the mechanism of action of these compounds, which show activity in Staphylococcus aureus strains that are resistant to traditional beta-lactam antibiotics, is under investigation.",ucb,,https://escholarship.org/uc/item/0723n24v,,,eng,REGULAR,0,0
110,1546,Measurement and Causal Inference in Patent Strategy,"Kuhn, Jeffrey Michael","de Figueiredo, Rui;",2017,"Determining the effect of patents on firms has long presented significant challenges to innovation scholars due to the specific and idiosyncratic nature of patent protection. This thesis develops new techniques for measuring the impact, technological relatedness, and extent of patents. It then describes two new approaches to performing causal inference on patent protection. Finally, it applies these new measures and causal instruments to investigate the effect of receiving a broader patent on patent sales and the effect of winning a patent race on follow-on innovation.Patent scope is central to the sale of ideas, which can spur economic growth and provide significant gains from trade. Awarding an inventor a patent on a new idea partially solves a commitment problem that would otherwise prevent the inventor from selling the idea. (Arrow, 1962). In the absence of a patent, a prospective buyer cannot credibly promise not to steal the idea should the inventor reveal it, while the inventor cannot credibly promise to reveal the idea should the prospective buyer pay for it. A firm's ability to use a particular patent to overcome this transactional hurdle derives from two factors: (1) the scope of the patent's legal right to exclude and (2) the effectiveness of that legal right in providing market exclusivity. This thesis first shows that a broader patent is more likely to be sold by employing a causal instrument that provides a plausibly exogenous shock to the scope of a patent's legal right to exclude, holding fixed the underlying idea. It then examines variation in the effectiveness of the right by interacting the instrument with endogenous firm, industry, and market characteristics. These results shed light on how firms profit from innovation and also connect the important but understudied market for patents, widely believed to be illiquid and inefficient, with fundamental research about how markets function in other contexts.Competition between firms to invent and patent an idea, or ""patent racing,""' has been much discussed in theory, but seldom analyzed empirically. This thesis introduces an empirical way to identify patent races, and provides the first broad-based view of them in the real world. It reveals that patent races are common, particularly in information-technology fields. The analysis is then extended to identify the causal impact of winning a patent race, using a regression-discontinuity approach. It shows that both the winners and losers of patent races typically receive patent protection, but that the winners receive much broader patent scope. It also shows that patent race winners do significantly more follow-on innovation, and the follow-on research that they do is more similar to what was covered by the patent.Underlying both of these analysis is a new measure of patent-to-patent similarity. Current measures of patent similarity rely on the manual classification of patents into taxonomies. This thesis describes and validates a machine-automated measure of patent-to-patent similarity developed by leveraging information retrieval theory and Big Data methods. It also demonstrates that the measure significantly improves upon existing patent classification systems.",ucb,,https://escholarship.org/uc/item/0791g94s,,,eng,REGULAR,0,0
111,1547,Peace Agreements as Counterinsurgency,"Brandt, Caroline M.","Mattes, Michaela;",2020,"Peace agreements are heralded as tools for ending civil war. However, exclusive peace agreements, accords that include only a subset of a conflict's warring parties, are unlikely to bring an end to a civil conflict. I argue that exclusive peace agreements serve a purpose beyond conflict resolution. Exclusive peace agreements are a counterinsurgency strategy.Combatting more than one rebel group strains governments’ military abilities by dividing resources across multiple wars. Governments that would otherwise be able to defeat a rebel group may be unable to do so when tied down by multiple insurgencies. Based on this logic, and in contrast to the literature on spoilers in peace processes, I argue that the threat posed by other insurgent groups increases the likelihood that a government and rebel group sign an exclusive negotiated settlement. Exclusive peace agreements allow governments to consolidate military resources into the fight against the remaining insurgency. Exclusive peace agreements can further strengthen a government's counterinsurgency capabilities by including provisions for military power-sharing that transform conflict adversaries into war-fighting allies. To test this hypothesis, I analyze all multiparty civil wars from 1975-2013. In support of the theory, I find that the threat posed by other rebel groups is positively correlated with the likelihood that a government and rebel group sign an exclusive peace accord. I then use a case study of civil war in the Southern Philippines to bring to light the mechanisms that undergird these correlations. In line with the theory, the threat posed by other rebel groups jump-started stalled peace talks with Moro insurgent groups. I also find that exclusive peace agreements were successful mechanisms for incorporating rebel soldiers into the government's fight against other rebel groups.",ucb,,https://escholarship.org/uc/item/0798055m,,,eng,REGULAR,0,0
112,1548,"Low Molecular Weight Organic Contaminants in Advanced Treatment: Occurrence, Treatment and Implications to Desalination and Water Reuse Systems","Agus, Eva","Sedlak, David L;",2011,"Water reuse and desalination are increasingly considered as viable sources of potable water because improvements in materials and designs have decreased the cost of reverse osmosis (RO) membranes and their operation. Although most contaminants are efficiently rejected by reverse osmosis membranes, compounds with neutral charge and low molecular weight have proven to be difficult to remove. Depending on the characteristics of the membrane and the feed water, some contaminants may be present in reverse osmosis permeate at concentrations that are high enough to compromise water quality. 	When chemical disinfection is applied in desalination systems, compounds that pose potential risks to the human health and aquatic ecosystems or impact the aesthetic quality of drinking water may be formed. In particular, several compounds of concern are produced when chlorine is used as pretreatment. The formation and speciation of chlorination byproducts in desalination systems is affected by the elevated concentrations of bromide and iodide in seawater and desalinated product water. To gain insight into byproducts most likely to be formed in desalination systems, disinfection byproduct formation studies conducted in saline source waters, coastal power stations and existing desalination systems were reviewed. These prior studies suggested that chlorine, chloramine and chlorine dioxide all pose potential risks in desalinated water systems. Chlorination of seawater intakes to prevent membrane fouling and disinfection of blended product water both pose potential risks to water quality.	To assess the formation and fate of chlorination byproducts under different conditions likely to be encountered in desalination systems, trihalomethanes, dihaloacetonitriles, haloacetic acids, and bromophenols were analyzed in water samples from a pilot-scale seawater desalination plant. In the pilot plant, the rejection of neutral, low-molecular-weight byproducts ranged from 45% to 92%, while charged species of similar molecular weights ranged between 77% to 97% rejection. Bench-scale chlorination experiments conducted on seawater from various locations indicated significant temporal and spatial variability for chlorination byproduct formation that could not be explained by bulk measurements of dissolved organic carbon concentration and UV absorbance.  	When desalinated water was blended with freshwater, elevated concentrations of bromide in the blended water enhanced dihaloacetonitrile formation through a shift in the active disinfecting agent from hypochlorous acid (HOCl) to hypobromous acid (HOBr). In most situations, data from the pilot plant and bench-scale studies indicated that chlorination byproducts formed from continuous chlorination of seawater or blending of desalinated water and freshwater will not compromise water quality or pose significant risks to aquatic ecosystems. However, blends of desalinated seawater with water rich in humic substances could lead to higher-than-expected production of haloacetonitriles and other chlorination byproducts.	When reverse osmosis is used for the treatment of municipal wastewater effluent, compounds that exhibit low taste and odor thresholds could compromise water quality. To assess potential for odors in wastewater effluent to compromise potable water reuse schemes, we evaluated odors in secondary effluent using flavor profile analysis and gas chromatography with olfactometry detection (GC/Olfactometry or GC/Olf). The primary odor reported in secondary effluent samples was classified as earthy/musty and was typically present at an intensity well above the odor threshold. Using GC/Olfactometry on samples prior to reverse osmosis, we identified sixteen peaks present at high intensity in more than 80% of the wastewater effluent samples. Odor descriptors reported in GC/Olfactometry analysis of secondary effluent were categorized as fragrant, sulfide, rancid, and hydrocarbon/chemical. Potential odorants associated with olfactometry peaks were identified by comparing the odorant with sensory descriptors and gas chromatography and mass spectrometry (GC/MS) analysis of an authentic standard of the putative compound. Other than organosulfide, aldehydes and volatile acid odorants previously identified in wastewater treatment, compounds including 2-pyrrolidone, lactones, chlorophenol, and vanillins were also identified as odorants associated with olfactometry peaks.	Potent odorous compounds were detected in secondary effluent by quantitative GC/MS. The most prominent compounds were 2,4,6-trichloroanisole (median concentration 9.5 ng/L) and geosmin (median 7 ng/L). Both compounds exhibited earthy/musty sensory profiles at these concentrations. During advanced treatment, olfactometry peaks exhibited variable fate depending on their abundance, molecular structures and odor thresholds. Reverse osmosis significantly decreased the concentrations of low molecular weight odorous compounds in wastewater, but did not eliminate all odors. Odor peaks were typically reduced to below their odor thresholds during advanced oxidation processes (i.e., UV/H2O2¬) but also in a system employing biologically activated carbon (BAC) with ozone pretreatment. Odors can be removed from secondary effluent by applying multiple barrier treatment trains that combine reverse osmosis or another physical treatment method with chemical oxidation.",ucb,,https://escholarship.org/uc/item/014004j9,,,eng,REGULAR,0,0
113,1549,Using morphology and structure to tune solid-state thermal properties,"Hippalgaonkar, Kedar","Grigoropoulos, Costas;",2013,"Diffusive phonon transport in nanostructured materials has been a subject of intense interest and micro-fabricated platforms have been used to measure the thermal conductivity of nanowires. In this work, we demonstrate how the limits of heat transport can be tested in three novel material systems by extending this platform to probe material structure and provide a direct correlation to their thermal properties.Phonons are lattice vibrations and their scattering in solids has largely been explained like collision of particles. Since the development of nanostructures, diffusive boundary scattering from large surface-to-volume ratio materials has been studied in nanowires and superlattices. To beat this diffusive scattering limit, we designed integrated silicon nanowires with rough surfaces with 30% reduction in thermal conductivity. Subsequently, we took a significant step further by making nanostructures with broadband roughness close to the dominant phonon wavelength (1-10 nm) at room temperature. The decrease in thermal conductivity of intrinsic silicon by a factor of ~30 from 140 W/m-K to 5 W/m-K in this sub-diffusive regime might be due to multiple scattering stemming from coherent phonon wave effects. Transmission Electron Microscopy (TEM) based techniques including three- dimensional tomography were then used to map out the morphology and find that we can reduce the thermal conductivity to as low as 1 W/m-K, while preserving the single-crystalline core, which is as low as that amorphous silicon or silica. Correlating the surface roughness and porosity to the measured thermal conductivity opens up a new paradigm to observing wave physics in thermal phonons at room temperature in nanomaterials.Secondly, the platform developed previously was extended to be compatible with TEM, allowing us to characterize the crystal structure of measured nanowires. While phonon optics experiments in the 1970s showed a crystallographic direction dependent thermal conductivity, we performed the first 1-1 mapping of nanowire growth direction and thermal conductivity in Bismuth Nanowires. In the boundary scattering regime with diameter 100 nm, a nanowire in the [-102] direction had k = 8.5 W/m-K, ~6 times higher than a nanowire in the [110] direction with k = 1.5 W/m-K.Finally, this thesis also studies tapered Vanadium Oxide beams to study asymmetric phonon physics that manifest in temperature dependent thermal rectification. The interplay between electrons and phonons and the possibility of asymmetric scattering rates prompted us to look closely at the existence of Metal-Insulator interfaces that could result in thermal rectification. Between 150K and 340K, the Vn O2n-1 phases could be either metallic or insulating with nanoscale domains. We performed high resolution Auger spectroscopy on single-crystal Vanadium Oxide beams that show a stoichiometry variation and measured thermal rectification as high as 22%. The rectification behavior turns off (<4%) once the whole beam reaches the insulating phase, higher than 340K.Our platform thus couples materials characterization, especially TEM, with thermal property measurement to enhance understanding of thermal phonons.",ucb,,https://escholarship.org/uc/item/01h2v1jw,,,eng,REGULAR,0,0
114,1550,Multiscale multichroic focal planes for measurements of the cosmic microwave background,"Cukierman, Ariel Jozef","Lee, Adrian T;",2018,"We report on the development of multiscale multichroic focal planes for measurements of the cosmic microwave background (CMB). A multichroic focal plane, i.e., one that consists of pixels that are simultaneously sensitive in multiple frequency bands, is an efficient architecture for increasing the sensitivity of an experiment as well as for disentangling the contamination due to galactic foregrounds, which is increasingly becoming the limiting factor in extracting cosmological information from CMB measurements. To achieve these goals, it is necessary to observe across a broad frequency range spanning roughly 30-350 GHz. Depending on the foreground complexity, it may become necessary to observe across an even larger range. For this purpose, the Berkeley CMB group has been developing multichroic pixels consisting of planar superconducting sinuous antennas coupled to extended hemispherical lenslets, which operate at sub-Kelvin temperatures. The sinuous antennas, microwave circuitry and the transition-edge-sensor (TES) bolometers to which they are coupled are integrated in a single lithographed wafer. We describe the design, fabrication, testing and performance of multichroic pixels with bandwidths of 3:1 and 4:1 across the entire frequency range of interest. Additionally, we report on a demonstration of multiscale pixels, i.e., pixels whose effective size changes as a function of frequency. This property keeps the beam width approximately constant across all frequencies, which in turn allows the sensitivity of the experiment to be optimal in every frequency band. We achieve this by creating phased arrays from neighboring lenslet-coupled sinuous antennas, where the size of each phased array is chosen independently for each frequency band. We describe the microwave circuitry in detail as well as the additional benefits of a multiscale architecture, e.g., mitigation of beam non-idealities, reduced readout requirements and polarization-wobble cancellation. Finally, we discuss the design and fabrication of the detector modules and focal-plane structures including cryogenic readout components, which enable the integration of our devices in current and future CMB experiments.",ucb,,https://escholarship.org/uc/item/01p6c0bx,,,eng,REGULAR,0,0
115,1551,How Can Child Labor Lead to an Increase in Human Capital of Child Laborers and What Are Policy Implications?,"Luong, Quoc Viet","Rausser, Gordon C.;",2011,"This dissertation attempts to answer three critical questions that have remained largely misunderstood in the literature of child labor. The first question is whether child labor can help child laborers gain more human capital, including both formal education and health status.  The second question focuses on the mechanisms through which child labor impacts human capital. It asks how a positive causal impact from child labor to human capital can possibly take place. The third question discusses policy implications. Given the gain in human capital of child laborers due to child labor, what are the unintended consequences of current policies and what can we do to effectively combat child labor and at the same time help child laborers acquire more human capital? Because these three questions are intrinsically related I find it more productive to present them in form of one major study rather than in three separate papers. To provide empirical evidence to the first question of whether child labor can help child laborers gain more human capital, I exploit a quasi-controlled experiment that took place between 2004-2009 in a poor rural area in Vietnam. Most children in this area were so poor that they dropped out of school prematurely. In order to help these children sustain their education, a non-governmental organization (NGO) decided to provide a cow to each poor household with school-aged children so that the children could spend time tending the cows, earn some income to pay for their schooling. Practically this intervention provided the children a means to convert their time into income.Due to limited resources, the NGO could provide cows to only a subset of the poor children, effectively creating a controlled experiment in which some of the children had work (the treatment group) and the others did not (the control group). Since the children were not randomized into the treatment and control groups, the main concern was the selection bias. An examination of the bias shows that the children were selected into the treatment group on the basis of most urgent needs - which means those determined to be more likely to drop out of school in absence of the treatment were selected to receive the cows. The data collected verified that at baseline those in the treatment had indeed acquired less education, had higher dropout ratio, were poorer, had less land, lived further away from school, and their parents had lower levels of education. All of these socio-economic indicators suggest that the selected children would have been more likely to drop out of school if the status-quo had continued. Since the selection bias (being more likely to drop out) works against the treatment effect (acquiring more education), estimates of the impact are likely to be the lower bounds of the true effect and should be valid. I find that the poor children who worked gained a significant average of 0.59 years of education over a period of 5 years compared to those who did not have any work opportunity. While the finding of a positive causal relationship between child labor and education is striking, this outcome per se is not very useful in terms of proposing new policy interventions because it does not explain how child labor results in more human capital. Imagine even if we have the luxury of running a perfect randomized controlled trial and the experiment shows that child labor leads to an increase in human capital, there remains a ""black box"". We still cannot explain how the positive impact takes place. Clearly unless we can explain what happen in the black box - unless we can explain with economic theories how child labor can positively affect human capital, we cannot construct informed policy interventions.This immediately leads to the second question: what are the mechanisms through which child labor can result in more human capital? To answer this question, I construct a theoretical model which examines how a household would choose optimal levels of human capital under the treatment (where children can work) and under the control (where children are not allowed to work). This framework shows that child labor affects child laborers' human capital through a positive income effect and a negative time cost. On one hand, child work brings home more income to acquire more education and consumption (a positive impact on health status). On the other hand, child labor takes away time, a necessary input for schooling. The most important finding is that while child labor always generates a positive income effect, its opportunity cost of time in terms of the education forgone can be zero, leading to a positive net effect. To see this, consider a household's choices of child labor and human capital as in a controlled experiment. Note that under the control when child labor is not allowed, the optimal level of schooling can be zero. For example, a hungry family that can afford only one meal per day would choose zero schooling, a costly expenditure in poor countries, in order to spend all income on food. In this case, the time cost of child labor in terms of forgone schooling is zero because in the absence of work children stay home anyway. When these hungry children can work, there are only two possibilities. First, they might choose to work full-time and spend all additional income on food. This case would lead to an increase in health status of the child laborers with no change in their education. Second, they might choose to work part-time and go to school part-time, using their additional income to pay for more food and more schooling. This case would lead to an increase in both their health status and education. The model shows that the income effect of child labor can dominate the time cost (because it can be zero), resulting in a positive net effect on human capital. The critical point that separates this research from the literature is that I use the amount of school time that would be chosen in the absence of work as the benchmark, not school time endowment, to measure the time cost of child labor.New answers to question 1 and 2 immediately bring up question 3: what are the unintended consequences of current policies and what can we do to effectively combat child labor and promote human capital? I find that current interventions such as trade sanctions, consumer boycotts, legal penalties or an outright ban against child labor, which would diminish or eliminate child work opportunity, would unambiguously reduce the human capital of the poorest child laborers. Note that child labor restricting policies are grounded on the belief that a loss in household welfare due to the loss of child labor income can be offset by an increase in child schooling due to reduced child labor. However, this study shows that by restricting child labor, these policies would reduce not only household welfare but also the schooling and physiological capital of the poorest children. Such instruments will have unintended consequences on children's human capital, the very point that they advocate for. Instead I find policies that make schooling more affordable such as such as reducing school fees, providing free meals and textbooks, providing cash transfer conditional on schooling, improving teacher/student ratios, improving curriculum, would simultaneously increase education and reduce child labor. In addition, I suggest new market-based policies that can enhance child laborers' education and health status without consuming additional public resources more than the status-quo. For example, encouraging the private sector to provide work to children with unemployed non-school time conditional on their school attendance would maximize education gain by capturing the income effect while excluding the substitution effect.My research adds to the literature in a number of ways. This is the first study to provide empirical evidence that child labor can lead to an increase in the human capital of child laborers. Moreover, this research is also the first to provide a theoretical framework that explains the mechanism at work - that is child laborers can gain more human capital from working because child labor always generates a positive income effect while its opportunity cost of time in terms of forgone education can be zero. Most importantly, my work suggests a need for a major overhaul of current policies that are adversely affecting hundreds of million poor children around the globe. Reducing child labor by enforcing interventions that restrict child work opportunities will have the exact unintended consequences of reducing the human capital of the poorest child laborers. The best way to get the more than 100 million hungry children worldwide out of work is to subsidize schooling (i.e. even pay them to go to school). Such a subsidized education in poorest countries, however, more often than not is practically out of the question. In this situation, the hungry children need, not less, but more work opportunities to buy more food, and at times also buy more education.",ucb,,https://escholarship.org/uc/item/07m2v2db,,,eng,REGULAR,0,0
116,1552,Inverse Modeling of Geological Heterogeneity for Goal-Oriented Aquifer Characterization,"Savoy, Heather Marie","Rubin, Yoram;",2017,"Characterizing the spatial heterogeneity of aquifer properties, particularly hydraulic conductivity, is paramount in groundwater modeling when the transport and fate of contaminants need to be predicted. The field of geostatistics has focused on describing this heterogeneity with spatial random functions. The field of stochastic hydrogeology uses these functions to incorporate uncertainty about the subsurface in groundwater modeling predictions. Bayesian inference can update prior knowledge about the spatial patterns of the subsurface (e.g. plausible ranges of values) with a variety of information (e.g. direct measurements of hydraulic conductivity as well as indirect measurements such as water table drawdown at an observation well) in order to yield posterior knowledge. This dissertation focuses on expanding the tools for Bayesian inference of these spatial random functions.First, the development of open-source software tools for guiding users through the Bayesian inference process are described. There is an desktop application that implements the Method of Anchored Distributions and is referred to as MAD#. It is built in a modular fashion such that it can be coupled with any geostatistical software and any numerical modeling software. This modularity allows for a wide variety of spatial random functions and subsurface processes to be incorporated in the Bayesian inference process. There is also a R package, called anchoredDistr, that supplements the MAD# software. While the MAD# software handles the communication between the geostatistical software and the numerical modeling software, the anchoredDistr package provides more flexibility in analyzing the results from MAD#. Since R is an open-source statistical computing language, the anchoredDistr package allows users to take advantage of the plethora of statistical tools in R to calculate the posterior knowledge in the Bayesian process. Although MAD# provides a post-processing module to calculate this posterior knowledge, it does not provide all of the options that the R community can provide for modifications. Second, the expansion of which kinds of data and knowledge can be incorporated into the Bayesian process is explored. Incorporating time series (e.g. the drawdown of a water table from pumping over time) as indirect data in Bayesian inference poses a computational problem referred to as the `curse of dimensionality'. Since each additional measurement in time is correlated with the measurements before and after it, the calculation of probability distributions of these data become multi-dimensional. A synthetic case study incorporating drawdown time series in the Bayesian inference process is explored. A second form of information, conceptual models of geology, is also explored with a synthetic case study. Conceptual models of geology (e.g. a graphical representation of assumed geologic layering) can be described with images. There is a geostatistical technique called Multipoint Statistics that uses images as its input. The synthetic case study provides a proof-of-concept example in which the Bayesian inference process can infer conceptual models of geology using Multipoint statistics. Third, the issue of devising spatial models with realistic geology while constraining the complexity of the model is explored. An aquifer analog is used as the basis for an example. An aquifer analog is a data set with data of hydraulic properties at high spatial resolution, i.e. much higher than expected for ordinary field measurements. The aquifer analog used in this dissertation has ten soil types distributed in three-dimensional space. The objective posed is to predict the early arrival time of a contaminant traveling through the analog. Given this prediction goal, the task is to simplify the analog into a simplified structure without changing the prediction outcome. The purpose of this exercise is to take a goal-oriented approach to defining a parsimonious spatial model for describing this complex aquifer analog such that a geostatistical model can be inferred for this kind of geology in a computationally efficient manner.   Ultimately, any uncertainty quantification regarding the spatial heterogeneity of subsurface properties has the goal of improving groundwater modeling prediction efforts. With the addition of freely available software tools, the ability to integrate more forms of information, and methodology for translating complex geological structures into parsimonious spatial models, the characterization of our groundwater resources improves.",ucb,,https://escholarship.org/uc/item/07n5t8pw,,,eng,REGULAR,0,0
117,1553,The Mechanism of Processivity and Directionality of Cytoplasmic Dynein,"Cleary, Frank Banta","Yildiz, Ahmet;",2014,"Cytoplasmic dynein is a dimeric motor that transports intracellular cargoes towards the minus-end of microtubules (MTs). In contrast to other processive motors, microtubule release of the dynein heads is not precisely coordinated and the mechanism of dynein processivity remains unclear. By engineering the mechanical and catalytic properties of the motor we show that dynein processivity minimally requires a single active motor domain (head) and a second inert MT binding domain. The AAA+ ring and the linker of the other head are dispensable, suggesting that processivity arises from a high ratio of MT bound to unbound time, and not from interhead communication. Additionally, nucleotide-dependent microtubule release is gated by tension on the linker domain.We find that dynein releases rapidly from the MT when force is applied in the forward direction (towards the minus-end), but remains bound under backward forces. This finding suggests that the asymmetric release properties of the microtubule binding domain (MTBD) control dynein directionality. Consistent with this hypothesis, replacing the MTBD with a catalytically inactive kinesin motor domain, which favors release towards the plus end, results in reversal of motor directionality. Furthermore, a dynein dimer maintains its minus-end directed motility when the released monomer is allowed to orient freely during the search for a new binding site. The results rule out directionality models based on a swinging mechanism of the linker domain. We propose that the mechanism of dynein directionality is fundamentally distinct from kinesin and myosin motors and determined by asymmetric release and binding properties of its MT binding interface. We develop a quantitative model for dynein stepping that reproduces the velocity and stepping characteristics of dynein motors and their response to chemical and mechanical perturbation.",ucb,,https://escholarship.org/uc/item/07t3x6xg,,,eng,REGULAR,0,0
118,1554,Symplectic approaches in geometric representation theory,"Jin, Xin","Nadler, David;",2015,"We study various topics lying in the crossroads of symplectic topology and geometric representation theory, with an emphasis on understanding central objects in geometric representation theory via approaches using Lagrangian branes and symplectomorphism groups. The first part of the dissertation focuses on a natural link between perverse sheaves and holomorphic Lagrangian branes. For a compact complex manifold $X$,  let $D_c^b(X)$ be the bounded derived category of constructible sheaves on $X$, and $Fuk(T^*X)$ be the Fukaya category of $T^*X$. A Lagrangian brane in $Fuk(T^*X)$ is holomorphic if the underlying Lagrangian submanifold is complex analytic in $T^*X_{\mathbb{C}}$, the holomorphic cotangent bundle of $X$. We prove that under the quasi-equivalence between $D^b_c(X)$ and $DFuk(T^*X)$ established by Nadler and Zaslow, holomorphic Lagrangian branes with appropriate grading correspond to perverse sheaves.The second part is motivated from general features of the braid group actions on derived category of constructible sheaves. For a semisimple Lie group $G_\mathbb{C}$ over $\mathbb{C}$, we study the homotopy type of the symplectomorphism group of the cotangent bundle of the flag variety and its relation to the braid group. We prove a homotopy equivalence between the two groups in the case of $G_\mathbb{C}=SL_3(\mathbb{C})$, under the $SU(3)$-equivariancy condition on symplectomorphisms.",ucb,,https://escholarship.org/uc/item/07t5s31b,,,eng,REGULAR,0,0
119,1555,Live Imaging of Segmentation Clock Dynamics in Zebrafish,"Shih, Nathan Pui-Yin","Amacher, Sharon L;",2013,"Segmentation is a developmental program in animals that generates semi-repetitive structures along the body axis. In vertebrates, somites are formed sequentially from the mesoderm of the extending tailbud, eventually giving rise to structures such as axial muscles and vertebrae. Somites form with great regularity, every thirty minutes in zebrafish. The periodicity of somite formation is controlled by a set of oscillating genes known as the segmentation clock. To better understand the dynamics of the clock and its role in patterning somites, I have explored its behavior through an in vivo clock reporter, her1:her1-venus. I show that individual cells in the presomitic mesoderm express oscillating clock expression, consistent with predictions made by mathematical modeling and analysis in fixed embryos. I am able to rapidly track a large number of PSM cells using novel semi-automated cell tracking and fluorescence quantification programs. Through these methods, I find that the clock oscillations are coordinated through the Notch pathway, and a loss of Notch function causes slower and desynchonized clock oscillations. I also explore the interaction of mitosis and the segmentation clock, and find the two processes are connected. Finally, I investigate the slowing of the segmentation clock in real-time, and find that oscillations in the anterior PSM are about twice the periodicity of somitogenesis. This has interesting implications for the role of the segmentation clock in patterning somites, including the potential to polarize somites. By studying the segmentation clock in real-time, I am able to better investigate and understand the mechanisms driving somitogenesis.",ucb,,https://escholarship.org/uc/item/0813z4j6,,,eng,REGULAR,0,0
120,1556,"Design, Synthesis, and Evaluation of Next Generation Technologies in Stimulus-Responsive Materials and Organic Electronics","Unruh, Jr, David Allen","Frechet, Jean M. J.;",2011,"Future advancements in technology are fundamentally limited by materials research and development.  New materials are most often found by exploring new chemistries, but the nature of these chemistries can vary widely.  This dissertation is a compilation of some of the chemical insights that can lead to the development of new stimulus-responsive materials and organic electronics.  For example, a tool that can consistently display specific, precise types of reactivity can have dramatic effects on the ability to make never-before seen materials.  Chapter 1 discusses a multifunctional mixed monolayer resist for scanning probe nanolithography that can respond to two different electrochemical stimuli to produce two chemically different products, each with nanoscale resolution.  The synthesis of a novel reductively active monolayer precursor, the preparation of this mixed monolayer surface, and its use in a proof-of-principle bottom-up assembly of complementary semiconductor components are described.  A combination of older chemistries can result in the development of new materials as well.  In Chapter 2, the synthesis of a thermally-triggered single component epoxy monomer is described.  A small library of these monomers is prepared, and structure-property relationships are established between monomer molecular structure and cure temperature, enthalpy of cure, and glass transition temperature.  In addition, effects of cure on the production of voids in the fully cured thermosetting polymer are investigated.  New materials can also be made by altering the way other materials pack together, as commonly found in supramolecular chemistry.  Chapter 3 discusses the pre-organization of small molecules into nanoscale crystalline domains as a means to establishing long range order in organic electronics.  A small molecule previously used in bulk heterojunciton organic photovoltaics is shown to make high aspect ratio nanowires through solution-phase nanocrystal synthesis, and the degree of crystallinity and device performance in organic field effect transistors are compared to the same molecule cast as a thin film.  In addition, different derivatives of the same molecule are evaluated for their ability to pack differently in the solid state.  Finally, new materials can be inspired by the limitations of existing materials and the challenges of an emerging technology.  Chapter 4 introduces a new thienooxypyrroline acceptor building block for donor-acceptor materials in organic photovoltaics and organic field effect transistors.  The synthetic sequence to access this chromophore is described, and a small library of thienooxypyrroline monomers are prepared.   Small molecules and polymers using these monomers are synthesized and shown to have promising device performance characteristics in preliminary testing of both OPVs and OFETs.  Based upon the device data, possible next steps are presented for use of this building block in future materials for organic electronics.",ucb,,https://escholarship.org/uc/item/081604mr,,,eng,REGULAR,0,0
121,1557,Kinetic isotope and trace element partitioning during calcite precipitation from aqueous solution,"Nielsen, Laura Christina","DePaolo, Donald J;",2012,"Precipitation of carbonate minerals is ubiquitous in the near-surface environment, and the isotopic and trace element composition of carbonates may be used to reconstruct the conditions of growth. Little is known about the mechanisms controlling isotope and trace element distribution into carbonates. Proposed growth mechanisms are typically inferred from the supersaturation dependence of CaCO3 precipitation rates. As different experimental techniques often generate different apparent reaction orders, numerous hypothesized mechanisms can be found in the literature. These descriptions of crystallization pathway cannot be used to identify processes controlling trace element and isotope partitioning. Recent advances in experimental methodology allow us to observe microscopic to nano-scopic structures at the mineral surface during growth. The mechanisms controlling calcite growth have been directly determined by using fluid flow cells placed in an atomic force microscope (AFM; chapter 3). Calcite growth below the threshold oversaturation for amorphous calcium carbonate (ACC) formation &ndash typical of seawater and most terrestrial fluids &ndash occurs primarily through the attachment of ions to kink sites on the surface. The net flux of ions to kink sites governs both overall growth rate and mineral composition. In this thesis, I present a self-consistent model based on observed calcite growth mechanisms that may be used to predict growth rate (chapter 1), and isotopic (chapter 2; Nielsen et al., 2012) and trace element (chapter 5; Nielsen et al., in prep.) partitioning as a function of solution composition. I apply this model to calcium isotope fractionation during the precipitation of synthetic calcite, which I grew and analyzed using novel secondary ion mass spectrometry (SIMS) methods (chapter 3). In chapter chapter 4 (Nielsen and DePaolo, in review), I model calcium isotope fractionation in carbonate minerals that I collected from the highly alkaline Mono Lake, CA. The mechanistic framework developed here may be extended to multicomponent systems, and may be adapted for use in reactive transport models. When interpreted through the lens of this model, trace element and isotope signatures preserved in carbonates may eventually be used to reconstruct the chemistry of natural aqueous fluids.",ucb,,https://escholarship.org/uc/item/03t4933z,,,eng,REGULAR,0,0
122,1558,Resonant Interactions of Surface and Internal Waves with Seabed Topography,"Couston, Louis-Alexandre","Alam, Mohammad-Reza;",2016,"This dissertation provides a fundamental understanding of water-wave transformations over seabed corrugations in the homogeneous as well as in the stratified ocean. Contrary to a flat or mildly sloped seabed, over which water waves can travel long distances undisturbed, a seabed with small periodic variations can strongly affect the propagation of water waves due to resonant wave-seabed interactions--a phenomenon with many potential applications.  Here, we investigate theoretically and with direct simulations several new types of wave transformations within the context of inviscid fluid theory, which are different than the classical wave Bragg reflection. Specifically, we show that surface waves traveling over seabed corrugations can become trapped and amplified, or deflected at a large angle ($\sim 90\degree$) relative to the incident direction of propagation. Wave trapping is obtained between two sets of parallel corrugations, and we demonstrate that the amplification mechanism is akin to the Fabry-Perot resonance of light waves in optics. Wave deflection requires three-dimensional and bi-chromatic corrugations and is obtained when the surface and corrugation wavenumber vectors satisfy a newly found class I$_2$ Bragg resonance condition. Internal waves propagating over seabed topography in a stratified fluid can exhibit similar wave trapping and deflection behaviors, but more surprising and intricate internal wave dynamics can also be obtained. Unlike surface waves, internal waves interacting with monochromatic seabed corrugations can simultaneously generate many new waves with different wavenumbers and directions of propagation--a phenomenon which we call chain resonance. Here, we demonstrate that the chain resonance leads to significant energy transfer from long internal waves to short internal waves for almost all angles of the incident waves relative to the orientation of the oblique seabed corrugations. Since short internal waves are prone to breaking, the resonance mechanism, therefore, may have important consequences on the spatial variability of ocean mixing and energy dissipation. Potential applications of the theory of resonant wave-seabed interactions for wave energy extraction and coastal protection are also discussed.",ucb,,https://escholarship.org/uc/item/04077138,,,eng,REGULAR,0,0
123,1559,On the Biosynthesis of Triacsins,"Twigg, Frederick Fairbank","Zhang, Wenjun;",2020,"Natural products are a source of engineering innovation and design for small molecules due to their relevance to wide swaths of the chemical sector including medical, agricultural, food and fragrance, and commodity chemical fields. Their structural complexity comprising of numerous chiral centers and an abundance of heteroatoms makes organic synthesis challenging, expensive, and generally infeasible. While combinatorial chemistry hoped to address these obstacles by enabling rapid diversification and screening methods, it is still limited by access to an initial scaffold on which to act upon. As such there is great potential for leveraging biosynthesis in combination with synthetic strategies to facilitate sustainable production of new bioactive compounds. In this manuscript we present our findings on the biosynthesis of NN bonds in the context of the triacsin natural product family. By elucidating the biosynthesis of a compound with multiple NN chemical bonds, we have discovered multiple enzymatic strategies employed by nature to create a linkage which is synthetically challenging due to the inherent nucleophilicity of nitrogen atoms. As such this research provides insight into the biogenesis of NN bonds and addresses the aforesaid synthetic challenges in chemical access to new bioactive compounds.Triacsins are notable for the conserved N-hydroxytriazene moiety that all members of the family bear. In addition to two sequential NN bonds, the terminal nitrogen itself is a member of an additional heteroatom-heteroatom linkage in the form of a nitrogen-oxygen bond. As with many stories in natural product biosynthesis, this manuscript begins with the genomic sequencing of the originally reported native producer of triacsins. Mutagenesis and isotopically labeled precursor feeding led to the identification of the essential genes required for triacsin biosynthesis and led to the discovery of another native triacsin-producing organism. Cultivation of mutant strains and analysis of organic extracts from said strains led to the structural characterization of a key late-stage chemical intermediate that informed the biochemical timing of N-hydroxytriazene biosynthesis. Subsequent in vitro reconstitution of several encoded enzymes has advanced our knowledge of biochemical strategies for NN bond biogenesis. Finally, nascent work on the detailed characterization of an NN bond-forming enzyme will provide a full mechanistic understanding of this catalytic transformation. Collectively, this work contributes to the biocatalytic formation of NN bonds.",ucb,,https://escholarship.org/uc/item/0414x28r,,,eng,REGULAR,0,0
124,1560,Inertial and Aerodynamic Tail Steering of a Meso-scale Legged Robot,"Kohut, Nicholas Jospeh","Fearing, Ronald S;O'Reilly, Oliver M;",2013,"Legged robots have excellent potential for all-terrain mobility, and are capable of many behaviors wheeled and tracked robots are unable to perform. However, legged robots have difficulty turning rapidly, or turning while running forward. For maximum maneuverability, terrestrial robots need to be able to turn precisely, quickly, and with a small radius. Previous efforts at turning in legged robots primarily have used leg force or velocity modulation. This work presents two novel methods of legged robot turning. The first of these methods is inertial tail turning, or using a rapidly actuated, weighted tail to cause a sudden change in angular momentum, turning the body of the robot. The tailed robot presented here is able to make rapid, precise turns. By rapidly rotating the tail as the robot runs forward, the robot was able to make sudden 90 degree turns at 360 degrees per second, making it the fastest turning legged robot known to the author at the time of this publication. Unlike other turning methods, this turn is performed with almost no change in forward running speed. The dynamics of this maneuver have also been modeled, to examine how features, such as tail length and mass, affect the robot's turning ability. This approach has produced turns with a radius of 0.4 body lengths at 3.2 body lengths per second running speed. Using a nonlinear feedback controller, turns with an accuracy of  5 degrees for a 60 degree turn have been achieved.The second method of turning presented here allows a legged robot to turn continuously while running at high speeds. This remains a difficult task for legged robots, but is crucial for maneuvering quickly in a real-world environment. SailRoACH is the first running robot that uses aerodynamic forces to turn. A flat plate serves as an aerodynamic surface, and depending on its position, can be used to impart positive or negative aerodynamic yaw torques on the robot as it runs forward, causing turns of up to 70 degrees per second at a radius of 1.2 meters and a running speed of 1.8 meters per second. A scale analysis of aerodynamic steering is also presented, showing this method is most effective for small robots. Comparisons to other steering methods are made, showing that inertial and aerodynamic steering are superior for high speed turns at high forward velocity, compared to existing methods.",ucb,,https://escholarship.org/uc/item/048207h4,,,eng,REGULAR,0,0
125,1561,Context in Constructions,"Lee-Goldman, Russell Rafael","Sweetser, Eve E.;",2011,"Traditional associations between syntax and grammar include the notions of anaphora, deixis, ellipsis, speech acts, and information structure. However, there exist numerous other layers of communicative organization, including the structure of conversation and turn-taking, quasi-ritualized interactions, and genre and register. Long recognized as analytically important categories in the fields of Conversation Analysis, contrastive pragmatics, Interactional Sociolinguistics, and others, they have been largely ignored in linguistic and especially syntactic theory. This study aims to begin an integration of formal and social/interactional approaches to linguistics from the perspective of a flexible and precise grammatical framework: Sign-based Construction Grammar.Through a series of close studies of grammatical constructions in English and Japanese, it is shown that grammatical structure and the interactional contexts in which language is used have a far closer and more integrated relationship than is usually assumed. I introduce the script as a way to capture the fact that not only can language reflect context, but context can exert a significance force on speakers' linguistic choices. The case studies proceed from relatively low-level contextual features to higher levels of organization, showing at each point the necessity to recognize grammatical constructions sensitive to that level of interaction.Chapter 1 introduces the question of the syntax-context interface. Chapters 2 and 3 present a syntactic representation for lexically- and constructionally-licensed argument omission (null instantiation). Aside from contextual features normally associated with ellipsis, it is seen that to fully account for argument omission it is necessary to incorporate into grammatical constructions references to a fine-grained categorization of speech acts and attitudinal and epistemic stances. Chapters 4 and 5 show that broader regions of conversational context are crucial to licensing constructions. Chapter 4 examines means of providing identification in situations where there is no visual contact (on the phone, at the front door). Chapter 5 illustrates the existence of Japanese and English constructions which function to project future linguistic actions. Chapter 6 extends the framework to consideration of persistent contextual features, namely kinship relations, and how these determine or influence linguistic choices in referring to other family members. Chapter 7 concludes and discusses directions for future work.",ucb,,https://escholarship.org/uc/item/04g389dr,,,eng,REGULAR,0,0
126,1562,Toward the Systematic Design of Complex Materials from Structural Motifs,"SMIDT, TESS Eleonora","Neaton, Jeffrey B;",2018,"With first-principles calculations based on density functional theory, we can predict with good accuracy the electronic ground state properties of a fixed arrangement of nuclei in a molecule or crystal. However, the potential of this formalism and approach is not fully utilized; most calculations are performed on experimentally determined structures and stoichiometric substitutions of those systems. This in part stems from the difficulty of systematically generating 3D geometries that are chemically valid under the complex interactions existing in materials. Designing materials is a bottleneck for computational materials exploration; there is a need for systematic design tools that can keep up with our calculation capacity. Identifying a higher level language to articulate designs at the atomic scale rather than simply points in 3D space can aid in developing these tools.Constituent atoms of materials tend to arrange in recognizable patterns with defined symmetry such as coordination polyhedra in transition metal oxides or subgroups of organic molecules; we call these structural motifs. In this thesis, we advance a variety of systematic strategies for understanding complex materials from structural motifs on the atomic scale with an eye towards future design.In collaboration with experiment, we introduce the harmonic honeycomb iridates with frustrated, spin-anisotropic magnetism. At the atomic level, the harmonic honeycomb iridates have identical local geometry where each iridium atom octahedrally coordinated by oxygen hosts a $J_{eff}=1/2$ spin state that experiences interactions in orthogonal spin directions from three neighboring iridium atoms. A homologous series of harmonic honeycomb can be constructed by changing the connectivity of their basic structural units.Also in collaboration with experiment, we investigate the metal-organic chalcogenide assembly [AgSePh]$_\infty$ that hosts 2D physics in a bulk 3D crystal. In this material, inorganic AgSe layers are scaffolded by organic phenyl ligands preventing the inorganic layers from strongly interacting. While bulk Ag$_2$Se is an indirect band gap semiconductor, [AgSePh]$_\infty$ has a direct band gap and photoluminesces blue. We propose that these hybrid systems present a promising alternative approach to exploring and controlling low-dimensional physics due to their ease of synthesis and robustness to the ambient environment, contrasting sharply with the difficulty of isolating and maintaining traditional low-dimensional materials such as graphene and MoS$_2$.Automated density functional theory via high throughput approaches are a promising means of identifying new materials with a given property. We automate a search for ferroelectric materials by integrating density functional theory calculations, crystal structure databases, symmetry tools, workflow software, and a custom analysis toolkit. Structural distortions that occur in the structural motifs of ferroelectrics give rise to a switchable spontaneous polarization. In ferroelectrics lattice, spin, and electronic degrees of freedom couple leading to exotic physical phenomena and making them technologically useful (e.g. non-volatile RAM). We also propose a new neural network architecture that encodes the symmetries of 3D Euclidean space for learning the structural motifs of atomic systems. We describe how these networks can be used to speed up important components of the computational materials discovery pipeline and generate hypothetical stable atomic structures.Finally, we conclude with a discussion of the materials design tools deep learning may enable and how these tools could be guided by the intuition of materials scientists.",ucb,,https://escholarship.org/uc/item/04g5p8ph,,,eng,REGULAR,0,0
127,1563,A Grammar of Chilliwack Halkomelem,"Galloway, Brent",,1977,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/08q2g527,,,eng,REGULAR,0,0
128,1564,Matrix Factorization and Matrix Concentration,"Mackey, Lester","Jordan, Michael I;",2012,"Motivated by the constrained factorization problems of sparse principal components analysis (PCA) for gene expression modeling, low-rank matrix completion for recommender systems, and robust matrix factorization for video surveillance, this dissertation explores the modeling, methodology, and theory of matrix factorization.We begin by exposing the theoretical and empirical shortcomings of standard deflation techniques for sparse PCA and developing alternative methodology more suitable for deflation with sparse ""pseudo-eigenvectors."" We then explicitly reformulate the sparse PCA optimization problem and derive a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.We next develop a fully Bayesian matrix completion framework for integrating the complementary approaches of discrete mixed membership modeling and continuous matrix factorization. We introduce two Mixed Membership Matrix Factorization (M3F) models, develop highly parallelizable Gibbs sampling inference procedures, and find that M3F is both more parsimonious and more accurate than state-of-the-art baselines on real-world collaborative filtering datasets.Our third contribution is Divide-Factor-Combine (DFC), a parallel divide-and-conquer framework for boosting the scalability of a matrix completion or robust matrix factorization algorithm while retaining its theoretical guarantees. Our experiments demonstrate the near-linear to super-linear speed-ups attainable with this approach, and our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.Finally, inspired by the analyses of matrix completion and randomized factorization procedures, we show how Stein's method of exchangeable pairs can be used to derive concentration inequalities for matrix-valued random elements. As an immediate consequence, we obtain analogues of classical moment inequalities and exponential tail inequalities for independent and dependent sums of random matrices. We moreover derive comparable concentration inequalities for self-bounding matrix functions of dependent random elements.",ucb,,https://escholarship.org/uc/item/08q2q9vw,,,eng,REGULAR,0,0
129,1565,Learning to Learn with Gradients,"Finn, Chelsea","Levine, Sergey;Abbeel, Pieter;",2018,"Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches.",ucb,,https://escholarship.org/uc/item/0987d4n3,,,eng,REGULAR,0,0
130,1566,Three Essays on Legitimacy and Organizational Outcomes,"Shin, Shoonchul","Haveman, Heather;",2016,"This research demonstrates that CEO dismissal can be one form of activism utilized by a board of directors against CEOs’ prior records of violating institutional logics (i.e., cultural beliefs, norms, and assumptions about appropriate conduct) especially during poor performance. To this end, it examines two organizational outcomes of CEO dismissal and post-dismissal strategic changes in large U.S. Fortune companies between 1984 and 2007. During this period, the field of large firms was characterized by the rise of a shareholder-value logic, in which maximizing shareholder returns was the ultimate goal of the firm and corporate refocusing and employment downsizing were appropriate and necessary means to that end. According to event-history analyses, poor performance generally led to an increase in the CEO dismissal rate, but this effect was even stronger for CEOs who were reluctant to refocus and downsize during their tenures in the position. Moreover, when their predecessors were dismissed, new CEOs were more inclined to refocus and downsize, especially during their early tenures. These results indicate that when firms deviating from logics perform poorly, directors attribute the performance problem to the deviation itself, and thus seek the removal of the CEO as a means of strategically reorienting towards prevailing logics. This study contributes to institutional theory and upper echelon research.",ucb,,https://escholarship.org/uc/item/0993h3dz,,,eng,REGULAR,0,0
131,1567,Unsupervised Models of Entity Reference Resolution,"Haghighi, Aria Delier","Klein, Dan;",2010,"A central aspect of natural language understanding consists of linking information across multiple sentences and even combining multiple sources (for example: articles, conversations, blogs and tweets). Understanding this global information structure requires identifying the people, objects, and events as they evolve over a discourse. While natural language processing (NLP) has made great progress on sentence-level tasks such as parsing and machine translation, far less progress has been made on the processing and understanding of large units of language such as a document or a conversation.The initial step in understanding discourse structure is to recognize the entities (people, artifacts, locations, and organizations) being discussed and track their refer- ences throughout. Entities are referred to in many ways: with proper names (""Barack Obama""), nominal descriptions (""the president""), and pronouns (""he"" or ""him""). Entity reference resolution is the task of deciding to which entity a textual mention refers.Entity reference resolution is influenced by a variety of constraints, including syntactic, discourse, and semantic constraints. Even some of the earliest work (Hobbs, 1977, 1979), has recognized that while syntactic and discourse constraints can be declaratively specified, semantic constraints are more elusive. While past work has successfully learned many of the syntactic and discourse cues, there has yet to be an entity reference resolution system that exploits semantic cues and operationalizes these observations into a coherent model.This dissertation presents unified statistical models for entity reference resolu- tion that can be learned in an unsupervised way (without labeled data) and models soft semantic constraints probabilistically along with hard grammatical constraints. While the linguistic insights which underlie this model have been observed in some of the earliest anaphora resolution literature (Hobbs, 1977, 1979), the machine learning techniques which allow these cues to be used collectively and effectively are relatively recent (Blei et al., 2003; Teh et al., 2006; Blei and Frazier, 2009). In particular, our models use recent insights into Bayesian non-parametric modeling (Teh et al., 2006) to effectively learn entity partition structure when the number of entities is not known ahead of time. The primary contribution of this dissertation is combining the linguistic observations of past researchers with modern structured machine learning techniques. The models presented herein yield state-of-the-art reference resolution results against other systems, supervised or unsupervised.",ucb,,https://escholarship.org/uc/item/09h6j8kp,,,eng,REGULAR,0,0
132,1568,"Relations Among Neighborhood, Parenting, and Effortful Control in Chinese American Children","Lee, Erica","Zhou, Qing;",2014,"Although an extensive literature links children's self-regulation abilities with both distal (e.g., neighborhood) and proximal (e.g., parenting) contexts, scarce attention has been paid to identifying potential cultural factors (e.g., ethnic density) in this association. Methodologically, most research has utilized cross-sectional data, with few studies examining the cumulative impact of multiple contexts on children's development and adjustment. Furthermore, it is unknown how these contexts are linked with different self-regulation outcomes among Asian American children. To address these gaps in the literature, I studied 258 first- and second- generation Chinese American immigrant children, using a combination of structured interviews, questionnaire measures, and behavioral tasks completed by children, parents, and teachers. Across two waves of data collection, I investigated the mediated and interactive relations of neighborhood (disadvantage, ethnic density) and parenting style (authoritarian, authoritative) to children's effortful control outcomes. Contrary to expectations, parenting style did not mediate the relation between neighborhood disadvantage and children's effortful control. However, children of families residing in neighborhoods with a higher Asian concentration were more likely to rate their parents as using an authoritarian parenting style, which subsequently predicted lower levels of effortful control. I also found that authoritative parenting for these children had a weaker benefit on their effortful control compared to children residing in less ethnically dense neighborhoods, as rated by their teachers. Taken together, these findings suggest that use of an authoritarian parenting style is more culturally normative in ethnically dense Asian American neighborhoods and may serve as a risk factor for poorer effortful control in this population. Additionally, residing in ethnically dense neighborhoods may confer some risk to Chinese American immigrant children's development of effortful control, regardless of their parents' parenting style. As children reach middle childhood, their perception of their parents' parenting style appears to play a role in determining their self-regulation outcomes, and this extends to both home and school contexts (i.e., both child and teacher reports of effortful control).",ucb,,https://escholarship.org/uc/item/09p297zj,,,eng,REGULAR,0,0
133,1569,"I. Seismic Moment Tensor Analysis of Micro-Earthquakes in an Evolving Fluid-Dominated System, II. Ambient Noise Cross-Correlation for Evaluating Velocity Structure and Instrument Orientations in a Geothermal Environment","Nayak, Avinash","Dreger, Douglas S;",2017,"This dissertation presents a detailed analysis of recorded seismic waves in terms of their source and their propagation through the Earth in multiple scenarios. First, I investigate the source mechanisms of some highly unusual seismic events associated with the formation of a large sinkhole at Napoleonville salt dome, Assumption Parish, Louisiana in August 2012. I implemented a grid-search approach for automatic detection, location and moment tensor inversion of these events. First, the effectiveness of this technique is demonstrated using low frequency (0.1-0.2 Hz) displacement waveforms and two simple 1D velocity models for the salt dome and the surrounding sedimentary strata for computation of Green’s functions in the preliminary analysis. In the revised, and more detailed analysis, I use Green’s functions computed using a finite-difference wave propagation method and a 3D velocity model that incorporates the currently known approximate geometry of the salt dome and the overlying anhydrite-gypsum cap rock, and features a large velocity contrast between the high velocity salt dome and low velocity sediments overlying and surrounding it. I developed a method for source-type-specific inversion of moment tensors utilizing long-period complete waveforms and first-motion polarities, which is useful for assessing confidence and uncertainties in the source-type characterization of seismic events. I also established an empirical method to rigorously assess uncertainties in the centroid location, MW and the source type of the events at the Napoleonville salt dome through changing network geometry, using the results of synthetic tests with real seismic noise. During 24-31 July 2012, the events with the best waveform fits are primarily located at the western edge of the salt dome at most probable depths of ~0.3-0.85 km, close to the horizontal positions of the cavern and the future sinkhole. The data are fit nearly equally well by opening crack moment tensors in the high velocity salt medium or by isotropic volume-increase moment tensors in the low velocity sediment layers. The addition of more stations further constrains the events to slightly shallower depths and to the lower velocity media just outside the salt dome with preferred isotropic volume-increase moment tensor solutions. I find that Green’s functions computed with the 3D velocity model generally result in better fit to the data than Green’s functions computed with the 1D velocity models, especially for the smaller amplitude tangential and vertical components, and result in better resolution of event locations and event source type. The dominant seismicity during 24- 31 July 2012 is characterized by the steady occurrence of seismic events with similar locations and moment tensor solutions at a near-characteristic inter-event time. The steady activity is sometimes interrupted by tremor-like sequences of multiple events in rapid succession, followed by quiet periods of little of no seismic activity, in turn followed by the resumption of seismicity with a reduced seismic moment-release rate. The dominant volume- increase moment tensor solutions and the steady features of the seismicity indicate a crack- valve-type source mechanism possibly driven by pressurized natural gas.Accurate and properly calibrated velocity models are essential for the recovery of correct seismic source mechanisms. I retrieved empirical Green’s functions in the frequency range ~ 0.2–0.9 Hz for interstation distances ranging from ~1 to ~30 km (~0.22 to ~6.5 times the wavelength) at The Geysers geothermal field, northern California, from cross-correlation of ambient seismic noise recorded by a wide variety of sensors. I directly compared noise- derived Green’s functions with normalized displacement waveforms of complete single-force synthetic Green’s functions computed with various 1D and 3D velocity models using the frequency-wavenumber integration method, and a 3D finite-difference wave propagation method, respectively. These comparisons provide an effective means of evaluating the suitability of different velocity models to different regions of The Geysers, and assessing the quality of the sensors and the noise cross-correlations. In the T-Tangential, R-Radial, Z- Vertical reference frame, the TT, RR, RZ, ZR and ZZ components (first component: force direction, second component: response direction) of noise-derived Green’s functions show clear surface-waves and even body-wave phases for many station pairs. They are also broadly consistent in phase and relative inter-component amplitudes with the synthetic Green’s functions for the known local seismic velocity structure that was derived primarily from body wave travel-time tomography, even at interstation distances less than one wavelength. I also found anomalous large amplitudes in TR, TZ, RT and ZT components of noise-derived Green’s functions at small interstation distances (≲4 km) that can be attributed to ~10°-30° sensor misalignments at many stations inferred from analysis of longer period teleseismic waveforms. After correcting for sensor misalignments, significant residual amplitudes in these components for some longer interstation distance (≳ 8 km) paths are better reproduced by the 3D velocity model than by the 1D models incorporating known values and fast axis directions of crack-induced shear-wave anisotropy in the geothermal field. I also analyzed the decay of Fourier spectral amplitudes of the TT component of the noise-derived Green’s functions at 0.72 Hz with distance in terms of geometrical spreading and attenuation. While there is considerable scatter in the amplitudes of noise-derived Green’s functions, the average decay is consistent with the decay expected from the amplitudes of synthetic Green’s functions and with the decay of tangential component local-earthquake ground-motion amplitudes with distance at the same frequency.",ucb,,https://escholarship.org/uc/item/09p8w3bj,,,eng,REGULAR,0,0
134,1570,Mousike and Mythos: The Role of Choral Performance in Later Euripidean Tragedy,"Weiss, Naomi Alison","Griffith, Mark;Kurke, Leslie;",2014,"This dissertation takes a new approach to the study of Greek theater by examining the dramatic function of mousike (music, song, dance) in the plays of Euripides. Previous scholarship has tended to see the many references to mousike in his later work only in connection with the ""New Music"" (the changes in musical style, language, and instruments in fifth-century Athens), and to disregard their place within the plays themselves, often deeming especially meta-musical choral odes to be irrelevant to the surrounding drama. In contrast, I explore the dynamics of choreia (choral song and dance) and the sociocultural meanings of different musical images in four plays to show how mousike plays a vital role in directing and complementing the movement of the plot. I demonstrate how Euripides uses traditional as well as new images of mousike, and argue that this combination of musical motifs is essential to an understanding of each play's dramatic structure.The dissertation is divided into four studies of individual plays, which span roughly the last fifteen years of Euripides' career. The first chapter focuses on Electra, the earliest extant tragedy to include multiple, extended descriptions of mousike. I argue that choreia both frames our understanding of Electra and has a generative power, anticipating and even enacting pivotal moments of the plot. In Chapter Two I examine how Hecuba and the chorus in Troades create the illusion of an absence of choreia, even while they sing and dance on stage, and liken this to the concept of ""embodied absence"" within Performance Studies. I also argue that the chorus' proclamation in the first stasimon that they will sing ""new songs"" refers not only to Euripides' experimentation at this point in his career, but to musical change within the drama itself. Chapter Three explores patterns of mousike and choreia in Helen, showing how the dominance of such imagery in the play's choral odes shapes the audience's understanding of Helen's relationship with the chorus. I suggest that the play's mousike creates an aetiology not only of Helen's cult in Sparta, but also of the Dionysiac performance of the chorus of Athenian citizens in the theater. Chapter Four examines the dynamics of chorality and monody in Iphigenia in Aulis, showing how, through the performance of mousike, the audience's attention is directed away from the panhellenic choreia of the parodos and toward the sacrifice of Iphigenia. I also explore how representations of instrumental mimesis provide a poignantly vivid impression of pastoral calm before the beginning of the Trojan War, and argue for the authenticity of contested lines at the end of the tragedy on the basis of their style of musical performance. Throughout the dissertation, my methodology centers on the idea that a complex interaction between described and performed mousike encourages the audience to see and hear a performance in a particular way--a form of aesthetic suggestion through choreia.",ucb,,https://escholarship.org/uc/item/09q2x7nq,,,eng,REGULAR,0,0
135,1571,Energy-Efficient 60GHz Phased-Array Design for Multi-Gb/s Communication Systems,"Kong, Lingkai","Alon, Elad;",2012,"Recent advance in wireless technologies has enabled rapid growth of mobile devices. Consequently, emerging applications for mobile devices have begun demanding data rates up to multiple Gb/s. Although advanced WiFi systems are approaching such data rates, the narrow bandwidth at ISM band fundamentally limits the achievable data-rate. Therefore, the unlicensed 7GHz of bandwidth at 60GHz band provides an opportunity to efficiently implement these communication systems with a potential to achieve $>$10Gb/s throughput. Besides the wider bandwidth, operating at higher frequency theoretically has higher achievable signal-to-noise ratio in area limited applications. This is because the maximum achievable antenna gain within limited aperture increases with frequency and it can be achieved using phased-array technique. This thesis therefore focuses on the design of 60GHz phased-array transceivers to support energy-efficient high data-rate communication systems.Despite the advantages of 60GHz, mobile applications often require low power consumption as well as low cost implementation, making the design of 60GHz phased-array systems challenging. Taking into account the limited power budget, this research investigates the design choices of the number of elements in phased-array transceivers, and identifies that the overhead power is the bottleneck of energy efficiency. In order to reduce the overhead power in the transmitter, a new architecture using a fast start-up oscillator is proposed, which eliminates the need of explicit modulator and 60GHz LO delivery. Measurements has shown that the transmitter efficiency is boosted by more than 2X. More importantly, the overhead power is significantly reduced down to 2mW, making this architecture a good candidate for large number phased-array. On the other hand, suffering from the similar overhead problem, the receiver unfortunately could not share the same architecture. A different architecture that stacks the mixer on top of LO generation is thus proposed to reduce the power consumption in the receiver. This approach demonstrated a 2X power reduction in receiver overhead, and the resulted optimum number of receiver elements is close to 4.Besides using CMOS technologies, on-chip antenna is also studied in order to further reduce the system cost. Slot-loop antenna is identified as a good candidate because that its intrinsic ground plane eases the integration with the rest of circuitry. Although the simulation shows an efficiency as high as $30\%$, the planar nature of the on-chip antenna limits its coverage in end-fire directions. Antenna diversity is thus proposed to overcome this limitation by utilizing multiple drive points on the same antenna. Because the antenna is fully integrated on-chip, antenna diversity can be implemented without extra high frequency I/Os, eliminating the loss that would be introduced otherwise.Using the proposed transceiver architectures, a 4-element phased-array with on-chip antennas was fabricated on TSMC's 65nm CMOS technology as a test vehicle. Consuming 50mW in the transmitter and 65mW in the receiver, this 10.4Gb/s phased-array covers a range larger than 45cm in all directions. This achieves a state-of-art energy-efficiency of 11pJ/bit. The 29mW/element power consumption also demonstrates the lowest power of a single phased-array element.",ucb,,https://escholarship.org/uc/item/09z098bd,,,eng,REGULAR,0,0
136,1572,"The Effects of Complex Optical Environments on the Development, Progression and Control of Myopia","Liu, Yue","Wildsoet, Christine;",2011,"Myopia or nearsightedness is a condition in which the axial length of the eye is too long relative to its optical focal length. This condition is reaching epidemic levels worldwide, and has become a tremendous public health burden. Myopia is one of the leading causes of vision loss and high myopia significantly increases the risk of permanent blindness. Consequently, myopia cannot be considered as a benign condition and early interventions aimed at slowing down or even stopping the progression of myopia rather than merely correcting the associated optical focusing error is of great importance. The consistent evidence from animal model studies showing that imposed hyperopic defocus, if sustained, comprises an effective myopogenic stimulus, accelerating eye growth, while imposed myopic defocus slows ocular elongation, has motivated clinical studies using bifocal and progressive addition spectacles for myopia control. While these studies, reviewed in Chapter 2 of this dissertation, failed to provide convincing evidence for a clinically significant treatment effect, i.e., slowed myopia progression; smaller scale, non-randomized studies using multifocal soft contact lenses and orthokeratology lenses have shown much more promising effects in terms of slowing myopia progression. However, the mechanism(s) underlying the latter anti-myopia treatment effects are poorly understood, thereby limiting their further refinement. In addition to the aforementioned systematic review of relevant clinical evidence for optical interventions for myopia control, this dissertation described 3 other studies using chick model, representing efforts to further understand how multifocal optical environments affect normal ocular development, specifically the process of emmetropization. These manipulations have also been used as a tool to investigate the mechanism underlying the myopia controlling effect of the novel contact lens applications referred above. Chapter 2 describes a systematic review and meta-analysis performed on randomized controlled trials investigating the effects of three traditional optical interventions, bifocal and progressive addition spectacles, and rigid contact lenses, all believed to control myopia progression, based on anecdotal evidence. The overall treatment effect was estimated to be only small and clinically insignificant. A number of reasons were proposed to explain the discrepancy between the results of related animal studies and these clinical trials - strong support for optical control of myopia from the former studies and inconclusive results from the latter studies. Poor compliance to the spectacle corrections in clinical studies, inaccurate measurement of adherence to the treatments, inadequate and potentially, ambiguous classification systems for clinical myopia, and potential flaws in the optical designs are likely to have contributed to the discouraging results from the clinical trials. Chapter 3 describes the first of three studies in which the chick model was used to test the effects on ocular growth of a series of custom-designed 2-zone multifocal ""spectacle"" lenses. This and the two follow-up studies using this model have the following merits; they made use of standardized experimental paradigms and objective methods of measurement to track ocular changes. The first study applied the lenses in a simple experimental paradigm, in which monocular lenses were attached to the normal eyes of young chickens. The 2-zone lenses included a plano zone, either in the center or peripheral surround, with either positive or negative power (+5 or -5 D) incorporated into the other zone. The size of the central zone was also allowed to vary, to control the size of the unifocal central and peripheral retinal areas, and intermediate multifocal zone, as was the placement of the powered zone, i.e. in the center or periphery of the lenses. Single vision lenses were included as a control treatment. Two important observations from this study were that 1) peripheral optical defocus can influence both peripheral (off-axis) and central (on-axis) refractive error development and 2) the inhibitory effect on axial ocular growth of myopic defocus imposed using 2-zone lenses (positive zones) could exceed that induced by single vision lenses of the same power. These results suggested complex interactions between adjacent retinal regions, with the peripheral retina apparently able to decode optical defocus, as well as complex interactions between the eyes own optical aberrations and those of the 2-zone lenses, which introduced large amounts of spherical aberration.  Chapter 4 described a closely related study in which subsets of the same 2-zone lenses were tested on eyes that had undergone surgical manipulations (sectioning of the ciliary nerve, CNX or iridectomy, ID) to investigate the influence of the pupil size of the eye as well as accommodation on the effects of the 2-zone lenses. Both were uncontrolled in the first study; yet create a dynamic optical system on which the multifocal optical environment was imposed. The ID surgery produced a fixed dilated pupil without any effect on accommodation (confirmed in another study, reported in Appendix I), while the CNX surgery produced a similarly enlarged pupil while also eliminating accommodation. This study revealed pupil size to be a critical factor in the treatment effects of 2-zone lenses, likely to reflect at least in part, its influence on the optical experience of various retinal zones (center to periphery), and also suggested a significant role of accommodation in the decoding of imposed optical defocus stimuli, in this case, complex multifocal optical stimuli. Chapter 5 described a third study which attempted to develop a clinically more relevant scenario; specifically, 2-zone lenses that incorporated two different negative powers (-5 & -10 D) in two optical zones (center & periphery or vice versa), were tested on both normal eyes and eyes made myopic before being fitted with one of the 2-zone lenses. The latter combination was intended to simulate the ocular conditions created when concentric multifocal contact lenses, i.e. with a near addition, are prescribed to human myopes, one of the novel, myopia control treatments currently being explored. When the 2-zone lenses were fitted to normal eyes, they induced myopia, of a magnitude falling between the values expected, had single vision lenses of the same powers been used. However, on myopic eyes, the lenses had a strong myopia inhibiting effect with the already induced myopia undergoing substantial regression. This result supports the further investigation of appropriately designed concentric multifocal contact lenses for the control of myopia progression. In summary, the studies reported in this dissertation indicate complex interactions between central and peripheral retinal regions in decoding and responding to complex defocus signals as well as critical influences of pupil size and accommodation on these processes. The strong and consistent inhibitory effects on ocular growth of concentric 2-zone lenses incorporating a zone of either positive power or a near addition, 2-zone designs lend plausibility to the notion of using custom-designed novel optical treatments for the control of myopia progression.",ucb,,https://escholarship.org/uc/item/0b6310p2,,,eng,REGULAR,0,0
137,1573,Exploring the Reliability and Validity of Pilot Teacher Ratings in a Large California School District,"Makkonen, Reino","Fuller, Bruce;",2013,"Many states and school districts have recently instituted revamped teacher evaluation policies in response to incentives from the federal government as well as a changing political climate favoring holding teachers accountable for the performance of their students. Many of these overhauls have mandated the incorporation of multiple performance indicators -- often including rubric-based classroom observation scores, estimated contributions to student test score outcomes, and surveys of students and parents -- into teacher evaluations. This three-paper dissertation explores the pilot implementation of a new standards-based multiple-measure teacher evaluation system in a large California school district in 2011/12. It examines both participants' views about the new system (particularly the challenges they faced and the early outcomes they felt were achieved), as well as the reliability and validity of the teacher observation ratings that resulted during pilot implementation. Results indicated that this self-selected group of pilot teachers and administrators generally appreciated the district's new teaching framework and pre/post-observation conferencing process, and participants also tended to report that certain key early outcomes were achieved, including increased reflection by teachers about their performance against the new teaching framework and better understandings of teachers' individual needs for instructional support (although a higher proportion of administrators than teachers reported that this latter outcome was achieved). Time constraints, staffing shortfalls and technology problems were all key challenges cited by both teachers and administrators during the pilot year. Analyses of the ratings for the small sample of participating teachers who received a complete set of observational focus element (item) scores from both of their raters across both observation cycles indicated that these teachers tended to be scored higher during the second cycle -- although such improvement wasn't universal -- and that across cycles the scores from second raters (who typically did not work at the school site) tended to be slightly lower than those awarded by the teachers' supervising site administrator. But ultimately, good agreement was evident between the primary and second raters who scored common teachers. Generalizability analyses indicated that approximately two-thirds of the variation in participating pilot teachers' total scores was attributable to systematic differences among teachers, while the variability associated with the observation cycle (approximately 25 percent) was larger than that associated with rater group (approximately 6 percent). These results were then used to forecast reliability coefficients based on different combinations of rater groups and observed lessons (cycles), and suggested that, based solely on pilot implementation and results from this particular analysis sample, varying the number of observations influenced reliability estimates far more than varying the number of observers. Finally, the group of participating pilot teachers who completed end-of-year surveys generally felt that the observations of their practice conducted during the pilot year represented a valid measure of their effectiveness, and pilot teachers' classroom observation-based ratings were not related to their ethnicity or the grade span they taught (factors that should theoretically be unrelated to performance). Low to moderate correlations were evident between pilot teachers' classroom observation-based ratings and their student survey ratings and value-added scores for the 2011/12 year. The uniqueness of this pilot context restricts the generalizability of these findings, however. The pilot consisted primarily of volunteers, and there was attrition during the pilot year -- approximately one-third of the teachers trained in fall 2011 never had any ratings entered online by an observer. In turn, the final pilot sample was comprised of a self selected group of experienced, mostly elementary school teachers who administrators from case study sites tended to characterize as particularly hard working and high performing. Moreover, our research team's limited capacity for qualitative data collection in spring 2012 (we were only able to visit five participating schools) and our low survey response rates (52 percent for teachers and 54 percent for administrators) also limit our ability to generalize findings more broadly. We did not hear the perspectives of those who dropped out of the pilot. Finally, the tools and processes under study were still being revised and fine-tuned by the district during the pilot year; observers were still learning the tools and teachers and administrators were just becoming familiar with the processes and measures. All told, these results likely do not reflect what will be found in any eventual full-scale roll out.",ucb,,https://escholarship.org/uc/item/0b63n98b,,,eng,REGULAR,0,0
138,1574,Media Aesthetics: Pergolesi's Stabat mater and its Circulation in the Long Eighteenth Century,"Meci, Jonathan Patrick","Mathew, Nicholas;",2019,"This dissertation takes as its starting point Clifford Siskin and William Warner’s recent contention that the Enlightenment is best understood as ""an event in the history of mediation,"" specifically the “proliferation” of ""new and newly important"" media that ""establish the conditions for the possibility of Enlightenment.""  Media for the circulation of music likewise proliferated during this period, resulting in the unprecedented mobility and iterability of musical works.  Giovanni Battista Pergolesi’s Stabat mater, as one of the most celebrated and copiously mediated works of the eighteenth century, offers a unique entry point into this changing mediascape and the effects of media proliferation on music discourse.  By following Pergolesi's Stabat on its global peregrinations, this dissertation surveys society's evolving relationship with new musical media, how these media became naturalized in different places, how different media interfaced with each other and how media proliferation reshaped generations of listeners’ musical experiences.   	 The first three chapters serve as archaeologies of three media particularly enmeshed in the reception of Pergolesi’s Stabat.  Chapter 1 focuses on the Neapolitan conservatory system, the medium through which the musical style that characterizes the Stabat was distilled and transmitted to Pergolesi along with his fellow students.  Not only did the conservatory system serve as a medium for the transmission of Neapolitan style, but it was also crucial in fostering a musical diaspora that enabled Neapolitan music to traverse the Alps and spread across Europe.  The second chapter examines Lenten public concerts and the new concert societies that reorganized local and supralocal sociabilities around musical performance.  Originating in a symbiotic relationship with the opera season, these concerts became a new media format in their own right.  By the end of the century, most cities in Europe had experimented with some form of “spiritual concert” and many concert organizations had also experimented with at least semi-annual performances of Pergolesi’s Stabat.  (A short intermezzo between this and the next chapter looks at the influence of Pergolesi’s example on other Stabat settings, exploring how composers managed Pergolesi’s legacy through reference and allusion).  The third chapter follows a parallel media development to that of the second chapter: the emergence of the new genre of composer biography.  The popularity of Pergolesi’s music and the misfortune of his early death drove interest in his life.  Biography offered writers and readers an opportunity to use the character of Pergolesi to (re)imagine musical communication, musical labor and musical history in ways that addressed music’s increasing mobility and iterability.  	In the process of excavating these media forms, a reoccurring theme is the material underpinning of the Stabat’s exceptional and novel fame.  Contemporary writers heaped praise on the expressivity of the Stabat.  Rather than demonstrating the origin of this unparalleled expressivity in the music itself, the reception of the Stabat strongly indicates that its vaunted ability to depict and illicit feeling was nothing less than the sum of the sentimental valences that accrued around the work in the process of its constant mediation.  The mediacy of the work, stemming from the peculiarities of Neapolitan style, allowed it to accumulate sentimental meaning, while allowing it to factor conspicuously in aesthetic debates concerning music’s new mobility and iterability.  Its early entrance into cosmopolitan circulation, though, ensured an ambiguous position in discourses surrounding notions of progress, emergent nationalism, ideas of canon formation as well as concerns over sacred music’s religious propriety and music’s gradual commercialization.  The final chapter investigates the aesthetics of media saturation, just as the first three explore those of media proliferation.  With the naturalization of once new media, the currency of the Stabat’s mediacy became devalued, initiating a decline in prestige.  But even as the Stabat’s European reputation waned, media proliferation into Europe’s colonies brought the Stabat into contact with non-European music, prompting a clash between competing aesthetic values.",ucb,,https://escholarship.org/uc/item/0bd6m68n,,,eng,REGULAR,0,0
139,1575,Some Periodic Solutions of the Two-Dimensional Stokes-Oldroyd-B System with Stress Diffusion,"Isaacson, Erica Amy","Wilkening, Jon A;",2012,"We use a limited memory BFGS optimization method to seek time-periodic solutions of the Stokes-Oldroyd-B system of equations with a 4-roller forcing field and periodic boundary conditions. The gradient of the objective function for the optimization is found using a method which is based on the calculus of variations, and employs a pseudo-spectral implicit-explicit Runge-Kutta scheme.  Once solutions are found, their  asymptotic stability is calculated via an eigenvalue method.  A variety of stationary and periodic solutions are found, plotted and systematized in a manner that suggests a global structure of periodic solutions.",ucb,,https://escholarship.org/uc/item/0bp939gf,,,eng,REGULAR,0,0
140,1576,Queer Kinship: An Exploration of the Rewards and Challenges of Planned Parenting among Gay Fathers,"Barr, Ben-David","Gambrill, Eileen;",2011,"Gay fathers are creating family forms and parenting practices that reach beyond the nuclear family model. Analysis suggests that fathers in this study are developing unique and queer versions of kinship. Fathers' desire for emotional connection leads to the creative assemblage of paid caregivers, friends, children's non-legal biological kin, and gay men's families of origin into kinship networks. These creative mixtures may be perceived as unusual family formations, but they assist gay fathers in creating social support and connected lives for themselves and their children. These findings are based on a qualitative research project that consisted of interviews with 15 gay fathers who resided in 8 households and who were raising 13 children. The participants were all self-identified gay men who had formed planned families outside of heterosexual relationships. Research aims included: To explore the lived experience of gay men raising children; To explore how gay fathers adapt concepts of kinship; To describe the role of social support in the lives of gay fathers; To present emergent and unanticipated topics.  Data-collection methods included development of a genogram that described each family and their social support networks. In-depth interviews based on a semi-structured interview guide were then conducted with participants. Data analysis techniques were based within a grounded theory framework. Analysis resulted in development of 47 repeating ideas, which were then organized into nine themes: 1) Methods of family formation and Desire to parent; 2) Challenges of parenting; 3) Rewards of parenting; 4) Kinship is about connection; 5) Biology is less and more important than I thought; 6) Importance of non-kin social supports; 7) Changes in sense of connection to the gay community; 8) I always knew I would be a dad; and 9) Experiences with social welfare institutions. Implications and recommendations for future research and practice are included.",ucb,,https://escholarship.org/uc/item/0c52x7tp,,,eng,REGULAR,0,0
141,1577,"Self-presentation, Interpersonal Perception, and Partner Selection in Computer-mediated Relationship Formation","Fiore, Andrew Rocco Tresolini","Cheshire, Coye;",2010,"The use of social and technological intermediaries to seek intimate partners has a long history. Yet the affordances and limitations of modern computer-mediated communication (CMC) systems built for this purpose — specifically, online dating sites — present new challenges and opportunities for those who use them to initiate intimate relationships. The sheer number of potential mates available on such sites is tremendous, but accurately gauging their appeal and suitability for a relationship can be difficult through CMC.This dissertation presents a longitudinal survey of users of a major U.S. online dating service as they interact with potential dates online, meet them in person, and in some cases establish intimate relationships. The survey addresses two research questions: how interpersonal perceptions change when online daters meet in person for the first time, and how online and offline perceptions are associated with relationship duration, satisfaction, and intimacy.With respect to the first research question, I find that on average measures of liking and willingness to enter into a romantic relationship decline after participants meet their dates face-to-face for the first time. This result held for both inexperienced and experienced online daters. With regard to the second research question, I find that participants' perceptions of their dates before they have met in person generally do not predict the duration of the subsequent relationship, if any. However, their perceptions on many dimensions shortly after meeting in person are significantly associated with relationship duration. At the same time, among those who do begin dating, perceptions on numerous dimensions both pre-meeting and post-meeting are associated with intimacy and, to a lesser degree, relationship satisfaction in the weeks after the first date. That is, it appears that initial judgments from online interaction do not predict whether a couple will form a relationship, but these judgments do predict metrics of relationship quality if they choose to do so.",ucb,,https://escholarship.org/uc/item/0cd9r9ws,,,eng,REGULAR,0,0
142,1578,New Transcriptional Regulators of Non-shivering Thermogenesis,"Dempersmier, Jon Michael","Sul, Hei Sook;",2015,"Unlike white adipose tissue (WAT), which stores excess energy as triglycerides, brown adipose tissue (BAT) burns fatty acids and glucose to produce heat. The thermogenic ability of BAT is due to the specialized inner mitochondrial proton transporter named uncoupling protein 1 (UCP1), which dissipates the proton motive force generated by the electron transport chain to create heat instead of ATP.  Despite data suggesting that increasing BAT activity may be a promising antiobesity therapy, an inclusive model of the transcriptional regulation of thermogenic genes remain unclear. The aim of this dissertation work was to identify and characterize novel regulators of the UCP1 promoter and nonshivering thermogenesis. 	Chapter 1 reviews BAT in both mice and human, profiling the basic mechanism of uncoupled respiration and cold-induced nonshivering thermogenesis. Unlike classical BAT, which has constitutive UCP1 expression, brown adipocyte-like cells arise in WAT depots following prolonged cold exposure and contribute to whole body thermogenic capacity. While having similar functions, these cells arise from different precursor populations, having unique gene signatures and potentially depot specific regulation. Human BAT resembles either classical BAT or brown adipocyte-like cells in a depot specific manner, with differing levels of basal UCP1 expression and expression profiles. Finally, known transcriptional and hormonal regulators of BAT are discussed.	Chapter 2 profiles my screening efforts to identify novel transcriptional regulators of the UCP1. Briefly, a library of over 1100 transcription factors was screened for activation of the UCP1 promoter. Expression profiling of the positive factors identified 6 novel, brown fat enriched transcriptional activators of UCP1. The first such transcription factor identified was the previously uncharacterized C2H2 type zinc-finger protein, Zfp516.  Zfp516 is induced by cold where it binds and activates a brown fat gene program. Zfp516 ablation is embryonic lethal, but Zfp516 knockout embryos have little to no UCP1 expression and aberrant morphology. On the other hand, adipose specific transgenic overexpression in aP2-Zfp516 resulted in marked browning of inguinal WAT, increased body temperature and whole body energy expenditure, and prevention of diet-induced obesity.	Chapter 3 profiles a second transcription factor identified in my screening efforts, the CCCH-type zinc finger protein, Zc3h10.  Zc3h10, together with multiple cofactors, binds and activates the distal UCP1 promoter. Ablation of Zc3h10 results in defective BAT differentiation in cells, while adipose specific transgenic overexpression in aP2-Zc3h10 mice results in a lean phenotype.	Finally, chapter 4 concludes this work, discussing my findings in the context of the field of brown adipocyte biology and presents future directions and remaining questions.	This study has identified novel transcriptional regulators of UCP1, contributing significantly to the understanding of brown adipocyte biology and nonshivering thermogenesis, and providing new targets for future antiobesity therapeutics.",ucb,,https://escholarship.org/uc/item/0cf4h6j1,,,eng,REGULAR,0,0
143,1579,Integration Of Locational Decisions with the Household Activity Pattern Problem and Its Applications in Transportation Sustainability,"Kang, Jee E",,2013,"Examining the Cycle: How Perceived and Actual Bicycling Risk Influence Cylcing Frequency, Roadway Design Preferences, and Support for Cycling Among Bay Area Residents",ucb,,https://escholarship.org/uc/item/0cm3b1gq,,,eng,REGULAR,0,0
144,1580,Nanomagnetism research: benefit from reduced dimensionality and interfaces,"Wu, Jie","Qiu, Ziqiang;",2010,"Along the effort of integrating the spin degree of freedom in electronic devices, magnetic structures at the nanometer scale are intensely studied because of their importance in both fundamental research and technological applications. In this dissertation, I present my Ph.D research on several subjects to reflect the broad topics of nanomagnetism research. Single-crystalline, magnetic, ultrathin films are synthesized by Molecular Beam Epitaxy (MBE) and measured by state-of-art techniques such as Magneto-Optic Kerr Effect (MOKE), Photoemission Electron Microscopy (PEEM), X-ray Circular and Linear Dichriosm (XMCD and XMLD) Spectroscopy. First, I will present my work on the quantum well state in metallic thin films. Second, I will present my study on the magnetic long range order in two-dimensional magnetic systems, particularly on the observation of stripe and bubble magnetic phases and the universal laws governing the stripe-to-bubble phase transition. Third, I will present my result on a new type of magnetic anisotropy resulting from the spin frustration at ferromagnetic/antiferromagnetic interfaces. Fourth, I will present our studies on the mechanism of the abnormal interlayer coupling in ferromagnet/antiferomagnet/ferromagnet sandwiches structure. Fifth, I will show a new method to control the oxidation process to realize the control of exchange bias. Sixth, I will revisit the topic of exchange bias and show that the exchange bias actually takes place even before the antiferromagentic spins are frozen. In the last chapter, I will summarize my research and discuss the future of this exciting field.",ucb,,https://escholarship.org/uc/item/0cm863nn,,,eng,REGULAR,0,0
145,1581,"Metapopulations in miniature: connectivity, subpopulation extinction, and recovery in microbial microcosms","Kurkjian, Helen","Simms, Ellen L;",2018,"Metapopulations occupy spatially divided habitats and understanding how that fragmentation affects their survival, growth, dispersal, and persistence is critical to their conservation.  Researchers in many sub-fields of ecology and evolutionary biology test hypotheses relating to metapopulation dynamics and landscape spatial structure. Key aspects of these hypotheses are sometimes (a) large numbers of subpopulations and dispersal corridors and (b) their positions relative to each other.  Comparing such spatial hypotheses using traditional lab equipment and methods is impractical, unwieldy, expensive, or impossible.I invented the Metapopulation Microcosm Plate (MMP) to overcome these drawbacks. This device resembles a 96-well microtiter plate; the 96 wells represent habitat patches and they are connected by dispersal corridors that can be modified in their spatial position to create various artificial landscapes, with hundreds of non-intersecting dispersal corridors of varying lengths.  The device can be filled with nutrient broth and used to culture microbial metapopulations.In Chapter One, I first demonstrate that bacterial travel time is significantly faster through MMP dispersal corridors that are shorter, but is unaffected by corridor vertical position within the plate.  Thus, MMPs satisfy the necessary assumptions for use in metapopulation experiments.  Furthermore, travel time by bacteria with fully functional flagella was significantly faster than that of bacteria with disabled flagella, indicating that the bacteria actively swim through the corridors, rather than traveling by simple diffusion. Thus, MMPs can test hypotheses that account for behavioral responses.  MMPs can be used to test many spatial hypotheses that have previously been prohibitively difficult to test. Further, by incorporating individual behavioral responses to within-patch conditions, MMPs incorporate greater realism than do directed pipetting or other artificial dispersal methods.In Chapter Two, I used MMPs to explore how recolonization and recovery after subpopulation extinction differs in metapopulations in which the dispersal corridors have different spatial arrangements.  Some metapopulations have corridors spread relatively evenly through space in a homogeneous arrangement such that most subpopulations are connected to a few neighbors, while others have corridors clustered in a heterogeneous arrangement, creating a few highly connected subpopulations and leaving most subpopulations with only one or two neighbors.  Graph theory and empirical data from other biological and non-biological networks suggest that heterogeneous metapopulations should be the most robust to subpopulation extinction.  Here, I compared the recovery of metapopulations with homogeneous and heterogeneous corridor arrangements following small, medium, and large subpopulation extinction events.  I found that while metapopulations with heterogeneous corridor arrangements had the fastest rates of recovery following extinction events of all sizes and had the shortest absolute time to recovery following medium-sized extinction events, metapopulations with homogeneous corridor arrangements had the shortest time to recovery following the smallest extinction events.  Finally, for Chapter Three I conducted an experiment to test whether metapopulations with heterogeneous corridor arrangements recover more slowly from extinctions targeted at high connectivity subpopulations than random extinctions in low connectivity subpopulations.  Simulations of the World Wide Web and other heterogeneous networks have demonstrated that, while they are very robust to random loss of nodes, targeted attacks on highly connected nodes can lead to failure of the entire network.  Based on these simulations, I predicted that metapopulations with heterogeneous corridors would recover fastest when extinctions occurred in low connectivity wells, regardless of extinction event size.  Unlike in theoretical networks, however, the corridor arrangements of metapopulations cultured in MMPs cannot be completely homogeneous, because wells on the edge will be slightly less connected than those in the center.  However, I predicted that a small deviation in connectivity would be unimportant and that recovery in metapopulations with homogeneous corridors would not be affected by whether extinctions were in low connectivity or high connectivity wells.  Instead, I found that, at both low and medium levels of extinction targeted at highly connected subpopulations, both heterogeneous and homogeneous metapopulations recovered more quickly when those extinctions were targeted at high connectivity wells, but that when many subpopulations went extinct, all metapopulations recovered fastest when those extinctions were in low connectivity wells. This work demonstrates that MMPs can be used to test the assumptions of metapopulation theory, especially those involving large numbers of subpopulations and dispersal corridors.  I have shown that metapopulations with heterogeneous corridor arrangements have the fastest rates of recovery from subpopulation extinction, but that that faster rate only translates to a shorter absolute time of recovery after larger extinction events.  Furthermore, when smaller numbers of subpopulations go extinct, metapopulations recover more quickly when those extinctions are targeted at high connectivity subpopulations, but when large numbers of subpopulations go extinct, recovery is faster when low connectivity subpopulations are targeted.  This suggests that dispersal corridors that are clustered in space may help to alleviate the effects of habitat fragmentation in some circumstances, but exacerbate them in others.",ucb,,https://escholarship.org/uc/item/0d11302q,,,eng,REGULAR,0,0
146,1582,Exotic Species and Temporal Variation in Hawaiian Floral Visitation Networks,"Imamura, Jennifer Lynn","Roderick, George;",2019,"Many studies have documented the negative impact of invasive species on populations, communities, and ecosystems, although most have focused solely on antagonistic rather than mutualistic interactions.  For mutualistic interactions, such as pollination, a key to understanding their impacts is how invasive species interact with native species and alter interaction networks.  Chapter 1 explores the impacts of invasive species on islands, particularly in regard to plants, pollinators, and how these exotic species attach to existing pollination interaction networks.  Island pollination networks differ from mainland counterparts in several important characteristics, including fewer species, more connectance, and increased vulnerability to both invasion and extinction.  A progression of invasion has been previously proposed, through which supergeneralist native species facilitate the entry of new exotic species, then are eventually replaced by a few supergeneralist invader species that ultimately dominate the interaction networks.  As a result, highly-linked exotic supergeneralists become central nodes in the networks, thus altering network topology and community structure and functioning.  Here, I evaluate the evidence for (1) native supergeneralists that provide attachment points for exotic species, (2) exotic supergeneralists that are potentially replacing the function of native species, and (3) the consequences for the replacement of native species with exotics.  Both native and exotic supergeneralist species are found on islands, which may therefore represent different points along the invasion trajectory, with consequent concerns for future conservation. Chapter 2 utilizes a long-term series of observed floral visitations to break apart the potential differences between plants and pollinators as invaders of a community.  When plants are introduced into a new environment, their reproductive success can be limited by the lack of a suitable pollinator.  If there is no suitable native pollinator, the success of exotic plants may depend on the presence of exotic pollinators, a situation mirrored for exotic plant visitors.  Yet, rarely are the distinct roles for native and non-native species of both plants and pollinators examined in the same community. This study examines the role of exotic plants and insects in floral visitation networks in Hawaii, in simple ecological communities with a depauperate native pollinator fauna.  On the island of Hawaii, in sites that differed with respect to the presence of exotic plants, floral visitors were observed and quantified across multiple years and seasons.  Where exotic plants were present, exotic insects were observed to visit both native and exotic plant species, while native insects rarely utilized exotic plant resources.  Additionally, the majority of floral visitors comprised exotic bees and syrphid flies.  In contrast, where the vegetation was dominated by native plants, native bees were major visitors. Thus, the impact of exotic plants and insect visitors on visitation networks was non-symmetrical.  Exotic plants relied upon exotic insect taxa, while exotic insect taxa were able to utilize both native and exotic plants. This study demonstrates that the role of mutualistic interactions on the success and impact of invasive species cannot be predicted by looking at isolated interactions, but must also consider the context of the interactions.Chapter 3 evaluates how these floral visitation networks vary over time.  Pollination systems provide important ecosystem services in both natural and managed ecosystems, but their future ecological stability is uncertain as a result of global change, including the impacts of invasive species, habitat loss, and a changing climate.  Understanding how these systems vary naturally through time, including intra-annually, can provide critical context for evaluating future change, as well as elucidating the complexity of interspecific interactions in the community.   This study examines temporal variation in floral visitation networks in a tropical system in Hawaii characterized by both native and non-native pollinators and plants, and less seasonal variation than in temperate regions.  The three most common floral visitors exhibited unique seasonal visitation patterns.  In the presence of only native plant species, both the exotic honeybee Apis mellifera and the endemic Hylaeus bees had similar seasonal variation in floral foraging.  However, when the vegetation was a mix of native and exotic species, Apis visitation tracked the peak blooming of exotic plants while Hylaeus only visited native plants, leading to seasonal variation in resource partitioning.  In contrast, visitation by the invasive yellowjacket Vespula pensylvanica consistently peaked during the fall, unrelated to plant blooming cycles.  Thus, even in a system with minimal seasonal climate variation, there were marked differences in the patterns of pollination interactions between seasons, suggesting that intra-annual variation must be considered in predictions for stability of pollination networks in a changing world.Finally, Chapter 4 reviews and assesses the range of conservation threats to these Hawaiian pollination systems.  Pollination interactions worldwide are facing a wide variety of threats, including habitat loss/change, agricultural intensification, pesticide/herbicide use, invasive species, parasites/disease, and global climate change.  Pollination networks in Hawaii are of special concern, because of the unique nature of Hawaii’s terrestrial biota, including both plants and pollinators.  As the sites from this study were located within a protected national park, the most likely sources for their endangerment are exotic/invasive species, the introduction and spread of parasites/disease, and the slow but potentially devastating effects of climate change.  Hawaiian ecosystems, and these sites in particular, are additionally subject to the changes and hazards associated with a zone of active geologic activity.  In this chapter, I address specifically both the rising global threats of parasites/disease and climate change and the unique local dangers of active volcanoes for Hawaiian pollination interactions.  The variety and magnitude of potential effects provide a wealth of opportunities for future research utilizing existing network data to evaluate how these factors operate both independently and interactively to create change.",ucb,,https://escholarship.org/uc/item/0d1544kt,,,eng,REGULAR,0,0
147,1583,Consequences of Confinement in Zeolite Acid Catalysis,"Gounder, Rajamani Pachayappan","Iglesia, Enrique;",2011,"The catalytic consequences of confinement within zeolite voids were examined for several elimination (alkane cracking and dehydrogenation, alkene cracking, alkanol dehydration) and addition (alkene hydrogenation, alkylation and oligomerization) reactions catalyzed by Brønsted solid acids. These reactions are mediated by cationic transition states that are confined within voids of molecular dimensions (0.4-1.3 nm) and proceed at rates that reflect the Gibbs free energies of late ion-pairs at transition states relative to those for the relevant reactants. Ion-pair stabilities depend on electrostatic interactions between organic cations and catalyst conjugate anions and on dispersion interactions between these cations and framework oxygen atoms. The former interactions are essentially unaffected by confinement, which influences weakly Brønsted acid strength, while the latter depend strongly on the sizes and shapes of voids and the species confined within them. The catalytic effects of confinement in stabilizing ion-pairs are prevalent when transition states are measured relative to gaseous reactants, but are attenuated and in some cases become irrelevant when measured with respect to confined reactants that are similar in composition and size.Zeolite voids solvate confined species by van der Waals forces and mediate compromises in their enthalpic and entropic stabilities. Confinement is generally preferred within locations that benefit enthalpic stability over entropic freedom at low temperatures, in which free energies depend more strongly on enthalpic than entropic factors. For example, the carbonylation of dimethyl ether (400-500 K) occurs with high specificity within eight-membered (8-MR) zeolite voids, but at undetectable rates within larger voids. This specificity reflects the more effective van der Waals stabilization of carbonylation transition states within the former voids. In contrast, entropic consequences of confinement become preeminent in high temperature reactions. Alkane activation turnovers (700-800 K) are much faster on 8-MR than 12-MR protons of mordenite zeolites because the relevant ion-pairs are confined only partially within shallow 8-MR side pockets and to lesser extents than within 12-MR channels. The site requirements and confinement effects found initially for elimination reactions were also pertinent for addition reactions mediated by ion-pair transition states of similar size and structure. Ratios of rate constants for elimination and addition steps involved in the same mechanistic sequence (e.g., alkane dehydrogenation and alkene hydrogenation) reflected solely the thermodynamic equilibrium constant for the stoichiometric gas-phase reaction. These relations are consistent with the De Donder non-equilibrium thermodynamic treatments of chemical reaction rates, in spite of the different reactant pressures used to measure rates in forward and reverse directions. The De Donder relations remained relevant at these different reaction conditions because the same elementary step limited rates and surfaces remained predominantly unoccupied in both directions.Rate constants for elementary steps catalyzed by zeolitic Brønsted acids reflect the combined effects of acid strength and solvation. Their individual catalytic consequences can be extricated using Born-Haber thermochemical cycles, which dissect activation energies and entropies into terms that depend on specific catalyst and reactant properties. This approach was used to show that thermal, chemical and cation-exchange treatments, which essentially change the sizes of faujasite supercage voids by addition or removal of extraframework aluminum species, influence solvation properties strongly but acid strength only weakly. These findings have clarified controversial interpretations that have persisted for decades regarding the origins of chemical reactivity and acid strength on faujasite zeolites. Born-Haber thermochemical relations, together with Marcus theory treatments of charge transfer reaction coordinates, provide a general framework to examine the effects of reactant and catalyst structure on ion-pair transition state enthalpy and entropy. The resulting structure-function relations lead to predictive insights that advance our understanding of confinement effects in zeolite acid catalysis beyond the largely phenomenological descriptions of shape selectivity and size exclusion. These findings also open new opportunities for the design and selection of microporous materials with active sites placed within desired void structures for reasons of catalytic rate or selectivity. The ability of zeolite voids to mimic biological catalysts in their selective stabilization of certain transition states by dispersion forces imparts catalytic diversity, all the more remarkable in light of the similar acid strengths among known aluminosilicates. This offers significant promise to expand the ranges of materials used and of reactions they catalyze.",ucb,,https://escholarship.org/uc/item/0d18z4sg,,,eng,REGULAR,0,0
148,1584,"Du Naturel, or Philippe de Champaigne Against Nature. Portraiture, artifice and the natural in seventeenth-century France","Douplitzky, Karine","Olson, Todd P.;",2020,"Philippe de Champaigne (1602-1674) is mostly remembered for his triple portrait of Richelieu and his hieratic series of Jansenist leaders' portraits but rarely considered for his rapport with nature despite his training as a Flemish landscape artist. By introducing the unexpected question of the natural in the context of his artistic practice, I reconsider Champaigne's rich corpus of portraits, which map his contemporary society and provide a new perspective on the evolving web of social identities.I explore how the concept of the natural, as opposed to artifice, is a shifting term that questions the ability of the painter to imitate nature, create a prototype, and give it ""life."" I successively qualify Champaigne's artistic praxis in relation to its contemporary reception within different communities – the nobility, the Crown, the Jansenist community, and finally the Royal Academy of Painting and Sculpture. Champaigne's approach to portraiture raises the issues of exemplarity, resemblance, and presence of the model. These three problematics offer a chronological and thematic understanding of the painter as a multifaceted artist who leads portraiture into different paths – decoration, diplomacy, and even the sacred.The position of the portraitist within the complex social and political agenda of the French Grand siècle, provides a particularly interesting and underexamined insight into the intricate relations between power and religion under Louis XIII's reign and later, during the Regency's social unrest. By combining formal analysis with anthropologically rich archival evidence, I consider Champaigne's portraits as active agents in history, thus providing a conceptual framework to analyze the different actors' strategies of representation.",ucb,,https://escholarship.org/uc/item/0dk8j1th,,,eng,REGULAR,0,0
149,1585,Three Essays on Management and Organization,"Hong, Bryan","de Figueiredo, Rui;",2012,"This dissertation examines how managers influence firm behavior and performance. Managers play an important role in the performance and activities of firms, given their decision-making role within organizations. I conduct three separate empirical analyses examining specific factors that influence the impact that managers have on firm behavior and performance.The first chapter investigates the following question: How does the performance impact of supervisor changes differ across levels in a hierarchy? In my results, I find that supervisor changes at higher levels result in more severe performance declines relative to lower levels in the hierarchy, even when accounting for differences in span of control. The findings suggest that reassignment and turnover of managers at higher levels may be more costly for firms, independent of their ability and other individual characteristics.The second chapter examines the following: What is the effect of replacing experienced managers with rookie managers on firm performance? And, how does this change if they are instead replaced with experienced managers? At the individual store level, I observe the behavior and performance effects of management changes when successors are newly promoted store managers, and compare this to changes where successors are experienced store managers that are reassigned. In my results, newly promoted store managers systematically cut costs that briefly lead to profit increases, but ultimately result in profit declines in subsequent months. By contrast, successors that have prior experience as a manager do not make any changes observable in my data, and I find no evidence of performance changes. These findings suggest that inexperienced managers within firms may engage in well-intentioned behavior that may be costly for firms, at least in the short run. However, managerial experience may reduce the likelihood that the same costly behavior is repeated. The results shed additional insight into how managerial experience may matter for performance, and provide a tangible estimate of the performance costs of being a rookie manager.In the final essay, I investigate the influence of top managers on corporate social responsibility (CSR). A growing body of literature suggests that individual managers may play a critical role in determining corporate social responsibility (CSR) activities.  However, attempts to quantitatively measure the individual influence managers have on CSR face significant empirical challenges.  Estimation methods unable to adequately control for firm-specific factors influencing CSR are likely to overstate the importance of individual managers in their findings.  To address these concerns, I use an identification approach allowing for the simultaneous estimation of manager and firm fixed effects, and provide quantitative estimates of the degree to which individual managers might influence CSR.  The results suggest that managers do exert some degree of individual influence on CSR outside of firm-specific factors, but that the magnitude of their effect is relatively small.  Also, when managers switch firms, I find no evidence of a relationship between their influence on CSR in their first and second firm, suggesting that managers do not exert a persistent influence on CSR independent of the firm where they are employed.",ucb,,https://escholarship.org/uc/item/0dt4c8n2,,,eng,REGULAR,0,0
150,1586,Structure and Dynamics of Cu and Cu-Ag Nanocrystal Catalysts during Electrochemical CO2 Reduction,"Osowiecki, Wojciech Tomasz","Alivisatos, Paul;",2019,"In recent years, the CO2 reduction reaction (CO2RR) has been a popular topic in the field of electrocatalysis for its potential to help in mitigating climate change effects. As renewable energy sources such as solar and wind are rising in the market, there is a growing need for energy storage due to the intermittency of production. Renewable energy can be stored in batteries, but chemical bonds offer energy densities that are orders of magnitude higher. CO2 reduction is one of the electrocatalytic reactions that can store electrons in chemical bonds, simultaneously decreasing the amount of harmful greenhouse gas and creating useful fuels and chemical feedstocks. However, this vision is only possible if new catalysts for CO2 reduction are developed, because currently the efficiency and selectivity of the reaction are not high enough to allow for an industrially-viable technology.	Due to the fundamental restrictions between the bonding strengths of CO2RR intermediates, complex nano-engineered catalysts are particularly well suited for achieving substantially better catalytic selectivity as compared to state-of-the-art Cu bulk materials. Creating high-energy facets with low-coordination atoms as well as alloying Cu with other metals are among widely pursued strategies, and for such objectives, we find small (<10 nm) synthetically-tunable nanocrystals to be promising candidates. That said, industrially-viable catalysts must not only be efficient and selective but also stable. Ironically, the very properties that offer favorable catalytic performance also render the material prone to morphological restructuring, namely sintering, under the operating conditions. We believe that in order to prevent sintering in the future, it needs to be studied first, so we directly focus on this issue by systematically probing the reaction conditions during electrocatalysis. We hope that this topic will be further investigated as the complexity of the experimental parameters calls for efforts of the same magnitude as to what has been done to understand the CO2RR selectivity and reaction mechanism.  	Chapter 1 discusses the opportunities and challenges of electrocatalysis in the 21st century. We draw some analogies to other fields where an understanding of the theoretical limits as well as an ambitious pursuit of chemical reaction control were needed to create industrially-viable technologies. We then motivate the need of for a fundamental understanding of morphological changes occurring during electrocatalysis in light of these considerations.	Chapter 2 describes the synthesis, characterization, and thermodynamic understanding of different morphologies of Cu-Ag bimetallic nanocrystals. Cu and Ag do not alloy, so the bulk Cu-Ag materials used for catalysis possess monometallic domains of a specified size, but with nano-colloidal synthesis, it is possible to bring Cu and Ag into a more intimate contact which may give rise to a change in material properties. As such, we have synthesized a new structure, the nanocrescent, and its formation, based on thermodynamic principles, is explained.	Chapter 3 presents the catalytic performance of Cu-Ag bimetallic nanocrystals for CO2RR and compares it with that of physical mixtures of monometallic Cu and Ag particles. This chapter illustrates precisely why an understanding of sintering and its prevention are crucial, as the studied structures undergo a complete morphological restructuring and can no longer be confidently distinguished as distinct particles. Nevertheless, we observe a significant shift in catalytic selectivity as compared to pure Cu nanocrystals. Cu-Ag materials decrease the activity towards undesired H2 production and increase the efficiency of oxygenates formation.	Chapter 4 focuses directly on the issue of electrochemical sintering by studying the behavior of Cu nanocrystals under the standard conditions of CO2RR as well as a range of control experiments. We hypothesize some possible driving factors that could lead to the morphological restructuring and aim at distinguishing between them by changing variables such as the gas environment or pH. Ligand presence is probed with spectroscopic techniques, and morphological structures are imaged with electron microscopy. The presented set of evidence demonstrates that CO, a CO2RR intermediate, plays an important role in the sintering process by changing nanoparticle surface properties, leading to the formation of larger single-crystal facets.",ucb,,https://escholarship.org/uc/item/0f090711,,,eng,REGULAR,0,0
151,1587,"Hats off, Galileo: Early Richard Serra","Byrd, Anne Elizabeth","Wagner, Anne M;",2011,"This dissertation examines the first decade of Richard Serra's career, beginning with the European travels that followed his graduation with a Masters of Fine Arts from Yale and continuing through the mid-1970s. This period is especially interesting because it was during these years that Serra initiated the sculptural practice for which he is now best known, yet he was not so single-mindedly devoted to it as he would become - he was also very actively involved in the production of film, video, photo essays, conceptual proposals, and occasional ephemeral works. This dissertation studies these projects in conjunction with Serra's sculpture, arguing that they are in some respects parallel investigations, and arguing further that it therefore becomes necessary to find language that allows us to address the possibility that Serra's sculpture has some kind of content - whether psychological, political, or philosophical - despite the artist's assiduous avoidance of representation.I begin with a discussion of Serra's movement into ""process art."" Tracing a line through the visually very dissimilar sculptures that Serra made just prior to his process works, I argue that the tendency of Serra's earliest sculpture to privilege logical contradiction and perversity sets it apart from contemporary minimalist literalism, and opens it up to models of meaning found in the writings of the philosopher Alfred North Whitehead and the psychologically-minded art educator Anton Erhrenzweig, both of whom Serra was reading at the time. Then I turn to Serra's Props, lead sculptures propped up with no fixed joints that have often prompted viewers to focus on their threatening aspects. Tying these sculptures to works in other media that took the Vietnam War and Cold War technocratic theories as their materials, I argue that the Props did not simply (literally) enact violence but communicate about it. Finally I address the earliest of Serra's large-scale steel sculptures and landscape works, tying them to contemporaneous films, photo projects, and videos in order to argue that Serra's approach to sculpture here, while very much focused on embodiment, is more mediated by the mechanical image than has previously been acknowledged.",ucb,,https://escholarship.org/uc/item/0f0923k6,,,eng,REGULAR,0,0
152,1588,Progress in Xenon and Proton Relaxation Based Sensing,"Gomes, Muller De Matos","Pines, Alexander;",2017,"In this dissertation, the sensitivity of xenon relaxation to changes in its environment is used to both develop new types of biosensors and also to develop new techniques that make use of xenon’s intrinsic interactions with its environment. A proton based relaxation experiment is also discussed due to its similarity to relaxation experiments done with xenon biosensors. Contrast agents are developed for xenon NMR. These agents consist of a cryptophane cage covalently attached to a DOTA chelating agent, allowing one to bring xenon close to chelated paramagnetic ions, enhancing the bulk relaxation of xenon. Both the T1 and T2 relaxivity of these contrast agents are tested. Adding paramagnetic metal ions seems to affect T1 more than T2 for most ions, possibly because the cage itself drastically affects the T2 of xenon because of the slow exchange rate and large chemical shift difference. In general, metal ions known to have long electronic relaxation times relax xenon more efficiently than ions with shorter electronic relaxation times. Gadolinium (III) and manganese (II) have the greatest effect on the T1 and T2 of xenon, with gadolinium (III) affecting T2 more and manganese (II) affecting T1 more. Adding gadolinium (III) increases the T1 relaxivity of M2 cages to 0.002 mM-1s-1 from 0.0009 mM-1s-1 and the T2 relaxivity to 92.5 mM-1s-1 from 26.1 mM-1s-1 . After testing the effect of these contrast agents, a relaxation based xenon biosensor is developed. This sensor consist of a cryptophane cage attached to a DOTA chelating agent and a biotin. The sensor works by binding to avidin, thereby increasing the rotational correlation time of the xenon inside the cage. This increases the relaxation rate of xenon inside the cage. Upon binding of a biotin-containing sensor to avidin at 1.5 µM concentration, the free xenon T2 is reduced by a factor of 4. Changes in relaxation were more easily seen in T2 due to the strength of the field used in this experiment. At high magnetic fields, T1 hardly responds to changes in the rotational correlation time. A proton based relaxation agent, developed by the IBS institute from the Republic of Korea, is discussed in this dissertation. This group developed a sensor consisting of two parts: a super paramagnetic nanoparticle quencher and a paramagnetic metal ion enhancer. When the two are close together, the paramagnetic enhancer cannot efficiently relax water. Separating the two, done by either cleaving the bond keeping them together or by a conformational change in the linker binding them, prevents the super paramagnetic nanoparticle from quenching the enhancer, making water relaxation extremely rapid. Cleaving the bond between the quencher and enhancer increases the R1 of water by 1.5 s-1. This sensor was used to detect MMP2, an enzyme seen in certain tumors, both invitro and invivo. Concentrations as low as 15 ng per mL of MMP2 were detected invitro. This sensor is less sensitive invivo, with a lowest detected concentration of MMP2 being 450 ng per mL. After studying many varieties of sensors developed to functionalize xenon, the direct interactions between xenon and its target were studied. Xenon interacts with many substances, including proteins, leading to rapid relaxation of the entire xenon ensemble. This is due to both nonspecific interactions with the protein surface relaxing xenon and also because many proteins have hydrophobic pockets xenon can occupy. This leads to rapid xenon relaxation, which can be perturbed by the protein binding to another ligand. Adding a ligand to a solution of protein, such as a small molecule drug, alters the relaxation of xenon in that solution. This effect was exploited in order to develop a method for measuring the binding affinity of certain drugs for albumin by monitoring their effect on the relaxation of xenon. Of the drugs studied, warfarin, tenoxicam, and sodium salicylate had the strongest effects due to their high affinity for albumin, with warfarin lowering the T2 of xenon from 5 seconds to 2 seconds.",ucb,,https://escholarship.org/uc/item/0f13c7g6,,,eng,REGULAR,0,0
153,1589,"Structural, Biochemical Characterization, and Homology Based Modeling of Target Protein Interactions with Natural and Synthetic Indolecarbinol Compounds that Control Anti-Proliferative Signaling in Human Melanoma and Breast Cancer Cells","Quirit, Jeanne","Firestone, Gary L;",2016,"Cancer unforgivingly impinges upon millions of lives daily and is a predominant cause of death worldwide.  Conventional cancer treatment is comprised of surgery, radiation, chemotherapy, hormone, immune, and targeted therapy, however, with the deleterious side effects and eventual tumor resistance that invariably ensue, there is an urgency to develop safer, less invasive therapies.  Within the past decade, a significant transition in cancer therapeutics has unfurled as molecular targeted therapies have emanated as an alternative treatment for an array of cancers which include breast, colorectal, lung, pancreatic, lymphoma leukemia, and multiple myeloma.  Molecular targeted therapies suppress critical biochemical pathways or mutant proteins that are required for tumor cell growth and survival.  Within a molecularly defined cluster of patients, these drugs have the capacity to arrest tumor progression and can impel dramatic regressions.  Identifying novel classes of highly potent therapeutic agents that specifically act on molecular targets with diminished side effects after continued treatment has been a challenging obstacle to overcome in the treatment of cancer.  Among the array of molecular targeted agents employed, indole-3-carbinol (I3C), has emerged as a viable anti-cancer agent.  I3C is a bioactive component in cruciferous vegetables that is derived by hydrolysis from glycobrassicin in Brassica and exhibits multiple anticarcinogenic and antitumorigenic properties as well as chemo-preventative and strong anti-tumor properties in vivo in rodent model systems and in human cancer cell xenograft tumors.  A major headway in understanding the I3C anti-proliferative mechanism is our discovery that I3C triggers distinct and overlapping sets of anti-proliferative signaling events by direct interactions with specific target proteins.  Through a multi-faceted series of cellular and biochemical experiments, our lab has cemented three I3C target proteins which include human neutrophil elastase, E3 ubiquitin ligase NEDD4-1, and oncogenic B-RAF V600E serine/threonine kinase.  Because of I3C’s off-target and nonspecific cytotoxic effects and its propensity to dimerize into its natural condensation product DIM, which triggers distinctly different antiproliferative cascades, there is a need to generate more potent, target-specific compounds this ultimately lead to a promising new compound, 1-benzyl-I3C, which is substantially more effective in suppressing enzymatic activity, inducing anti-proliferative effects in melanoma and breast cancer cells, and diminishing tumor size in mouse xenografts .  Here, we demonstrate that both I3C and 1-benzyl-I3C serve as molecular scaffold for creating a novel, robust enzymatic small molecule inhibitors aimed at disrupting specific target proteins in melanoma and breast cancer cells, particularly NEDD4-1 and elastase respectively.  By executing an in silico approach, we were able to make predictions about the mechanistic and structural nature of a set of five synthetic I3C and 1-benzyl-I3C derived analogs and employed a combination of in vitro protein thermostability assays and enzymatic assays to substantiate our predictions.  Notably, compounds 2242 and 2243, the two indolecarbinol analogues with added methyl groups that result in a more nucleophilic benzene ring π system, further inhibited NEDD4-1 enzymatic activity more dramatically with IC50s of 2.7 µM and 7.6 µM, respectively.  Interestingly, compounds 2242, 2160, and 2243 inhibited elastase activity much more significantly as well with and an IC50 of 30.4 μM, 25.1 μM, and 16.9 μM respectively.  In both NEDD4-1 and elastase enzymatic studies, the potency of compound 2242, 2160, and 2243 appeared to be sensitive to structural changes on the phenyl ring, more notably when methyl substituents were added to the para and ortho positions.  The activity was abolished with the addition of bulky chemical groups like the thiophene substituent attached to the indole of 2163 and the electron donating methoxy group attached to the phenyl moiety of 2244 in the meta and ortho positions.    The quest for additional I3C target proteins has been facilitated through our understanding of homology modeling and identification of patterns in protein folds and ligand binding sites.  In order to make predictions about additional indolecarbinol target proteins, we obtained the crystal structures of the four I3C target proteins to date (human neutrophil elastase, ubiquitin E3 ligase NEDD4-1, oncogenic BRAF V600E serine/threonine kinase, WNT) and used them as the starting template structure for creating homologous protein models.  Homology models for various homologues of the target proteins were generated to ascertain whether or not similarities in potential indolecarbinol binding sites could be visibly discerned after performing a computational docking analysis of indolecarbinol compounds into their most feasible predicted binding sites.  After examining the indolecarbinol binding sites in the homology models, it is evident that the homologous proteins that have greater than 50% sequence identity share a distinct structural architecture that confers binding to the indolecarbinol compounds, but generally, homologous proteins that share 30% sequence identity or below tend to lose essential I3C contact residues.Examining the binding modes of I3C, 1-benzyl-I3C, and their corresponding synthetic derivatives in complex with their target proteins, namely, elastase, NEDD4-1, oncogenic BRAF V600E serine/threonine kinase, and wnt, can illuminate various patterns in binding sites and can hence provide valuable information for locating additional indolecarbinol target proteins.  Conceivably, information garnered from these studies will be useful in making predictions allowing identification of additional indolecarbinol compound target proteins.  The results presented here provide the fundamental framework for understanding the mode of inhibition of natural and synthetic indolecarbinol compounds and the proteins they effectively target.",ucb,,https://escholarship.org/uc/item/0f19g6f0,,,eng,REGULAR,0,0
154,1590,Prevention of Mother-to-Child HIV Transmission: Predictors of Utilization & Future Policy Implication,"Martz, Tyler Elizabeth","Bertozzi, Stefano M;",2015,"Despite the availability of highly efficacious antiretroviral drug regimens for the prevention of mother-to-child HIV transmission (PMTCT), transmission rates remain higher than those achieved in clinical trials. Access to these efficacious drug regimens continues to expand rapidly in countries most affected by HIV. Such expansion is an important first step in dramatically reducing mother-to-child HIV transmission rates. However, beyond access to drug regimens, programs must also identify and address individual and structural factors impeding the utilization of and adherence to PMTCT services by the women they are designed to serve. Additional research into factors both positively or negatively associated with PMTCT service utilization could help improve PMTCT programs to further reduce transmission rates. Each of the three papers included in this dissertation examined different factors of PMTCT service utilization. The first two papers analyze secondary data from a large-scale impact evaluation of Zimbabwe’s PMTCT program. Specifically, the first paper explores the association between costs (service costs, travel time, and transportation costs) and utilization of all recommended PMTCT services. The second paper explores the relationship between the timing of a pregnant woman’s HIV-positive diagnosis, either prior to pregnancy or during antenatal care, and her utilization of PMTCT services. The hypotheses was that women who were diagnosed prior to pregnancy, having had more time to cope with their diagnosis, would complete more of the recommended PMTCT services. The final paper utilizes policy analysis methodology to examine two different implementation strategies Malawi could consider to improve lifelong adherence to antiretroviral therapy (ART) among women living with HIV who initiate treatment during pregnancy or breastfeeding. The two strategies proposed were: 1) for all pregnant/breastfeeding women to initiate and indefinitely receive treatment at maternal and child health clinics rather than be transferred to an ART specialty clinic, and 2) to expand access to lifelong ART, regardless of stage of disease, to any individual living with HIV in the pregnant/breastfeeding woman’s household.",ucb,,https://escholarship.org/uc/item/0f83h1kd,,,eng,REGULAR,0,0
155,1591,Heat Pipe Performance Enhancement with Binary Mixture Fluids that Exhibit Strong Concentration Marangoni Effects,"Armijo, Kenneth Miguel","Carey, Van P;",2011,"This research investigates the impact of Marangoni phenomena, with low mixture concentrations of alcohol and water, to enhance thermal transport capability of gravity-assisted heat pipes. The use of binary mixture working fluids in gravity-assisted heat pipes are shown to improve the critical heat flux (CHF) and operating performance, more so than with pure fluids. The CHF is responsible for dryout when the pumping rate of a liquid flow structure is not sufficient to provide enough fluid to the evaporator section.     In the first study, heat pipe performance experiments were conducted for pure water and 2-propanol solutions with varying concentrations. Initial tests with pure water determined the optimal working fluid charge for the heat pipe; subsequent performance tests over a wide range of heat input levels were then conducted for each working fluid at this optimum value. The results indicated that some mixtures significantly enhance the heat transfer coefficient and heat flux capability of the heat pipe evaporator. For the best mixture tested, the maximum evaporator heat flux carried by the coolant without dryout was found to be 52% higher than the value for the same heat pipe using pure water as a coolant under comparable conditions. Peak evaporator heat flux values above 100 W/cm2 were achieved with some mixtures. Evaporator and condenser heat transfer coefficient data are presented and the trends are examined in the context of the expected effect of the Marangoni mechanisms on heat transfer.     Analytical modeling effort was also conducted investigating the impact of Marangoni phenomena for low concentrations of 2-propanol/water and methanol/water mixtures. In real systems the addition of small levels of surface-active contaminants can affect the surface tension of the liquid-vapor interface and thermodynamic conditions in this region. Analysis was performed for three widely accepted binary mixture correlations to predict heat flux and superheat values for subatmospheric experimental data using bulk fluid and film thermodynamic properties. Due to the non-ideal nature of these alcohol/water mixtures, this study employs an average pseudo single-component (PSC) coefficient in place of an ideal heat transfer coefficient (HTC) to improve the correlation predictions. This investigation evaluates the ability for these correlations to predict strong Marangoni effects of mixtures that have large surface tension variation with concentration under subatmospheric conditions. It is not always clear that evaluation of bulk fluid properties will satisfactorily account for Marangoni effects.  Analysis is also performed to assess correlation predictions for interfacial film properties rather than that of the bulk fluid. The results indicate that the use of film properties along with the PSC coefficient improves heat flux model predictions of  subatmospheric experimental data by as much as 59.3% for 0.015M 2-propanol and 49.1% for 0.04M methanol/water mixtures, where strong Marangoni effects are believed to be more evident.     A second experimental study was also performed of a 37° inclined, gravity-assisted, brass heat pipe with a 0.05M 2-Propanol/water binary mixture. The device design was developed from the first study by enlarging the evaporator and condenser surface areas. Strip heaters were also employed to provide larger input heat flux levels, for enhanced heat pipe performance testing. These experiments were carried out for varying liquid charge ratios between 30% and 70%, to determine an optimal value that would enhance heat transport performance by maximizing the critical heat flux (CHF) condition, while reducing the evaporator wall superheat. A 45% fill ratio was found to have the lowest overall superheat and highest thermal conductance by as much as 7.5W/K, as well as an enhanced CHF condition of 114.8W/cm2. A heat pipe analytical model, that characterizes binary mixture pool boiling is also presented, which was developed based on modeling efforts presented in studies 1 and 2. Model results with a 45% liquid charge ratio were found to provide good correspondence with the experimental data with an average rms evaporator vaporization heat flux deviation of 6.5%.     The final study of this investigation assesses the cooling of single and dual-junction solar cells with the inclined, gravity-assisted, brass heat pipe, with a 0.05M 2-propanol/water mixture. Thermal behavior of this heat pipe solar collector system was investigated theoretically and semi-empirically through experimentation of varying input heat loads from attached strip-heaters to simulate waste heat production of single-junction monocrystalline silicon (Si), and tandem multijunction GaInP/GaAs solar cells. It was also found that the 45% liquid charge was capable of achieving the lowest superheat levels and highest critical heat flux (CHF) condition of 114.8 W/cm2, at a predicted solar concentration of 162 suns. Solar cell semiconductor theory was employed to evaluate the effects of increasing temperature and solar concentration on solar cell performance. Results showed that a combined PV/heat pipe system had a 1.7% higher electrical efficiency, at a concentration ratio 132 suns higher than a stand-alone PV system. The dual-junction system also exhibited enhanced performance at elevated system temperatures with a 2.1% greater electrical efficiency, at an operational concentration level of 560 suns higher than a stand-alone PV system. Waste heat recovery analysis of the silicon solar cell, revealed respective thermal and system efficiencies as high as 56.3% and 66.3% as the incident solar radiation and corresponding condenser heat removal factor increased to 82 suns.",ucb,,https://escholarship.org/uc/item/0f86g1pf,,,eng,REGULAR,0,0
156,1592,Numerics and stability for orbifolds with applications to symplectic embeddings,"Wormleighton, Ben","Eisenbud, David;",2020,"This thesis studies the geometry of orbifolds - primarily via variation of GIT, derived category methods, and numerics - and develops connections of equivariant algebraic geometry with embedding problems in symplectic geometry, and with lattice point counting for rational polytopes. We also compile many aspects of the disparate toolkit required to rigorously study orbifolds.",ucb,,https://escholarship.org/uc/item/0fh5q00w,,,eng,REGULAR,0,0
157,1593,Lowness For Computational Speed,"Bayer, Robertson Edward","Slaman, Theodore;",2012,"From the original definition of a set whose jump is as simple as possible (A' =T 0'), to more recent definitions involving randomness, notions of lowness appear throughout recursion theory.  In that spirit, a non-recursive set A will be said to be low for speed if for any recursive set R and any computation of R from A, there is an oracle-free computation of R that is no more than polynomial-time slower than the A-computation.  We will construct such an r.e. set and discuss some properties of these sets.  We will show that promptly simple r.e. sets cannot be low for speed, and also that there are non-prompt sets that are not low for speed. We conclude by showing that generic sets are low for speed if and only if P = NP.",ucb,,https://escholarship.org/uc/item/0fj0k334,,,eng,REGULAR,0,0
158,1594,Constraining sources and sinks of atmospheric trace gases: Spectroscopy and kinetics of C1-C3 Criegee intermediates and the isotopic composition of lightning-produced N2O,"Smith, Mica","Boering, Kristie A.;",2016,"This dissertation presents a series of research projects designed and carried out to elucidate the physical chemistry and assess the atmospheric relevance of (1) carbonyl oxide radicals (i.e., Criegee intermediates) produced in alkene ozonolysis and (2) nitrous oxide (N2O) produced in lightning-induced corona discharges. The results provide UV absorption spectra and reaction rate coefficients for Criegee intermediates that will help constrain the formation and loss pathways of aerosol nucleation precursors such as H2SO4 and oxidized volatile organic compounds, and the isotopic signature of N2O formed in lightning that can help distinguish various N2O sources in atmospheric measurements.Criegee intermediates are byproducts of the reaction of alkenes with ozone. Bimolecular reactions of Criegee intermediates can lead to the production of low-volatility organic compounds and acids in the atmosphere, which in turn play a role in determining the concentration, size, and optical properties of aerosols. Recently, a novel method for producing measurable quantities of stabilized Criegee intermediates in the laboratory paved the way for the development of new experimental techniques to study their chemical properties and predict their importance in the atmosphere. For this dissertation, a unique apparatus combining time-resolved UV absorption in a flow cell with laser depletion in a molecular beam was adapted to obtain the absolute absorption spectrum of CH3CHOO with high resolution and accuracy relative to previous spectral measurements by other groups. The resulting absorption cross sections imply a photolysis lifetime of about seven seconds in the atmosphere, long enough for CH3CHOO to participate in unimolecular and bimolecular reactions. The broad absorption band with weak structure in the long-wavelength region of the spectrum represents a “spectral fingerprint” for identifying CH3CHOO in future studies, and the cross sections provide valuable benchmarks for theory to characterize electronically excited states of CH3CHOO.The fast reaction of CH2OO with water dimer is thought to dominate CH2OO removal in the atmosphere. However, reaction rates can vary considerably under different conditions of temperature, humidity, and pressure. A temperature-controlled flow cell was designed to measure the transient absorption of CH2OO and obtain rate coefficients for its reaction with water dimer from 283 to 324 K. The rate of the reaction of CH2OO with water dimer was found to exhibit a strong negative temperature dependence, pointing to the participation of a hydrogen-bonded pre-reactive complex between CH2OO and two water molecules. Due to the strong temperature dependence, and shifting competition between water dimer and water monomer (which has a positive temperature dependence), the effective loss rate of CH2OO by reaction with water vapor is highly sensitive to atmospheric conditions. The role played by the stable pre-reactive complex suggests that similar complexes could form between water dimer and other larger Criegee intermediates, and that the stability and relative energy of these complexes control the reaction rate with water and its temperature dependence.Effective loss rates of Criegee intermediates due to bimolecular reactions in the atmosphere are limited by their rates of unimolecular decomposition. The rates of decomposition depend strongly on the molecular geometry, which affects the accessible isomerization pathways and dissociation products. (CH3)2COO is the main product of tetramethylethylene ozonolysis, and has been found to react slowly with water dimer and rapidly with SO2. While CH2OO decomposes slowly via isomerization to dioxirane, (CH3)2COO may decompose faster via intramolecular hydrogen transfer to form vinyl hydroperoxide. Fast (CH3)2COO decomposition could affect the significance of the Criegee intermediate H2SO4 source, as well as the non-photolytic production of OH radicals. In this dissertation, measurements of the transient absorption of (CH3)2COO to obtain thermal decomposition rate coefficients from 283 to 323 K by extrapolating the observed loss rate to zero concentration are reported. The rate of unimolecular decomposition is ~400 s-1 at 298 K and varies by nearly an order of magnitude within the studied temperature range. The effective loss rate of (CH3)2COO in the atmosphere due to thermal decomposition is thus competitive with its loss due to reaction with water vapor and with SO2, suggesting that the unimolecular decomposition pathway is a significant sink for (CH3)2COO and possibly other di-substituted Criegee intermediates, and should be included in models of Criegee chemistry in the atmosphere as well as in kinetic models of tetramethylethylene ozonolysis.N2O is the third most important greenhouse gas after CO2 and methane, and is mainly emitted to the atmosphere as a byproduct of microbial activity in soils. The expanding use of nitrogen-containing fertilizers in agriculture has led to an increase in N2O atmospheric concentrations since preindustrial times. Isotopic measurements are a valuable tool to distinguish the influence of different sources of N2O, but the isotopic composition of N2O formed from corona discharge in lightning has not previously been measured. Here, a corona discharge cell apparatus was used to generate a corona discharge in flowing or static zero air, and the N2O formed at discharge cell pressures from ~0.1 to 10 Torr and discharge voltages from 0.25 to 5 kV was collected and measured with isotope ratio mass spectrometry to determine its isotopic composition. The results show enrichments in 15N of N2O up to 32‰ relative to the reactant N2, and even larger enrichments in 15N of up to 77‰ at the central nitrogen atom. Large depletions in 18O as large as -71‰ relative to reactant O2 were also measured. The isotopic composition measured here may help to elucidate the chemical mechanisms leading to N2O formation and destruction in a corona discharge. Furthermore, the isotope-isotope relationships of the N2O produced in the corona discharge experiments are distinct from those of N2O from other sources, implying that isotopic measurements can be used to determine whether local variations in the atmospheric concentration of N2O – e.g., the enhanced N2O levels recently measured in the upper tropical and subtropical troposphere – are due to lightning activity, soil emissions, or biomass burning.",ucb,,https://escholarship.org/uc/item/0fj5x2w6,,,eng,REGULAR,0,0
159,1595,Influences in Voting and Growing Networks,"Racz, Miklos Zoltan","Mossel, Elchanan;",2015,"This thesis studies problems in applied probability using combinatorial techniques. The first part of the thesis focuses on voting, and studies the average-case behavior of voting systems with respect to manipulation of their outcome by voters. Many results in the field of voting are negative; in particular, Gibbard and Satterthwaite showed that no reasonable voting system can be strategyproof (a.k.a. nonmanipulable). We prove a quantitative version of this result, showing that the probability of manipulation is nonnegligible, unless the voting system is close to being a dictatorship. We also study manipulation by a coalition of voters, and show that the transition from being powerless to having absolute power is smooth. These results suggest that manipulation is easy on average for reasonable voting systems, and thus computational complexity cannot hide manipulations completely. The second part of the thesis focuses on statistical inference questions in growing random graph models. In particular, we study the influence of the seed in random trees grown according to preferential attachment and uniform attachment. While the seed has no effect from a weak local limit point of view in either model, different seeds lead to different distributions of limiting trees from a total variation point of view in both models. These results open up a host of new statistical inference questions regarding the temporal dynamics of growing networks.",ucb,,https://escholarship.org/uc/item/0fk1x7zx,,,eng,REGULAR,0,0
160,1596,"Global Mental Health Policy Diffusion, Institutionalization, and Innovation","Shen, Gordon Chit-Nga","Snowden, Lonnie R;",2013,"Mental health is an integral part of health and well-being. Mental health enables people to realize their potential, cope with the stressors of everyday life, and make contributions to society. Mental, neurological and substance use (MNS) disorders constitute 13% of the global burden of disease. And yet, across all countries, public investment in preventing and treating this cluster of disorders is disproportionately low relative to this disease burden. Health systems have not adequately or sufficiently responded to the burden of MNS disorders: the gap between the need and supply of treatment ranges from 76% to 85% in low- and middle-income countries, and from 35% to 50% in high-income countries. Mounting evidence underlines the inequitable distribution, poor quality, and inefficient use of scarce resources to address mental health needs. Globally, annual spending on mental health is less than US $2 per person in high-income countries and less than US $0.25 per person in low-income countries, with 67% of these financial resources allocated to stand-alone mental hospitals. Flagrant abuse of human rights and discrimination against people with mental disorders and psychosocial disabilities have been found in such psychiatric institutions. The redirecting of mental health budgets toward community-based services, including the integration of mental health into general health care settings, is needed. To address this state of affairs, this dissertation takes a fresh look at the actions taken to formulate a comprehensive, coordinated response from health and social sectors. It is founded at the nexus of new institutional, world culture, and diffusion of innovation theories.This dissertation employs a mixed methods approach, combining statistical and survey analyses. A mental health policy is an official statement of a government that defines its vision, values, principles, and objectives to improve the mental health of a population. It also outlines the areas of actions, strategies, timeframes, budgets, targets and indicators used to realize the vision and achieve the objectives of the policy. In the first study, I examine the coercive and emulative isomorphic effects on the diffusion of mental health policy across geopolitical borders. Using discrete-time data for 193 countries covering the period from 1950 to 2011, I conduct an event history analysis to examine the influence of WHO accession, foreign aid, and peer influence on mental health policy adoption. The results confirm that the act of adopting mental health policy is partly owed to membership in the World Health Organization, as well as influence of neighbors in the same World Bank and World Health Organization regions. National mental health policy adoption is trumpeted as a milestone for mental health reform. Is mental health policy limited to a rhetorical plane or taken up for pragmatic reasons? The effectiveness of this ""upstream"" factor could be realized based on examining ""downstream"" models of deinstitutionalized programming. While mental health policy adoption is treated as an outcome of interest in the first study, it is treated as a predictor in the second study. More specifically, I test the phase of policy adoption as a determinant of psychiatric bed rate changes using panel data for the same 193 countries between 2001 and 2011. The analysis finds that late-adopters of mental health policy are more likely to reduce psychiatric beds in mental hospitals and other biomedical settings than innovators, whereas they are less likely than non-adopters to reduce psychiatric beds in general hospitals. Deinstitutionalization is a much more complex and sophisticated process than reducing dehospitalization, or the reduction of psychiatric beds. It is also about improving the quality of care provided by inpatient facilities while increasing access to care through the development of mental health services in other medical and community settings. However, progress towards mental health reform is often stalled because it is an essentially contested issue in professional and advocacy circles and a highly politicized one among governments. For these reasons, the third study gathers contemporary perspectives on deinstitutionalization from 78 mental health experts. The survey administered assesses their knowledge, attitude, and practices of expanding community-based mental health services and/or downsizing institution-based care. The respondents also attested to the enabling, reinforcing, and constraining factors prevalent in the 42 countries they collectively represent. The qualitative evidence is complementary to the quantitative evidence in that it portrays the contemporary mental health system as being controlled by a nucleus of inpatient care. It further suggests that innovations are made in linking specialty services with primary and social services to support people with mental, neurological, and substance use disorders and their families as they (re)integrate into their communities. Mental health care has branched out in new directions at the turn of the 21st century. Time and again when governments are in the throes of strengthening their mental health systems, a closer look into the setup of infrastructure, essential medicines, human resources, and civil society involvement becomes necessary. This dissertation demonstrates that deinstitutionalization is a result of mental health policies imposed from the top down by the government. The experience with deinstitutionalizing mental health care also involves grassroots mobilization of social change by citizens, clients, families, and other advocates. In parallel with service reorganization, advances have been made in training lay personnel to offer services to people with MNS disorders. Research and development have made treatment more cost-effective and accessible. Cutting across temporal and geographic borders, tradition and modernity, this dissertation probes into the permeability of mental health policy and unpacks the complexity of deinstitutionalization.",ucb,,https://escholarship.org/uc/item/0fs1v90r,,,eng,REGULAR,0,0
161,1597,Design and Transient Analysis of Passive Safety Cooling Systems for Advanced Nuclear Reactors,"Galvez, Cristhian","Peterson, Per F;",2011,"The Pebble Bed Advanced High Temperature Reactor (PB-AHTR) is a pebble fueled, liquid salt cooled, high temperature nuclear reactor design that can be used for electricity generation or other applications requiring the availability of heat at elevated temperatures. A stage in the design evolution of this plant requires the analysis of the plant during a variety of potential transients to understand the primary and safety cooling system response. This study focuses on the performance of the passive safety cooling system with a dual purpose, to assess the capacity to maintain the core at safe temperatures and to assist the design process of this system to achieve this objective.  The analysis requires the use of complex computational tools for simulation and verification using analytical solutions and comparisons with experimental data. This investigation builds upon previous detailed design work for the PB-AHTR components, including the core, reactivity control mechanisms and the intermediate heat exchanger, developed in 2008. In addition the study of this reference plant design employs a wealth of auxiliary information including thermal-hydraulic physical phenomena correlations for multiple geometries and thermophysical properties for the constituents of the plant. Finally, the set of performance requirements and limitations imposed from physical constrains and safety considerations provide with a criteria and metrics for acceptability of the design. The passive safety cooling system concept is turned into a detailed design as a result from this study. A methodology for the design of air-cooled passive safety systems was developed and a transient analysis of the plant, evaluating a scrammed loss of forced cooling event was performed. Furthermore, a design optimization study of the passive safety system and an approach for the validation and verification of the analysis is presented. This study demonstrates that the resulting point design responds properly to the transient event and maintains the core and reactor components at acceptable temperatures within allowable safety margins. It is also demonstrated that the transition from steady full-power, forced-cooling mode to steady decay-heat, natural-circulation mode is stable, predictable and well characterized.",ucb,,https://escholarship.org/uc/item/0g2353c7,,,eng,REGULAR,0,0
162,1598,Efficient inference algorithms for near-deterministic systems,"Chatterjee, Shaunak","Russell, Stuart J;",2013,"This thesis addresses the problem of performing probabilistic inference in stochastic systems where the probability mass is far from uniformly distributed among all possible outcomes. Such near-deterministic systems arise in several real-world applications. For example, in human physiology, the widely varying evolution rates of physiological variables make certain trajectories much more likely than others; in natural language, a very small fraction of all possible word sequences accounts for a disproportionately high amount of probability under a language model. In such settings, it is often possible to obtain significant computational savings by focusing on the outcomes where the probability mass is concentrated. This contrasts with existing algorithms in probabilistic inference---such as junction tree, sum product, and belief propagation algorithms---which are well-tuned to exploit conditional independence relations.The first topic addressed in this thesis is thestructure of discrete-time temporal graphical models ofnear-deterministic stochastic processes.  We show how the structuredepends on the ratios between the size of the time step and theeffective rates of change of the variables. We also prove that accurateapproximations can often be obtained by sparse structures even for verylarge time steps. Besides providing an intuitive reason for causal sparsity in discrete temporal models, the sparsity also speeds up inference.The next contribution is an eigenvalue algorithm for a linear factored system (e.g., dynamic Bayesian network), where existing algorithms do not scale since the size of the system is exponential in the number of variables. Using a combination of graphical model inference algorithms and numerical methods for spectral analysis, we propose an approximate spectral algorithm which operates in the factored representation and is exponentially faster than previous algorithms.The third contribution is a temporally abstracted Viterbi (TAV) algorithm. Starting with a spatio-temporally abstracted coarse representation of the original problem, the TAV algorithm iteratively refines the search space for the Viterbi path via spatial and temporal refinements. The algorithm is guaranteed to converge to the optimal solution with the use of admissible heuristic costs in the abstract levels and is much faster than the Viterbi algorithm for near-deterministic systems.The fourth contribution is a hierarchical image/video segmentation algorithm, that shares some of the ideas used in the TAV algorithm. A supervoxel tree provides the abstraction hierarchy for this application. The algorithm starts working with the coarsest level supervoxels, and refines portions of the tree which are likely to have multiple labels. Several existing segmentation algorithms can be used to solve the energy minimization problem in each iteration, and admissible heuristic costs once again guarantee optimality. Since large contiguous patches exist in images and videos, this approach is more computationally efficient than solving the problem at the finest level of supervoxels.The final contribution is a family of Markov Chain Monte Carlo (MCMC) algorithms for near-deterministic systems when there exists an efficient algorithm to sample solutions for the corresponding deterministic problem. In such a case, a generic MCMC algorithm's performance worsens as the problem becomes more deterministic despite the existence of the efficient algorithm in the deterministic limit. MCMC algorithms designed using our methodology can bridge this gap.The computational speedups we obtain through the various new algorithms presented in this thesis show that it is indeed possible to exploit near-determinism in probabilistic systems. Near-determinism, much like conditional independence, is a potential (and promising) source of computational savings for both exact and approximate inference. It is a direction that warrants more understanding and better generalized algorithms.",ucb,,https://escholarship.org/uc/item/0g63029f,,,eng,REGULAR,0,0
163,1599,The Drama in Disguise: Dramatic Modes of Narration and Textual Structure in the Mid-Nineteenth-Century Russian Novel,"Wiggins, Kathleen Cameron","Paperno, Irina;",2011,"My dissertation investigates the generic interplay between the textual forms of drama and the novel during the 1850s, a fertile ""middle ground"" for the Russian novel, positioned between the works of Pushkin, Lermontov, and Gogol and the psychological realist novel of the 1860s and 70s. My study begins with Turgenev's Rudin (1856) and then considers Goncharov's Oblomov (1859) and Dostoevsky's Siberian novellas (1859), concluding with an examination of how the use of drama evolved in one of the ""great novels"" of the 1860s, Tolstoy's Voina i mir (War and Peace, 1865-69). Drawing upon both novel and drama theory, my dissertation seeks to identify the specific elements of the dramatic form employed by these nineteenth-century novelists, including dramatic dialogue and gesture, construction of enclosed stage-like spaces, patterns of movement and stasis, expository strategies, and character and plot construction. Each chapter examines a particular combination of these dramatic narrative strategies in order to pinpoint the distinct ways in which the form of the drama aided writers in their attempts to create a mature Russian novel. I also address the ways in which both characters and narrators discuss and make reference to drama and theatricality, revealing their ambivalence toward a genre and expressive mode in which they themselves participate. Finally, my dissertation traces a trajectory in the use of dramatic modes of narrative throughout the decade of the 1850s; while Turgenev, Goncharov, and Dostoevsky foreground their use of drama, Tolstoy strives to place his under disguise. As a whole, my dissertation seeks to add to our understanding of the enigmatic rise of the Russian novel in the nineteenth century by illuminating the importance of the dramatic form in this process.",ucb,,https://escholarship.org/uc/item/0g68t49h,,,eng,REGULAR,0,0
164,1600,New Approaches to the Asymmetric Traveling Salesman and Related Problems,"Ahmadipouranari, Nima","Rao, Satish B;",2015,"The Asymmetric Traveling Salesman Problem and its variants are optimization problems that are widely studied from the viewpoint of approximation algorithms as well as hardness of approximation. The natural LP relaxation for ATSP has been conjectured to have an $O(1)$ integrality gap. Recently the best known approximation factor for this problem was improved from the decades-old $O(\log(n))$ to $O(\log(n)/\log\log(n))$ using the connection between ATSP and Goddyn's Thin Tree conjecture.In this work we show that the integrality gap of the famous Held-Karp LP relaxation for ATSP is bounded by $\log\log(n)^{O(1)}$ which entails a polynomial time $\log\log(n)^{O(1)}$-estimation algorithm; that is we provide a polynomial time algorithm that finds the cost of the best possible solution within a $\log\log(n)^{O(1)}$ factor, but does not provide a solution with that cost. This is one of the very few instances of natural problems studied in approximation algorithms where the state of the art approximation and estimation algorithms do not match.We prove this by making progress on Goddyn's Thin Tree conjecture; we show that every $k$-edge-connected graph contains a $\log\log(n)^{O(1)}/k$-thin tree.To tackle the Thin Tree conjecture, we build upon the recent resolution of the Kadison-Singer problem by Marcus, Spielman, and Srivastava. We answer the following question by providing sufficient conditions: Given a set of rank 1 quadratic forms, can we select a subset of them from a given collection of subsets, whose total sum is bounded by a fraction of the sum of all rank 1 quadratic forms?Finally we address the problem of designing polynomial time approximation algorithms, algorithms that also output a solution, matching the guarantee of the estimation algorithm. We prove that this entirely relies on finding a polynomial time algorithm for our extension of the Kadison-Singer problem. Namely we prove that ATSP can be $\log(n)^\epsilon$-approximated in polynomial time for any $\epsilon>0$ and that it can be $\log\log(n)^{O(1)}$-approximated in quasi-polynomial time, assuming access to an oracle which solves our extension of Kadison-Singer.",ucb,,https://escholarship.org/uc/item/0gf14980,,,eng,REGULAR,0,0
165,1601,The Large-scale Structure of the Universe: Probes of Cosmology and Structure Formation,"Noh, Yookyung","White, Martin;Quataert, Eliot;",2013,"The usefulness of large-scale structure as a probe of cosmology and structure formation is increasing as large deep surveys in multi-wavelength bands are becoming possible. The observational analysis of large-scale structure guided by large volume numerical simulations are beginning to offer us complementary information and crosschecks of cosmological parameters estimated from the anisotropies in Cosmic Microwave Background (CMB) radiation. Understanding structure formation and evolution and even galaxy formation history is also being aided by observations of different redshift snapshots of the Universe, using various tracers of large-scale structure.This dissertation work covers aspects of large-scale structure from the baryon acoustic oscillation scale, to that of large scale filaments and galaxy clusters. First, I discuss a large- scale structure use for high precision cosmology. I investigate the reconstruction of Baryon Acoustic Oscillation (BAO) peak within the context of Lagrangian perturbation theory, testing its validity in a large suite of cosmological volume N-body simulations. Then I consider galaxy clusters and the large scale filaments surrounding them in a high resolution N-body simulation. I investigate the geometrical properties of galaxy cluster neighborhoods, focusing on the filaments connected to clusters. Using mock observations of galaxy clusters, I explore the the correlations of scatter in galaxy cluster mass estimates from multi-wavelength observations and different measurement techniques. I also examine the sources of the correlated scatter by considering the intrinsic and environmental properties of clusters.",ucb,,https://escholarship.org/uc/item/0gg775z6,,,eng,REGULAR,0,0
166,1602,Bees in urban landscapes: An investigation of habitat utilization,"Wojcik, Victoria A.","McBride, Joe R;",2009,"Bees are one of the key groups of anthophilies that make use of the floral resources present within urban landscapes. The ecological patterns of bees in cities are under further investigation in this dissertation work in an effort to build knowledge capacity that can be applied to management and conservation. 	Seasonal occurrence patterns are common among bees and their floral resources in wildland habitats. To investigate the nature of these phenological interactions in cities, bee visitation to a constructed floral resource base in Berkeley, California was monitored in the first year of garden development. The constructed habitat was used by nearly one-third of the locally known bee species. Bees visiting this urban resource displayed distinct patterns of seasonality paralleling those of wildland bees, with some species exhibiting extended seasons.	Differential bee visitation patterns are common between individual floral resources. The effective monitoring of bee populations requires an understanding of this variability. To investigate the patterns and trends in urban resource usage, the foraging of the community of bees visiting Tecoma stans resources in three tropical dry forest cities in Costa Rica was studied. Substantial variability was noted between individual T. stans resources in each of the three populations. The observed variability is driven by the quality of the food resource as measured by the number of individual flowers available. Additionally, the regional landscape plays a role in general species occurrence patterns at a resource. 	The urban landscape presents a heterogeneous mosaic patchwork of habitat resources. To investigate the influence of this local variability on resource usage, the foraging patterns of bees in tropical and temperate landscapes were examined. In the dry forest of Costa Rica, bee foraging on T. stans was studied in the cities of Bagaces, Cañas, and Liberia. In the coastal grassland region of California, bee foraging on California poppy (Eschscholzia californica) was studied in the cities of Berkeley, Emeryville, and Oakland. In both regions, resource abundance and spatial distribution were the main drivers of bee visitation in all taxon groups. Land use and uniquely urban landscape variables influenced the occurrence of certain bee taxa.",ucb,,https://escholarship.org/uc/item/0gp9j2q7,,,eng,REGULAR,0,0
167,1603,"Diversity, Institutions and Economic Outcomes","Santacreu Vasut, Estefania","DeLong, J.Bradford;",2010,"Why may social diversity be bad for growth? In this thesis, I arguethat diversity affects the extent of information asymmetries thatdetermine the design of contracts and institutions. Becauseinformation asymmetries generate information rents, these contractsand institutions foster lower economic growth and persist over time.I proceed as follows: First, I model the impact of workforcediversity on the design of contracts and the shape of the firm. Ifind that diversity decreases the incentives given inprincipal-agent interactions and multiplies the number of layersbureaucracies need. Furthermore, the relation between diversity andproductivity is institution dependent. Second, I compare the spreadof industrialization in Japan and British India; and I provide newevidence of the organization, managerial beliefs, and workforcediversity of the three biggest textile centers in Bombay province. Ifind that workforce diversity was pervasive in British India, butnot in Japan, allowing the latter but not the former to introduceorganizational improvements and develop. In British India, centerswith higher workforce diversity had more supervisors per worker andtheir managers were the most likely to believe that their workerswere lazy.",ucb,,https://escholarship.org/uc/item/0gx5w2kw,,,eng,REGULAR,0,0
168,1604,Phonological Encoding in Aided Augmentative and Alternative Communication,"Dukhovny, Elena","Soto, Gloria;",2011,"Short-term memory for words is typically described in terms of phonological storage and rehearsal. However, research has shown that task demands, such as alternative means of output, may alter characteristics of short-term word storage. Alternative / Augmentative Communication (AAC) via high-technology Speech Generating Devices (SGDs), typically used by people with profound communication impairments, involves production of words via device-specific motor sequences. No study, however, has systematically considered potential effects of SGD-based production on short-term memory for words. In the current study, modality of short-term word storage was evaluated in a group of adult typical speakers trained to use SGDs, as well as a small group of authentic long-term users of SGDs. Results indicated that neurotypical subjects continued to store word lists phonologically when using SGDs, while authentic users of SGDs demonstrated phonological encoding more strongly during recall of high frequency `core' words than during recall of lower frequency `fringe' words. Thus, phonological encoding appears to remain a robust means of short-term word storage across output modalities.",ucb,,https://escholarship.org/uc/item/0h56354d,,,eng,REGULAR,0,0
169,1605,Cooperative Multiplexing in Wireless Relay Networks,"Nagpal, Vinayak","Nikolic, Borivoje;",2012,"Wireless networks are experiencing an explosive growth in the number of users and the demand for data capacity. One of the methods to improve capacity is to use tighter cooperation between terminals. In order to design a cooperative wireless link, several theoretical as well as practical challenges need to be addressed. In this dissertation we develop tools for the design of practical cooperative links that perform very close to fundamental limits.Using the tools of information theory, we begin by showing that cooperative relaying provides additional degrees-of-freedom for communication. For a simple network with a single-antenna source, single-antenna half-duplex relay and a two antenna destination, we show that cooperation allows the link throughput to increase approximately by a factor of 2.This gain is achievable using the recently introduced quantize-map-and-forward (QMF) cooperation scheme. However, QMF requires joint decoding of multiple information streams at the destination. The computational complexity of joint decoding is prohibitive for practical implementation. We address this problem by developing a low-complexity practical coding and system design framework for QMF relaying. The framework presents several pragmatic design choices to achieve cooperative degree-of-freedom gains in practice. The framework uses a combination of LDPC and LDGM codes decoded jointly over a low complexity factor graph. Signal processing requirements at all terminals are shown to have linear time complexity. Density evolution tools are developed for the design of specialized linear codes and mapping functions. Based on these tools, we demonstrate the design of cooperative links that perform within 0.5-1.0dB of information-theoretic limits.",ucb,,https://escholarship.org/uc/item/0h68q09r,,,eng,REGULAR,0,0
170,1606,Exposure to Ambient Air Pollution and Potential Biological Mechanisms/Biomarkers in Minority Children with Asthma Living in the United States,"LEE, EUNICE YUJUNG","Eisen, Ellen;",2017,"Rationale: Exposure to ambient air pollution is a major environmental risk factor for chronic diseases such as asthma. Children with asthma can be even more susceptible to the effects of air pollution since their respiratory system is not fully developed and some of the air pollutants can trigger asthma attacks. Over the past decades, scientists and researchers recognized the need to improve our understanding in the biological response mechanisms. The exact underlying mechanisms linking air pollution to disease outcomes, however, are not clear. Objectives: The overarching aims of this thesis are to investigate the association between exposure to ambient air pollutants and adverse health effects among minority children and identify potential biological pathways from exposure to health end points by considering genetic ancestry and, asthma endotype (atopy) as effect modifiers of the relation between air pollution and telomere length. In Chapter 1, we investigated the association between ambient air pollutants and asthma exacerbations in urban minority children, as well as effect modification by atopy status and African ancestry. In Chapter 2, we conducted a pilot study to gather preliminary information about how telomere length varies in relation to polycyclic aromatic hydrocarbons (PAH) exposure in children living in a highly polluted city. In Chapter 3, we examined the association between ambient air pollutants and telomere length in minority children to understand the potential damage caused by air pollution at the molecular level.    Methods: In Chapter 1, air pollutant exposures were estimated based on residence using U.S. EPA monitoring data and inverse distance weighting. The associations between average daily exposures and asthma exacerbations were estimated by the incident rate ratio (IRR) from a negative binomial regression model. In Chapter 2, we selected asthmatic and non-asthmatic subjects based on their annual average PAH level and described patterns of telomere length, measured by using uniplex polymerase chain reaction (PCR). In Chapter 3, the annual average daily exposure to each of four air pollutants was examined in relation to telomere length. Results: In chapter 1, exposure to ambient O3 and NO2 were associated with asthma exacerbations. Results for PM2.5 were null. Exposure-response relationships were linear for O3 and NO2 among non-atopic subjects and inconsistent among atopic subjects. Effect modification by African genetic ancestry was present only for O3; the impact of exposure appeared to be larger for those with higher African ancestry. In chapter 2, we found an inverse linear relationship between PAH and telomere length in a small pilot study. In chapter 3, the association between ambient SO2 and telomere length was significantly negative, whereas results for PM2.5, NO2 and O3 were null. Conclusions: Our results provide further evidence that exposure to ambient air pollution is a serious environmental risk factor that causes adverse health outcomes among minority children.",ucb,,https://escholarship.org/uc/item/0h8074z3,,,eng,REGULAR,0,0
171,1607,Vibration Harvesting using Electromagnetic Transduction,"Waterbury, Andrew","Wright, Paul K;",2011,"Embedded condition monitoring sensors that eliminate unanticipated failures of critical or high value equipment improve asset utilization while streamlining maintenance and support operations. General Electric and Rolls Royce use embedded sensors on their jet engines that have eliminated failures and allowed maintenance to be performed on an as-needed basis. As a result, airplane utilization increases while enabling the consolidation of maintenance operations. In more traditional industrial settings, condition monitoring of manufacturing and industrial equipment can quickly impact bottom lines through improved productivity and streamlined operations in ways comparable to what is already being realized in the aviation industry.Wireless sensor nodes provide embedded sensing with little overhead or infrastructure cost as long as appropriate power sources are available to sustain the node over its target lifetime. Energy is the limiting factor for sensor node lifetimes and data streams. Since most manufacturing and industrial equipment have some associated vibration spectrum when operating, transducing the mechanical energy of vibrations to a small amount of electrical energy electromagnetically was explored as a way of powering condition monitoring sensors.  Large pump motors and a machine tool were surveyed to characterize input vibration accelerations associated with manufacturing and industrial operations. Harvestable acceleration peaks occurred below 120 Hz and had magnitudes near or less than 0.1 g. Metal cutting vibrations were characterized and shown to have vibration frequencies proportional to the number of cutting teeth and the spindle RPM. It was also shown that, the 0.4-1.0 g acceleration impulses associated with the rapid axis motion of a machine tool are harvestable. Simple magnet and coil as well as coreless electromagnetic architectures were pursued using an overall device size constrain of a cube with 2.5 cm sides. That device size was roughly the same as a c-cell battery that is capable of powering a wireless sensor node for five to ten years. The target power for the harvester designs was the time-averaged powers of hundreds of microwatts to single milliwatts required by commercial wireless sensor nodes. Prototype vibration harvesters based on magnet-coil and voice-coil transducer designs were fabricated and evaluated. Both were able to produce about a milliwatt on a vibration platform for an input acceleration amplitude of 0.1 g at frequencies consistent with those characterized on the pump motors and machine tool. The power densities of the unoptomized proof of concept prototypes were comparable to commercial vibration harvester but at less than one seventh the size. The voice-coil prototype was installed on several 15-30 kW pump motors running support systems for a microfabrication lab, and unrectified powers of 0.2¬-1.5 mW were harvested. Similarly, 0.8-1.8 mW was harvested from metal cutting vibrations while facemilling cast iron and stainless steel, showing that powers comparable to commercial sense node requirements could be harvested from industrial settings.  Coreless motor architectures proved to be best suited for industrial settings because the unconstrained magnetic flux of simple magnet-coil designs interacted with the iron and steel mounting surfaces commonly found on large machines. Simulated coreless magnetic circuit designs showed that gap magnetic flux densities approaching one tesla could be possible but were not implemented.",ucb,,https://escholarship.org/uc/item/0hg645mt,,,eng,REGULAR,0,0
172,1608,Essays on Corruption and Political Favoritism,"Szucs, Ferenc","Finan, Frederico;",2018,"Corruption and political favoritism are considered major impediments to economic development. Although there is a growing consensus about the adverse efficiency consequences of corruption we still have a limited understanding of how corruption is shaped by political and economic institutions and how it affects our democracies. An increasing literature documents political favoritism and its welfare consequences relative to a no misallocation benchmark. In my dissertation, I complement this line of research by quantifying the effects of favoritism relative to relevant policy counterfactuals. My work highlights the importance of transparency and limiting regulatory discretion in improving the efficiency of public spending.In the first chapter, I investigate the determinants and consequences of increasing a buyer's discretion in public procurement. I study the role of discretion in the context of a Hungarian policy reform which removed the obligation of using an open auction for contracts under a certain anticipated value. Below this threshold, buyers can use an alternative  ""high-discretion"" procedure to purchase goods and services. At the threshold, I document large discontinuities in procurement outcomes, but I also find a discontinuity in the density of anticipated contract value, indicating that public agencies set contract values strategically to avoid auctions. To distinguish the causal effects of increased discretion from the self-selection of agencies into high-discretion procedures, I exploit the time variation of the policy reform. I find that discretion increases the price of contracts and decreases the productivity of contractors. To dig deeper into the motivations of public agencies, I use a structural model to identify discretion's impact on rents from corruption.  I also use the same structural approach to simulate the effect of alternative value thresholds. I find that the actual threshold redistributes about 2 percent of the total contract value from taxpayers to firms and decreases the average productivity of contractors by approximately 1.6 percent. My simulations suggest that the optimal threshold would be about a third of the actual. Moreover, case studies suggest that in addition to rent extraction corruption provides opportunities to buy political support in weakly institutionalized democracies (e.g. McMillan and Zoido (2004)). Consequently, detrimental effects of political favoritism may not be limited to misallocation of public resources but also constrain governmental accountability. In the second chapter, my coauthor Adam Szeidl and I confirm this conclusion by investigating political favoritism in the Hungarian media market. We scrutinize three different markets, printed media, billboards, and online newspapers. We establish three main results about favoritism in the Hungarian media. First, we document distortive two-way favors between politicians and the media, in the form of government advertising and media coverage. For both directions of favors, our empirical strategy is to compare the allocations of actors with changing versus unchanging connection status. More specifically we compare advertising behavior of state-owned and private companies and media content of outlets with more and less political connections. Since friendly news coverage systematically moves together with advertising favors we interpret our findings as media capture. Second, we document an organizational change in favoritism: a first phase when favored media was controlled by a single connected investor; a second phase when this relationship broke down and two-way favors were terminated; and a third phase when control of newly favored media was divided between multiple connected investors. Our preferred interpretation is that governments with more de-jure power shift the organization of favors towards a divide-and-rule style arrangement. Third, we develop and implement a portable structural approach to measure the economic cost of misallocative favoritism.",ucb,,https://escholarship.org/uc/item/0hg9234w,,,eng,REGULAR,0,0
173,1609,"Competition, Entrepreneurship, and Innovation","Wang, Xinxin","Malmendier, Ulrike;Morse, Adair;",2017,"What kind of market structure promotes innovation and growth? This dissertation delves into the relationship between market power, innovation, and returns. On one hand, competition exerts downward pressure on costs and provides incentives for efficient organization of production. On the other, the Schumpeterian hypothesis suggests that monopolist rents are necessary to encourage technological disruption. I study the effects of competition in two distinct settings: the acquisition of early-staged high-growth startups and horizontal mergers between public companies. In Chapter 1, Catering Innovation: Entrepreneurship and the Acquisition Market, I study the role of the financial market of acquisitions on an inventor's decision to begin a new venture and his or her subsequent innovation. After documenting its increasing importance as the dominant exit path for entrepreneurs, I test a novel catering theory of innovation: Does the market structure of potential acquirers have a measurable impact on inventors' start-up decisions? I construct a new dataset of early stage start-ups using the uniquely broad coverage of CrunchBase and VentureXpert data. I disambiguate and match the resulting data to employment data from LinkedIn and to the entire universe of patent data. Using the prior citation history of entrepreneurs for exogenous variation, I construct a formal proxy variable and employ the Heckman selection model to establish causality. I find that a one standard deviation increase in acquirer market concentration decreases the inventor's propensity to become an entrepreneur by 4%. This first result suggests that fragmented markets are appealing entry markets. My main finding is that a one standard deviation increase in acquirer concentration and market size increases the quality of patents, as measured by citations per patent, and the catering of entrepreneurs, as measured by technological overlap with potential acquirers. The magnitudes suggest that 5-16% of entrepreneurial innovation can be attributed to the influence of acquisition markets, particularly in the information technology and biotechnology industries. In Chapter 2, Market Power in Mergers and Acquisitions, I explore the value implications of market power changes in public mergers and acquisitions. Using a large sample of horizontal mergers from 1980 to 2012, I provide evidence that a significant portion of merger announcement returns are explained by changes in market power resulting from the merger. I find that a 0.1 expected increase in product market concentration leads to a 2.3% increase in cumulative abnormal returns over a three day period. In the long-run, mergers with high expected changes in market power revert to pre-announcement levels. My results suggest the ""announcement effect"" in M&A is due to misplaced market power expectations by investors.",ucb,,https://escholarship.org/uc/item/0hx4s77m,,,eng,REGULAR,0,0
174,1610,Studies in Optics and Optoelectronics,"Byrnes, Steven John Feinman","Wang, Feng;",2012,"This thesis will detail four projects aimed at understanding and applying the principles of optics and optoelectronics.In Chapter 1, we describe phase-sensitive sum-frequency vibrational spectroscopy (PS-SFVS), a nonlinear optical technique that can probe the molecular structure of the top few monolayers of a liquid-vapor interface. We use this technique to investigate the air-water interface, using a number of water samples with different dissolved salts. The information is used to draw inferences about the surface propensity of these salt ions—information that can shed light on both atmospheric chemistry and water solvation theory. We also give a detailed description of the experimental methodology for PS-SFVS, its rationale, and the issues that can arise.PS-SFVS measurements, such as those described in Chapter 1, can be fruitfully used by comparing them with the signal predicted by molecular simulation. However, the relationship between a molecular configuration and its nonlinear optical signal is not thoroughly understood in the theoretical chemistry community. In particular, the procedures used in the literature to predict an PS-SFVS signal within a molecular simulation have been ambiguous, depending on arbitrary parameters. In Chapter 2, we review PS-SFVS theory at a fundamental level, then map it to modern simulation methods, thereby explaining the ambiguities as consequences of improper truncation of a multipole expansion. A molecular-dynamics simulation of the water-air interface is used as an example, illustrating the consequences of different simulation methods and suggesting which ones should be most accurate.Chapter 3 explores a different aspect of nonlinear optics: The compression and characterization of ultrafast pulses of light. These pulses have been explored for a variety of scientific and technological applications. Ideally, an optical pulse can be reduced in duration up to the limit imposed by its spectral bandwidth via the uncertainty principle. However, the presence of ""nonlinear chirp"" (different frequencies arriving at different times in a nonlinear fashion), which is especially common in mode-locked fiber lasers,  can be a major factor preventing the shortening of a pulse. We describe a new technology, a type of patterned glass phase plate, that promises to reduce nonlinear chirp in a convenient, adjustable, inexpensive, and high-throughput manner. After showing simulations, we describe how we made the plate, and then how we used frequency-resolved optical gating (FROG) to watch the plate change the duration and shape of a pulse from a fiber laser.Finally, Chapter 4 discusses a new architecture for solar cells that uses the field effect, rather than the traditional p-n junction, to separate charge. This could be advantageous for semiconductor materials that are difficult to dope to both p- and n-type, such as oxides, sulfides, and nanoparticles. We discuss the underlying physics and rule-of-thumb design principles, along with both finite element simulations and experimental verifications.",ucb,,https://escholarship.org/uc/item/0k0918t7,,,eng,REGULAR,0,0
175,1611,Oracle-Guided Design and Analysis of Learning-Based Cyber-Physical Systems,"Ghosh, Shromona","Seshia, Sanjit A.;Sangiovanni-Vincentelli, Alberto L.;",2019,"We are in world where autonomous systems, such as self-driving cars, surgical robots, robotic manipulators are becoming a reality. Such systems are considered \textit{safety-critical} since they interact with humans on a regular basis. Hence, before such systems can be integrated into our day to day life, we need to guarantee their safety. Recent success in machine learning (ML) and artificial intelligence (AI) has led to an increase in their use in real world robotic systems. For example, complex perception modules in self-driving cars and deep reinforcement learning controllers in robotic manipulators. Although powerful, they introduce an additional level of complexity when it comes to the formal analysis of autonomous systems.  In this thesis, such systems are designated as Learning-Based Cyber-Physical Systems~(LB-CPS). In this thesis, we take inspiration from the Oracle-Guided Inductive Synthesis~(OGIS) paradigm to develop frameworks which can aid in achieving formal guarantees in different stages of an autonomous system design and analysis pipeline. Furthermore, we show that to guarantee the safety of LB-CPS, the design (synthesis) and analysis (verification) must consider feedback from the other.  We consider five important parts of the design and analysis process and show a strong coupling among them, namely (i) Robust Control Synthesis from High Level Safety Specifications; (ii) Diagnosis and Repair of Safety Requirements for Control Synthesis; (iii) Counter-example Guided Data Augmentation for training high-accuracy ML models; (iv) Simulation-Guided Falsification and Verification against Adversarial Environments; and (v) Bridging Model and Real-World Gap. Finally, we introduce a software toolkit \verifai{} for the design and analysis of AI based systems, which was developed to provide a common formal platform to implement design and analysis frameworks for LB-CPS.",ucb,,https://escholarship.org/uc/item/0tm3q8b5,,,eng,REGULAR,0,0
176,1612,Manipulation-resistant online learning,"Christiano, Paul Francis","Vazirani, Umesh;",2017,"Learning algorithms are now routinely applied to data aggregated from millions of untrusted users, including reviews and feedback that are used to define learning systems’ objectives. If some of these users behave manipulatively, traditional learning algorithms offer almost no performance guarantee to the “honest” users of the system. This dissertation begins to fill in this gap.Our starting point is the traditional online learning model. In this setting a learner makes a series of decisions, receives a loss after each decision, and aims to achieve a total loss which is nearly as low as if they had chosen the best fixed decision-making strategy in hindsight.We extend this model by introducing a set of users U. Each of the learner’s decisions is made on behalf of a particular user u ∈ U, and u reports the loss they incur from the decision. We assume that there is some (unknown ) set of “honest” users H ⊂ U, who report their losses honestly, while the other users may behave adversarially. Our goal is to ensure that the total loss incurred by users in H is nearly as small as if all users in H had used the single best fixed decision-making strategy in hindsight. We say that an algorithm is manipulation-resistant if it achieves a bound of this form.This dissertation proposes and analyzes manipulation-resistant algorithms for prediction with expert advice, contextual bandits, and collaborative filtering. These algorithms guarantee that the honest users perform nearly as well as if they had known each others’ identities in advance, pooled all of their data, and then used a traditional learning algorithm. This bounds the total amount of damage that can be done per manipulative user. More significantly, we give bounds that can be considerably smaller in the realistic setting where the users are vertices of a graph (such as a social graph) with disproportionately few edges between honest and manipulative users.As a key technical ingredient, we introduce the problem of online local learning, and propose a novel semidefinite programming algorithm for this problem. This algorithm allows us to effectively perform online learning over the exponentially large space of all possible sets H ⊂ U, and as a side-effect provides the first asymptotically optimal algorithm for online max cut.",ucb,,https://escholarship.org/uc/item/0w22c86t,,,eng,REGULAR,0,0
177,1613,SHAPE (Sound and Habitat Audio Prototyping Environment),"Cress, Jason","Campion, Edmund;",2020,"Sound and Habitat Audio Prototyping Environment (SHAPE) is a collection of nature-inspired electroacoustic devices created for sound art in public spaces. It is part of an audio feedback research project at the Center for New Music and Audio Technologies (CNMAT). By repurposing discarded electronics and manufactured objects, low-cost materials are used to make interactive sound sculptures and novel music instruments. Subtle gestures and actions by participants change the sound in real time. The project attempts to question the dichotomy between sound art and common environmental sounds through a zero-waste, collective action framework. With SHAPE, natural and artificial materials converge; construction and deconstruction hold equal weight; raw materials reclaim another existence; and sounds from unusual sources expand into fully resonating bodies. Audio transfer is based on two input types for each device: a piezoelectric contact mic and an electret air mic. These elements combine to sense both vibration in material and pressure waves in the air. Sound energy is then converted into an analog and a digital signal. Both analog and digital electronic environments are highly programmable, allowing for quick on-site prototyping. Six devices from this project will be highlighted and described in detail. Aside from the PCB fabrication, smartphone, and case construction, all of the e-components for these devices can be easily found in old discarded speaker systems and reused. Proprietary devices such as the iRig are currently being used, but these will be reverse engineered for future open-access integration. Open-source software such as Pure Data and MobMuPlat make any Android or iOS device compatible with this system, thus facilitating second-hand use of virtually all smartphone models. Considering the portability and cost effectiveness of this project, SHAPE is particularly adept at facilitating outdoor applications such as sound installations or musical performances.",ucb,,https://escholarship.org/uc/item/0wh27245,,,eng,REGULAR,0,0
178,1614,Studies on Effects of Arsenic on Human Beta-Defensin-1,"Dangleben, Nygerma Laurent","Smith, Martyn T;Skibola, Christine F;",2012,"Arsenic (As) is a well established cause of cancer in humans, and increasing evidence indicates that As has deleterious effects on the immune system that are not directly related to carcinogenesis. However, the mechanisms of As toxicity remain poorly understood. Our laboratory previously reported decreased urinary levels of human beta-defensin-1 (HBD1) peptides in As-exposed individuals from two cross-sectional studies based in Nevada and Chile, and confirmed in vitro that As exposure suppressed HBD1 mRNA expression which is encoded by the DEFB1 gene. DEFB1 is constitutively expressed in epithelial tissues, plays a role in both the innate and adaptive branches of the immune system, and is implicated in anti-tumor immunity. Therefore, the objectives of this dissertation are to review the immunotoxicological effects of As, characterize the effects of As on DEFB1 gene and protein expression in relevant in vitro model systems, investigate the molecular mechanisms mediating these effects, and explore the influence of other metals on HBD1 levels. A comprehensive review of the literature on the immune-related effects associated with As exposure in humans, animals and in vitro models reveals that chronic exposure to As can severely impair various aspects of immune function and consequently result in elevated risk of infections and chronic diseases. However, further investigation is needed to better understand the relationship between As exposure and the development of disease, and several recommendations are discussed to help bridge the gaps in knowledge.The current research investigated the effects of As exposure on DEFB1 in cells derived from target tissues of toxicity using immortalized non-tumorigenic human HOK-16B keratinocytes and HK-2 kidney epithelial cells. DEFB1 mRNA levels were more abundant in HK-2 cells than in HOK-16B cells, and were suppressed by exposure to arsenite (AsIII) or monomethylarsonous acid (MMAIII), the postulated more toxic metabolite. The suppressive effect of AsIII and MMAIII treatments on DEFB1 transcript levels continued for several passages after removal of As. HBD1 peptide levels were significantly reduced following exposure to AsIII, but were not affected by treatment with lead, cadmium or chromium, suggesting that decreased HBD1 may be a specific response to As. Finally, AsIII treatment was found to suppress DEFB1 promoter activity, indicating that the inhibition of DEFB1 mRNA by As is likely due to transcriptional down-regulation. Taken together, the research presented here provides evidence that our previous findings of decreased urinary HBD1 levels are likely due to a direct effect of As on the kidney, and suggest a novel mechanism by which As exposure may promote cancer development.This dissertation summarizes the known in vivo and in vitro effects of As on the immune system, characterizes the effects of As on DEFB1 using relevant cell culture models, and establishes DEFB1 as a potentially relevant biomarker of response to As. Future studies should address the role of DEFB1 inhibition in As immunotoxicity and carcinogenicity.",ucb,,https://escholarship.org/uc/item/0xm6v10q,,,eng,REGULAR,0,0
179,1615,Microseismic event location with multiple arrivals: application in the Newberry Enhanced Geothermal System and the Marcellus Shale,"Zhang, Zhishuai","Rector, James W.;",2017,"Multistage fracturing technique, together with horizontal drilling, make production from organic-rich shale possible. Microseismic monitoring of hydraulic fractures has been an important technology for far-field fracture diagnostics. It can provide us hydraulic fracture geometry and its growth behavior vs. time. Getting accurate microseismic event location is important to interpretation. Various methods originally developed for earthquake location have been used for microseismic event location.The main objective of this dissertation is to make use of multiple arrivals of microseismic data to improve microseismic event location accuracy. The improvement can be achieved from two aspect: (1) simultaneous inversion of multiple microseismic data for event locations and velocity model and (2) improving microseismic event location accuracy with head wave arrival time. We begin this dissertation by laying out the inverse problem theory as the basis of the simultaneous inversion. Then, we built a Bayesian framework to simultaneously invert for microseismic event locations and the velocity model. We developed a software package, BayesTomo, based on the simultaneous inversion framework. The first application is the simultaneous inversion of microseismic event locations and the velocity in a microseismic survey at Newberry Enhanced Geothermal System (EGS). We successfully applied the developed method on both synthetic examples and real data from the Newberry EGS. Comparisons with location results based on a traditional predetermined velocity model method demonstrated that we can construct a reliable effective velocity model using only microseismic data and determine microseismic event locations without prior knowledge of the velocity model.The second application is on the microseismic data acquired from a geophone array deployed in the horizontal section of a well drilled in the Marcellus Shale near Susquehanna County, Pennsylvania. We identified the existence of prominent head waves in some of the microseismic data. The head waves are refractions from the interface between the Marcellus and the underlying Onondaga Formation. The source locations of microseismic events can be significantly improved by using both the P-, S-wave direct arrival times and the head wave arrival times in place the traditional method of using direct arrival times and P-wave polarizations. The traditional method had substantially greater uncertainty in our data due to the large uncertainty in P-wave polarization direction estimation. Our method was applied to estimate the locations of perforation shots as well as microseismic events. Comparison with traditional location results shows improved location accuracy thanks to head wave arrival times.",ucb,,https://escholarship.org/uc/item/0xt5x2k6,,,eng,REGULAR,0,0
180,1616,"Crafting Words and Wood: Myth, Carving and Húsdrápa","Schjeide, Erik","Lindow, John;",2015,"In the poem Húsdrápa, ca. 985, Úlfr Uggason described woodcarvings of mythological scenes adorning an Icelandic hall owned by the chieftain Óláfr pái. The performance, of which some verses have been passed down in writing, was an act of referential intermedia, insofar as the art form of skaldic poetry presented with woven words the content of a wood-carved medium that has long since rotted away. Hence, the composition of the poem combined with the carvings created a link that opens up a union between extant literary sources and material culture which contributes to expanding cultural insight. This study draws from a range of sources in order to answer central research questions regarding the appearance and qualities of the missing woodcarvings. Intermedia becomes interdisciplinary in the quest, as archaeological finds of Viking Age and early medieval woodcarvings and iconography help fill the void of otherwise missing artifacts. Old Norse literature provides clues to the mythic cultural values imbued in the wooden iconography. Anthropological and other theories drawn from the liberal arts also apply as legend, myth and art combine to inform cultural meaning. The study reveals that the appearance and function of the woodcarvings merge as they were understood not only aesthetically but also to possess a certain agency. The dissertation is in two parts. The first portion provides background information regarding woodcarving and iconography found in Northern Europe as it refers to poetry, sagas and legends for contextualization. The second portion continues the investigation with an emphasis on Iceland and a close reading of the poem Húsdrápa. In these sections a synthesis of saga scenes, skaldic poetry, myth and applicable iconography informs analysis and hypothetical prototypes of the carvings.In addition to figures such as sketches and photos included in the appendix, there are eight animations available for download labeled Animation 1 - 8. Each of these animations help illustrate the appearance and qualities of both extant and hypothetical wood-carved mythic scenes from the Viking Age.",ucb,,https://escholarship.org/uc/item/1017z7d7,,,eng,REGULAR,0,0
181,1617,Cryo-EM Studies of Inflammasomes,"Haloupek, Nicole","Nogales, Eva;",2017,"Following invasion by disease-causing biological entities, before a threat-specific response is mounted by the adaptive immune system, the innate immune system initiates a campaign to restrict the pathogen. In animals and plants, members of the Nucleotide-binding domain, Leucine-rich repeat-containing (NLR) superfamily of proteins are sentinels of the innate immune system that detect a wide array of pathogens’ molecular signatures. NAIP5 (NLR family, apoptosis inhibitory protein 5), for example, is activated by binding the bacterial protein flagellin—a component of the flagellum many bacteria use for locomotion. After binding, the NAIP5–flagellin complex associates with multiple NLRC4 (NLR family, CARD [Caspase Activation and Recruitment Domain]-containing 4) protomers, forming an inflammasome that activates the protease Caspase-1 by juxtaposing the protease’s CARDs. The active Caspase-1 sparks a cascade that results in pyroptosis, a programmed form of cell death that summons an immune response and causes inflammation. Another such protein, NLRP1 (NLR family, pyrin domain containing 1), also forms inflammasomes that, when activated by cleavage by anthrax lethal factor, activate Caspase-1.I used cryo-electron microscopy (cryo-EM) to determine the structure of the NAIP5–NLRC4 inflammasome. Analysis of the structure of the NAIP5-NLRC4 inflammasome revealed how the bacterial ligand flagellin is detected and how the complex assembles and uncovered a possible mechanism by which NLRs restrict pathogen evasion of detection by the innate immune system. Structural investigation of the NLRP1 inflammasome is at an early—but promising—stage. In this thesis, I also describe the new methods for preparing cryo-EM samples that made this work possible and may be useful for cryo-EM studies of other macromolecular complexes.",ucb,,https://escholarship.org/uc/item/1087x3p1,,,eng,REGULAR,0,0
182,1618,Examining Factors Associated with Learning and Performance in Primary Care Graduate Medical Education Organizations,"Kim, Jung Gook","Rodriguez, Hector P;",2019,"Despite calls to improve Graduate Medical Education (GME), little is known about the organizational factors influencing training design, resident learning, and assessment. This dissertation examines the organizational behavior factors in primary care GME associated with time spent training in ambulatory care, resident clinical competency learning rates, and quality of care. Linked databases from medical education accreditors and policymakers, population health sources, federal cost reports, and an integrated health system were analyzed to investigate the extent to which primary care GME’s competing internal and external organizational factors influence the professional training environment and performance of primary care residents. Key findings include: 1. Experience in ambulatory care for residents varies among their ACGME-accredited programs, with more time in ambulatory care settings most strongly associated with additional faculty, receipt of federal Teaching Health Center GME funding, and accreditation warnings; 2. Improved resident learning rates in the Accreditation Council for GME (ACGME) Milestones for family medicine and internal medicine programs were more associated with external factors than internal factors. Patient care, practice-based learning and improvement, and systems-based practice learning rates were dependent on the program’s geographic setting, organizational structural characteristics, and the type of resident learning experiences; and 3. Healthcare Effectiveness Data and Information Set (HEDIS) measure reliability in ambulatory care for residents varies among ACGME-accredited primary care residency programs with potential opportunities to utilize publicly reported quality data for GME programs. Overall, these empirical studies help clarify the organizational and associated environmental factors influencing training design, resident learning, and performance in order to assist policymakers in understanding the fragmented GME learning environment and move GME toward improved accountability. As trainee experiences may have a downstream impact on patient care, the systematic study on primary care GME organizations helps improve the design of the resident learning environment and training the future primary care workforce, especially in ambulatory care, the most common delivery setting for primary care health services today.",ucb,,https://escholarship.org/uc/item/10h629k6,,,eng,REGULAR,0,0
183,1619,Phonetic Attention and Predictability: How Context Shapes Exemplars and Guides Sound Change,"Manker, Jonathan Taylor","Johnson, Keith;",2017,"In this dissertation, I investigate how word predictability in context modulates the listener’s attention to phonetic details, and how this in turn affects sound change.  Three sets of experiments are designed to investigate these questions:  In the first set of experiments, involving discriminability tasks, I demonstrate that (1) contextual predictability affects speech perception, and that listeners attend more to the phonetic details of unpredictable speech.  In the second set of experiments I use the phonetic accommodation paradigm to show that (2) the effect of contextual predictability on speech perception in turn affects speech production.   This by itself suggests relevance in sound change.  In the third set of experiments I apply the model to a specific example of sound change: the reduction of function words.  Using an error detection task I show that (3) listeners attend to the details of content words more than function words (with all other variables controlled for) which is linked to their differences in contextual predictability.  I then propose a two-step model of sound change involving the propagation of contextually-modulated variation with a perceptual (rather than production) bias followed by the acquisition of new variants.	The results build and expand on several strands of literature which have not been fully connected previously.   The findings for the effect of predictability on speech perception corroborate a number of past experiments showing that higher level linguistic information can have the effect of aiding speech recognition (Miller, Heise & Lichten 1951, Pollack & Pickett 1963), perceptually restoring missing information (Warren 1970, Marslen-Wilson, & Welsh 1978, Samuel 1981), or generally diverting attention from the raw auditory signal (Cole, Jakimik, & Cooper 1978, Ganong 1980).  Additionally, this research considers dual-processing models of speech perception (Norris & Cutler 1979, Lindblom et al. 1995, Hickok and Poeppel 2004, 2007) in a broader context, considering how word predictability and expectancy modulate the type of listening used.   The findings also add to the literature on exemplar theory (Johnson 1997, Pierrehumbert 2002, Goldinger 2007), particularly to hybrid models including both abstractions and exemplar clouds within the lexicon.  Finally, I propose a new model of perception-based sound change driven by contextual predictability that can account for cross-linguistically common patterns of function word and morpheme reduction (Bell et al. 2001, Jurafsky et al. 2001, Beckman 1998) that does not rely on teleological production-based accounts of reduction (Lindblom 1990, Alyett & Turk 2004).",ucb,,https://escholarship.org/uc/item/10r90282,,,eng,REGULAR,0,0
184,1620,Earthquake Resilient Tall Reinforced Concrete Buildings at Near-Fault Sites Using Base Isolation and Rocking Core Walls,"Calugaru, Vladimir","Panagiotou, Marios A;",2013,"This dissertation pursues three main objectives: (1) to investigate the seismic response of tall reinforced concrete core wall buildings, designed following current building codes, subjected to pulse type near-fault ground motion, with special focus on the relation between the characteristics of the ground motion and the higher-modes of response; (2) to determine the characteristics of a base isolation system that results in nominally elastic response of the superstructure of a tall reinforced concrete core wall building at the maximum considered earthquake level of shaking; and (3) to demonstrate that the seismic performance, cost, and constructability of a base-isolated tall reinforced concrete core wall building can be significantly improved by incorporating a rocking core-wall in the design.First, this dissertation investigates the seismic response of tall cantilever wall buildings subjected to pulse type ground motion, with special focus on the relation between the characteristics of ground motion and the higher-modes of response. Buildings 10, 20, and 40 stories high were designed such that inelastic deformation was concentrated at a single flexural plastic hinge at their base. Using nonlinear response history analysis, the buildings were subjected to near-fault seismic ground motions as well as simple close-form pulses, which represented distinct pulses within the ground motions. Euler-Bernoulli beam models with lumped mass and lumped plasticity were used to model the buildings. The response of the buildings to the close-form pulses fairly matched that of the near-fault records. Subsequently, a parametric study was conducted for the buildings subjected to three types of close-form pulses with a broad range of periods and amplitudes. The results of the parametric study demonstrate the importance of the ratio of the fundamental period of the structure to the period of the pulse to the excitation of higher modes. The study shows that if the modal response spectrum analysis approach is used--considering the first four modes with a uniform yield reduction factor for all modes and with the square root of sum of squares modal combination rule--it significantly underestimates bending moment and shear force responses. A response spectrum analysis method that uses different yield reduction factors for the first and the higher modes is presented. Next, this dissertation investigates numerically the seismic response of six seismically base-isolated (BI) 20-story reinforced concrete buildings and compares their response to that of a fixed-base (FB) building with a similar structural system above ground. Located in Berkeley, California, 2 km from the Hayward fault, the buildings are designed with a core wall that provides most of the lateral force resistance above ground. For the BI buildings, the following are investigated: two isolation systems (both implemented below a three-story basement), isolation periods equal to 4, 5, and 6 s, and two levels of flexural strength of the wall. The first isolation system combines tension-resistant friction pendulum bearings and nonlinear fluid viscous dampers (NFVDs); the second combines low-friction tension-resistant cross-linear bearings, lead-rubber bearings, and NFVDs. The designs of all buildings satisfy ASCE 7-10 requirements, except that one component of horizontal excitation is used in the two-dimensional nonlinear response history analysis. Analysis is performed for a set of ground motions scaled to the design earthquake (DE) and to the maximum considered earthquake (MCE). At both the DE and the MCE, the FB building develops large inelastic deformations and shear forces in the wall and large floor accelerations. At the MCE, four of the BI buildings experience nominally elastic response of the wall, with floor accelerations and shear forces being 0.25 to 0.55 times those experienced by the FB building. The response of the FB and four of the BI buildings to four unscaled historical pulse-like near-fault ground motions is also studied. Finally, this dissertation investigates the seismic response of four 20-story buildings hypothetically located in the San Francisco Bay Area, 0.5 km from the San Andreas fault. One of the four studied buildings is fixed-base (FB), two are base-isolated (BI), and one uses a combination of base isolation and a rocking core wall (BIRW). Above the ground level, a reinforced concrete core wall provides the majority of the lateral force resistance in all four buildings. The FB and BI buildings satisfy requirements of ASCE 7-10. The BI and BIRW buildings use the same isolation system, which combines tension-resistant friction pendulum bearings and nonlinear fluid viscous dampers. The rocking core-wall includes post-tensioning steel, buckling-restrained devices, and at its base is encased in a steel shell to maximize confinement of the concrete core. The total amount of longitudinal steel in the wall of the BIRW building is 0.71 to 0.87 times that used in the BI buildings. Response history two-dimensional analysis is performed, including the vertical components of excitation, for a set of ground motions scaled to the design earthquake and to the maximum considered earthquake (MCE). While the FB building at MCE level of shaking develops inelastic deformations and shear stresses in the wall that may correspond to irreparable  damage, the BI and the BIRW buildings experience nominally elastic response of the wall, with floor accelerations and shear forces which are 0.36 to 0.55 times those experienced by the FB building. The response of the four buildings to two historical and two simulated near-fault ground motions is also studied, demonstrating that the BIRW building has the largest deformation capacity at the onset of structural damage.",ucb,,https://escholarship.org/uc/item/1256t25p,,,eng,REGULAR,0,0
185,1621,Heaven is Empty: A Cross-Cultural Approach to Religion and Human Agency in Early Imperial China,"Marsili, Filippo","Nylan, Michael;",2011,"This dissertation is about the religious (extra-human) legitimation of political power during the Western Han dynasty (206 BCE- 9CE). It reexamines the correlation between religious, cultural, and political unity, closely analyzing Sima Qian's (ca. 145-86 BCE) Records of the Grand Historian (Shiji), the first universal narrative of Chinese civilization from its origins through the first century of the Western Han empire. This text became the model for all dynastic histories until 1911, when the imperial age came to an abrupt end. The contrast between Sima Qian's treatment of religious practices, official and unofficial, and accounts in the classical Greco-Roman historiography about imperial cults and propaganda provides an intriguing point of departure from which my thesis questions the applicability of paradigms imported and applied to the case of early China from the ancient Mediterranean world (e.g., “religion,” “metaphysics,” “divinity,” “sacred vs. secular,” “scripture,” “myth,” and “ritual”). This dissertation contributes to our understanding of the relationship between “religion,” “morality,” and “cultural identity” in China by calling into question those very categories.  By adopting a comparative approach, it shows how the discourse on the sacred by historians and philosophers has been often informed by intellectual prejudices and pre-formed conceptions that have hindered the mutual understanding between East and West.  To overcome these obstacles, this dissertation proposes a new trans-cultural attitude aimed both at the deconstruction of these ethnocentric biases and at the reconstruction of Sima Qian's own analytical criteria and concerns.",ucb,,https://escholarship.org/uc/item/12f937fh,,,eng,REGULAR,0,0
186,1622,"Iron, Oil, and Emeryville: Resource Industrialization and Metropolitan Expansion in the San Francisco Bay Area, 1850-1900","Lunine, Seth","Walker, Richard;",2013,"Scholars have largely overlooked the formative role of industry in both California's economic development and the San Francisco Bay Area's metropolitan expansion during the late nineteenth century.  Beginning in the early 1880s, leading firms in San Francisco's specialized industries, such as the iron and chemicals sectors, dispersed to the metropolitan periphery.  This process of industrial suburbanization created an integrative metropolitan economy, as well as individual suburbs. In this dissertation, I explore the creation of one of the Bay Area's earliest industrial suburbs, Emeryville.  I argue that an analysis of industrial dynamism on the regional scale is integral for understanding metropolitan development and industrial suburbanization.  Symbiotic relations between resource extraction and industrial dynamism structured California's distinct mode of capitalist development.  The expansion and diversification of resource extraction and processing industries fueled metropolitan growth.  Within the broader context of regional capitalism, I examine the process of industrial suburbanization and the formation of Emeryville.  I show how two processes greatly influenced industrial dispersal and factory relocation:  the creation of an industrial property market and the endogenous logic of industrial production.  A coalition of land developers, politicos, and transportation entrepreneurs created a new suburban industrial space.  High rates of innovation and accumulation, as well as fierce competition, enabled certain firms to eschew the industrial core and locate their factories in early Emeryville.  I draw on array of archival material and primary sources to weave together this distinctive story of California landscapes and industries, and cities and suburbs.  My examination of the formation of Emeryville also presents a case study of how industry engenders metropolitan transformation.  This dissertation provides insights into the necessarily conjoined processes of city development and industrial suburbanization.",ucb,,https://escholarship.org/uc/item/13q7r2gq,,,eng,REGULAR,0,0
187,1623,"States of Disunion: American Marriage and Divorce, 1867–1906","Roehrkasse, Alexander Fort","Fligstein, Neil;",2019,"This dissertation comprises three essays on the historical relationship between capitalist development, state formation and marriage and divorce patterns in the United States.The first examines the effects of liberalizing women’s property rights on divorce. In the late nineteenth century, most American states gave married women new rights to own and control assets and earnings. Using administrative data on most U.S. divorces between 1867 and 1906, I show that rights transfers gave women financial independence from husbands that enabled them to exit undesirable unions at greater rates. However, husbands also filed for more divorces following women’s economic gains, suggesting that the violation of traditional gender norms of household governance also destabilized unions.The second essay explores the legal behavior of men and women who faced significant restrictions on divorce. Before the 1970s, U.S. states allowed legal divorces only for specified causes. Examining data on the causes cited by divorce seekers, I document the routinization of divorce procedure over historical time. I also exploit legal changes to demonstrate that individuals adapted to divorce regulations by changing the causes they cited. Strategic legal behavior was widespread but differed by gender, with men being more prone toward routinization and women being more likely to adapt to new rules. The third essay reflects critically on the quality of available data on nineteenth-century marriages. I compare vital records of marriages to census microdata on marital duration, showing that the latter exhibit significant measurement error. Despite growing interest by elite state actors in measuring marriage and divorce at the population level, vital recording, which had administrative origins in the clarification of individual legal statuses, seems to have elicited more reliable participation in official knowledge projects. I analyze the extent and distribution of mismeasurement, which has consequences for both the study of American political development and the validity of quantitative historical research on the family.",ucb,,https://escholarship.org/uc/item/1412c4hj,,,eng,REGULAR,0,0
188,1624,The Effect of Unreliable Commuting Time on Commuter Preferences,"Koskenoja, Pia Maria K.",,1996,"Unreliable travel time is defined to mean a distribution of possible commute durations. This dissertation identifies occupational groups and shows how an individual's occupation can be expected to indicate how that person is going to behave in risky commuting stations. Individual occupations attract a certain personality type. Also, individual occupations require different amounts of team work and pose idiosyncratic supervisory requirements for the employer. These effects create systematic variations among employer imposed work rules concerning employee's time use and employee expectations and reactions to the rules. The outcome is both personality driven and situation specific response to risky commuting situations. A psychological construct -- locus of control -- draws a boundary between what an individual believes is influenced by her own actions and what is caused by factors external to her. A person with an internal locus of control is optimistic about her possibilities to influence the outcomes of risky situations, while a person with an external locus of control tends to see the cause of events as random or influenced by some powerful others. Commuters with an external locus of control take fewer planned risks, reserving more slack time between planned arrival and official work start time. If something unanticipated throws them off the habitual path, they are less likely to go out of their way to maintain the planned arrival time. The commuters with more internal locus of control are more willing to take planned risks and are more committed to see that the risk pays off. I use occupational classification developed by John Holland and resource exchange theory of Uriel Foa to establish a partial order from most external to most internal occupational groups. The dissertation also includes models where the commuter trades off different elements of unreliable travel time: expected mean travel time, expected schedule delay early, and expected schedule delay late. Occupations affect these tradeoffs even when income and family composition are controlled.",ucb,,https://escholarship.org/uc/item/0rj9z9cv,,,eng,REGULAR,0,0
189,1625,"Deployment, Design, and Commercialization of Carbon-­Negative Energy Systems","Sanchez, Daniel Lucio","Kammen, Daniel M;Callaway, Duncan S;",2015,"Climate change mitigation requires gigaton-scale carbon dioxide removal technologies, yet few examples exist beyond niche markets. This dissertation informs large-scale implementation of bioenergy with carbon capture and sequestration (BECCS), a carbon-negative energy technology. It builds on existing literature with a novel focus on deployment, design, commercialization, and communication of BECCS. BECCS, combined with aggressive renewable deployment and fossil emission reductions, can enable a carbon-negative power system in Western North America by 2050, with up to 145% emissions reduction from 1990 levels. BECCS complements other sources of renewable energy, and can be deployed in a manner consistent with regional policies and design considerations. The amount of biomass resource available limits the level of fossil CO2 emissions that can still satisfy carbon emissions caps. Offsets produced by BECCS are more valuable to the power system than the electricity it provides. Implied costs of carbon for BECCS are relatively low (~$75/ton CO2 at scale) for a capital-intensive technology.Optimal scales for BECCS are an order of magnitude larger than proposed scales found in existing literature. Deviations from optimal scaled size have little effect on overall systems costs – suggesting that other factors, including regulatory, political, or logistical considerations, may ultimately have a greater influence on plant size than the techno-economic factors considered.The flexibility of thermochemical conversion enables a viable transition pathway for firms, utilities and governments to achieve net-negative CO2 emissions in production of electricity and fuels given increasingly stringent climate policy. Primary research, development (R&D), and deployment needs are in large-scale biomass logistics, gasification, gas cleaning, and geological CO2 storage. R&D programs, subsidies, and policy that recognize co-conversion processes can support this pathway to commercialization. Here, firms can embrace a gradual transition pathway to deep decarbonization, limiting economic dislocation and increasing transfer of knowledge between the fossil and renewable sectors.Global cumulative capital investment needs for BECCS through 2050 are over $1.9 trillion (2015$, 4% real interest rate) for scenarios likely to limit global warming to 2 °C. This scenario envisions deployment of as much as 24 GW/yr of BECCS by 2040 in the electricity sector. To achieve theses rates of deployment within 15-20 years, governments and firms must commit to research, development, and deployment on an unprecedented scale.Three primary issues complicate emissions accounting for BECCS: cross-sector CO2 accounting, regrowth, and timing. Switchgrass integration decreases lifecycle greenhouse gas impacts of co-conversion systems with CCS, across a wide range of land-use change scenarios. Risks at commercial scale include adverse effects on food security, land conservation, social equity, and biodiversity, as well as competition for water resources. This dissertation argues for an iterative risk management approach to BECCS sustainability, with standards being updated as more knowledge is gained through deployment. Sustainability impacts and public opposition to BECCS may be reduced with transparent measurement and communication.Commercial-scale deployment is dependent on the coordination of a wide range of actors, many with different incentives and worldviews. Despite this problem, this dissertation challenges governments, industry incumbents, and emerging players to research, support, and deploy BECCS.",ucb,,https://escholarship.org/uc/item/0rs8n38z,,,eng,REGULAR,0,0
190,1626,Studies of Auction Bidding with Budget-Constrained Participants,"Kotowski, Maciej Henryk","Kariv, Shachar;",2011,"Consider a first-price, sealed-bid auction where participants have affiliated valuations and private budget constraints; that is, bidders have private multidimensional types. We offer sufficient conditions for the existence of a monotone equilibrium in this environment along with an equilibrium characterization. Hard budget constraints introduce two competing effects on bidding. The direct effect depresses bids as participants hit their spending limit. The strategic effect encourages more aggressive bidding by participants with large budgets. Together these effects can yield discontinuous equilibrium strategies stratifying competition along the budget dimension. The strategic consequences of private budget constraints can be a serious confound in interpreting bidding behavior in auctions. Evidence from an experimental auction market lends support to the strategic importance of budget constraints.",ucb,,https://escholarship.org/uc/item/0s84s1bw,,,eng,REGULAR,0,0
191,1627,Exploring the Localization of Transportation Planning: Essays on research and policy implications from shifting goals in transportation planning,"King, David Andrew",,2009,"Transportation planning has long focused on large scale projects using a civil engineering approach of maximizing throughput and minimizing interactions with the surrounding environment. Such efforts greatly increased the overall mobility and accessibility of individuals within and across metropolitan regions, but it is clear that in the future such enormous initiatives are unrealistic due to political, financial, spatial and social concerns. The field of transportation planning is shifting away from this old model of planning towards one where transportation systems are considered part of the overall quality of life of communities.This dissertation explores how local transportation planning is adapting to these changing dynamics of transportation planning through three essays. The first considers how cities are already planning for transportation through their general plans without strong mandates from regional governments. The second essay estimates the spatial variation in commute mode choice in order to show the complexity of travel due to geographic factors of infrastructure provision and land uses. The final essay discusses what flexible localized transportation policies look like, using cruising for parking as an example. Ultimately this research highlights a way forward for transportation planning as a quality-of-life issue, traditionally the purview of local governments.",ucb,,https://escholarship.org/uc/item/0tx230gc,,,eng,REGULAR,0,0
192,1628,Theoretical and Computational Tools for Analyzing the Large-Scale Structure of the Universe,"Hand, Nicholas","Seljak, Uros;",2017,"The analysis of the large-scale structure (LSS) of the Universe can yield insights into some of the most important questions in contemporary cosmology, and in recent years, has become a data-driven endeavor. With ever-growing data sets, optimal analysis techniques have become essential, not only to extract statistics from data, but also to effectively use computing resources to produce accurate theoretical predictions for those statistics. Future LSS experiments will help answer fundamental questions about our Universe, including the physical nature of dark energy, the mass scale of neutrinos, and the physics of inflation. To do so, improvements must be made to theoretical models as well as the computational tools used to perform such analyses.This thesis examines multiple aspects of LSS data analysis, presenting novel modeling techniques as well as a software toolkit suitable for analyzing data from the next generation of LSS surveys. First, we present nbodykit, an open-source, massively parallel Python toolkit for analyzing LSS data. nbodykit is both an interactive and scalable piece of scientific software, providing parallel implementations of many commonly used algorithms in LSS. Its modular design allows researchers to integrate nbodykit with their own software to build complex applications to solve specific problems in LSS. Next, we derive an optimal means of using fast Fourier transforms to estimate the multipoles of the line-of-sight dependent power spectrum, eliminating redundancy present in previous estimators in the literature. We also discuss potential advantages of our estimator for future data sets. We then present a novel theoretical model for the redshift-space galaxy power spectrum and demonstrate its accuracy in describing the clustering of galaxies down to scales of k = 0.4 h/Mpc. Finally, we analyze the large-scale clustering of quasars from the extended Baryon Oscillation Spectroscopic Survey to constrain the deviation from Gaussian random field initial conditions in the early Universe, known as primordial non-Gaussianity.",ucb,,https://escholarship.org/uc/item/0vk2x0cs,,,eng,REGULAR,0,0
193,1629,Energy Demands and Efficiency Strategies in Data Center Buildings,"Shehabi, Arman","Nazaroff, William W;Horvath, Arpad;",2009,"Information technology (IT) is becoming increasingly pervasive throughout society as more data is digitally processed, stored, and transferred.  The infrastructure that supports IT activity is growing accordingly, and data center energy demands have increased by nearly a factor of four over the past decade.  Data centers house IT equipment and require significantly more energy to operate per unit floor area than conventional buildings.  The economic and environmental ramifications of continued data center growth motivate the need to explore energy-efficient methods to operate these buildings.  A substantial portion of data center energy use is dedicated to removing the heat that is generated by the IT equipment.  Using economizers to introduce large airflow rates of outside air during favorable weather could substantially reduce the energy consumption of data center cooling.  Cooling buildings with economizers is an established energy saving measure, but in data centers this strategy is not widely used, partly owing to concerns that the large airflow rates would lead to increased indoor levels of airborne particles, which could damage IT equipment.  The environmental conditions typical of data centers and the associated potential for equipment failure, however, are not well characterized.  This barrier to economizer implementation illustrates the general relationship between energy use and indoor air quality in building design and operation.  This dissertation investigates how building design and operation influence energy use and indoor air quality in data centers and provides strategies to improve both design goals simultaneously.As an initial step toward understanding data center air quality, measurements of particle concentrations were made at multiple operating northern California data centers.  Ratios of measured particle concentrations in conventional data centers to the corresponding outside concentrations were significantly lower than those reported in the literature for office or residential buildings.  Estimates using a material-balance model match well with empirical results, indicating that the dominant particle sources and losses - ventilation and filtration - have been characterized.  Measurements taken at a data center using economizers show nearly an order of magnitude increase in particle concentration during economizer activity.  However, even with the increase, the measured particle concentrations are still below concentration limits recommended in most industry standards. The research proceeds by exploring the feasibility of using economizers in data centers while simultaneously controlling particle concentrations with high-quality air filtration.  Physical and chemical properties of indoor and outdoor particles were analyzed at a data center using economizers and varying levels of air filtration efficiency.  Results show that when improved filtration is used in combination with an economizer, the indoor/outdoor concentration ratios for most measured particle types were similar to the measurements when using conventional filtration without economizers.  An energy analysis of the data center reveals that, even during the summer months, chiller savings from economizer use greatly outweigh the increase in fan power associated with improved filtration.  These findings indicate that economizer use combined with improved filtration could significantly reduce data center energy demand while providing a level of protection from particles of outdoor origin similar to that observed with conventional design.  The emphasis of the dissertation then shifts to evaluate the energy benefits of economizer use in data centers under different design strategies.  Economizer use with high ventilation rates is compared against an alternative, water-side economizer design that does not affect indoor particle concentrations.  Building energy models are employed to estimate energy savings of both economizer designs for data centers in several climate zones in California.  Results show that water-side economizers consistently provide less energy savings than air-side economizers, though the difference in savings varies by location.  Model results also show that conventional limits on humidity levels in data centers can restrict the energy benefits of economizers.  The modeling efforts are then extended to estimate national data center energy use.  Different size data centers are modeled to represent the national variation in efficiency and operation of associated mechanical equipment.  Results indicate increased energy efficiency opportunities with larger data centers and highlight the importance of temperature setpoints in maximizing economizer efficiency.  A bottom-up modeling approach is used to estimate current (2008) United States data center energy use at nearly 62-70 billion kWh annually.  The model indicates that more about 65-70% of this energy demand can be avoided through energy efficient IT and cooling infrastructure design, equivalent to an annual energy efficiency resource of approximately 40-50 billion kWh available at a national level.  Within the context of greenhouse gas emissions, benefits can be significantly increased by incorporating site location into energy-efficient design strategies.  The framework of this dissertation contributes to general building energy efficiency efforts by shifting the perspective of building design to address indoor and outdoor environmental impacts simultaneously, ensuring that one design goal does not eclipse the other.  More specifically, the results presented here outline opportunities to temper the growing data center energy demand, so that IT can evolve into an energy efficient utility with the potential to facilitate a more sustainable expansion of goods and services.",ucb,,https://escholarship.org/uc/item/0xd1t108,,,eng,REGULAR,0,0
194,1630,Elucidating Heterogeneities and Dynamic Processes at the Nanoscale with Cathodoluminescence and Cathodoluminescence-Activated Microscopies,"Bischak, Connor Gregory","Ginsberg, Naomi S;",2017,"Super-resolution imaging has revolutionized how the structure of biological systems are observed at the nanoscale. Yet, observing dynamic processes in biology with high temporal and spatial resolution remains a significant challenge. Additionally, elucidating the nanoscale structure and dynamics in functional materials, particularly in optoelectronics, would greatly aid in the development of more efficient devices, such as solar cells and light-emitters. Unfortunately, most super-resolution microscopy platforms are designed for imaging biological samples and are incompatible with complex, functional materials. To extend super-resolution imaging to capture both biological dynamics and nanoscale material properties, we have developed cathodoluminescence imaging with low electron exposure (CILEE) and cathodoluminescence-activated imaging by resonance energy transfer (CLAIRE). Both imaging methods use cathodoluminescence (CL) microscopy to achieve nanoscale spatial resolution. The main drawback of CL microscopy is damage caused by the relatively high energy electron beam. In CILEE, the electron beam dose is significantly reduced to image samples only moderately robust to the electron beam. In CLAIRE, more fragile samples can be imaged by placing a thin scintillator film between the sample and electron beam. When excited by a focused electron beam, the scintillator film acts as a nanoscale optical excitation source, providing contrast based on interactions between luminescent dopant atoms in the sctillator and the adjacent sample in the near field. In this dissertation, the development of CILEE and CLAIRE are outlined, as well as many examples of uncovering new nanoscale phenomena with both imaging platforms.Part I of this dissertation, which includes Chapters 2-5. focuses on using CILEE to elucidate the nanoscale structure and dynamic properties of lead halide hybrid perovskites, which are promising materials for optoelectronics. Using CILEE, we reveal a surprising degree of heterogeneity at the surface of hybrid perovskite thin films that differs greatly from the more homogeneous environment found in the bulk. Our CILEE study suggests that solar cells composed of a hybrid perovskite active layer can improve in efficiency by decreasing the heterogeneity through synthetic approaches. We also use CILEE to investigate the process by which mixed halide hybrid perovskites phase separate upon photoexcitation, a process that severely limits solar cell efficiency. Through a combination of CILEE and multiscale modeling, we find that phase separation is driven by polaronic strain in the lattice. Our results represent a new type of nanoscale phase transformation that is unique to hybrid materials. The emergence of CILEE as new approach to non-invasive super-resolution imaging has led to a greater understanding of the complex structure and dynamics in hybrid perovskite materials.Part II of this dissertation, which includes Chapters 6-11, introduces CLAIRE as a new super-resolution imaging platform designed to image soft materials, such as organic or biological samples. In this dissertation, we describe the production of thin, free-standing scintillator films for CLAIRE and the incorporation of these scintillator films into a functional imaging device. We demonstrate that CLAIRE is capable of imaging soft materials and dynamic processes. The capability of CLAIRE to image biological samples with endogenous chromophores, such as photosynthetic membranes, is also demonstrated. Together, CILEE and CLAIRE extend non-invasive super-resolution optical imaging to new classes of soft materials that are incompatible with current super-resolution optical imaging approaches and traditional electron microscopy. These new nanoscale imaging methods provide promising opportunities to visualize biological dynamics at high spatial and temporal resolution and to interrogate the nanoscale optical properties of functional optoelectronic materials to understand their fundamental properties, leading to higher efficiency devices.",ucb,,https://escholarship.org/uc/item/11j686v3,,,eng,REGULAR,0,0
195,1631,Rejuvenation of the Aged Stem Cell Niche: Signal Transduction and Reversing the Decline of Adult Hippocampal Neurogenesis and Myogenesis,"Yousef, Hanadie","Schaffer, David;Conboy, Irina;",2013,"Although functional organ stem cells persist in the elderly, tissue damage invariably overwhelms tissue repair, ultimately leading to the failure of major organ systems. It has been demonstrated that the microenvironment, or niche in which adult stem cells reside critically influences stem cell function. The delicate balance between positive and negative signaling regulators controls the decision of adult stem cells to remain quiescent, self-renew or differentiate, a crucial balance for the maintenance of tissue homeostasis. In this dissertation, we provide evidence that the same key morphogenic signaling pathways become deregulated with age and contribute to the decline of both hippocampal neurogenesis and skeletal muscle regeneration with aging, leading to the decline in regenerative performance of both brain and muscle tissue stem cells. Furthermore, we demonstrate that the aged tissue niches can be rejuvenated to enhance native stem cell function in muscle and brain by youthful calibration of the intensity of these morphogenic signaling pathways. In particular, local attenuation of BMP signaling in the aged hippocampus, systemic and local attenuation of TGF-beta signaling in both the aged hippocampus and aged skeletal muscle, and specific proteins secreted by human embryonic stem cells that act through MAPK and Notch signaling rejuvenate brain and muscle tissue precursor cell function by normalizing the signaling strength of the pathways that are chronically overexpressed or underexpressed with aging. Summarily, by better understanding the age-imposed decline in the regenerative capacity of stem cells, the debilitating lack of organ maintenance in the old, including decline in neurogenesis and skeletal muscle regeneration, can be ameliorated.",ucb,,https://escholarship.org/uc/item/11s7t6kd,,,eng,REGULAR,0,0
196,1632,Gesturing Through Time: Holds and Intermodal Timing in the Stream of Speech,"Park-Doob, Mischa Alan","Sweetser, Eve E.;Hanks, William F.;",2010,"Most previous work examining co-speech gestures (the spontaneous bodily movements and configurations we engage in during speaking) has emphasized the importance of their most salient or energetically expressive moments, known as gesture 'strokes' (Kendon 1980). In contrast, in this dissertation I explore the potential functions of intervals of gestural stasis, or gesture 'holds', in which the hands or body maintain particular configurations across variable spans of time, interwoven with the stream of speech. Through the embodiment of a constant form within continuously evolving face-to-face interactions, holds make possible a unique and understudied array of functions relating to the maintenance of ideas and contexts across time. Chapter 1 introduces the corpus of videotaped dyadic conversations from which all of the examples are drawn, discusses the history of the concepts of 'stroke' and 'hold', and illustrates the structural possibilities for the timing of holds with respect to co-expressive speech: they bear content that is not just simultaneous with, but also 'retrospective' and/or 'prospective' of, portions of the full composite utterances in which they occur. Chapter 2 illustrates that holds lasting across pauses and disfluencies support continued expressiveness and interpretability, alternately presaging new content that will also be part of a fluent resumption, or maintaining retrospective links to prior content that can contextualize the resumption. Chapter 3 discusses the frequent expressive complementarity of co-timed speech and gesture, as it relates to the debate on speech-gesture synchrony, and further demonstrates that preliminary commitments to utterances are often partially fulfilled from the earliest moments because of gestural cues that are interpretable at all points of their lifecycles, including preparatory phases. Chapter 4 discusses the implications for attention and memory of gesture holds acting as temporary cognitive artifacts, forming 'bridges' across interruptions and competing representations by interlocutors, thereby functioning retrospectively as 'recall cues' to previous moments of the interaction. Chapter 5 focuses on instances of gesture holds combined with listener-directed gaze that are maintained across turn transitions, then released, allowing speakers to 'hand off' control while enforcing a context for the next turn. Chapter 6 synthesizes the preceding chapters and suggests directions for future research.",ucb,,https://escholarship.org/uc/item/12f968ck,,,eng,REGULAR,0,0
197,1633,"Lived Experience in New Mexico, 1754-2019: A Historical Archaeology With and For a Genizaro Community","McCleary, Alexandra","Sunseri, Jun U;",2020,"Deep contestations of essentialized identity categories are a contemporary reality for communities for whom cultural patrimony of land and water resources play a crucial role. Yet, archaeology has not been able to adequately recognize the dynamics of the changing nature of identity practices which shaped interactions between groups of people, particularly in areas with a sustained colonial presence and resource-challenged ecologies. The high-elevation, semi-arid climate and historical complexity of Northern New Mexico provide such a context. My research objective is to understand how Genizaro Indian communities are sensitive to the historically particular dynamics of ethnopolitical empowerment and racialization in the 18th to early 20th centuries. This project uses an examination of the documentary record, faunal remains, and commensurate data from excavated materials from Genizaro communities in New Mexico to build upon existing models of cultural hybridity and ethnogenesis.",ucb,,https://escholarship.org/uc/item/1415b7n3,,,eng,REGULAR,0,0
198,1634,Fully Integrated Complementary Metal Oxide Semiconductor (CMOS) Bio-Assay Platform,"Florescu, Octavian","Boser, Bernhard E;",2010,"We present a post-processed 2.5mm x 2.5mm 0.18um Complementary Metal Oxide Semiconductor (CMOS) platform that leverages the advantages of super-paramagnetic bead labeling to integrate on-chip the label separation and detection functionalities required for high sensitivity bio-assays. The surfaces of the CMOS chip and of the magnetic beads are functionalized with bio-chemicals complementary to a target analyte. In a sandwich capture format, the presence of the target analyte will strongly bind 4.5um magnetic bead labels to the surface of the chip. The undesired background signal is minimized by the removal of the unbound magnetic beads from the detection array via magnetic forces generated on-chip. The remaining strongly bound magnetic beads are respectively magnetized and detected by an array of 128 stacked micro-coil/Hall sensor elements. This single chip solution does not require any external components like pumps, valves or electromagnets and is capable of detecting purified Human antibodies down to concentrations of 100pg/ml as well as anti-Dengue antibodies in human serum samples.A whole blood sample preparation system based on membrane filtration alleviates the need for centrifugation and can be readily combined with the assay platform into a high performance, Point-of-Need (PON) In-Vitro Diagnostic (IVD) device.The present prototype relies on an electronic reader for controlling the assay and reporting results. Implementing this function on the bio-sensor front-end enables a very simple design consisting of only the bio-sensor and an attached results display. Such low-cost, easy-to-use, high performance devices are needed for lowering health costs through more decentralized distribution of medical care.",ucb,,https://escholarship.org/uc/item/1519g7kt,,,eng,REGULAR,0,0
199,1635,Essays on Environmental Economics,"Tang, Qu","Rausser, Gordon C.;",2015,"AbstractEssays on Environmental EconomicsbyQu TangDoctor of Philosophy in Agricultural and Resource EconomicsUniversity of California, BerkeleyProfessor Gordon C. Rausser, ChairThis dissertation is comprised of three essays that apply microeconomics theory and econometric methods to study important issues in environmental economics. In the first essay, I investigate the impacts of imposing inter-state trade restrictions on the compliance costs of coal-fired electric generating units (EGUs) in the context of a U.S. SO2 emissions trading program (the Acid Rain Program). Over the past decade, tremendous efforts have been devoted to modifying emissions trading programs to address cross-state air pollution problems. The modification involves imposing more restrictions on emissions trading across geographical areas. The empirical question is how severe trade restrictions affect the regulated firms’ compliance costs. Using rich data from the Acid Rain Program, this essay developed a discrete-continuous model to estimate electric generating units’ compliance strategies and marginal abatement costs associated with the nationwide uniform emissions trading as the program was implemented in practice. Based on the estimation results, this essay then simulated units’ compliance behaviors and the corresponding compliance costs if interstate trading had been prohibited. The results show that the aggregate compliance costs would increase more than one and a half times for the same emissions reduction goal due to the narrower trading markets in the counterfactual policy design with trade restrictions, and the costs would vary dramatically across space. Combined with the analysis on the benefit side, the results of this essay could be used to predict welfare impacts associated with trade restrictions at both national level and state level. And it may shed light on the future modification and implementation of EPA’s cross-state air pollution regulations.The second essay applies an equilibrium sorting model to a brand-new housing market in Beijing, China to estimate household preferences for neighborhood public goods provision, including public transportation services, public primary schools, and environmental amenities. The equilibrium sorting model is based on a discrete choice model of household residential location decisions. Relying on a unique, detailed data set on housing location, price, and other household characteristics, I estimate the model following the two-step BLP method, taking into account the heterogeneity of household preferences, incorporating neighborhood-specific unobservable characteristics, and addressing the endogeneity of housing prices using instrumental variables. The results suggest that in general, lower housing price, better environmental amenities, and being closer to job centers will increase the choice opportunity of a neighborhood, and public transportation systems play a more important role in the neighborhoods far away from urban centers. Moreover, different households show varying preferences for these public goods. A distinct fact is that in addition to income, people’s preferences vary greatly with generation (head age of households) and job type (whether there are public employees), which reveal the significant differences between generations and illustrate the welfare for public employees within the context of the transitional economy in China. This preference heterogeneity implies that future policies should be more geographically asymmetric, locally targeted and tailored based on specific socio-economic characteristics.The third essay estimates the impact of climate change on the crop yields in China. I use a 11-year county-level panel data set covering more than 1,000 counties to estimate the effects of random year-to-year variation in weather on three major crops yields, including rice, wheat, and corn. Because it is not easy for small-scale farmers to adapt to climate change quickly in short time, these estimates could be used to plausibly predict the short to medium-run impacts of climate change on crop yields in China. The essay finds that over the period 2040-2060, projected climate change would reduce rice yield by 1.18% under a comparatively high emission scenario and by 0.08% under a medium-low scenario, reduce corn yield by 2.21% and 1.64% under the two emission scenarios, respectively, and increase wheat yield by 6.68% and 5.48% under the two emission scenarios, respectively. These findings may shed light on future policy designs to enhance the adaptive capacity of agriculture in China and thus ensure food security in the context of climate change.",ucb,,https://escholarship.org/uc/item/159713r1,,,eng,REGULAR,0,0
200,1636,"Our Fanatics: Figurations of Religious Fanaticism in Ian McEwan, Chimamanda Ngozi Adichie, and Marilynne Robinson","Sambrooke, Jerilyn","Lye, Collen;Naddaff, Ramona;",2017,"Our Fanatics: Figurations of Religious Fanaticism in Ian McEwan, Chimamanda Ngozi Adichie, and Marilynne Robinson examines how three contemporary novelists complicate oft-repeated accounts that oppose religious fanaticism to reasoned argumentation and secular politics. My dissertation features novels that focus intently on the interiority of protagonists who encounter figures of religious fanaticism, portraying religious fanaticism as something to be negotiated rather than defended against. By analyzing twenty-first century novels that variously figure religious fanaticism in oppositional, paradoxical, and genealogical terms, this project examines how religious fanaticism is constitutive of—rather than external to—the worlds of these novels.The first chapter reaches back to Ian McEwan’s Enduring Love (1998), comparing it to his 9/11 novel, Saturday (2005), and, more recently, The Children Act (2014). I argue McEwan’s novels frame religious fanaticism as a form of irrational certainty that generates epistemological uncertainty for the novels’ protagonists. These texts frustrate a simple triumphant narrative whereby secular rationalism prevails over religious fanaticism. More recently, however, McEwan’s fiction resolves such tensions with increasing authority, gradually eliminating the experimental dimensions of McEwan’s early work. Chapter two features Chimamanda Ngozi Adichie’s Purple Hibiscus (2003), which develops an apparently paradoxical religious fanatic— politically admirable but privately violent. I investigate this paradox by analyzing the novel’s cyclical plot, which echoes the Catholic liturgical calendar and which distinguishes it from Chinua Achebe’s Things Fall Apart (1958), a comparison that has dominated Adichie’s critical reception. The third chapter reads Marilynne Robinson’s Gilead trilogy—Gilead (2004), Home (2008), and Lila (2014)—as an extended meditation on the lingering effects of religious fanaticism across the generations of a small mid-Western town. The trilogy’s genealogical figuration of religious fanaticism ties abolitionism to civil rights activism, delivering a resounding critique of “mainline” Protestant disavowals of such fanaticism.The religious fanatics that appear across this dissertation cannot be described in any easy sense as “ours.” My title draws attention to the smaller, subtler way that these novels approach religious fanaticism through intimate relationships and private spaces, positioning religious fanaticism as internal to communities, to families, and, particularly in Adichie and Robinson, to Christian traditions.",ucb,,https://escholarship.org/uc/item/15r2c4vf,,,eng,REGULAR,0,0
201,1637,"Race Across Borders: Transnationalism and Racial Identity in African-American Fiction, 1929-1945","Agbodike, Kanayo Jason","JanMohamed, Abdul R;",2012,"Race Across Borders: Transnationalism and Racial Identity in African-American Fiction, 1929-1945, examines four African-American literary texts that employ transnational themes and aesthetics as a means of resisting a logic of racial essentialism that governed the production and reception of black literature in the United States during the early 20th century. I examine the ways in which Dark Princess by W. E. B. Du Bois, Their Eyes Were Watching God by Zora Neale Hurston, Banjo: A Story Without a Plot by Claude McKay, and If He Hollers Let Him Go by Chester Himes employ various formal and stylistic techniques to critique and reconfigure the dominant codes of racial identity that shaped their context. I argue that each of these texts exemplifies a conflict between a nationalist mode of racial representation and a transnational orientation that destabilizes received notions of race. Whereas the cultural field in which interwar African-American novels were situated involved a manifest nationalist topography which reproduced a racially divided polity, these texts inscribe transnational forces that disrupt the racial underpinnings of the 20th-century American national narrative. Because of the hegemonic status of the nationalist framework, the critique of that framework tends to appear in the formal aspects of the novels rather than their explicit contents.	The first chapter considers how Dark Princess explores the intersections between African-American and anti-colonial politics by way of the story of a romantic relationship between an African American man involved in local politics and an Indian woman involved in an international Third World liberation movement. I consider how the juxtaposition of national and transnational forms of solidarity within the text is paralleled by a tension between naturalism and romance in  its formal economy. While the techniques of naturalism tend to characterize the parts of the novel that represent national and racial politics, the parts that imagine a transnational anti-colonial movement draw on the codes of literary romance. Through this utopian gesture, the novel gives shape to the conflict between national and transnational perspectives on minority politics without offering a clear resolution to that conflict. The second chapter challenges dominant critical interpretations of Their Eyes Were Watching God, which construe the novel as a written representation of African American oral tradition. While such readings are illuminating, they overlook significant aspects of the text's racial thematic by emphasizing how it presupposes racial forms of identity. Although the novel does reproduce such forms, I argue that it simultaneously resists them, particularly in some of its more marginal characters and moments, and that it is precisely through the representation of dialect speech that these hidden resistances become visible. 	The third chapter examines McKay's use of the aesthetic concept of the sublime in articulating the problematic gulf separating modern Blacks from metropolitan culture and society. In Banjo the sublime mediates between these terms rather than the rationally free subject and a causally determined Nature. Banjo differs from the mainstream European realist novel by denying the teleological narrative of reconciliation as unsuitable to the concerns of a radically excluded black collective. By taking as his protagonists an international band of black vagabonds based in the cosmopolitan French port city Marseilles, McKay imagines an alternative to the grand narrative of national identity. The final chapter focuses on notions of embodiment and psychological affect within Himes's narrative of thwarted integration. Simultaneously foreclosing on both a successful outcome for such a project and the death of the protagonist, the novel moves towards an ambivalent and open-ended reflection on the possibilities of social transformation. In light of this ambivalence, I view the brief but frequent points at which the protagonist identifies with marginalized Mexican-Americans and Japanese-American internment-camp prisoners as moments that both disrupt the received black/white binary as a schema for American social reality and contrasts a trans-national anti-colonial solidarity with racial nationalism as an alternative mode of political agency.",ucb,,https://escholarship.org/uc/item/16v2k9z2,,,eng,REGULAR,0,0
202,1638,Nonparametric Optimization with Objective Operational Statistics,"Yu, Lian","Lim, Andrew E.B.;",2010,"In the first part of this thesis, we study the non-parametricmethods for estimation and optimization. In particular, a newnon-parametric method, objective operational statistics, is proposedto inventory control problems, where the only information availableis the sample data and we do not assume any relationship betweendemands and order quantities. A kernel algorithm based on objectiveoperational statistics is constructed to approximate the objectivefunction directly from sample data. Moreover, we give conditionsunder which the operational statistics approximation functionconverges to the true objective. Numerical results of the algorithmwith applications to newsvendor problem show that the objectiveoperational statistics approach works well for small amount of dataand outperforms the previous parametric and non-parametric methods.In the second part of this thesis, we present a robust hedgingproblem under model uncertainty and the bounds of the optimalobjective value are derived by duality analysis.",ucb,,https://escholarship.org/uc/item/18z443nf,,,eng,REGULAR,0,0
203,1639,Biomechanics and evolution of flight in stick insects,"Zeng, Yu","Dudley, Robert;Wake, David;",2013,"Many unresolved questions in animal flight evolution relate to the transition between flightless and volant forms. Functional analysis of transitional modes using anatomical intermediates may help to assess the biomechanical underpinnings to such transitional processes. The group of stick insects exhibits tremendous diversity in wing sizes, which is potentially correlated with selection gradient for wing size. This dissertation work uses stick insects as a model system to address the ecological context of controlled aerial behaviors and the evolutionary consequences of flight loss. Chapter 1 explores the behavioral context of controlled aerial descent in the newly hatched larvae of an Australian stick insect  Extatosoma tiaratum, which exhibit ephemeral ant-mimicry and hyperactive dispersal activities. Through lab experiments, we documented the ontogenetic variation in various taxic behaviors and voluntary drop as an escaping response. Chapter 2 explores the visual ecology of directed aerial descent in larval  E. tiaratum. We investigated how visual contrasts are used as locomotor references during directed aerial descent. Our results suggest the use of vertically constant contrast edges is a major component of directional targeting. The utility of contrast as visual cues was further shown to be dependent on both the heterogeneity of the visual environment and the quality of perceived signals. Chapter 3 addressed the biomechanics of aerial righting behaviors in larval  E. tiaratum. Through high--speed filming and three--dimensional motion reconstruction, the results showed highly controlled leg movements are involved in righting maneuvers. Through posture control, the insects achieve effective righting rotation. More subtle leg movements were also shown in stroke--like patterns, which are instantaneously correlated with whole--insect rotation. This study provided useful information for understanding the ecology and evolution of controlled aerial behaviors in invertebrates. Chapter 4 addressed the functional consequences of progressive wing size reduction along an altitudinal gradient in three populations of a stick insect  Asceles tanarata  native to Malay Peninsula. We investigated how wing and body morphology change along the altitudinal gradient, and further studied the biomechanics of flight in each flight morph. The results indicate the reduction of fight apparatus leads to changes in wing design and wing kinematics. Reduced flapping fight performances resemble parachuting and gliding. Due to both altered kinematics and flight trajectories different from conventionally recognized flapping flight, the aerodynamics of wing flapping is characterized by large advance ratio and reduced half--stroke asymmetries. Although the reduction in flight performance is closely correlated with the reduction of wing size, wing aerodynamics shows more complex pattern along the performance gradient.",ucb,,https://escholarship.org/uc/item/1cr698rw,,,eng,REGULAR,0,0
204,1640,Constraints on slow slip from landsliding and faulting,"Delbridge, Brent Gregory","Burgmann, Roland;",2017,"The discovery of slow-slip has radically changed the way we understand the relative movement of Earth’s tectonic plates and the accumulation of stress in fault zones that fail in large earthquakes. Prior to the discovery of slow-slip, faults were thought to relieve stress either through continuous aseismic sliding, as is the case for continental creeping faults, or in near instantaneous failure. Aseismic deformation reflects fault slip that is slow enough that both inertial forces and seismic radiation are negligible. The durations of observed aseismic slip events range from days to years, with displacements of up to tens of centimeters. These events are not unique to a specific depth range and occur on faults in a variety of tectonic settings. This aseismic slip can sometimes also trigger more rapid slip somewhere else on the fault, such as small embedded asperities. This is thought to be the mechanism generating observed Low Frequency Earthquakes (LFEs) and small repeating earthquakes.I have preformed a series of studies to better understanding the nature of tectonic faulting which are compiled here. The first is entitled “3D surface deformation derived from airborne interferometric UAVSAR: Application to the Slumgullion Landslide”, and was originally published in the Journal of Geophysical Research in 2016. In order to understand how landslides respond to environmental forcing, we quantify how the hydro-mechanical forces controlling the Slumgullion Landslide express themselves kinematically in response to the infiltration of seasonal snowmelt. The well-studied Slumgullion Landslide, which is 3.9 km long and moves persistently at rates up to ∼2 cm/day is an ideal natural laboratory due to its large spatial extent and rapid deformation rates. The lateral boundaries of the landslide consist of strike-slip fault features, which over time have built up large flank ridges.The second study compiled here is entitled “Temporal variation of intermediate-depth earthquakes around the time of the M9.0 Tohoku-oki earthquake” and was originally published in Geophysical Research Letters in 2017. The temporal evolution of intermediate depth seismicity before and after the 2011 M 9.0 Tohoku-oki earthquake reveals interactionsbetween plate interface slip and deformation in the subducting slab. I investigate seismicity rate changes in the upper and lower planes of the double seismic zone beneath northeast Japan. The average ratio of upper plane to lower plane activity and the mean deep aseismic slip rate both increased by factor of two. An increase of down-dip compression in the slab resulting from coseismic and postseismic deformation enhanced seismicity in the upper plane, which is dominated by events accommodating down-dip shortening from plate unbending.In the third and final study included here I use geodetic measurements to place a quantitative upper bound on the size of the slow slip accompanying large bursts of quasi-periodic tremors and LFEs on the Parkfield section of the SAF. We use a host of analysis methods to try to isolate the small signal due to the slow slip and characterize noise properties. We find that in addition to subduction zones, transform faults are also capable of producing ETSs. However, given the upper-bounds from our analysis, surface geodetic measurements of this slow slip is likely to remain highly challenging.",ucb,,https://escholarship.org/uc/item/1dv2x1sk,,,eng,REGULAR,0,0
205,1641,Fantasies of Friendship: Ernst Jünger and the German Right's Search for Community in Modernity,"Bures, Eliah Matthew","Jay, Martin E.;",2014,"This dissertation argues that ideas and experiences of friendship were central to the thinking of German radical conservatives in the twentieth century, from the pre-WWI years to the emergence, beginning in the 1970s, of the New Right.I approach this issue by examining the role of friendship in the circle around the writer Ernst Jünger (1895-1998). Like many in his generation, Jünger's youthful alienation from a ""cold"" bourgeois society was felt via a contrast to the intimacy of personal friendship. A WWI soldier, Jünger penned memoirs of the trenches that revealed similar desires for mutual understanding, glorifying wartime comradeship as a bond deeper than words and a return to the ""tacit accord"" that supposedly marked traditional communities. After 1933, Jünger turned from a right-wing opponent of democracy into a voice of ""spiritual resistance"" to the Nazi regime. For Jünger and other non-Nazi Germans, friendship was a crucial space of candid communication and nonconformity to the norms of the Third Reich. Jünger's writings from these years also issued coded signals to sympathetic readers to keep alive conservative values for a post-Nazi future. After WWII, Jünger became one of Germany's most controversial figures, a critic of modernity who was at the center of a friendship network that joined the veterans and heirs of Weimar's radical right into a counterculture opposed to what they believed was the decadence of German life. In Jünger's later works, he portrayed friendship as the last true site of community, an idea that shaped the elitist attitudes of new members of the German right.  I use published texts and letters alongside new archival material to make two broad contributions. First, by investigating friendship among twentieth-century German radical conservatives, I bring to light the important work that friendship has done for those facing quintessentially modern problems like alienation and social fragmentation. I argue that the work of friendship for figures like Ernst Jünger has primarily been the provision of needs for affirmation, communication, and mutual understanding. Recognizing these needs helps us see that anxieties about being understood, longings for fellowship, and concerns for the quality of interpersonal relationships have often underlain radical conservatism's explicit ideas about, say, the virtues of ""organic"" community or the perils of democratic leveling. I show how these needs and anxieties were closely bound up with the radical conservative critique of modernity, including its elitism, ultra-nationalism, and disdain for mass society and mass culture. It is through friendship, I argue, that German radical conservatives have understood the shortcomings of modern life and envisioned ways to overcome or cope with modernity.My second contribution is methodological. The study of friendship, I argue, can uncover emotional needs and intimate states of mind that are otherwise difficult for the historian to bring to light. Examining friendship among twentieth-century radical conservatives provides fundamental insights into motives, helping us understand why certain emotional demands were felt at certain moments in German history, and how these emotions in turn drove the decision for particular ideological positions. Asking these questions of the German radical right offers a fresh angle on a group usually dealt with through a reductive focus on cultural pathologies and formal ideology. Taking Ernst Jünger and his many friends and interlocutors as a case study, I provide a rich biographical historicization of German radical conservative thinking as it developed over multiple stages throughout the twentieth century. Stressing recurring needs for communication and mutual understanding, I locate new motives for radical conservative ideas.",ucb,,https://escholarship.org/uc/item/1f32711j,,,eng,REGULAR,0,0
206,1642,Role of Cdc48 unfoldase in the ubiquitin-proteasome proteolytic pathway,"Olszewski, Michal M","Martin, Andreas;",2018,"The ubiquitin proteasome system (UPS) controls essential cellular processes such as cell cycle progression, cellular signaling, stress responses and maintenance of protein folding homeostasis via regulated proteolysis of substrates by the 26S proteasome. Proteins destined for degradation need to meet two criteria: i) carry a polyubiquitin degron signal and ii) have an unstructured initiation region that allows for the engagement by the proteasome and subsequent unfolding, translocation and degradation. Yeast Cdc48, also known as p97 or VCP in higher eukaryotes, is an essential, abundant and highly conserved AAA+ ATPase that uses its unfoldase activity to extract ubiquitinated protein substrates from complexes and membranes. It has been hypothesized that Cdc48 also plays an important role in protein degradation pathways, working upstream of the 26S proteasome. For my dissertation, I established an in vitro reconstituted system that allowed me to study the interactions between Cdc48 and its cofactors, determine the kinetics of substrate unfolding by Cdc48 and demonstrate the functional role of Cdc48 in the proteasomal degradation.First, I established that recombinant Cdc48 can bind to the Ufd1/Npl4 substrate recognition heterodimer, as well as to the Otu1 deubiquitinase. Those interactions are mutually exclusive, meaning that Ufd1/Npl4 and Otu1 cannot both form a complex with Cdc48 in the absence of substrate. I also observed interaction between the Cdc48 unfoldase and the Ufd2 ubiquitin ligase. Additionally, I tested the activity of Ufd2 and developed conditions under which it can ubiquitinate model substrates containing linear ubiquitin fusion. That substrate can be used to study the activity of Otu1 and its modulation by Cdc48, as well as to measure unfolding by Cdc48. Furthermore, by utilizing the fluorescent protein mEOS3.2 N-terminally fused to tetra ubiquitin as a model substrate, and extending this ubiquitin using Ufd2, I was able to measure kinetics of Cdc48 unfolding under single and multiple turnover conditions. Finally, I demonstrated for the first time that compact substrates lacking an initiation segment can still be degraded by the 26S proteasome when first processed by Cdc48.  Overall, my data suggest that Cdc48’s primary role in the UPS is to create unstructured initiation regions in compact substrates refractory to proteasomal engagement.",ucb,,https://escholarship.org/uc/item/1h25d3p8,,,eng,REGULAR,0,0
207,1643,Association with Foci,"Toosarvandani, Maziar Doustdar","Mikkelsen, Line;",2010,"Association with focus has, since Jackendoff's (1972) dissertation, been the object of intense study. Most researchers, however, have concentrated on explaining the semantic variability of only and even, whose truth conditions vary with the position of focus. I take as my starting point another property of associating expressions. Both only and even restrict the distribution of focus, a property that, I argue, they share with a range of other lexical items. But, while only and even take a single argument and require there to be a focus somewhere inside that argument, expressions like adversative but and let alone take two arguments, thereby associating with two foci.Associating expressions, of both the one- and two-place varieties, have two things in common. First, they are crosscategorial in their syntax, taking arguments of a variety of different types. Second, they evoke multiple alternatives—different possible answer to a question. Together, these two independent properties of associating expressions interact with the question under discussion (Roberts 1996, 2004) to give rise to the restriction on the distribution of focus.  My approach to association with focus departs from previous ones in important ways. Associating expressions neither make reference to focus in their lexical entry (Rooth 1985, 1992, 1996) nor to the question under discussion (Beaver and Clark 2008), providing a more satisfying answer to the question of why only some expressions associate with focus.",ucb,,https://escholarship.org/uc/item/0ns534g1,,,eng,REGULAR,0,0
208,1644,"Essays on Crime, Unemployment and Health","Chaidez, Lilia","Magruder, Jeremy;",2015,"This dissertation is composed of three chapters and studies issues related to crime, unemployment and health.  The first chapter looks at the effect of funding for public safety on drug related violence.  The second chapter, which is joint work with Santiago Guerrero, examines the effect of unemployment on crime during the latest great recession.  The third chapter examines the effect that the introduction of ultra-low sulfur diesel has had on infant mortality.  The first paper develops a simple framework to describe the effect of increases in fighting capacity on violence and uses a large program in Mexico to empirically estimate the effect of funding for public safety on violence, specifically drug related violence.  Starting in 2008, Mexico implemented a large program designed for the strengthening of the municipal police, the assignment of which was based on an index.  The main areas of allowed expenditures for these funds were: the purchase of fighting equipment, technology infrastructure and training of the police force.  Instrumenting funding with the arbitrary initial eligibility cutoff, I find that the funds led to large increases in drug related violence.  Evidence is consistent with the funds allowing the police to fight criminal organizations which weakened organizations and in turn led to turf wars.  The effect is not higher for PAN municipalities, the party whose main platform during the study period was to fight organized crime.  Also, there does not seem to be an increase in violence in politically stable municipalities as a result of the program, but there is a decrease in areas with low land productivity.  Consistent with theory, I also find suggestive evidence of an inverted U-shaped relationship between baseline funding for public safety and the effect of the program. The second paper estimates the effect of unemployment on crime in Mexico.  This study uses the variation in unemployment across metropolitan areas in Mexico induced by the latest great recession.  Areas that were highly dependent on the US economy experienced the largest increases in unemployment, thus we instrument unemployment with the initial manufacturing and tourism labor share interacted with US GDP and find that increases in unemployment have led to decreases in crime in Mexico.  The results are consistent with the decrease in potential targets due to the increases in unemployment outweighing the positive effect coming from the decrease in the opportunity cost of engaging in criminal activities as unemployment increases. The third paper estimates the effect of the introduction of ultra-low sulfur diesel on infant health in Mexico.  In 2006 the Mexican government began the rollout of ultra-low sulfur diesel in metropolitan areas, starting with border municipalities.  Using a difference in differences approach, I find that, despite its potential to improve health outcomes, there is no evidence that sulfur regulation had a substantial effect on infant mortality outcomes.",ucb,,https://escholarship.org/uc/item/0021q7dw,,,eng,REGULAR,0,0
209,1645,"Making Ivan-Uzbek: War, Friendship of the Peoples, and the Creation of Soviet Uzbekistan, 1941-1945","Shaw, Charles David","Slezkine, Yuri;",2015,"This dissertation addresses the impact of World War II on Uzbek society and contends that the war era should be seen as seen as equally transformative to the tumultuous 1920s and 1930s for Soviet Central Asia. It argues that via the processes of military service, labor mobilization, and the evacuation of Soviet elites and common citizens that Uzbeks joined the broader “Soviet people” or sovetskii narod and overcame the prejudices of being “formerly backward” in Marxist ideology. The dissertation argues that the army was a flexible institution that both catered to national cultural (including Islamic ritual) and linguistic difference but also offered avenues for assimilation to become Ivan-Uzbeks, part of a Russian-speaking, pan-Soviet community of victors. Yet as the war wound down the reemergence of tradition and violence against women made clear the limits of this integration. The dissertation contends that the war shaped the contours of Central Asian society that endured through 1991 and created the basis for thinking of the “Soviet people” as a nation in the 1950s and 1960s. The first chapter addresses the experience of soldiers in the Red Army, paying special attention to the army’s policies to support Central Asian men with agitation. The second chapter focuses on the laborers who faced high mortality in the mines and industrial sites of the Urals and Siberia. Deprived of cultural support, agitators, and segregated from Slavic workers, they offer a case study in how the Soviet war-time state could operate both as a nation and an empire at the same time. The next two chapters address the Uzbek homefront, the contributions of Uzbek women who stayed in the region, and changing gender roles. Via an “emancipation of necessity” Uzbek women continued the professional gains they made during collectivization and replaced men in mechanized agriculture and in leadership positions. I examine the wartime contributions of three noteworthy women to show how the state both respected cultural mores that prevented them from serving at the front, but also pressed them into new, public roles. The next chapter focuses the interaction between evacuated Russian and Uzbek writers. I argue that their cooperation facilitated the narrative of Friendship of the Peoples while also allowing the evacuees to assert their tutorial rights as elder brother and masters of socialist realism. The final chapter addresses the durability of the Ivan-Uzbek identity in the face of social breakdown and resurgent religious tradition after the war.",ucb,,https://escholarship.org/uc/item/00p3q4kq,,,eng,REGULAR,0,0
210,1646,Essays on Financial Risk Taking and Embedded Heuristics,"Stimmler, Mary","Staw, Barry;",2013,"This dissertation investigates the relationship between the use of financial risk taking and the complex mathematical models used to quantify risks. In the first essay, laboratory experiments demonstrate that students who have taken courses in finance and economics take more risk when they are exposed to complex mathematical models, even when these models do not provide more information. Furthermore, students who have a strong belief in the power of risk quantification to produce accurate assessments of future events are more likely to take more risk when it is accompanies by complex quantitative models. The second essay uses data on U.S. banks from 1994-2007 and investigates popular claims that innovative risk measures, such as value at risk (VAR) resulted in greater risk seeking by banks prior to the financial crisis. It theorizes that as formal risk assessment models, such as the risk models used by banks, become institutionalized within organizations, a model's abstract representation of reality becomes reified and treated as though it is real and complete. This results in organizationally-embedded decision-making heuristics that shape how choices are made within the firm. For risk models, this meant that when banks encouraged the use of risk models, they generated an implicit belief that these models represented accurate assessments of future outcomes. This reduced uncertainty about future outcomes and led to greater risk taking.",ucb,,https://escholarship.org/uc/item/01z2b0hj,,,eng,REGULAR,0,0
211,1647,Flipping the Hemoglobin Switch and Discovering Regulators Involved in Fetal Hemoglobin Reactivation,"Boontanrart, Mandy","Corn, Jacob E;",2020,"The fetal to adult hemoglobin switch is a developmental process by which fetal hemoglobin becomes silenced after birth and replaced by adult hemoglobin.  Diseases caused by defective or missing adult hemoglobin, such as Sickle Cell Disease or β-Thalassemia, can be ameliorated by reactivating fetal hemoglobin. We discovered that knockdown or knockout of β-globin, a subunit of adult hemoglobin, led to robust upregulation of γ-globin, a subunit of fetal hemoglobin. This phenomenon suggested that red blood cells have an inherent ability to upregulate fetal hemoglobin in the event that adult hemoglobin is lacking.We developed multiple gene-editing tools in an immortalized erythroid cell model to investigate the molecular mechanisms behind the increase in fetal hemoglobin. Time-course transcriptomics identified ATF4, a transcription factor, as a causal regulator of this response. Further analysis also converged upon downregulation of MYB and BCL11A, known repressors of γ-globin, described in detail in chapter 2. Further work in chapter 3 explores other possible fetal hemoglobin regulators as discovered by CRISPRi arrayed mediated knockdown experiments. This work furthers our understanding of fundamental mechanisms of gene regulation and how cellular and molecular events influence red blood cell differentiation.",ucb,,https://escholarship.org/uc/item/02m6p6v6,,,eng,REGULAR,0,0
212,1648,"Synthesis, Characterization, and Fabrication of Boron Nitride and Carbon Nanomaterials, their Applications, and the Extended Pressure Inductively Coupled Plasma Synthesis of Boron Nitride Nanotubes","Fathalizadeh, Aidin","Zettl, Alex K.;",2016,"Nanoscale materials made of carbon, boron, and nitrogen, namely BCN nanostructures, exhibit many remarkable properties making them uniquely suitable for a host of applications. Boron nitride (BN) and carbon (C) nanomaterials are structurally similar.  The forms studied here originate from a two-dimensional hexagonally arranged structure of sp2 bonded atoms.  These nanomaterials exhibit extraordinary mechanical and thermal properties.  However, the unique chemical compositions of carbon and boron nitride result in differing electrical, chemical, biological, and optical properties.  In this work, we explore the single layer sheets of sp2 bonded carbon (graphene), and their cylindrical forms (nanotubes) of carbon and boron nitride.In the first part of this work, we look at carbon based nanomaterials.  In Chapter 2, the electron field emission properties of carbon nanotubes (CNTs) and their implementation as nanoelectromechanical oscillators in an integrated device will be discussed.  We show a technique hereby a single CNT is attached to a probe tip and its electron field emission characterized.  We then delve into the fabrication of a field emitting CNT oscillator based integrated device using a silicon nitride membrane support.  We then present the electron field emission capabilities of these devices and discuss their potential use for detection of nuclear magnetic resonance (NMR) signals.Graphene is the subject of study in Chapter 3.  We begin by extensively examining the synthesis of graphene using a chemical vapor deposition (CVD) process, ultimately establishing techniques to control graphene domain size, shape, and number of layers.  We then discuss the application of the single-atom thick, but ultra-mechanically strong graphene as a capping layer to trap solutions in a custom fabricated silicon nitride membrane to enable transmission electron microscopy (TEM) of liquid environments.  In this manner, the volume and position of liquid cells for electron microscopy can be precisely controlled and enable atomic resolution of encapsulated particles.In the second portion of this work, we investigate boron nitride nanostructures and in particular nanotubes.  In Chapter 4, we present the successful development and operation of a high-throughput, scalable BN nanostructures synthesis process whereby precursor materials are directly and continuously injected into a novel high-temperature, Extended-Pressure Inductively-Coupled plasma system (EPIC).  The system can be operated in a near-continuous fashion and has a record output of over 35 g/hour for pure, highly crystalline boron nitride nanotubes (BNNTs).  We also report the results of numerous runs exploring the wide range of operating parameters capable with the EPIC system.In Chapter 5, we examine the impurities present in as-synthesized BNNT materials.  Several methods of sample purification are then investigated.  These include chemical oxidation, using both gas and liquid phase based methods, as well as physical separation techniques.The large scale synthesis of BNNTs has opened the door for further studies and applications.  In Chapter 6, we report a novel wet-chemistry based route to fill in the inner cores of BNNTs with metals.  For the first time, various metals are loaded inside of BNNTs, forming a plethora of structures (such as rods, short nanocrystals, and nanowires), using a solution-based method.  We are also able to initiate and observe dynamics of the metallic nanoparticles, including their movement, splitting, and fusing, within a BNNT.",ucb,,https://escholarship.org/uc/item/03m2x0zt,,,eng,REGULAR,0,0
213,1649,Improving Exposure-Response Estimation in Air Pollution Health Effects Assessments,"Beckerman, Bernard Sam","Jerrett, Michael L.B.;",2014,"Of the 3.7 million deaths attributed to outdoor air pollution, ischemic heart disease (IHD) represents 40% of the total deaths, or approximately 1.48 million deaths, which occur mainly in older adults. IHD is the largest single causes of death attributable to ambient air pollution. Research on the progression and incidence of IHD are pointing to ambient fine particulate matter (PM) as a major contributor to morbidity and mortality outcomes. In this context, improvements in air pollution exposure assessment methods and health effects assessments are developed and investigated in this thesis.  With the exposure assessment, methods and tools were created that had utility for improving air pollution exposure assessment. Two exposure assessment chapters are presented. The first of these is focused on the creation of a national-level spatio-temporal air pollution exposure model. In the second exposure chapter, emphasis is placed on the development and evaluation of methods used to estimate annual average daily traffic - a local source of ambient particulates and other air pollutants thought to have heightened toxicity.A model was created to predict ambient fine particulate matter less than 2.5 microns in aerodynamic diameter (PM2.5) across the contiguous United States to be applied to health effects modeling (Chapter 2). We developed a novel hybrid approach that combine a land use regression model (LUR) and Bayesian Maximum Entropy (BME) interpolation of the LUR space-time residuals,. The PM2.5 dataset included observations at 1,464 monitoring locations with approximately 10% of locations reserved for cross-validation across the contiguous United States. In the LUR, variables based on remote sensing estimates of PM2.5, land use and traffic indicators were made available to the Deletion/Substitution/Addition machine learning algorithm used to select predictive models describing local variability in PM2.5.  Two modeling configurations were tested.  The first included all of the available covariates; and the second did not include the remote sensing.  The remote sensing variable was not based on any ground information. Specific results showed that normalized cross-validated R2 values for LUR were 0.63 and 0.11 with and without remote sensing, respectively; suggesting remote sensing is a strong predictor of ground-level concentrations. In the models including the BME interpolation of the residuals, cross-validated R2 were 0.79 for both configurations; the model without remotely sensed data described more fine-scale variation than the model including remote sensing.   Our results suggest that our modeling framework effectively predicts ground-level concentrations of PM2.5 at multiple scales over the contiguous U.S.The network interpolation tool used to estimate traffic is described in Chapter 3.  The program was created using free open-source software, namely Python 2.7 and its related libraries. It was applied to two county study areas in California, USA (Alameda and Los Angeles), where inverse distance weighted (IDW) and kriging annual average daily traffic (AADT) models were estimated.  These estimates were compared to: each other; to an entirely independent dataset; and against a traffic model using similar methods to those used in the traffic estimates employed in the exposure model in Chapter 2.Results show different levels of predictive agreement.  Using cross-validation methods, the R2 for these models were 0.36 and 0.32 in Alameda and 0.46 and 0.47 in Los Angeles, for IDW and Kriging, respectively.  Differences in model performance seen between and within the study area suggest that data issues may have materially contributed; these include: temporal discordance in the measurements and mischaracterization of road types. A comparison of network interpolation methods to those used to estimate traffic in Chapter 2 found the network methods to be superior.For the health effects analysis that that estimated an exposure response curve describing the effect of PM2.5 on ischemic heart disease mortality, monthly ambient PM2.5 estimates (from the model outlined in Chapter 2) were averaged to represent long-term exposure at the home.  Super Learner evaluated 14 models that fell within the classes of parametric, semi-parametric, and non-parametric models.   A generalized additive model with splined terms was identified as being most predictive of life expectancy.  Over the range of exposure 3-27 µg/m3 the estimated years of life lost over this interval was 0.6 years.  This relationship, however, was not linear.  It followed the pattern reported in previous studies with increased risk rates at lower exposures and a flattening out of the curve at higher exposures.  An inflection point appeared to occur near 10 µg/m3.  These estimates failed to reach significance at the 95% confidence criteria but were close enough to be suggestive of a relationship. Results from a complementary simulation showed that left truncation characteristics of the cohort likely biased to results towards the null.  In addition, the use of inverse probability of censoring weights to control for bias induced by right censoring added variability to the estimator that likely reduced the power to detect and effect.  This research has shown the utility of machine-learning algorithms for improving health effects assessments in the field of air pollution epidemiology. In exposure science, they have proven their utility in creating estimates of exposure that can be used to characterize multiple scales of variability. In health effects assessments, in combination with causal inference methods, this work has shown the utility of these methods to detect non-linear effects in novel parameter estimates in individual cohort studies. In addition to the methodological contribution, the health effects results contribute to the discussion about the burden of disease attributable to particulate matter.",ucb,,https://escholarship.org/uc/item/0416s9hf,,,eng,REGULAR,0,0
214,1650,Charge accommodation dynamics of cluster and molecular anions produced by photo-initiated intracluster charge transfer,"Yandell, Margaret Ashley","Neumark, Daniel M;",2014,"Time-resolved photoelectron imaging spectroscopy is used to examine the dynamics of charge accommodation by solvent species and biomolecules upon photo-initiated intracluster charge transfer.  Excitation of a charge transfer state of an iodide-complexed molecule or cluster with a UV pulse and subsequent interrogation by photodetachment with a lower energy probe enables detection of changes in photoelectron signals over hundreds of femtoseconds.  Velocity map imaging detection permits simultaneous collection of electron kinetic energy (eKE) and photoelectron angular distributions that provide insight into the strength and structure of the association between the cluster or molecule and the excess electron. 	Application of this methodology to iodide-containing clusters of small polar molecules such as water, methanol, and ethanol elucidates the stability and extent of intramolecular forces within a given cluster.  In complexes of iodide with small solvent clusters (≤ 10 molecules), iodide is situated somewhat outside of the solvent network.  Interaction of iodide-water clusters with a UV pulse to produce iodine and a free electron results in the partial solvation of the excess charge through hydrogen bonding interactions over hundreds of picoseconds before electron autodetachment.  In contrast, methanol and ethanol cluster networks can only support the excess charge for tens of ps.  Notably, stable bare water cluster anions have previously been measured with as few as two molecules, while upwards of seventy methanol molecules are necessary to stabilize an excess electron.  Drawing an analogy between electron autodetachment and statistical unimolecular decay, an excited iodide-water cluster with a given number of water molecules might be expected to decay most rapidly given its significantly smaller density of states.  The observation of the opposite pattern, as well as the similarity between iodide-methanol and -ethanol cluster anion lifetimes, suggests that energetics, rather than molecular structure, play a larger role in stabilizing an excess charge to autodetachment.  Applying a thermionic emission model confirms this result.          The dynamics of charge accommodation are also examined for small biomolecules.  Radiative damage to DNA caused by low energy electrons is thought to originate in the attachment of an electron to a nucleobase unit of a nucleotide in the DNA double helix.  Previous experiments have examined binding motifs and fragmentation patterns of transient negative ions (TNIs) of nucleobases using Rydberg electron transfer from excited noble gas atoms or collision of the nucleobase with a beam of electrons of defined energy.  Here, nascent TNIs of the nucleobase uracil are created by intracluster charge transfer from a complexed iodide ion and their decay examined with time-resolved photoelectron imaging.  Anions created with several hundred meV of excess energy appear as valence anions and are observed to decay biexponentially with time constants of hundreds of fs and tens of ps by iodine atom loss and autodetachment.  Repetition of these experiments with uracil molecules methylated at the N1, N3, or C5 positions results in a dramatic reduction of the longer time constant.  The addition of the methyl group may hasten the intramolecular vibrational energy redistribution process preceding autodetachment.          Photoelectron spectroscopy of isolated nucleobase anions has measured only the dipole-bound state (DBS) of the anion consisting of an electron weakly associated with the molecular dipole moment and very delocalized over the molecular structure.  Though the valence anion has not been directly measured, the DBS has been posited to serve as a `doorway' to the valence-bound state (VBS).  Such a mechanism has also been proposed for nitromethane.  In contrast, acetonitrile should only support a DB anion state.  Examination of nascent acetonitrile and nitromethane anions excited near the vertical detachment energies of their corresponding iodide-molecule complexes indeed produces the DB acetonitrile anion, which then decays biexponentially with time constants of few and hundreds of ps by iodine atom loss and autodetachment.  The nitromethane DB anion decays rapidly over hundreds of fs to form the valence anion, which decays biexponentially with time constants similar to those measured for the acetonitrile DB anion.  This study marks the first direct observation of a transition from a dipole-bound anion to a valence anion and will inform future studies of iodide-nucleobase complexes.",ucb,,https://escholarship.org/uc/item/05h0f36c,,,eng,REGULAR,0,0
215,1651,Decolonizing the White Colonizer?,"Lucas, Cecilia Cissell","Baquedano-Lopez, Patricia;",2013,"This interdisciplinary study examines the question of decolonizing the white colonizer in the United States. After establishing the U.S. as a nation-state built on and still manifesting a colonial tradition of white supremacy which necessitates multifaceted decolonization, the dissertation asks and addresses two questions: 1) what particular issues need to be taken into account when attempting to decolonize the white colonizer and 2) how might the white colonizer participate in decolonization processes? Many scholars in the fields this dissertation draws on -- Critical Race Theory, Critical Ethnic Studies, Coloniality and Decolonial Theory, Language Socialization, and Performance Studies -- have offered incisive analyses of colonial white supremacy, and assume a transformation of white subjectivities as part of the envisioned transformation of social, political and economic relationships. However, in regards to processes of decolonization, most of that work is focused on the decolonization of political and economic structures and on decolonizing the colonized. The questions pursued in this dissertation do not assume a simplistic colonizer/colonized binary but recognize the saliency of geo- and bio-political positionalities. As a result of these different positionalities, white U.S. citizens committed to participating in our own decolonization and in the decolonization of our (social, political, educational, and economic) structures and relationships with others must learn from but cannot simply imitate or appropriate decolonial methodologies developed by indigenous people and people of color.The title of this dissertation posits decolonization as an active ongoing process (through the use of the verb-form, i.e. ""decolonizing"") without guarantees (through the use of the question mark). Each chapter addresses a different yet interrelated aspect of this process:Chapter One intervenes in the reconstructionism versus abolitionism debate in Whiteness Studies, and offers p/reparations as a framework for redistributory practices and (inter)personal transformation and as a methodology through which the white colonizer might contribute to racial justice and decolonization projects. P/reparations processes are open-ended and include apologies, material and cultural redress, and structural change to ensure non-recurrence. By highlighting historical and contemporary processes of accumulation by dispossession, p/reparations processes emphasize interconnectedness and challenge the illusion of autonomous individuals, groups and nation-states. Thus, a p/reparations framework intervenes into discourses of meritocracy and equal opportunity; denaturalizes notions of citizenship, immigration, and the borders of nation-states; and provides counter-narratives to discourses of aid and charity which assume the assets being redistributed were legitimately acquired and that acts of redistribution should thus be met with gratitude. Chapter Two examines the ways in which the geographical control of bodies has been a key technology of white supremacist colonialism. Given the entanglement of geographical (im)mobility with social (im)mobility and an unequal racialized distribution of premature death, decolonization and the dismantling of white supremacy necessitates not only the redistribution of political and economic resources but divesting from U.S.-ness itself. As such, decolonization requires not only white abolitionism but also U.S.-abolitionism. This chapter interrogates the use of the trope of ""the criminal"" by both the nation-state and the prison industrial complex, and the ways in which these discourses are mobilized as threats to the white colonizer's ""home."" As such, this chapter argues that, for the white colonizer, one aspect of decolonization may require developing a relationship to home as a foreign concept as well as (in many cases) pursuing downward rather than upward mobility.Chapter Three suggests power-conscious hybridity as a technology the white colonizer can employ in the face of this challenge of needing to claim whiteness and U.S.-ness even as we seek to participate in their abolition. Hybridity emphasizes that no one is reducible to any particular ""identity."" In order not to disappear into colorblind ""humanness,"" engage in cultural appropriation, and/or revalorize whiteness, however, the white colonizer's employment of hybridity must simultaneously involve (de)facing whiteness. (De)facing implies a double movement: facing whiteness, in all of its horror, without resorting to white flight; and defacing whiteness, both in the sense of destroying it and in the sense of de-facing it, i.e. undoing the notion that whiteness is human.Chapter Four examines issues of pedagogy and curricula inside and outside the classroom as they pertain to processes of recreating and transforming colonial white supremacy. This chapter critiques discourses of ""equality of opportunity"" as a primary ideological mechanism supporting colonial white supremacy in the current age of colorblind racism. Through participant-observation of two different attempts at ""social justice"" schooling (one at the high school level, one at the college level), it examines the creation of what Michel Foucault calls ""docile bodies,"" and draws on pedagogies from theater as possibilities for cultivating counter-disciplines of the body. This chapter ends with a list of specific skills the white colonizer needs to learn for the purpose of decolonization. ""Chapter"" Five attempts to ""practice what I preach"" (in particular in relation to the colonial white supremacy institutionalized as epistemological hierarchies in the academy) by revisiting the topics of this dissertation in a live performance. This theoretical and methodological intervention enacts a response to critiques of the mind/body split in colonial epistemologies, and positions performance as analysis which must be engaged on its own terms -- rather than only as a methodology or phenomenon that is then analyzed in writing. This is also a pedagogical intervention which insists on the importance and legitimacy of multiple modalities of communication beyond writing within academia, and seeks to make academia feel accessible to a wider range of people with a range of learning and teaching styles.The Inconclusion addresses the question of why the white colonizer would want to decolonize. It argues that the prerequisite for wanting to decolonize is recognizing oneself as colonizer and all beings as interconnected. Then decolonization becomes not so much a choice as a spiritual--which is also to say political--imperative. As such, this dissertation argues not only against the mind/body split, but also against the mind/body/soul split by emphasizing the importance of politicizing and embodying spirituality and infusing political movements with spiritual convictions.",ucb,,https://escholarship.org/uc/item/06g372z8,,,eng,REGULAR,0,0
216,1652,A Utility-Theory-Consistem System-of-Demand-Equations Approach to Household Travel Choice,"Kockelman, Kara Maria",,1998,"Modeling personal travel behavior is complex, particularly when one tries to adhere closely to actual casual mechanisms while predicting human response to changes in the transport environement. There has long been a need for explicitly modeling the underlying determinant of travel- the demand for participation in out-of-home activities; and progress is being made in this area, primarily through discrete-choice models coupled with continous-duration choices. However, these models tend to be restircted in size and conditional on a wide variety of other choices that could be modeled more endogenously. ",ucb,,https://escholarship.org/uc/item/06x0k5r4,,,eng,REGULAR,0,0
217,1653,"""Vous êtes hombre de bien”: A study of bilingual family letters to and from colonial Louisiana, 1748-1867","Thomas, Jenelle","McLaughlin, Mairi;",2017,"In this dissertation, I use a bilingual epistolary corpus to examine the interaction of language contact, language- and genre-specific conventions, and speakers’ individual communicative strategies. Although multilingualism was a common historical condition, many traditional language histories and studies of historical speech only consider monolinguals, or bilingualism as it affects a particular language, rather than seeing multilingualism as part of an individual and community repertoire. In contrast, this study uses both quantitative and qualitative methods to examine the linguistic practices of a bilingual network of speakers in both of their languages. The corpus analyzed here is a collection of private letters which I selected and transcribed from the family correspondence of Francisco Bouligny, a soldier and military governor in colonial New Orleans. These letters were written in both French and Spanish as the family and acquaintances corresponded between New Orleans, France, and Spain for a period spanning from the mid-18th to the mid-19th century.I address the following research questions through case studies at three levels: phonological/orthographic, morphosyntactic, and pragmatic.(1)	In what way(s) does bilingualism affect language usage? How do bilingual and monolingual usage differ?(2)	How does bilingualism interact with other factors such as convention, genre, audience, and stance, and how do bilingual writers use the resources of their two languages in their production?In Chapter 3, I find widespread orthographic variation conditioned by factors such as age and geographic location. However, I argue for the inclusion of language-specific education and literacy as an additional factor in variation, as I find that the written standard can obscure variation and contact effects while also serving as a resource for bilinguals when writing in a second language. Chapter 4 addresses the contrast between the complex and simple past tenses in both Spanish and French. As the monolingual patterns of use in each language diverge, I find that bilinguals as a group do not follow monolinguals in showing an increased use of the complex past in French but not Spanish. Although individual patterns vary greatly, bilinguals use the French complex past overall less frequently than monolinguals, arguably because of the restraining influence of Spanish contact. This contact influence can be seen in discourse-pragmatic uses of the two tenses, used to shape the narrative or create temporal contrast. In Chapter 5, I consider the construction of identity and interpersonal relationships in bilingual correspondence through choice of language, forms of address, and expressions of sincerity. I find that speakers choose and continue to use only one language with their addressees, even when both speakers are bilingual, and that this choice is motivated largely by the characteristics of the addressee. The choice of second person pronouns (T or V) patterns fairly rigidly according to language and familial relationship, but I argue that speakers vary opening formulae to express more subtle distinctions in the intimacy of the relationship. Some speakers similarly appeal to existing formulae to convince the addressee of their sincerity, but this can be shown to be particularly true of less literate or second-language speakers, while other speakers eschew explicit mentions of sincerity for other strategies. Overall, although speakers show awarenes of epistolary norms in each language and in many ways adhere to language-specific practices, they also manipulate these conventions in meaningful ways. This study is the first to delve into issues of historical bilingualism through a balanced bilingual epistolary corpus and one of few to explore language use in Spanish Louisiana. I find that individual speakers show evidence of using resources from their bilingual repertoire to aid in composition and construction of meaning in various ways. This is particularly true of loci of variation in the individual language systems or structures that share typological similarities across the systems. However, at the community level the variety of individual patterns and the force of the monolingual norms and written formulae appear to inhibit the spread of change. I underscore the importance in future studies of considering a speaker’s (and community’s) language use in its entire context, including other languages, literacies, and considerations of genre, as we explore how speakers use the available resources for interpersonal communication and how that translates to the community level.",ucb,,https://escholarship.org/uc/item/07b228j2,,,eng,REGULAR,0,0
218,1654,The Hecke Stability Method and Ethereal Forms,"Schaeffer, George Johann","Venkatesh, Akshay;",2012,"The purpose of this thesis is to outline the Hecke stability method (HSM), a novel method for the computation of modular forms.The HSM relies on the following idea: A finite-dimensional space of ratios of modular forms that is stable under the action of a Hecke operator should consist of modular forms (i.e., without poles). This principle is correct over the complex numbers, but more care is required over finite fields due to complications arising near the supersingular points on modular curves. Formalizing this main idea as a theorem comprises most of our theoretical work.Though it can be utilized in a variety of settings, the main application of the Hecke stability method is the computation of weight 1 modular forms. These spaces cannot be computed using the algorithms (e.g., modular symbols algorithms) that are typically employed to compute modular forms of higher weight.Furthermore, to provide a complete picture of the weight 1 modular forms of level N, we must account for certain sporadic discrepancies between the space of classical forms and the space of mod p modular forms. Ultimately, our approach is motivated by the effect this ""ethereality"" phenomenon may have on the statistics of number fields via the theory of modular Galois representations.",ucb,,https://escholarship.org/uc/item/07n8235q,,,eng,REGULAR,0,0
219,1655,Ca Isotopes in Igneous and High-Temperature Metamorphic Systems and the Hydrothermal Chemistry of Paleoseawater,"Antonelli, Michael A","DePaolo, Donald J;",2018,"In the last two decades it has been increasingly recognized that Ca isotope fractionation at high-temperatures can greatly exceed fractionation at surface conditions. These effects have been used to trace recycling of crustal materials into the mantle, to estimate equilibration temperatures in mantle rocks, and to understand kinetic isotope fractionation during diffusion in experimental melts. However, little is known about the competition between kinetic and equilibrium Ca isotope effects in natural samples, which suggests that the use of stable Ca isotope variations as a proxy for carbonate recycling (or for mantle temperature) is significantly underdetermined.The Ca isotope evolution of continental crust, which is currently understudied, is intimately linked to the geochemical evolution of Earth’s mantle, oceans, and atmosphere. As such, this dissertation focuses dominantly on understanding radiogenic, kinetic, and equilibrium Ca isotope variations in igneous and metamorphic rocks from the continental crust, but also includes radiogenic Sr isotope models of hydrothermal circulation at mid-ocean ridges, which are critical to understanding continental weathering rates and the geochemical evolution of seawater over time.Chapter 2 explores the potential feed-back between the chemical and isotopic composition of paleoseawater and high-temperature hydrothermal fluids, which has been implied by Sr-isotope measurements in hydrothermal minerals over time (Antonelli et al., 2017). Chapter 3 presents results from radiogenic Ca isotope measurements in lower-crustal rocks and minerals, confirming that K is effectively lost from the lower crust during high-T metamorphism (Antonelli et al., 2018). Chapter 4 focuses on stable Ca isotope measurements in lower-crustal rocks and minerals, demonstrating that kinetic Ca effects are abundant in nature, both at the whole-rock and inter-mineral scales, and that they can be used to understand paragenesis and Ca diffusion in lower-crustal rocks (Antonelli et al., 2019b). Chapter 5 presents stable Ca isotope results from volcanic and sub-volcanic rocks and minerals, which imply that Ca isotopes can be used to estimate crystal growth-rates in igneous crystals (e.g. phenocrysts, comb-layers, and orbicules) and that associated volcanic eruption/recharge events are generally short-lived (Antonelli et al., 2019a).",ucb,,https://escholarship.org/uc/item/0825f42v,,,eng,REGULAR,0,0
220,1656,Engineering Magnetoelectric-Multiferroic Composites Using FeRh,"Clarkson, James David","Ramesh, Ramamoorthy;",2016,"This thesis presents the study of magnetoelectricity in artificial multiferroic heterostructures, focussing on the FeRh metamagnetic phase transition. Spin based electronic systems are ideal for many applications, primarily as a result of long term stability. Toward this goal, there is a need for electrically modifiable and differentiable magnetic states. First, new methodologies to determine and control the antiferromagnetic axis in the low temperature  antiferromagnetic phase of FeRh are considered. Second, In an effort to increase the ability to control magnetism with an applied electric field through the converse magnetoelectric effect, FeRh is combined with piezoelectric materials. In association with the large volumetric magnetostriction coefficient of FeRh, a composite multiferroic with the largest magnetoelectric coupling is observed. Finally, the out-of-plane magnetic anisotropy is investigated as a function of the ferromagnetic phase fraction and epitaxial strain. This is applied toward investigation of zero magnetic field magnetoelectric control of the magnetic phase and the apparent memory of the magnetization direction across repeated phase reversal.",ucb,,https://escholarship.org/uc/item/09979501,,,eng,REGULAR,0,0
221,1657,Influencing cell fate decisions using physical and chemical cues,"Sia, Junren","Li, Song;",2016,"Directed genetic reprogramming of cells from one identity to another offers tremendous potential in regenerative medicine, disease modelling and drug testing.  However, its application is limited by the low efficiency at which it occurs, and existing methods to improve efficiency mostly utilize additional molecular biology and biochemical manipulations. This thesis explored an alternative paradigm for improving reprogramming efficiency: presentation of physical cues. To this end, I first showed that simply agitating an adherent culture with an orbital shaker enhanced its efficiency of reprogramming to induced pluripotent stem cells (iPSCs). I further demonstrated that convective mixing of the culture medium by orbital agitation blunted the upregulation of CDK inhibitor p57/Kip2 that was caused by the culture becoming overconfluent, which in turn enhanced the efficiency of reprogramming to iPSCs. Next, I showed that culturing reprogramming cells on solid supports scored with microgrooves enhanced their reprogramming into cardiomyocytes. I demonstrated that the microgrooves caused upregulation of the activity of the transcription factor megakaryoblastic leukemia-1 (Mkl1) / myocardin-related transcription factor A (Mrtf-a) and also enhanced organization of sarcomeric structure, with both effects contributing to better reprogramming efficiency. In addition to physical cues, I also explored whether treatment with only small molecules could reprogram fibroblasts into skeletal muscle cells. Indeed, I found that an optimized basal medium (10% FBS in DMEM with 50 μg/ml of ascorbic acid and 50 ng/ml of basic fibroblast growth factor (bFGF)) containing just 2 small molecules— 616452 [an inhibitor of the protein kinase activity of the transforming growth factor-beta (TGF-β) type I receptor (R1)] and forskolin (a plant diterpene that stimulates adenylyl cyclase and elevates the intracellular level of 3',5'-cyclic-AMP)—was sufficient to achieve reprogramming at high efficiency. In summary, this thesis described how both physical and chemical cues can contribute to enhancing the reprogramming of cell identity.",ucb,,https://escholarship.org/uc/item/0bq39484,,,eng,REGULAR,0,0
222,1658,Synthesis of Doped Graphene Nanoribbons from Molecular and Polymeric Precursors,"Cloke, Ryan","Fischer, Felix R;",2015,"Abstract Synthesis of Doped Graphene Nanoribbons from Molecular and Polymeric Precursorsby Ryan Randal ClokeDoctor of Philosophy in ChemistryUniversity of California, Berkeley Professor Felix Fischer, Chair As electronic devices continue to shrink and energy problems continue to grow, nanoscale materials are becoming increasingly important.  Graphene is a material with exceptional promise to complement silicon in next-generation electronics because of its extraordinary charge carrier mobility, while also finding a role in cutting-edge energy solutions due to its high surface area and conductivity.  Improving on this material even further by reducing the width of graphene to nanoscale dimensions with atomically-precise dopant patterns is the subject of this thesis.  Nanometer-wide strips of graphene, known as graphene nanoribbons (GNRs), offer the advantages of semiconducting behavior, combined with more accessible surface area compared to bulk graphene (Chapter 1).  Additionally, it is demonstrated that GNRs can be doped with atomic precision, allowing for intricate modulation of the electronic properties of this material, which was studied by STM, STS, and nc-AFM (Chapter 2).  Controlled growth of GNRs on surfaces is still an outstanding challenge within the field, and to this end, a variety of porphyrin-GNR template materials were synthesized (Chapter 3).  The GNRs obtained in this work were also synthesized in solution, and it was shown that these materials possess excellent properties for applications in hydrogen storage, carbon dioxide reduction, and Li-ion batteries (Chapter 4).  A prerequisite for solution-synthesized GNRs, conjugated aromatic polymers are an important class of materials in their own right.  Therefore, Ring-Opening Alkyne Metathesis Polymerization was developed using conjugated, strained diynes (Chapter 5).  The resulting conjugated polymers were explored both for their own materials properties due to a remarkable self-assembly process that was discovered, and also as precursors to GNRs (Chapter 6).  This work advances the fundamental understanding of carbon-based nanostructures, as well as the large-scale production of GNRs for next-generation energy and electronics applications.",ucb,,https://escholarship.org/uc/item/0cm8d86r,,,eng,REGULAR,0,0
223,1659,"Approximate counting, phase transitions and geometry of polynomials","Liu, Jingcheng","Sinclair, Alistair;",2019,"In classical statistical physics, a phase transition is understood by studying the geometry (the zero-set) of an associated polynomial (the partition function).  In this thesis, we will show that one can exploit this notion of phase transitions algorithmically, and conversely exploit the analysis of algorithms to understand phase transitions.  As applications, we give efficient deterministic approximation algorithms (FPTAS) for counting $q$-colorings, and for computing the partition function of the Ising model.",ucb,,https://escholarship.org/uc/item/0f2195k5,,,eng,REGULAR,0,0
224,1660,Decoupled Vector-Fetch Architecture with a Scalarizing Compiler,"Lee, Yunsup","Asanovic, Krste;",2016,"As we approach the end of conventional technology scaling, computer architects are forced to incorporate specialized and heterogeneous accelerators into general-purpose processors for greater energy efficiency.  Among the prominent accelerators that have recently become more popular are data-parallel processing units, such as classic vector units, SIMD units, and graphics processing units (GPUs).  Surveying a wide range of data-parallel architectures and their parallel programming models and compilers reveals an opportunity to construct a new data-parallel machine that is highly performant and efficient, yet a favorable compiler target that maintains the same level of programmability as the others.In this thesis, I present the Hwacha decoupled vector-fetch architecture as the basis of a new data-parallel machine.  I reason through the design decisions while describing its programming model, microarchitecture, and LLVM-based scalarizing compiler that efficiently maps OpenCL kernels to the architecture.  The Hwacha vector unit is implemented in Chisel as an accelerator attached to a RISC-V Rocket control processor within the open-source Rocket Chip SoC generator. Using complete VLSI implementations of Hwacha, including a cache-coherent memory hierarchy in a commercial 28 nm process and simulated LPDDR3 DRAM modules, I quantify the area, performance, and energy consumption of the Hwacha accelerator.  These numbers are then validated against an ARM Mali-T628 MP6 GPU, also built in a 28 nm process, using a set of OpenCL microbenchmarks compiled from the same source code with our custom compiler and ARM's stock OpenCL compiler.",ucb,,https://escholarship.org/uc/item/0fm0z48h,,,eng,REGULAR,0,0
225,1661,Creating Community Among Leaders: Leveraging Shared Practices for School Improvement,"Williams, Sarah June","Gifford, Bernard;",2017,"Principals work within a complex context where multiple stakeholders make many competing demands of them. Chief among these demands includes district initiatives, which serve to create leadership expectations but often do not contain clear methods or practices for implementation. Additionally, demands of the local community and interests of teachers and students create layers of complexity which can confound and isolate leaders. While principals may feel cut off from their peers dealing with these intricacies, the reality is, regardless of initiative or priority, principals have many common problems of practice. Establishing highly effective teacher collaborative groups, is an example of an implementation most principals come to face.The Early Release Wednesday Toolkit was developed to support leadership practices for implementing highly effective collaborative teacher groups. To create the toolkit, a sample of principals engaged in a co-development process to capture effective practices already in place, and share them with the larger principal group. In so doing, the principal Community of Practice was strengthened. The findings of this study suggest that principals gain from relying on each other for problem solving where their leadership is concerned, and may serve to inform other leaders about effective ways to learn from one another. This design study is centered on action research and includes two primary research elements, evaluation of the design outcome and assessment of the design process.",ucb,,https://escholarship.org/uc/item/0gq200m9,,,eng,REGULAR,0,0
226,1662,Promoting Learning of Instructional Design via Overlay Design Tools,"Carle, Andrew Jacob","Canny, John;",2012,"Design is a notoriously difficult profession to practice, and it is even more difficult to learn.  Traditionally, learning of design skills has been situated in the context of apprenticeships or formal design studios.  Unfortunately, these methods are inaccessible to practicing professionals due to constraints on time and location.  And, indeed, professional designers must continuously update their knowledge as paradigm shifts in design practice threaten to make their skills obsolete.  An ideal resolution to this problem is to situate the learning of design skills within the professional practice of design.  This dissertation studies an approach to this mode of situated learning, focusing on integrating learning mechanisms into practical design tools.  These tools provide scaffolding for novices as they construct an understanding of best practices in design while engaging in real design work.I begin by introducing Virtual Design Apprenticeship (VDA), a learning model — built on a solid foundation of education principles and theories — that promotes learning of design skills via overlay design tools.  In VDA, when an individual needs to learn a new design skill or paradigm she is provided accessible, concrete examples that have been annotated with design rationale.  These annotations make expert thinking visible and allow the novice to immediately use, and gradually understand, new best practices.  By combining abstract rationale with concrete design instances, annotated artifacts become more useful than either could be alone.  I describe the essential components of the VDA framework: annotated design artifacts, a repository of carefully chosen annotated examples, and a community of experts and learners.  I walk the reader through an example of how VDA scaffolds learners as they move from a novice's understanding of a design space towards that of an expert.  Within the context of this example, I present a set of design principles that guide the creation of VDA design tools — user interfaces built to mediate an individual's interactions with the three core VDA components.While VDA is applicable to most design fields, I narrow the scope of consideration to one particular domain of design by focusing in-depth on the instructional design difficulties that university-level faculty members face and how the VDA approach can address them.  These instructors face precisely the type of design paradigm shift that VDA was developed to ease as they attempt to move away from traditional, lecture-based pedagogical methods and towards more modern, learner-centered techniques.I engaged with these instructors and a curriculum design research group in a six-year period of contextual inquiry.  Findings from this study influenced my formulation of the VDA framework and the design of PACT, a design tool that leverages the learning principle of making thinking visible to assist novices as they transition from concrete to abstract reasoning about curriculum design.  The central focus of PACT is the incorporation of annotated references to pedagogical design patterns — abstract representations of best practices in instructional design.  I discuss the iterative design and implementation of PACT in detail, highlighting the ways in which it embodies the VDA design principles for promoting learning of instructional design via overlay design tools.  Next, I study the challenges of converting abstract best practices and design patterns into concrete annotations that can be applied directly to content.  My solution, the PACT Annotation Schema, is a formal mechanism for generating tags and pattern annotations from freeform pattern text.  Formal representations of patterns are far more useful than generic references, both as scaffolds for learning and for structuring user interactions with design artifacts.  Using this schema, I have generated the PACT Annotation Library, a collection of 56 tags and 74 pattern annotations based on the work of the Pedagogical Patterns Project.  Visual representations of these formal annotations are the centerpiece of PACT's user interface.The PACT tool was evaluated in two distinct stages.  First, I present a formative study conducted with early, prototype versions of the PACT tool.  This study examines the utility of PACT for expert curriculum designers and curriculum research groups, using a sample annotation process — and reflection on the outcomes of that process — to demonstrate that my approach is feasible and useful for those groups. I then present a summative user study of the utility of PACT for novice learner-centered curriculum designers.  I demonstrate PACT's significant impact on how novice designers learn from expert-generated examples, how they perceive the credibility of those examples, and the quality of curriculum designs those novices can produce.  These findings show that the VDA approach to learning works and that the PACT overlay curriculum design tool is a successful realization of VDA's design principles.Last, I discuss future directions for this work.  PACT is a fully developed design tool that can and should be used by curriculum designers as they create new courses and build their own understanding of the principles of learner-centered design.  The PACT Annotation Schema is a useful mechanism that can be further improved to allow the generation of more accurate and complete annotations based on design patterns.  The PACT Annotation Library should be continuously expanded as new patterns and principles are developed.  Finally, the Virtual Design Apprenticeship model for learning is a robust and highly-principled approach to integrating design learning and design practice.  It is applicable across a wide range of design domains and can help promote learning of design skills in them all.",ucb,,https://escholarship.org/uc/item/0hh0r9rp,,,eng,REGULAR,0,0
227,1663,Mass Transport of Condensed Species in Aerodynamic Fallout Glass from a Near-Surface Nuclear Test,"Weisz, David Gabriel","van Bibber, Karl;Knight, Kim;",2016,"In a near-surface nuclear explosion, vaporized device materials are incorporated into molten soil and other carrier materials, forming glassy fallout upon quenching. Mechanisms by which device materials mix with carrier materials have been proposed, however, the specific mechanisms and physical conditions by which soil and other carrier materials interact in the fireball, as well as the subsequent incorporation of device materials with carrier materials, are not well constrained. A surface deposition layer was observed preserved at interfaces where two aerodynamic fallout glasses agglomerated and fused. Eleven such boundaries were studied using spatially resolved analyses to better understand the vaporization and condensation behavior of species in the fireball. Using nano-scale secondary ion mass spectrometry (NanoSIMS), we identified higher concentrations of uranium from the device in 7 of the interface layers, as well as isotopic enrichment (>75% 235U) in 9 of the interface layers. Major element analysis of the interfaces revealed the deposition layer to be chemically enriched in Fe-, Ca- and Na-bearing species and depleted in Ti- and Al-bearing species. The concentration profiles of the enriched species at the interface are characteristic of diffusion. Three of the uranium concentration profiles were fit with a modified Gaussian function, representative of 1-D diffusion from a planar source, to determine time and temperature parameters of mass transport. By using a historical model of fireball temperature to simulate the cooling rate at the interface, the temperature of deposition was estimated to be ∼2200 K, with 1σ uncertainties in excess of 140 K. The presence of Na-species in the layers at this estimated temperature of deposition is indicative of an oxygen rich fireball. The notable depletion of Al-species, a refractory oxide that is highly abundant in the soil, together with the enrichment of Ca-, Fe-, and 235U-species, suggests an anthropogenic source of the enriched species, together with a continuous chemical fractionation process as these species condensed.",ucb,,https://escholarship.org/uc/item/0hp34897,,,eng,REGULAR,0,0
228,1664,Pattern Matching for Advanced Lithographic Technologies,"Rubinstein, Juliet Alison","Neureuther, Andrew R;",2010,"This dissertation extends fast-CAD kernel convolution methods for the identification of unintended effects in optical lithography, including OPC-induced sensitivities, high-NA and polarization vector effects.  A more accurate through-focus physical model is incorporated, and the application of layout decomposition guidance for double patterning is demonstrated.  All layout regions react differently to lithographic processes such as aberrations, and the vulnerabilities are non-intuitive and hard to capture with design rules.  The pattern matcher is a fast tool, developed by Gennari and Neureuther, for quickly scanning layouts to find vulnerabilities to unintended effects of lithographic processes.  Kernel convolutions are performed between Maximum Lateral Test Patterns (MLTPs) and mask layouts, and are over a factor of 104 times faster than rigorous simulation.  Challenges faced in pattern matching extensions include MLTP derivation, edge movement prediction, defocus accuracy improvement, and integration of image effect estimation into real-time guidance for layout decomposition.As motivation for why variability and yield are important, a study is presented in which a probabilistic distribution of transistor Critical Dimensions (CD) is generated given a focus-exposure joint distribution.  An interpolation model is used to generate CD response surfaces, producing a fast method for the analysis of average CD variation for each transistor, the spread of individual variations, the OPC performance, the Across Chip Linewidth Variation (ACLV), and yield distribution.  This study motivates the importance of understanding the variability in a layout; the remainder of the dissertation demonstrates how pattern matching can provide a fast approximation to variability due to lithographic effects.MLTPs, derived as the inverse Fourier Transform of the Zernike polynomials, are the theoretically most sensitive patterns to lens aberrations. As well as being used as input to the pattern matcher, MLTPs can also be etched onto a mask to function as aberration monitors.  However, MLTPs are inherently very costly and unfriendly for mask manufacturing, due to round edges and touching phases.  Both a mask-friendly handmade pattern and an automated method of monitor modification are presented. The handmade pattern retains 68% of its sensitivity to defocus and orthogonality to other aberrations, and the automatically generated pattern passes all DRC checks with only minimal modifications.Use of the pattern matcher on pre-OPC layouts admits the identification of problematic hot-spots earlier in the design flow.  Several studies are presented on the effects of different OPC algorithms on match factors.  In most cases, match factors do not vary significantly between the pre-OPC layout and the post-OPC layout, and the pre-OPC match factor is a good indicator for the sensitivity of the post-OPC layout area.  However, in some circumstances, especially when SRAFs are present, the pre- and post-OPC match factors can vary by a larger amount.  It is shown that defocus and proximity sensitivities occur in different locations on a layout, and if OPC targets the best-case simulation, then it is possible for OPC to worsen sensitivities to aberrations.  As a consequence, pattern matching should be used on post-OPC layouts to check for any created sensitivities.Extensions of the pattern matcher are presented for high-NA and polarization vulnerabilities.  This involves the generation of three to five match patterns for either on-axis or off-axis illumination, with a vulnerability score being calculated as a weighted sum of the match factors.  The patterns are tested against simulation, and found to be good predictors of vulnerability to high-NA and polarization vector effects.  High-NA and polarization vector effects are significant, causing intensity changes of 40% or 10% respectively for the on-axis case, and 8% for the off-axis case.The accuracy of the pattern matcher is evaluated, and improved.  A method for predicting edge movement through coma, rather than just change in intensity, takes the image slope into account and improves the R2 from 0.73 to 0.95.  A major contribution of this dissertation is the improvement of the pattern matching model for defocus.  A quadratic model for defocus is presented, using both the Optical Path Difference (OPD) and OPD2, rather than just the linear term.  OPD2 expands to yield two new patterns, Z0 and Z8, to be used in addition with Z3, which is derived from the OPD term.  Using the three match patterns, prediction of the change in intensity through focus improves from completely non-predictive to an R2 value of 0.92.  Results show that the Z3, pattern and a combined Z0 and Z8,  pattern predict change in intensity through defocus at line ends with an R2 of 0.96, indicating that two match factors with algebraic weighting factors are likely possible.  These results are of great importance, as defocus is not a small aberration, reaching typical values of nearly one Rayleigh unit, and the ability to find defocus-induced hot-spots is of practical interest.Double patterning is identified as an emerging technique that benefits from the application of pattern matching. In double patterning, a layout is split into two masks, each mask being exposed separately, effectively doubling the pitch.  A process flow is presented showing that pattern matching can add value both within the double patterning decomposition algorithm, and also on the post-decomposition layout.  Pattern matching is tested on post-decomposition layouts, showing that in one particular case using complementary dipole illumination, the match factors for coma are increased significantly on the post-decomposition layout.  In another case for annular illumination, introducing an extra split is shown to reduce the variability through coma, and reduces the match factor by 55%.  Furthermore, when splitting an H-structure, a number of different splits are scanned by the pattern matcher, and the split with the lowest intensity change through defocus (which was two thirds smaller than the largest change) is correctly identified.  These examples show that the pattern matcher is an appropriate tool for double patterning, that can quickly provide a measure of intensity change through defocus during the layout decomposition process.",ucb,,https://escholarship.org/uc/item/0hx624s1,,,eng,REGULAR,0,0
229,1665,"The New Interculturalism: Race, Gender and Immigration in Post-Celtic Tiger Ireland","McIvor, Charlotte Ann","Glazer, Peter R;Steen, Shannon;",2011,"""There are wonders that I want to perform"" says the name of Ireland's first African-Irish theatre company, Arambe Productions, which derives from the Nigerian saying ara m be ti mo fe da. The company performs stories of the African-Irish community, yet their dramatizations ponder a larger reality of an Ireland that has gone from a country of emigrants to a nation re-shaped by inward-migration. The sudden shifts brought on by the mid-1990s Celtic Tiger economic boom and unprecedented immigration have plunged the Irish population at large into a state of wondering. What does it mean that the non-Irish born population residing in the Republic grew from less than 5% to more than 12% in a little over a decade?  How will Ireland model a vision of interculturalism that avoids the failures of multiculturalism in Western Europe and the U.S.?  How have race and gender created a hierarchy amongst migrant communities and subjects?  Through performance, Arambe Productions transforms such wondering into a process of ""working together,"" signaling a second meaning of the company's name: harambee in Swahili means ""work together.""  The company's collective labors aim to create a post-Celtic Tiger intercultural vision of Irish identity and belonging.  But can this vision be performed into existence?My dissertation project, ""Performing the `New Irish': Race, Gender, and Interculturalism in the Post-Celtic Tiger Nation,"" argues that performance is at the center of conceptualizing interculturalism as social policy, philosophy and aspiration in contemporary Ireland.  While some might see interculturalism as referring to two cultures meeting in the moment of performance, I argue, rather, that in Ireland today, the term refers to the process of inventing a new pluralistic Irish identity, one that accommodates Irish-born as well as migrant communities.  Irish interculturalism connotes practical policy measures regarding integration, access to social benefits and services, and public eduction about racism, but it also translates into cultural initiatives that stress the arts as a zone of contact between diverse populations.  My research examines theatres, public festivals and arts/social organizations that make use of performance to theorize interculturalism as embodied practice.  Theatre companies like Arambe, Camino de Orula Productions, Calypso Productions, and NGOs like Spirasi, Migrant Rights Centre Ireland, and the Forum on Migration and Communication bid for cultural recognition of minority groups through performance, arts, and media activism. These efforts are endorsed by diverse governmental and non-governmental bodies, which range from the Office of the Minister of Integration, the now-defunct National Consultative Committee on Racism and Interculturalism, to the Irish government Task Force on Active Citizenship.  The diverse sponsors and forums for these projects, however, generate tension between state-managed visions for interculturalism and the goals of community-based or non-governmental groups advocating for an interculturalism from below which remains critical of the Irish state's treatment of minority groups and management of inward-migration more generally.     My investigation of the interplay between social and aesthetic theories of interculturalism exposes the embodied challenges of analyzing relationships between the Irish state, minority communities and the nation at large.  Using ethnographic methods, I position performance as the crucible in which Irish theories of interculturalism are tested and reimagined through the work of bodies who must bear the labor of social change.  I trace the struggles to craft an analytical language around race and ethnicity in Ireland frames these projects, and how the intersection of gender with these former categories complicates this task. My sites range from the Abbey Theatre stage to the Migrant Rights Center's photography exhibit by domestic workers and the Dublin St. Patrick's Festival Parade in order to capture the diversity of venues in which performing bodies are called upon to embody post-Celtic Tiger social change.  My case studies interrogate whether these projects have the power to push against material limits of social access, paths to citizenship and racism/discrimination and reveal that these performances frequently reinscribe relationships of power between minority and Irish-born communities by falling back on top-down models of interculturalism.  Perhaps it is through the reiterative power of performance that the wonders of an egalitarian Irish interculturalism can come into being, but these moving bodies must first be situated in broader matrixes of power which index the role of race and gender in shaping the future of post-Celtic Tiger Irish identities.",ucb,,https://escholarship.org/uc/item/0k75g3rv,,,eng,REGULAR,0,0
230,1666,"Competing Visions of the Modern: Urban Transformation and Social Change of Changchun, 1932-1957","Liu, Yishi","AlSayyad, Nezar;Yeh, Wen-hsin;",2011,"Examining the urban development and social change of Changchun during the period 1932-1957, this project covers three political regimes in Changchun (the Japanese, the Nationalist, and the Communist), and explores how political agendas operated and evolved as a local phenomenon in this city.  I aim to reveal connections between the colonial past and socialist ""present"". I also aim to reveal both the idiosyncrasies of Japanese colonialism vis-à-vis Western colonialism from the perspective of the built environment, and the similarities and connections of urban construction between the colonial and socialist regime, despite antithetically propagandist banners, to unfold the shared value of anti-capitalist pursuit of exploring new visions of and different paths to the modern.  The first three chapters relate to colonial period (1932-1945), each exploring one facet of the idiosyncrasies of Japanese colonialism in relation to Changchun's urbanism. Chapter One deals with the idiosyncrasies of Japanese colonialism as manifested in planning Changchun are the subject of the next chapter. Chapter Two charts out the plurality of architectural styles in the city, and analyzes the diversities, ambivalences, and ambiguities in the practice of statecraft and urban construction. Chapter Three gives a picture of how the downtown of Changchun was reconstructed to meet new political agenda when Socialist Realism took sway of aesthetic program. I also examine in this chapter the nature of Japanese colonialism in Manchukuo from the perspective of rituals and pubic pageantries, by using Yamamuro's analogy of the client state to a hybrid beast of chimera.The last two chapters examine Changchun's development since 1945. Chapter Four pictures how the downtown of Changchun was reconstructed to meet new political agenda. Chapter Five explores Changchun's urban expansion under Maoism: the construction of the First Automobile Works, a key project of Maoist industrialization. The purposes of the dissertation have been anchored by an overall objective to fill up this vacancy from the perspective of urban construction and urban life.This dissertation has unfolded a proliferation of competing formulations of the modern in Changchun's urban history, some inspired by Western creations but more competing with Western concerns. In the competition for the dominance of the world, Japanese colonialism in Manchukuo and Chinese socialism both represented massive anti-capitalist and anti-imperialist qualities.",ucb,,https://escholarship.org/uc/item/0149581v,,,eng,REGULAR,0,0
231,1667,Chip-Scale Lidar,"Behroozpour Baghmisheh, Behnam","Boser, Bernhard E;",2016,"The superiority of lidar compared to radio-frequency and ultrasonic solutions in terms of depth and lateral resolution has been known for decades. In recent years, both application pull such as 3D vision for robotics, rapid prototyping, self-driving cars, and medical diagnostics, as well as technology developments such as integrated optics and tunable lasers have resulted in new activities.Pulsed, amplitude-modulated continuous-wave (AMCW), and frequency-modulated continuous-wave (FMCW) lidars can all be used for ranging. The latter option enables excellent depth resolution at the micron level. Achieving this level of performance is contingent on a precision light source with accurate frequency modulation. This thesis presents a fully integrated solution realizing an electro-optical phase-locked loop (EO-PLL) fabricated on separate complementary metal-oxide-semiconductor (CMOS) and silicon-photonic wafers interconnected with through-silicon vias (TSVs).The system performs 180,000 range measurements per second, with a root-mean square (RMS) depth precision of 8 μm at distances of ±5cm from the range baseline increasing to 4.2 mm RMS error at a range of 1.4 m, limited by the coherence length of the laser used in these experiments. Optical elements including input light couplers, waveguides, and photodiodes are realized on a 3 mm by 3 mm silicon-photonic chip. The 0.18 μm CMOS application-specific integrated circuit (ASIC) of the same area comprises the front-end trans-impedance amplifier, analog electro-optical PLL, and digital control circuitry consuming 1.7 mA from a 1.8-V supply and 14.1 mA from a 5-V supply. The latter includes 12.5 mA bias current for the distributed Bragg reflector (DBR) section of the tunable laser. Also presented in the thesis is a novel dual mode lidar that combines FMCW and chirped-AMCW operation to simultaneously achieve precision depth resolution and a longer operating range not limited by Laser coherence length.",ucb,,https://escholarship.org/uc/item/01b3362w,,,eng,REGULAR,0,0
232,1668,"Theatre, Calvinism, and Civil Society in Eighteenth-Century Edinburgh and Geneva","Leyba, Ashley","Laqueur, Thomas;",2014,"Over the course of eighteen months in 1756 and 1757, theatre crises, large-scale debates about the morality of the stage, erupted in both Edinburgh and Geneva.  Traditionally, these debates have been explained away as examples of Calvinist anti-theatricality.  This dissertation argues, however, that this understanding is inaccurate.  Beyond the fact that there was no consistent tradition of Calvinist anti-theatricality in the early modern period, taking such a narrow view of the theatre crises undermines their importance.  The theatre debates of 1756 and 1757 must be understood in the context of the Enlightenment and changing notions about the relationship between the Calvinist church and civil society.  The theatre symbolized the birth of civil society and the end of a particular brand of Calvinism.  When the eighteenth-century debates about the stage are understood only as examples of ""Calvinist anti-theatricality,"" though, this importance is lost.  This project remedies the current gap in scholarship by demonstrating that these debates were not simply about the theatre; they were about the fate of Calvinism in an increasingly polite, enlightened society.",ucb,,https://escholarship.org/uc/item/01h927nz,,,eng,REGULAR,0,0
233,1669,"Institutional Determinants of Cyber Security Promotion Policies: Lessons from Japan, the U.S., and South Korea","Bartlett, Benjamin Gosnell","Aggarwal, Vinod;",2018,"Ensuring the cyber security of the private sector requires both theproduction of and consumption of cyber security technology.States vary in the degree to which they promote production and consumption.Taking an institutionalist approach, I argue that the differencebetween states can be explained as the result of two policylegacies. States with a policy legacy of maintaining strongtraditional national security capabilities have the instrumentsnecessary to promote production of cyber security technology, as wellas actors---the military and intelligence agencies---who are motivatedto do so. States with a policy legacy of economic guidance have theinstruments to promote the consumption of cyber security technology,and economically-oriented bureaucratic actors who see it as theirresponsibility to do so.To provide evidence for my hypotheses, I do a comparative case studyof Japan, the U.S., and South Korea. Japan, with a policy legacy ofrestrained traditional security capabilities and a legacy of economicguidance, does little to promote production but is active in promotingconsumption. The U.S., with a legacy of maintaining strong traditionalsecurity capabilities but without a legacy of economic guidance, isactive in promoting production but does little to promoteconsumption. South Korea, which has a policy legacy of maintainingstrong traditional security capabilities and a legacy of economicguidance, promotes both.The key implication of this research is that a state's ability topromote cyber security in the private sector is heavily determined notonly by past policies, but past policies that were unrelated to cybersecurity. States without the proper policy legacies will have to findways to build substituting institutions in order to promote bothproduction and consumption of cyber security.",ucb,,https://escholarship.org/uc/item/02f4879m,,,eng,REGULAR,0,0
234,1670,Discrete-Time H2 Guaranteed Cost Control,"Conway, Richard Anthony","Horowitz, Roberto;",2011,"In this dissertation, we first use the techniques of guaranteed cost control to derive an upper bound on the worst-case H2 performance of a discrete-time LTI system with causal unstructured norm-bounded dynamic uncertainty. This upper bound, which we call the H2 guaranteed cost of the system, can be computed either by solving a semi-definite program (SDP) or by using an iteration of discrete algebraic Riccati equation (DARE) solutions. We give empirical evidence that suggests that the DARE approach is superior to the SDP approach in terms of the speed and accuracy with which the H2 guaranteed cost of a system can be determined.We then examine the optimal full information H2 guaranteed cost control problem, which is a generalization of the state feedback control problem in which the H2 guaranteed cost is optimized. First, we show that this problem can either be solved using an SDP or, under three regularity conditions, by using an iteration of DARE solutions. We then give empirical evidence that suggests that the DARE approach is superior to the SDP approach in terms of the speed and accuracy with which we can solve the optimal full information H2 guaranteed cost control problem.The final control problem we consider in this dissertation is the output feedback H2 guaranteed cost control problem. This control problem corresponds to a nonconvex optimization problem and is thus ""difficult"" to solve. We give two heuristics for solving this optimization problem. The first heuristic is based entirely on the solution of SDPs whereas the second heuristic exploits DARE structure to reduce the number of complexity of the SDPs that must be solved. The remaining SDPs that must be solved for the second heuristic correspond to the design of filter gains for a estimator.To show the effectiveness of the output feedback control design heuristics, we apply them to the track-following control of hard disk drives. For this example, we show that the heuristic that exploits DARE structure achieves slightly better accuracy and is more than 90 times faster than the heuristic that is based entirely on SDP solutions.Finally, we mention how the results of this dissertation extend to a number of system types, including linear periodically time-varying systems, systems with structured uncertainty, and finite horizon linear systems.",ucb,,https://escholarship.org/uc/item/02w9h3kz,,,eng,REGULAR,0,0
235,1671,Cognitive Structures Underlying Gendered Language Usage in Germany: Narration and Linguistic Fieldwork,"Kolar, Meredith","Rauch, Irmengard;",2011,"This study intends to expand the historical language and gender debate (Chapter 1) by examining the cognitive structures that underlie human beliefs about gender.  Although the work does not profess to be a feminist work, it does seek to offer an opinion about how and why linguistic and social change can occur within a population. It examines the current state of gendered language usage and the potential for change in gendered language usage within a Western population.  The foundational methods for this study include cognitive linguistic and metaphor theories (Chapter 2) combined with narrative theory (Chapter 3), and the study incorporates Christian theological (Chapter 4) and feminist history (Chapters 1 & 4) as a basis for understanding the cultural conventions about gender in the West.  Narratives are considered to be ""Instruments of Mind"" (3.6).  They consist of systematic structures necessary for all human cognition, principally consisting of metaphorical mappings between source and target domains (2.6).  Narrative structures therefore enable us to reason throughout daily life.  As a crucial part of our reasoning strategies, narratives point to the details in our moral systems (Chapter 4).  A moral system is the coherent foundation of a person's beliefs and choices.  Moral systems are culturally shared, but there may be several versions of moral systems in any given culture (4.1).  Due to the prolific capacity of metaphorical reasoning, spreading activation in neural structures that enables such reasoning (2.4), and the radial characteristics of real human categorization strategies (2.2, 2.3), no human being reasons with complete consistency.  Exceptions abound and point to the blending of moral systems in individuals' reasoning strategies (Chapter 10).  Crucially, exceptions indicate both the potential for change and an innate human creativity (2.11, Chapter 10). We can draw inferences (3.1) about human reasoning structures and individuals' moral systems from the language individuals choose to discuss culturally shared stories.  Constellations of words, collocations, phrases, and metaphors point to the values, or moral systems, of each individual.  Constellations and collocations (3.4) often demonstrate beliefs in cultural folk models (2.3, 4.1.5).  Folk models primarily consist of prototypes and basic-level effects (2.2), and speakers employ these to make speedy and efficient judgments about people, things, and actions in everyday life.  Prototype categories, however, are radial categories (2.2, 2.3), which means that membership in a category is based on relationship to the central member, but that categories have indistinct boundaries and allow for unique or novel inclusion radiating from the central members.  The capacity for novel usage (2.11) is one of the most salient qualities of human cognition, and it is the quality that allows for both linguistic and social change through cognitive transformation.The primary folk models in the West point to two moral systems used by speakers to reason about daily, mundane and complex functions and actions.   Both prototypical moral systems stem from the Christian heritage: the Strict Father system of morality (SFM) and the Nurturant Parent system of morality (NPM) (Chapter 4).  SFM involves hierarchies, strict boundaries, moral strength, and purity, while NPM is based on empathy and dissolves notions of hierarchies.  This study demonstrates through interviews with 26 native speakers of modern German regarding stories of Christian saints (Chapters 5-9) that the leading moral system both historically and currently in this Western population segment is SFM (Chapter 10).  While many speakers demonstrate occasional features of NPM reasoning, female consultants tend to demonstrate more of these features than male consultants (Chapters 7-10).  It appears that women's historical status as a subordinate group under a SFM system may predispose them to the use of empathy (10.1) and therefore to the use of NPM reasoning.  Women tend to be the primary instigators of change in gendered language usage.  Finally, the analysis of the study suggests that language and social change occur over time as a result of the creative potential inherent in empathetic cognition, found more often in subordinate groups, due to their perception of a need for alternatives from the norm (Chapter 10).  Change rarely occurs ""from above"", through those who make up the status quo, but originates out of a need by subordinate groups to break down strict boundaries and rigid divisions.  Change is always possible, as human cognition is based on fuzzy boundaries and radial categories.  Nonetheless, change is a slow process because it requires long-term and often radical alterations in the tenacious narrative and cognitive structures of a shared culture.",ucb,,https://escholarship.org/uc/item/0329d50c,,,eng,REGULAR,0,0
236,1672,Empathic Communication During Mother-Adolescent Conflict Management,"Main, Alexandra","Zhou, Qing;Campos, Joseph J;",2013,"Interpersonal conflict management is a context in which empathy and emotion regulation can be both challenging and of vital necessity. The present study examined the effects of empathic communication on conflict management between mother-adolescent dyads (N = 50). Mother-adolescent dyads engaged in a 10-minute discussion of a topic of frequent conflict in their relationship. Following the discussion, mothers and adolescents independently completed a post-conflict discussion questionnaire to assess their satisfaction with the discussion. Emotional behaviors during the discussion were coded using the Specific Affect Coding System (SPAFF). Empathic communication was coded as (1) validation and (2) interest in the other's perspective and feelings. The present study explored several questions related to (1) adolescent age differences in mother and adolescent empathic communication and conflict management, and (2) relations between empathic communication and conflict management. Notably, older adolescents and their mothers displayed more validation than younger adolescents and their mothers. Furthermore, mother's validation was marginally positively correlated with adolescents' satisfaction with the discussion, and this relation was mediated by the degree to which adolescents perceived that their mother understood their point of view and feelings during the discussion. Findings indicate that empathic communication in response to adolescent negative emotion plays a unique role in effective conflict management between mothers and adolescents. Implications for research on empathy and interventions targeted at facilitating effective conflict management between parents and adolescents are discussed.",ucb,,https://escholarship.org/uc/item/04r9g4cz,,,eng,REGULAR,0,0
237,1673,Magnetic Exchange Coupling and Single-Molecule Magnetism in Uranium Complexes,"Rinehart, Jeffrey Dennis","Long, Jeffrey R;",2010,"This dissertation describes the research that led to the discovery of single-molecule magnetism in the actinides. Chapter One is an introduction to the concepts that lead to single-molecule magnet behavior with an emphasis on the specific qualities of the f-elements that make them interesting for such studies. A simple model for predicting ligand field environments that should be amenable to single-molecule magnet behavior is presented along with several examples of its application to lanthanide and actinide systems. The study of magnetic exchange coupling in uranium-containing multinuclear complexes is discussed and the literature on the subject is reviewed.	Chapter Two describes how the homoleptic dimer complex [U(Me2Pz)4]2 (Me2Pz- = 3,5-dimethylpyrazolate) can be cleaved via insertion of terminal chloride ligands, such that reactions with (cyclam)MCl2 (M = Ni, Cu, Zn; cyclam = 1,4,8,11-tetraazacyclotetradecane) in dichloromethane generate the linear, chloride-bridged clusters (cyclam)M[(μ-Cl)U(Me2Pz)4]2. Variable-temperature magnetic susceptometry is used to reveal the presence of weak ferromagnetic coupling between the Ni(II) (S = 1) and U(IV) centers and no coupling between the Cu(II) (S = 1/2) and U(IV) centers. Consistent with a simple superexchange mechanism for the coupling, density functional theory calculations performed on a [(Me2Pz)4UCl]− fragment of the cluster show the spin resides in 5fxyz and 5fz(x2-y2) orbitals, exhibiting delta symmetry with respect to the U-Cl bond.	Chapter Three extends the analysis of exchange coupling in Chapter Two to include the (cyclam)Co[(μ-Cl)U(Me2Pz)4]2 cluster. As in the Cu(II) case, Co(II) has a single unpaired electron (S = 1/2), however this unpaired electron resides in a dz2 orbital and is therefore oriented directly along the superexchange pathway. This provides a significantly better magnetic exchange pathway leading to the strongest magnetic coupling of the series.	Chapter Four deviates briefly from the pursuit of molecular magnets to study a series of multinuclear clusters formed from the activation of the 3,5-dimethylpyrazolate anion by uranium(III) via two-electron reductive cleavage of the N-N bond to form 4-ketimidopent-2-ene-2-imido (kipi3-) units, as isolated in three related tetranuclear uranium cluster compounds, two of which are mixed valent. The kipi3- ligand represents an exotic latecomer to the acetylacetonato (acac-) ligand family. Unlike the related and widely-utilized β-diketimido (nacnac-) ligands, kipi3- can be represented as containing both imido and ketimido functionalities. Thus, it provides a true nitrogen-based, isoelectronic analogue of acac-, a ligand that has played a long and vital role in coordination chemistry.	Chapter Five turns from the synthesis of exchange coupled clusters to mononuclear species. Drawing on the model of f-element anisotropy presented in Chapter One, the trigonal prismatic complex U(Ph2BPz2)3 was chosen for study. Ac magnetic susceptibility measurements performed on it demonstrate the presence of slow magnetic relaxation under zero applied dc field. Analysis of both the temperature and frequency dependence of the ac susceptibility indicate a temperature regime (T > ~3 K) where Arrhenius behavior dominates the relaxation processes, leading to a spin relaxation barrier of Ueff = 20 cm−1. The dc field dependence of the relaxation time is studied to reveal evidence of quantum tunneling processes occurring at lower temperatures. The results represent the first example of an actinide complex displaying single-molecule magnet behavior and confirm the general strategy for identifying further uranium(III)-based single-molecule magnets by concentrating ligand field contributions above and below the equatorial plane of an axially-symmetric coordination complex.	Chapter Six builds on the results presented in Chapter Five to characterize the related complex the trigonal prismatic complex U(H2BPz2)3. This tricapped trigonal prismatic complex is characterized by single crystal x-ray diffraction and ac magnetic susceptibility measurements. The ac susceptibility data demonstrate the presence of multiple processes responsible for slow magnetic relaxation. Out-of-phase signals observed at ac switching frequencies between 1 and 1500 Hz in dc fields of 500-5000 Oe indicate a thermal relaxation barrier of ca. 8 cm-1 for the molecule, with a temperature-independent process taking over at the lowest temperatures probed. Significantly, an unprecedented, slower relaxation process becomes apparent for ac switching frequencies between 0.06 and 1 Hz, for which a monotonic increase of the relaxation time with applied dc field suggests a direct relaxation pathway.",ucb,,https://escholarship.org/uc/item/06g3k0kp,,,eng,REGULAR,0,0
238,1674,"Ion Nanocalorimetry: Measuring Absolute Reduction Potentials, and Investigating Effects of Water on Electron Solvation and Ion Fluorescence","Donald, William Alexander","Williams, Evan R;",2010,"This dissertation reports on the development of a new gas-phase ion nanocalorimetry technique, in which electrochemistry is performed using large ""aqueous"" nanodrops in vacuo to obtain absolute half-cell potentials in bulk solution. Absolute recombination energies (REs) of nanometer-sized water droplets containing a divalent or trivalent metal ion are obtained from the number of water molecules lost upon electron capture (EC). REs are obtained from the experimentally measured average number of water molecules lost from the cluster, and from both the sum of the threshold water molecule binding energies and the sum of energy that is partitioned into the translational, rotational and vibrational modes of the products for each water molecule lost.  The energy removed by the lost water molecules is obtained from established theoretical models.  The width of the product ion distribution in these experiments is predominantly attributable to the distribution of energy that partitions into the translational and rotational modes of the water molecules that are lost.  These results are consistent with a singular value for the recombination energy.  Ion nanocalorimetry has been used to obtain a value for the absolute standard hydrogen electrode potential from three different nanocalorimetry based methods that all agree within 5% of each other (+4.05, +4.11, and +4.21 V).  Our extrapolation method, in which REs of size-selected and thermalized Eu3+(H2O)n, n = 55 to 140, are extrapolated to infinite size to obtain the absolute reduction potential of Eu3+(aq) and a value for the absolute SHE potential (+4.11 V), should be the most accurate because a solvation model is not used and therefore, errors associated with solvation models are eliminated.          Water clusters containing ions for which one-electron reduction potentials in aqueous solution are not readily measurable, such as alkaline earth divalent metal ions and most of the trivalent lanthanide ions, form solvent separated metal ion and electron ion pairs upon EC, as long as there are a sufficient number of water molecules to stabilize the ion pair.  The dependence of the RE values for Ca(H2O))n2+ on cluster size suggest that the electron is delocalized on the surface of the cluster for n = 32-47, but a transition to a more highly solvated electron is indicated for n = 47-62 by the constant RE values for these ions. For La3+(H2O)n (n = 42 to 160), the trend in recombination energies as a function of hydration extent is consistent with a structural transition from a surface-located excess electron at smaller sizes (n ≤ ~56) to a more fully solvated electron at larger sizes (n ≥ ~60). The recombination enthalpies for n > 60 are extrapolated as a function of the geometrical dependence on cluster size to infinite size to obtain the bulk hydration enthalpy of the electron (-1.3 eV), which is within the wide range of values obtained from previous methods (-1.0 to -1.8 eV).  The ion nanocalorimetry method has the advantage that it does not require estimates for the absolute solvation energy of the proton or the H atom.         Whereas EC by hydrated metal ions resulted in only the full internal conversion of the RE into the reduced precursor, some ions can fluoresce upon electronic excitation.  We report a new highly sensitive method for detecting the fluorescence of isolated, partially hydrated ions for the first time.  Fluorescence is indirectly detected based on the distribution of water molecules lost upon absorption of a UV photon. Photodissociation of hydrated protonated proflavine (n = 13-50) undergoes three photophysical processes upon absorption of a 248 nm photon and excitation to a high energy singlet excited state: full internal conversion and fluorescence to the ground electronic singlet state, and formation of a long-lived triplet state, which slowly undergoes non-radiative intersystem crossing to the ground singlet state.  The high sensitivity of this method should make it possible to perform Förster resonance energy transfer experiments with gas-phase biomolecules in a microsolvated environment to investigate how a controlled number of water molecules effects biomolecular structure and dynamics.         Although the precision in the nanocalorimetry method is excellent, the absolute uncertainty obtained is more difficult to assess because the energy removed by the lost water molecules has not been experimentally measured for large hydrated metal ions. Laser induced photodissociation experiments, in which M2+(H2O)n are dissociated by absorption of UV laser light at 193 (6.4 eV) and 248 nm (5.0 eV), are used to directly relate the average number of water molecules lost to the energy that is deposited into the cluster, which can be used to directly convert the average water molecules lost in EC experiments to experimentally measured RE values.  These results demonstrate that absolute solution phase reduction potentials can be obtained entirely from experimental data, with no modeling, and should provide the most direct route to establishing an absolute electrochemical scale with high accuracy.",ucb,,https://escholarship.org/uc/item/06x143rt,,,eng,REGULAR,0,0
239,1675,Molecular Imaging Approaches to Understanding the Roles of Hydrogen Peroxide Biology in Stress and Development,"Dickinson, Bryan Craig","Chang, Christopher J.;",2010,"The production of hydrogen peroxide (H2O2) in biological systems is associated with a variety of pathologies including neurodegenerative diseases, cancer, and the general process of aging. However, a growing body of evidence suggests that the reactivity of this particular reactive oxygen species (ROS) is also harnessed for physiological processes. Molecular imaging using fluorescence microscopy offers a valuable approach for deciphering the multifaceted roles of H2O2 in biological processes. The use of aryl boronates for the selective detection of H2O2 in biological systems is a validated approach to the development of H2O2-responsive fluorophores. This dissertation describes the design, synthesis, and characterization of an assortment of new boronate-based fluorescent probes for H2O2, as well as their application toward uncovering new roles for H2O2 in stress and development. Peroxyfluor-2, Peroxyfluor-3, Peroxy Yellow 1, and Peroxy Orange 1 are turn-on fluorescent probes that can detect physiological levels of H2O2 produced for cell signaling, as well as monitor multiple ROS simultaneously in single cells. Mitochondria Peroxy Yellow 1 is a bifunctional probe featuring a triphenylphosphonium group for mitochondrial targeting and a single boronate for H2O2 detection that allows for the detection of mitochondrial H2O2 associated with a Parkinson's diseases model. Nuclear Peroxy Emerald 1 is a nuclear-targeted probe that reveals Sirtuin-dependent changes in nuclear H2O2 metabolism in C. Elegans. Peroxyfluor-6 (PF6) is a bifunctional probe featuring acetoxymethylester-protected phenol and carboxylic acid functionalities for enhanced cellular uptake and retention. PF6 reveals endogenous H2O2 production within neural stem cells and molecular biological experiments expose a new role for H2O2 in growth signaling within this critical brain cell population in vitro and in vivo. Finally, Peroxy Yellow 1 Methyl-Ester, a probe designed for analysis by flow cytometry, reveals that Aquaporins 3 and 8, but not Aquaporin 1, can mediate H2O2 uptake across the plasma membrane of mammalian cells, and that Aquaporin 3 can facilitate the uptake of endogenous H2O2 relevant to cell signaling.",ucb,,https://escholarship.org/uc/item/07n8n80h,,,eng,REGULAR,0,0
240,1676,Electrocatalytic Oxidation of Formate with Rh(III) and Co(III) Electrocatalysts,"Kellenberger, Daniel Louis","Arnold, John;",2015,"AbstractElectrocatalytic Oxidation of Formate with Rh(III) and Co(III) ElectrocatalystsByDaniel Louis KellenbergerDoctor of Philosophy in ChemistryUniversity of California, BerkeleyProfessor John Arnold, ChairChapter 1. The hydrogen fuel cell is an environmentally friendly alternative to fossil fuel combustion for the powering of vehicles and other mobile applications. The storage of hydrogen in appreciable densities and the difficulty of its distribution are prohibitive factors for the large scale, societal adoption of current hydrogen fuel cell technologies. A hydrogen fuel cell based on the reversible storage of hydrogen equivalents in liquid, organic substrates would alleviate both of these issues. A hydrogen fuel cell based on the hydrogenated/dehydrogenated pair of formic acid and carbon dioxide is a promising example that would allow for the regeneration of the fuel at an external plant away from the point of release. There exist numerous examples of the dehydrogenation of formic acid to generate dihydrogen. However, the direct electrocatalytic oxidation of formic acid to its constituent protons, electrons, and carbon dioxide byproduct is presented as a more attractive alternative that would allow for the incorporation of a homogeneous electrocatalyst into the fuel cell itself. A generalized mechanism is envisaged that would allow for the direct electrocatalytic oxidation of formic acid by a homogenous, organometallic electrocatalyst.Chapter 2. The Rh(III)-centered complex [Cp*Rh(bpy)(MeCN)][PF6]2 (Cp* = pentamethylcyclopentadienyl, bpy = 2,2’-bipyridyl) was selected as a possible electrocatalyst for the electrocatalytic oxidation of formate. Analogues for each of the intermediates in the electrocatalytic cycle as presented in Chapter 1 were either isolated, modelled, or directly observed. The Rh(I) complexes Cp*Rh(bpy) and Cp*Rh(phen) (phen = 6,10-phenanthroline) were synthesized and the latter was structurally characterized. Reaction of the Rh(III) complex with formate in acetonitrile resulted in decomposition but coordination of formate was modelled with the isolation of the acetate analogue, [Cp*Rh(bpy)(OAc)][PF6]. The complex [Cp*Rh(6,6’-Me2-2,2’-bipyridyl)(MeCN)][PF6]2 featuring a bulkier chelating ligand was synthesized and monitoring the reaction with formate in acetonitrile by 1H NMR revealed the in situ generation of a Rh(III) hydride, [Cp*Rh(6,6’-Me2-2,2’-bipyridyl)(H)]+. Electrocatalytic oxidation of formate in benzonitrile was achieved as determined by constant potential coulometry experiments conducted at the impressive potential of -900 mV vs. Ag/Ag+.Chapter 3. A series of Rh(III) electrocatalysts were synthesized of the type [Cp*Rh(chelate)(MeCN)]2+ (1) featuring the chelates (a) 2,2’-bipyridyl, (b) 6,10-phenanthroline, (c) 4,4’-Me2-2,2’-bipyridyl, and  (d) 6,6’-Me2-2,2’-bipyridyl to determine the influence of the chelate on the complex’s observed reactivity. Computational chemistry was in agreement with the proposal that the observation of two electrochemical reductions for complexes 1a-c resulted from an equilibrium in solution of 1 with a 16-electron complex, [Cp*Rh(chelate)]2+. The solid state structures of 1a-d determined by single crystal, x-ray diffraction experiments and the gas phase structures calculated with computational chemistry suggested the observed irreversible electrochemical reduction of 1d was due to increased steric effects from its bulkier chelating ligand which would favor dissociation upon reduction. Additionally, a system of calculations were developed to address the competing abilities of the Rh(III) hydrides to act as proton sources or hydride donors with the former being desired for the electrocatalytic oxidation of formate observed in Chapter 2. Chapter 4. [Cp*Co(bpy)(MeCN)]2+ was synthesized and characterized as a first-row analogue the Rh(III) electrocatalysts studied in Chapter 2 and Chapter 3. A new, simplified synthesis of [Cp*Co(MeCN)3][PF6]2 was achieved by reaction of Cp*Co(CO)2 with two equivalents of AgPF6 in acetonitrile. Reaction of the tris-acetonitrile complex with either 2,2’-bipyridyl or 6,6’-Me2-2,2’-bipyridyl resulted in the generation of the complex [Cp*Co(chelate)(MeCN)][PF6]2. Unlike with the Rh(III) analogues, reaction of the [Cp*Co(bpy)(MeCN)][PF6]2  species with formate generated an isolable Co-formate adduct, [Cp*Co(chelate)(OC(O)H)][PF6]2. The addition of formate to [Cp*Co(bpy)(MeCN)][PF6]2 to generate the Co-formate adduct in situ resulted in the appreciable anodic shift of the oxidation potential of formate by ca. 750 mV.",ucb,,https://escholarship.org/uc/item/082970n6,,,eng,REGULAR,0,0
241,1677,"Estimation, Identification and Data-Driven Control Design for Hard Disk Drives","Bagherieh, Omid","Horowitz, Roberto;",2017,"The demand for online storage has been increasing significantly during the last few years. Hard disk drives are the primary storage devices used in data centers for storing these online contents. The servo assembly of the dual-stage Hard Disk Drive (HDD) is composed of the Voice Coil Motor (VCM) and the Mili-Actuator (MA), where the VCM is responsible for coarse positioning at low frequency regions and the MA is responsible for fine positioning at high frequency regions. Controlling these two actuators is very critical in precision positioning of the read/write head, which is mounted at the edge of the servo assembly. In this dissertation, the precision positioning of the head during the self-servo writing process as well as feed-forward and feedback controls in the track following mode are considered. This dissertation discusses three control design methodologies for hard disk drives servo systems, in order to improve their performance as well as their reliability. The first is a state estimator for non-uniform sampled systems with irregularities in the measurement sampling time, which estimates the states at a uniform sampling time. The second is an online uncertainty identification algorithm, which parameterizes and identifies the uncertain part of transfer functions in a dual-stage HDD. The third is a frequency based data-driven control design methodology, which considers mixed H_2/H_infinity control objectives and is able to synthesize track following servo systems for dual stage actuators utilizing only the frequency response measurement data, without the need of identifying the models of the actuators.The state estimator design for non-uniform sampled systems with irregularity in the measurement sampling time is considered, where it is proposed to design an observer to estimate the states at a uniform sampling time. This observer is designed using a time-varying Kalman filter as well as a gain-scheduling observer. The Kalman filter has the optimal performance, while the gain-scheduling observer requires relatively lower computational power. Simulations are conducted involving the self-servo writing process in hard disk drives, where performance as well as computational complexity of these two observers are compared under different noise scenarios.Uncertainties in system dynamics can change the closed loop transfer functions and affect the performance or even stability of the control algorithm. These uncertainties are parameterized as stable terms using coprime factorizations, and are identified in an online fashion. The uncertainty identification, in comparison to the complete transfer function identification, requires less computational power as well as a smaller order for the identified transfer function.The proposed online uncertainty identification algorithm is utilized to factorize and identify the uncertain part of transfer functions in a dual-stage Hard Disk Drive (HDD). The dual-stage actuators' gains and resonance modes are affected by temperature variations, which in turn affect all closed loop transfer functions. Therefore, these transfer functions must be periodically updated in order to guarantee the convergence and stability criteria for the adaptive Repeatable Run-Out (RRO) following algorithm proposed in [61, 62]. Experimental results conducted on a hard disk drive equipped with dual-stage actuation, confirm the effectiveness of the proposed identification algorithm.A frequency based data-driven control design considering mixed H_2/H_infinity control objectives is developed for multiple input-single output systems. The main advantage of the data-driven control over the model-based control is its ability to use the frequency response measurements of the controlled plant directly without the need to identify a model for the plant. In the proposed methodology, multiple sets of measurements can be considered in the design process to accommodate variations in the system dynamics. The controller is obtained by translating the mixed H_2/H_infinity control objectives into a convex optimization problem. The H_infinity norm is used to shape closed loop transfer functions and guarantee closed loop stability, while the H_2 norm is used to constrain and/or minimize the variance of signals in the time domain.The proposed data-driven design methodology is used to design a track following controller for a dual-stage HDD. The  sensitivity decoupling structure[34] is considered as the controller structure. Thecompensators inside this controller structure are designed and compared by decoupling the system into two single input-single-output systems as well as solving for a single input-double output controller.",ucb,,https://escholarship.org/uc/item/0998n097,,,eng,REGULAR,0,0
242,1678,Manipulating 1D Conduction Channels; from Molecular Geometry to 2D Topology,"Pedramrazi, Zahra","Crommie, Michael F.;",2019,"This dissertation is divided into two segments, both of which focus on creating and manipulating one-dimensional (1D) conduction channels in novel 1D and two-dimensional (2D) systems, characterized by scanning tunneling microscopy (STM).The first half describes how the electronic properties of quasi-1D graphene nanoribbons (GNRs) are manipulated by controlling their width and edge geometry at the atomic scale. A bottom-up approach is used for fabricating three different armchair GNR (AGNR) systems, allowing the geometry and hence the electronic properties of resultant AGNRs to be controlled. Successful molecular bandgap engineering in 1D AGNR heterojunctions is described, as well as the electronic and topographic characterization of the concentration dependence of boron-doped AGNRs. The discovery of two new in-gap dopant states with different symmetry is described. The successful fabrication and characterization of S-AGNRs having sulfur atoms substitutionally doped at the AGNR edges is also described. Our results indicate that S-doping induces a rigid shift of the energies for both the valence and conduction bands.The second half of this thesis describes how the 1T’ phase of monolayer transition metal dichalcogenides (TMDs) can be used as a platform to create 2D topological insulators (TIs). These novel TI systems are characterized in great detail. The successful growth and characterization of single-layer 1T’–WTe2 is described. This material is shown to host a bulk bandgap and helical edge states at the 1T’–vacuum interface. The growth and characterization of mixed phase-WSe2 is described. New techniques for creation and manipulation of edge conduction channels at interfaces between materials of different topologies are described.",ucb,,https://escholarship.org/uc/item/09z5w3jz,,,eng,REGULAR,0,0
243,1679,"Essays in Innovation, Past and Present","Gross, Daniel Pincus","Handel, Benjamin R.;",2015,"This dissertation studies the economics of historical and modern innovation. The first chapter makes inroads into understanding how competition and incentives shape the creative process which lies at the heart of all technological progress. The creative act is a classic example of a black box in academic research: we can see the inputs and outputs, but we know little about what happens in between. This paper uses new tools for measuring the content of digital media to see how commercial graphic designers’ work evolves in winner-take-all competition. In this chapter, I show that competition both creates and destroys incentives for innovation: some competition is necessary to motivate high-performers to experiment with novel, untested ideas over tweaking tried-and-true approaches, but heavy competition will drive them out of the market.In the second chapter, I study the effects of performance feedback on innovation in competitive settings. Feedback typically serves two functions: it informs agents of their relative performance, and it also helps them improve the quality of their product. The presence of these effects suggests a tradeoff between participation and improvement, as the revelation of asymmetries discourages effort. Using data from the same setting as chapter one, I first show that this tradeoff is real. I then develop a structural model of the setting -- the first of its kind in the literature -- and use the results to evaluate counterfactual feedback policies. The results suggest that feedback is on net a desirable mechanism for a principal seeking high-quality innovation.In the third chapter, I use the farm tractor as a case study to demonstrate that technologies diffuse along two distinct margins: scale and scope. Although tractors are now used in nearly every field operation and with nearly all crops, early models were far more limited in their capabilities, and only in the late 1920s did the technology begin to generalize for broader use with row crops such as corn. Diffusion prior to 1930 was accordingly heavily concentrated in the Wheat Belt, while growth in diffusion from 1930-1940 was concentrated in the Corn Belt. Other historically important innovations in agriculture and manufacturing share similar histories of expanding scope. The key to understanding the pace and path of technology diffusion is thus not only in explaining the number of different users, but also in explaining the number of different uses.A common theme across all three chapters is the focus on developing tools or strategies to study innovation that are less dependent on patent data than the extant literature, since the majority of innovation is not patented (and often not patentable), and doing so while advancing the empirical literature on innovation in new directions.",ucb,,https://escholarship.org/uc/item/0c65m44m,,,eng,REGULAR,0,0
244,1680,The Life and Death of a Tectonic Plate: Imaging the Juan de Fuca Plate with Amphibious Seismic Data,"Hawley, William Bythewood","Allen, Richard M;",2019,"Understanding of Earth’s evolution has been hindered, in part, by the technical challenges associated with placing seismic instruments on the seafloor. As technology improves, more arrays of ocean bottom seismometers are being deployed around the ocean basins. Perhaps the most ambitious such array, the Cascadia Initiative, covered the entirety of a small oceanic plate, the Juan de Fuca plate, with ocean bottom seismometers in a four-year experiment. That array is the primary motivation for this study.The two models presented in this work, CASC16-P and CASC19-S, are teleseismic tomographic studies. They are each the first of their kind to use offshore and onshore data to simultaneously image the P- and S-wave seismic velocity structure of the mantle beneath the Juan de Fuca and North American plates. These models have provided insight into the tectonics of oceanic plates: how they are created, how they might interact with the mantle over the course of their life, how they might be influenced by an overriding continental plate, how they behave in the mantle after subduction, and how subduction could cease.This work focuses on the on- and offshore regions of the Pacific Northwest of the United States—in particular, on the interaction between the Juan de Fuca plate, the North American plate, and the upper mantle. But it also demonstrates the value in oceanic arrays. The offshore instruments used in this study have allowed the identification of new features offshore, and better resolution of features near the coastlines. Ocean bottom seismology represents a rapidly growing new frontier that has the potential to investigate earth evolution in detail that was not previously possible. The work we present here has both helped illuminate the tectonics of the Pacific Northwest, and helped generate detailed hypotheses about the nature of global tectonics that will be tested elsewhere.",ucb,,https://escholarship.org/uc/item/0cg454nd,,,eng,REGULAR,0,0
245,1681,The construction of the Colombian territory: Images of the Colombian Armed Conflict 2002- 2010,"Salamanca, Claudia Liliana","Esmeir, Samera;Bates, David;",2015,"My dissertation, The construction of the Colombian territory: Images of the Colombian Armed Conflict 2002-2010, critically analyzes the Colombian internal conflict and its relationship with the narrative of the nation in a global theater of operations of war in its different forms of visualization.  Through a detail textual analysis of documentaries, films, recorded military operations, media military operations, and proof of life videos, I examine the effect moving images have had as part of configuring the battlefield. My research proceeds from the premise  that there is a visual culture of warfare - grammars, imaginaries, and technologies – that has organized the global space of security and has brought new forms of territory that paradoxically exceed the idea of the sovereign nation and at the same time confirm it. In other words, I examine the creation and exercise of national security in a globalized world and its effects on the concept of the nation and national territory through the concept of politics of the visual. Colombia is considered a successful example for nation building, counterinsurgency tactics and U.S. intervention. As my case of study, I analyze the epistemological assumptions that emerge in the global war on terror, specifically how they reflect in the construction of the idea of Colombia as a unified image, one sovereign nation, and one territory through the production of different visual narratives.",ucb,,https://escholarship.org/uc/item/0cm960g2,,,eng,REGULAR,0,0
246,1682,Combining Structure and Usage Patterns in Morpheme Production: Probabilistic Effects of Sentence Context and Inflectional Paradigms,"Cohen, Clara",,2014,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0cs612p7,,,eng,REGULAR,0,0
247,1683,Adaptive Optimization Methods in System-Level Bridge Management,"Liu, Haotian","Madanat, Samer M;",2013,"In 2012, over 25% of the bridges in the United States were rated as structurally deficient or functionally obsolete. Moreover, 35% of bridges are serving beyond their theoretical design lifespan and the number has been projected to increase over the next decade. The imperative needs of improving the overall condition of the bridge system has been impeded by the shortage of funding available for bridge repairs and maintenance. In 2006 the gap between Federal Highway Administration's (FHWA) estimates to eliminate the bridge maintenance backlog and the actual appropriations to bridges for repairs and maintenance from the Highway Bridge Program was $43.4 Billion. In 2009, the gap increased to $65.7 Billion. Such conflict has made effective bridge management more critical than ever.	In bridge management, agencies collect bridge condition data and develop deterioration models that predict the bridges' future conditions and associated costs, based on which maintenance, rehabilitation and reconstruction (MR&R) decisions are made. It is therefore critical to have accurate deterioration models. However, limited availability of data and incomplete understanding of the deterioration process result in inaccurate models, which lead to sub-optimal MR&R decisions and significant cost increases. 	To address the inaccuracy stemming from limited bridge condition data, researchers have proposed Adaptive Control (AC) methods that update the deterioration models successively as new data become available. The underlying belief is that agencies can obtain more accurate deterioration models through updating and subsequently improve their MR&R decisions and achieve cost savings. State-of-the-art bridge management systems, such as Pontis, use a class of AC procedures known as Certainty Equivalent Control (CEC). The procedure used in Pontis updates the transition probabilities (i.e., the parameters of the component deterioration models) after each condition survey, and uses the updated probabilities in subsequent planning of MR&R decisions. Unfortunately, CEC does not necessarily lead to more accurate models, or guarantee savings in system costs; in other words, updating of the type in Pontis is not necessarily beneficial. 	In the present dissertation, an AC method, Open-Loop Feedback Control (OLFC), is proposed for system-level bridge management. The performance of OLFC and the Pontis CEC is tested in a numerical study and empirical results show that OLFC has superior performance with respect to two criteria. In terms of 	improvement in model accuracy, the Pontis CEC yields systematic bias in model parameter estimates and therefore does not improve model accuracy. In all testing scenarios, the resulting deterioration models lead to faster deterioration than the true models. OLFC, on the other hand, results in consistent convergence to the true models in all testing scenarios and improves model accuracy. When evaluated by system costs, the Pontis CEC consistently results in higher system costs than the no-updating scenario. The increases are on the order of $180 Million at the level of the State of California. To the contrary, updating with OLFC consistently achieves system costs savings compared to the no-updating scenario, and results in system costs that do not differ significantly from the system costs when true models are used for MR&R decision-making.	In addition, a computationally tractable optimization routine is developed for MR&R decision-making. The routine ensures strict conformity to system budget constraints and achieves satisfactory computational efficiency even given high levels of heterogeneity in bridge systems.",ucb,,https://escholarship.org/uc/item/0dm7g2d2,,,eng,REGULAR,0,0
248,1684,Machine Learning: Why Do Simple Algorithms Work So Well?,"Jin, Chi","Jordan, Michael I;",2019,"While state-of-the-art machine learning models are deep, large-scale, sequential and highly nonconvex, the backbone of modern learning algorithms are simple algorithms such as stochastic gradient descent, gradient descent with momentum or Q-learning (in the case of reinforcement learning tasks). A basic question endures---why do simple algorithms work so well even in these challenging settings?To answer above question, this thesis focuses on four concrete and fundamental questions:- In nonconvex optimization, can (stochastic) gradient descent or its variants escape saddle points efficiently?- Is gradient descent with momentum provably faster than gradient descent in the general nonconvex setting?- In nonconvex-nonconcave minmax optimization, what is a proper definition of local optima and is gradient descent ascent game-theoretically meaningful?- In reinforcement learning, is Q-learning sample efficient?This thesis provides the first line of provably positive answers to all above questions. In particular, this thesis will show that although the standard versions of these classical algorithms do not enjoy good theoretical properties in the worst case, simple modifications are sufficient to grant them desirable behaviors, which explain the underlying mechanisms behind their favorable performance in practice.",ucb,,https://escholarship.org/uc/item/1h25x268,,,eng,REGULAR,0,0
