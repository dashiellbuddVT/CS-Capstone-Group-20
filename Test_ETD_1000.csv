number,id,title,author,advisor,year,abstract,university,degree,URI,department,discipline,language,schooltype,oadsclassifier,borndigital
0,1436,Plasma Diagnostics and Plasma-Surface Interactions in Inductively Coupled Plasmas,"Titus, Monica Joy","Graves, David B;",2010,"The semiconductor industry's continued trend of manufacturing device features on the nanometer scale requires increased plasma processing control and improved understanding of plasma characteristics and plasma-surface interactions. This dissertation presents a series of experimental results for focus studies conducted in an inductively coupled plasma (ICP) system. First novel ""on-wafer"" diagnostic tools are characterized and related to plasma characteristics. Second, plasma-polymer interactions are characterized as a function of plasma species and processing parameters. Complimentary simulations accompany each focus study to supplement experimental findings.Wafer heating mechanisms in inductively coupled molecular gas plasmas are explored with PlasmaTempTM, a novel ""on-wafer"" diagnostic tool. Experimental wafer measurements are obtained with the PlasmaTempTM wafer processed in argon (Ar) and argon-oxygen (Ar/O2) mixed plasmas. Wafer heating mechanisms were determined by combining the experimental measurements with a 3-dimensional heat transfer model of the wafer.  Comparisons between pure Ar and Ar/O2 plasmas demonstrate that two additional wafer heating mechanisms can be important in molecular gas plasmas compared to atomic gas discharges.  Thermal heat conduction from the neutral gas and O-atom recombination on wafer surface can contribute as much as 60 % to wafer heating under conditions of low-energy ion bombardment in molecular plasmas.Measurements of a second novel ""on-wafer"" diagnostic sensor, the PlasmaVoltTM, were tested and validated in the ICP system for Ar plasmas varying in power and pressure. Sensor measurements were interpreted with a numerical sheath simulation and comparison to scaling laws derived from the inhomogeneous sheath model. The study demonstrates sensor measurements are proportional to the RF-current through the sheath and the scaling is a function of sheath impedance. PlasmaVoltTM sensor measurements are proportional to the square root of the plasma density at the plasma-sheath interface, one-fourth root of the electron temperature, and one-fourth root of the RF bias voltage under conditions where the sheath is predominantly capacitive. When the sheath impedance becomes increasingly resistive, the sensor measurements deviate from the scaling law and tend to be directly proportional to the plasma density. Vacuum ultraviolet (VUV) emissions in Ar ICPs are characterized and the chemical and physical modifications to 193 nm photoresist (PR) polymer materials processed in Ar ICPs are investigated. Fourier transform infrared (FTIR) transmission measurements as a function of VUV photon fluence demonstrate that VUV-induced bond breaking occurs over a period of time.  A numerical model demonstrates that VUV photons deplete near-surface O-containing bonds, leading to deeper, subsequent penetration and more bond losses, while the remaining near-surface C-C bonds absorb the incident radiation and slow VUV photon penetration. The roughening mechanism of blanket and patterned 193 nm PR samples are explored in a well characterized Ar ICP.  FTIR and atomic force microscopy (AFM) analysis of plasma processed 193 nm PR suggests that ion-induced generation of a graphitized layer at high energies, combined with VUV bulk modification of 193 nm PR may initiate PR roughening. The roughness of blanket samples increases as a function of VUV fluence, ion energy, and substrate temperature.  Line width roughness (LWR) measurements of patterned samples demonstrate a similar trend suggesting that LWR may correlate with surface roughness of patterns. The results are compared to PR studies previously conducted in an ultra-high vacuum beam system demonstrating that the vacuum beam system is a useful tool that can deconvolute and simplify complex plasma systems.",ucb,,https://escholarship.org/uc/item/0hn5z4f1,,,eng,REGULAR,0,0
1,1437,Declarative Systems,"Condie, Tyson","Hellerstein, Joseph M;",2011,"Building system software is a notoriously complex and arduous endeavor.Developing tools and methodologies for practical system software engineeringhas long been an active area of research.  This thesis explores system softwaredevelopment through the lens of a declarative, data-centric programminglanguage that can succinctly express high-level system specifications and bedirectly compiled to executable code.  By unifying specification andimplementation, our approach avoids the common problem of implementationsdiverging from specifications over time.  In addition, we show that using adeclarative language often results in drastic reductions in code size (100Ã— andmore) relative to procedural languages like Java and C++.  We demonstrate theseadvantages by implementing a host of functionalities at various levels of thesystem hierarchy, including network protocols, query optimizers, and schedulingpolicies.  In addition to providing a compact and optimized implementation, wedemonstrate that our declarative implementations often map very naturally totraditional specifications: in many cases they are line-by-line translations ofpublished pseudcode.We started this work with the hypothesis that declarative languages --originally developed for the purposes of data management and querying -- couldbe fruitfully adapted to the specification and implementation of core systeminfrastructure.  A similar argument had been made for networking protocols afew years earlier [61].  However, our goals were quite different: we wanted toexplore a broader range of algorithms and functionalities (dynamic programming,scheduling, program rewriting, and system auditing) that were part of complex,real-world software systems.  We identified two existing system components --query optimizers in a DBMS and task schedulers in a cloud computing system --that we felt would be better specified via a declarative language.  Given ourinterest in delivering real-world software, a key challenge was identifying theright system boundary that would permit meaningful declarative implementationsto coexist within existing imperative system architectures.  We found thatrelations were a natural boundary for maintaining the ongoing system state onwhich the imperative and declarative code was based, and provided an elegantway to model system architectures.This thesis explores the boundaries of declarative systems via two projects.We begin with Evita Raced; an extensible compiler for the Overlog language usedin our declarative networking system, P2.  Evita Raced is a metacompiler -- anOverlog compiler written in Overlog -- that integrates seamlessly with the P2dataflow architecture.  We first describe the minimalist design of Evita Raced,including its extensibility interfaces and its reuse of the P2 data model andruntime engine.  We then demonstrate that a declarative language like Overlogis well-suited to expressing traditional and novel query optimizations as wellas other program manipulations, in a compact and natural fashion.  FollowingEvita Raced, we describe the initial work in BOOM Analytics, which began as alarge-scale experiment at building ""cloud"" software in a declarative language.Specifically, we used the Overlog language to implement a ""Big Data"" analyticsstack that is API-compatible with the Hadoop MapReduce architecture andprovides comparable performance.  We extended our declarative version of Hadoopwith complex distributed features that remain absent in the stock Hadoop Javaimplementation, including alternative scheduling policies, online aggregation,continuous queries, and unique monitoring and debugging facilities.  We presentquantitative and anecdotal results from our experience, providing concreteevidence that both data-centric design and declarative languages cansubstantially simplify systems programming.",ucb,,https://escholarship.org/uc/item/0sn1r9st,,,eng,REGULAR,0,0
2,1438,Portrait of the Rugged Individualist: The Nonverbal Pride Display Communicates Support for Meritocracy,"Horberg, Elizabeth Jane","Keltner, Dacher;",2010,"Emotions profoundly influence beliefs about morality and justice (Haidt, 2001) and emerging research suggests that expressions of emotion communicate an individual's moral attributes to others (e.g., Brown, Palameta, & Moore, 2002). The present research examines the moral beliefs signaled by the nonverbal pride display. Pride is triggered by appraisals that the self merits high status and greater access to resources (Tracy & Robins, 2004) and pride's nonverbal expression has been shown to convey these appraisals to observers (Shariff & Tracy, 2009). Guided by appraisal-tendency frameworks of the association between distinct emotions and moral beliefs (Horberg, Oveis, & Keltner, 2010), I predicted that the nonverbal expression of pride would communicate greater support for meritocracy--the belief that social and material resources ought to be distributed according to merit--relative to egalitarianism, or beliefs that resources ought to be distributed in ways that promote equality of outcomes. Study 1 demonstrated these effects using unfamiliar male and female targets posing pride or joy in photographs. Study 2 found that individuals previously shown a photo of Barack Obama expressing pride, relative to a neutral expression, subsequently rated Obama as more likely to endorse meritocracy. Finally, Study 3 tests the validity of pride-based inferences of support for meritocracy. This study demonstrated that individuals who spontaneously expressed pride to a greater degree were more likely to advocate dividing a resource between the self and another on the basis of merit rather than equally. Moreover, consistent with Studies 1 and 2, observers rated the high-pride expressers as more likely to support meritocracy and less likely to support egalitarianism.",ucb,,https://escholarship.org/uc/item/0v37d9g2,,,eng,REGULAR,0,0
3,1439,Essays in Empirical Macroeconomics,"Nelson Mondragon, John Alexander","Gorodnichenko, Yuriy;",2015,"This dissertation provides evidence on the eï¬€ects of changes in the supply of credit to households during the 2000s on employment and other outcomes of interest during the 2000s. In the ï¬rst chapter I study the eï¬€ects of contractions in household credit supply during the ï¬nancial crisis of 2007-2009. I exploit a countyâ€™s exposure to the collapse of a large and previously healthy lender as a natural experiment. I show that exposure to this shock appears to be uncorrelated with other important shocks at the time. Reduced form estimates suggest that this shock had large eï¬€ects on the ï¬‚ow of credit, housing and non-housing expenditures, and employment. Using exposure to this shock as an instrument gives an estimated elasticity of employment with respect to household credit of about 0.3, caused by declines in both housing and non-housing demand. In the second chapter I study the size of the credit supply shock using non-parametric methods. I identify lender-speciï¬c supply-side shocks, which I then aggregate into a simple  measure of credit supply shocks to counties. I provide conditions under which this measured shock can be used to quantify the importance of supply shocks to credit in both the cross-section and, in a partial equilibrium sense, the aggregate. Combining this measure with various estimates of the elasticity of employment with respect to the measure, I calculate that shocks to household credit can be responsible for 30 to 60% of the decline in employment from 2007 to 2010.",ucb,,https://escholarship.org/uc/item/0wh0h5bj,,,eng,REGULAR,0,0
4,1440,Control and Trajectory Generation of a Wearable Mobility Exoskeleton for Spinal Cord Injury Patients,"Swift, Timothy Alan","Kazerooni, Homayoon;",2011,"There are currently nearly 1.3 million people in the United States who have some form of lower extremity paralysis due to a spinal cord injury (SCI).  For many of these subjects, a wheelchair is their primary means of mobility which brings along with it a collection of health complications stemming from extended periods of time in a seated position.  To provide these SCI patients with a viable upright mobility option, this work presents the control structure and trajectory generation method for a mobility exoskeleton that allows them to ambulate while reliably generating a gait similar to that of an unimpaired subject.  Also included are theoretical extensions to the trajectory generation structure and results from initial rounds of subject testing.",ucb,,https://escholarship.org/uc/item/0xc9q3b6,,,eng,REGULAR,0,0
5,1441,Errors as a Productive Context for Classroom Discussions: A Longitudinal Analysis of Norms in a Classroom Community,"Leveille Buchanan, Nicole Therese","Saxe, Geoffrey;",2016,"How do teachers and students create classroom environments in which mathematical errors are regarded as important opportunities for learning?  What norms support students in learning from their errors, and how to these norms develop in a classroom community?  This dissertation addresses these questions through a longitudinal case study investigating the emergence of classroom norms related to the treatment of errors.  Classroom norms are here understood to be taken-as-shared expectations for behavior in a classroom. The fifth-grade case study classroom was selected because in prior research studies the experienced and highly-regarded teacher had engaged students in rich discussions of their mathematical errors and the opportunities presented by these errors for learning.  To answer the questions of (a) what norms related to mathematical errors were taken up, and (b) how these norms developed over the course of the school year, Saxeâ€™s (2012) framework describing the relation of micro-, onto-, and sociogenetic processes was used as a guide for determining methods.  Sociogenetic processes were the focus of this investigation, and Saxeâ€™s framework points to microgenetic constructions, when studied collectively over time, as likely to illuminate otherwise difficult to observe sociogenetic processes, such as norm development.  To provide information about microgenetic constructions related to errors, several types of evidence were collected throughout the school year during three data collection periods lasting two weeks each: at the beginning (September), middle (January), and end (April) of the 2014 to 2015 school year.  During all three time periods, the teacher was interviewed, five â€œfocalâ€ students were interviewed, classroom mathematics lessons were video-recorded daily, and all students in the classroom answered a paper-and-pencil multiple choice survey about their expectations related to errors. Interviews were analyzed using grounded analysis methods, and video-recordings were analyzed using a focused coding procedure and StudioCode software.  Sources of evidence were used in a triangulating fashion to identify norms in the classroom.Through this analysis, seven norms were identified as having been taken up by the majority of members of the class by the end-of-year data collection period. Two of these norms were selected for in-depth description. The norm everyone has some mathematical understandings to which you should pay attention provides a good example of a norm that was closely tied to a specific collective practice, the â€œcoachingâ€ practice that was used frequently in the case-study classroom.  The norm there are different types of errors, only some of which are acceptable provides an example of a norm that emerged part-way through the school year in response to a problem with the way errors were being treated.  Classroom interactions and teacher and student interview statements exemplifying these norms are described. The process through which these two norms emerged in the case study classroom over the course of the school year is detailed, using evidence collected throughout the school year.  In general, the teacher strongly promoted these norms by frequently and persistently modeling, describing, and praising behaviors consistent with these norms and by correcting inconsistent behaviors. Implications for how Saxeâ€™s framework may be productively applied in future investigations of classrooms norms are discussed.  In particular, attention to ontogenetic processes â€“ that is, individualsâ€™ shifting expectations over time â€“ was found to be useful as an access point for identifying the norms of a classroom community, and the teacherâ€™s actions and expectations were found to be especially important indicators of classroom norms.  Examination of collective practices related to errors was also useful for the identification of norms because some norms were strongly associated with collective practices, such as the â€œcoachingâ€ practice.  The results of this study also have implications for teaching practice.  The findings indicate that children are capable of taking up challenging practices related to the study of errors, and teachers who promote these practices in their classrooms may be successful if they are persistent in modeling, explaining, and praising the desired practices.",ucb,,https://escholarship.org/uc/item/0zz775v7,,,eng,REGULAR,0,0
6,1442,Interrogating the Role of Spatial Organization in Receptor Function: Eph-Ephrin Signaling in Breast Cancer,"Nair, Pradeep","Groves, Jay T.;",2010,"Cells in living tissue integrate multiple signals from their environment to govern numerous aspects of both healthy and diseased behavior.  The cell membrane serves as an exquisite functional filter that regulates information transmission between a cell and its surrounding environment.  This viscoelastic plasma membrane, which allows lateral diffusion while restricting the orientation of signaling molecules within the plane of a phospholipid bilayer, is uniquely well suited to make sense of the myriad biochemical, mechanical, and spatial cues that constantly stimulate receptors on the cell surface.  The chemical basis for the cell membrane, a fluid phospholipid bilayer, can be used to create a supported membrane that retains these properties while allowing precise control over the physical and chemical aspects of signaling molecules on the supported membrane surface.Cell communication is critical for proper maintenance of multicellular organisms, and tumorigenesis can occur when communication is not properly controlled.  Cancerous cells often display a vastly altered array of cell surface receptors compared to normal cells, and the abnormal signaling that these receptors trigger has grave consequences for the fate of the cell and the organism as a whole.  The dynamics by which these receptors bind to ligands within the environment are not well understood because the cell membrane is a chemically heterogeneous and physically irregular surface that is difficult to study in vivo.  Here we recapitulate signaling events that occur in live cancer cells using the supported membrane to present laterally mobile ligands to receptor-expressing human breast cancer cells.This platform allows for precise control of the spatial organization of signaling molecules on the supported membrane surface, as well as a detailed examination of subsequent changes in signaling events within living cells.  Using this approach we observe receptor reorganization responses that are strongly linked to tissue invasion and our observations reveal a mechanism by which cells respond to the spatial and mechanical aspects of their environment.",ucb,,https://escholarship.org/uc/item/1117n3zf,,,eng,REGULAR,0,0
7,1443,Hydrodynamic Exchange in Estuarine Perimeter Habitats,"HSU, KEVIN KAI-WIN","Stacey, Mark T.;",2013,"Hydrodynamic exchange in estuaries is forced by tides, freshwater input, density forcing, and winds, and controls transport of important quantities such as salinity, sediment, nutrients, and pollutants.  Previous work has characterized many aspects of estuarine transport and contributed to our understanding of transport mechanisms such as gravitational exchange, tidal dispersion processes, and residual flows due to tidal asymmetries.  In general, studies of estuarine transport have focused on large-scale transport processes in the along-channel direction of the estuary, which determine the overall salinity and flow structure in estuarine environments.  However, study of hydrodynamic exchange at the perimeter of estuaries has also been recognized to be important, as exchange at the perimeter is relevant for understanding questions related to environmental restoration and management and ecological habitat quality.In this work, hydrodynamic exchange in estuaries and perimeter habitats is studied using numerical modeling and field observations of South San Francisco Bay.  First, the exchange between the estuary and a small perimeter slough is measured using salinity and temperature as tracers to calculate hydrodynamic flushing of the slough through tidal exchange, using a modified tidal prism method.  This method applies quasi-Lagrangian analysis to Eulerian measurements of exchange, and the results are compared to previous results from larger-scale estuarine systems, where tidal flushing is found to be significantly affected by the scale of mixing volumes in the system.  Next, Lagrangian methods of particle-tracking and Lagrangian coherent structure (LCS) analysis, developed from dynamical systems theory in order to analyze complex, chaotic flows, are applied to analyze tidal transport.  The results reveal the significant effects of tidal interactions with perimeter estuarine features on Lagrangian tidal transport over the tidal cycle, where perimeter interactions are found to significantly contribute to longitudinal estuarine dispersion.  Finally, the effect of wind forcing on estuarine transport is examined, using Lagrangian analysis methods applied to cases of constant wind forcing with varying wind direction relative to the main axis of the estuary.  Wind forcing is found to have a significant effect on hydrodynamic exchange and connectivity between the estuary and perimeter habitats, where wind in all directions increases perimeter exchange and connectivity, with the greatest effect for winds aligned with the along-axis direction of the estuary.  The results of these studies are relevant to a wide range of applications requiring analysis of connectivity near the estuarine perimeter, including sediment exchange and transport and seagrass population colonization in the context of wetland habitat restoration.",ucb,,https://escholarship.org/uc/item/11r7t98v,,,eng,REGULAR,0,0
8,1444,An Environmental and Economic Trade-off Analysis of Manufacturing Process Chains to Inform Decision Making for Sustainability,"Robinson, Stefanie","Dornfeld, David A;",2013,"Increasing costs, consumer awareness, and environmental legislation have driven industry to reduce its resource consumption and the impact from that consumption.  So, both traditional economic objectives (e.g., cost, time, and quality) and environmental objectives (e.g., CO2 emissions) have become strategically relevant for the manufacturing sector. For many manufacturing companies, production systems have a major influence on the environmental footprint of a product and therefore represent a major opportunity to minimize the company's overall environmental impact. Currently within industry, there is not an accurate, effective, or widely accepted method to assess the resource consumption of process chains used to manufacture a product.  As a result, this information is often not considered when making decisions about what processes to use. A considerable part of the energy and resource demand in manufacturing is determined during production planning.  An important component of this planning is determining the process chains to be used.  Process chains are a combined sequence of specifically arranged, single processes used to manufacture a product. As manufacturing processes are very resource intensive, it is now necessary to assess the resource consumption as well as the economics of these process chains.  Because of this, additional information must be considered when selecting the process chains used to manufacture a product.     Many life cycle assessment (LCA) tools focus on the materials and final disposition of a product, but do not include detailed information or data on the manufacturing required to fabricate the product. Sustainability impacts of discrete manufacturing processes and product value streams are needed to develop more complete LCAs. The development of a methodology and user tool to quantify sustainability impacts, leading to the identification of gaps and opportunities, is essential to facilitate decision making to support sustainability in manufacturing facilities.     To address these issues, this dissertation proposes an approach to evaluate and quantify the resource use in addition to the environmental and economic impacts associated with discrete manufacturing processes as part of a complex process chain. A methodology to evaluate multiple process chain configurations will be presented.       First, a database of industrial assessment metrics was compiled.  This database allows users to sort and select from a list of key metrics in order to choose the metrics that are relevant for the performance that they want to measure.  Next, an industrial assessment methodology was developed.  This methodology gives users an overview of the key areas to address when conducting an industrial assessment.  This methodology, which was applied to three case studies, can be used in combination with the key metrics.      In the second part of the dissertation, a resource consumption assessment and mapping methodology for complex manufacturing process chains was developed. This systematic methodology was developed to identify and quantify the resource consumption (energy, water, materials) for discrete manufacturing processes.  The processes mapped include: welding (manual and robotic), cutting (plasma arc and laser), rework (air carbon arc cutting and hand grinding), and machining (milling).      Next, a model consisting of database modules for each process was developed. This model quantifies the sustainability impacts (energy, water, and material consumption, waste generation, emissions, and resource consumption cost) of manufacturing process chains.  The model was validated using a case study with Caterpillar Inc. for a process chain including welding, plasma arc cutting, laser cutting, and milling.       Next, a process chain assessment tool was created.  This tool enables manufacturers to assess the resource consumption and associated impacts of multiple fabrication process chain configurations. This enables a more comprehensive assessment compared to other software tools.  Finally, a methodology modeled after the Six Sigma DMAIC (Define, Measure, Analyze, Improve, Control) process was presented to show how to translate the results from the model and tool to an Environmental Value Stream Map and to translate those results into improvements in manufacturing systems. This methodology was validated on a machining operation in a Caterpillar facility.         This research has developed and evaluated an effective approach for the analysis of energy, water, and other resource use in multiple processes in a manufacturing process chain.  This allows manufacturers to better understand the resource consumption and environmental and economic impacts of fabrication process chains used to make a product. This dissertation helps to provide the technical understanding and tools to enable designers and manufacturing engineers to create manufacturing systems that are truly more sustainable.  The implementation of this work can be directly applied to assessing and optimizing manufacturing process chains and the work presented in this dissertation directly contributes to the realization of a sustainable and prosperous manufacturing sector.",ucb,,https://escholarship.org/uc/item/1513r3t9,,,eng,REGULAR,0,0
9,1445,Essays on Markets and Institutions in Emerging Economies,"GHANI, TAREK FOUAD","Tadelis, Steven;Dal Bo, Ernesto;",2015,"Market frictions pervade emerging economies and constrain private sector development. In such settings, formal institutions to help address contract enforcement, property rights and information asymmetries are typically weak or absent. Instead, market participants must rely on informal practices and institutions to mitigate uncertainty, instability and opportunism. For example, personalized exchange relationships are useful when contract enforcement is weak, and cash holdings can be attractive when financial institutions are unreliable. In three specific emerging economy settings, I explore how informal practices and institutions interact with formal market development, and in particular the role that market frictions play in determining outcomes for firms, technologies and employees. The first chapter of this dissertation explores how changes in formal upstream market structure affect the economics of downstream relationships using original data from the ice industry in Sierra Leone. In this setting, a monopoly ice manufacturer sells through independent retailers to fishermen buyers. I demonstrate that a shock that increases upstream competition among manufacturers improves the contractual terms offered by retailers to buyers.  Under the monopoly manufacturer, late deliveries are common due to outside demand shocks. To help mitigate this uncertainty, retailers prioritize loyal customers when faced with shortages, and buyers respond by rarely switching retailers. When manufacturers compete, prices fall, quantities increase and services improve with fewer late deliveries. Entry upstream also disrupts collusion among retailers by increasing the value of competing for buyer relationships. Competing retailers expand trade credit provision as a new basis for loyalty, and stable buyer relationships reemerge after a period of intense switching. The findings suggest that market structure shapes informal contractual institutions, and that competition can reconstitute the nature of relationships.  The second chapter addresses the relationship between violence and financial decisions in Afghanistan. In particular, I investigate how violence affects the tradeoff between informal cash holdings - which are liquid but insecure - and usage of a more secure but less liquid formal financial account. Using three separate data sources, I find that individuals experiencing violence retain more cash and are less likely to adopt and use mobile money, a new financial technology. First, combining detailed information on the entire universe of mobile money transactions in Afghanistan with administrative records for all violent incidents recorded by international forces, I find a negative relationship between violence and mobile money use. Second, in the context of a randomized control trial, violence is associated with decreased mobile money use and greater cash balances. Third, in financial survey data from nineteen of Afghanistan's 34 provinces, I find that individuals experiencing violence hold more cash. Collectively, the evidence indicates that individuals experiencing violence prefer cash to mobile money. More speculatively, it appears that this is principally because of concerns about future violence. These results emphasize the difficulty of creating robust financial networks in conflict settings.Finally, in the third chapter, I study how informal behaviors interact with incentives to affect employees' decisions to formally save in the context of a large firm in Afghanistan. I analyze a mobile phone-based account that allows savings to be automatically deducted from salaries. Employees who are automatically enrolled in this defined-contribution account are 40 percentage points more likely to contribute than individuals with a default contribution of zero. Analyzing randomly assigned employer matching contributions, I find that the effect of automatic enrollment on participation is approximately equivalent to providing financial incentives equal to a 50 percent match. To understand why default enrollment increases participation, some employees are randomly offered an immediate financial consultation, and others a financial consultation in one week. Employees are more likely to discuss changing their savings contributions in one week, suggesting that defaults raise contributions because of the perceived complexity of financial decisions, and because employees procrastinate in developing a financial plan for the future.",ucb,,https://escholarship.org/uc/item/15g1g7dv,,,eng,REGULAR,0,0
10,1446,A Nexus Between Two Disruptions: A Multiscale Analysis of Transportation Electrification to Forecast the Impacts of Vehicle Grid Integration,"Sheppard, Colin John Ritter","Walker, Joan;",2019,"In this dissertation, I present a body of work that advances our understanding of the technical and economic potential for vehicle grid integration based on a variety of methodological approaches that quantify the opportunity at multiple scales, across multiple geographies, and that cover scenarios with both personally owned plug-in electric vehicles (PEVs) and shared autonomous electric vehicles (SAEVs). The key research questions addressed in this dissertation include:* How can charging infrastructure be cost-effectively deployed to maximize utilization and value to PEV drivers?* How much flexibility exists in the charging demand from PEVs? * What is the economic opportunity to manage the charging of PEVs to occur at lower cost time periods?* How will fleets of electrified autonomous vehicles serving mobility on-demand differ in how they are managed to minimize the cost of charging or to serve as a source of electricity for buildings?These questions are motivated by the fact that transportation electrification and emerging forms of mobility are dramatically changing how the transportation system is planned, operated, and analyzed. PEVs present new challenges and constraints around the siting and operation of refueling infrastructure. Electric load from PEVs can exacerbate grid congestion at either transmission or distribution scales if left unmanaged. Sharing and autonomy are changing mobility which will have unique implications for the grid integration of PEVs. Meanwhile, there are strong social and environmental forces compelling planners, regulators, and private industry to electrify transportation as soon as possible. The transportation sector is the largest emitter of greenhouse gases in the United States. With the exception of the great recession, emissions in the transportation sector have been growing for the last three decades, in contrast to the electric power and industrial sectors which have been on a downward trend in emissions. Transportation, therefore, represents one of the primary challenges to achieving deep decarbonization of the U.S. economy.In the electric power sector, policy and economic forces are upending incumbent generation technologies (coal and natural gas) in favor of lower cost and lower carbon alternatives, particularly wind and solar power. As these intermittent renewable resources increase in capacity, the incidence of renewable energy (RE) curtailment increases due to time periods when supply is greater than demand and generators are turned down or shut off from the level that they would otherwise be producing. Curtailment raises the overall system cost of supplying electricity. In addition, some utilities must meet an energy production standard to satisfy state mandates for renewable production. Renewable curtailment forces utilities to either acquire more RE or introduce sources of grid flexibility to relieve the curtailment. One low cost strategy to mitigate these challenges is to manage the temporal profile of electricity demand to make use of the renewable resources when they are available.PEVs are generally analyzed through modeling using one of two approaches, statistical modeling and activity-based modeling. Statistical models typically summarize or infer travel patterns from travel survey data and use them to characterize the need for PEV charging and the temporal opportunities to charge. The key disadvantage of such approaches is that they cannot account for the individual mobility constraints of travelers and they typically require an assumption that charging infrastructure is unlimited. Another common approach is to develop Markov Chain models of mobility and PEV charging. In these models, transitions between states are treated as random events. Because they lack a representation of the causal mechanism for these transitions, these models are difficult to generalize and their utility is degraded if applied in prospective contexts assuming a transportation system with dramatically different characteristics than present. Activity-based models make use of travel diaries from surveys or GPS data logging which are then provided as input to mobility and charging simulations. Agent-based models are a subset of activity-based models, in so far as they treat travelers individually and require a representation of each individual's activity schedule in order to model the travel necessary to engage in those activities. What distinguishes agent-based models are two key features: 1) wrapping the individuals in a virtual environment (e.g. the transportation system) with detailed representation of transportation supply and 2) dynamically simulating the agents' interactions with the virtual environment and with each other. These interactions open the opportunity to model the choices of the agents based on empirical studies of human behavior as well as to make agent behavior contingent on the time-evolving state of their environment and other agents.In the electric power and grid modeling domain, load from PEVs are typically represented as static or derived from very rudimentary estimation techniques. Studies either ignore flexibility entirely or they make simplistic assumptions about the timing and degree to which PEV load can be shaped. The inaccuracy in these modeling choices have had a relatively low impact in the recent past due to the still relatively low penetration of PEVs in the national vehicle fleet. But within a decade it will no longer suffice to ignore or simplify PEV load, which could eventually make up more than 20% of U.S. electricity demand.This dissertation addresses these gaps by coupling models of electric mobility and the grid at multiple scales. Each paper presented in this dissertation was produced in collaboration with co-authors across multiple projects and contexts. I employ reduced-form models in the context of optimization to solve the charger scheduling and vehicle mobility problems, as well as detailed agent-based models that simulate context-specific traveler behaviors and the dynamics of resource-constrained charging infrastructure.To address the infrastructure siting problem, I develop a spatially explicit agent-based simulation model that represents charging infrastructure, charging behavior, competition for scarce chargers, and driver adaptation. A differential evolution and a heuristic optimization scheme are employed to find a cost-effective distribution of charging infrastructure. I then address the question of flexibility in two ways. First I develop a scheme for optimizing the charging profiles of individual PEV drivers based an objective that simultaneously considers the costs of charging and the benefits associated with providing ancillary services to the grid. Then I employ a much higher fidelity approach to simulate both the electrified mobility system as well as the power sector. I develop the BEAM modeling framework (Behavior, Energy, Mobility, and Autonomy), which is an agent-based model of PEV mobility and charging behavior designed as an extension to MATSim (the Multi-Agent Transportation Simulation model). I apply BEAM to the San Francisco Bay Area and conduct a preliminary calibration and validation of its prediction of charging load based on observed charging infrastructure utilization for the region in 2016.  I link the BEAM model with PLEXOS, an industry standard production cost model that accurately characterizes grid dispatch constraints.Finally, I consider the impact of grid-integrated fleets of SAEVs providing mobility on-demand. In two separate studies I develop models to consider how such fleets could be used to serve building energy demand during power outages as well as a more general analysis of the battery and charging infrastructure requirements to serve nationwide mobility.The key findings across all of this work are the following:* In today's energy markets, PEV flexibility can reach values of $155/year/vehicle for NYISO and $98/year/vehicle for CAISO. The annual cost savings due to optimizing dispatch come more from savings on the price of energy (74% in CAISO and 61% in NYISO) but providing ancillary services (in the form of regulation) also contributes value to the solution (26% in CAISO and 39% in NYISO).* When we project the energy market of California to a future year when renewables make up 50% of the annual energy produced, PEV flexibility is even more beneficial to the power sector, primarily in lowering grid operating cost and the amount of RE that must be curtailed to avoid over-generation when supply and demand are mismatched. For example, if treated as flexible loads, 2.5 million smart charging PEVs avoid 50\% of incremental system operating costs annually and reduce renewable energy curtailment by 27% annually relative to when the same number of unmanaged charging PEVs are added to the grid. * When SAEVs serve power to buildings during an extreme outage, the fleet can generate 32%-40% more revenue than is earned serving mobility alone. While the overall value of providing on-demand power depends on the frequency and severity of power outages, the results show that serving power demand during large-scale outages can provide a substantial value stream, comparable to what can be earned providing grid services. * All mobility in the United States currently served by 276 million personally owned vehicles could be served by 12.5 million SAEVs at a cost of $0.27/vehicle-mile or $0.18/passenger-mile. The energy requirements for this fleet would be 1142 GWh/day (8.5% of 2017 U.S. electricity demand) and the peak charging load 76.7 GW (11% of U.S. power peak). In total, this body of work contributes new insights into the opportunity for electric mobility to reduce the cost of operating the electric grid, enabling deeper and faster adoption of renewable power in the electric sector, and providing reliable mobility to travelers in the transportation sector. The domain of vehicle grid integration is still relatively new, there are many areas of research that require additional attention, such as increased research on traveler preferences around PEV charging, the intersection between electric mobility and the distribution grid, electrification of medium and heavy duty vehicles, as well as properly incentivizing electric vehicles to ride hail drivers in the gig economy.",ucb,,https://escholarship.org/uc/item/1663f91r,,,eng,REGULAR,0,0
11,1447,Tracing Patterns of Textiles in Ancient Java (8thâ€“15th century),"Sardjono, Sandra","Williams, Joanna;Klokke, Marijke;",2017,"Few attempts have been made to study the numerous textile depictions in Java from the eighth to fifteenth centuries, also known as the Hindu-Buddhist or the Ancient Javanese Period. This thesis seeks for the textiles that inspired these depictions and considers their techniques. It also traces the evolution of particular patterns in Java over time. To do so, I employ close art-historical analysis of works of art and draw supportive materials from archaeology, epigraphy and literature, as well as ethnography. After the introductory chapter, Chapters One and Two focus each on a different textile pattern: the connected circles and the overlapping circles patterns. These chapters follow the evolution of the patterns with particular interest to search for connections to current textile tradition in Indonesia. A similar approach of inquiry is applied in Chapter Three to a type of short sleeve jacket. Chapter Four investigates the depiction of weavers in Ancient Javanese textual and visual sources. This study of textile depictions will underscore the global connection between Java and the outside world, particularly China and India, from where many prototypes of the textile images originated. The study will also reveal that these images, in addition to being historical records, were also ornamentations, which the Javanese artists were adept at translating, decontextualizing, and re-contextualizingâ€”as a whole or in partâ€”into the local aesthetic and usage.",ucb,,https://escholarship.org/uc/item/16f914tm,,,eng,REGULAR,0,0
12,1448,"Microtopographical control of cell adhesion, organization, and proliferation in a cardiac tissue engineering scaffold","Patel, Anuj Ashwin","Kumar, Sanjay;",2011,"Myocardial infarction, commonly known as a heart attack, is caused by the blockage of blood flow to heart, resulting in the death of cardiomyocytes, or heart muscle cells. Scar tissue formation occurs in the area of the damage due to the heart's inability to regenerate myocardial tissue. Therefore, regeneration of myocardial tissue through the use of synthetic scaffolds requires strategies to promote cardiomyocyte attachment while minimizing proliferation of the fibroblast cells that contribute to scar tissue. Previous studies have demonstrated that a synthetic platform consisting of an array of microscale polydimethylsiloxane (PDMS)-based pillars (""micropegs"") can accomplish both of these goals, but the mechanism through which this occurs has remained a mystery.   In this work the interaction between microtopographical cues and both fibroblasts and cardiomyocytes is further explored. It is shown that a fibroblast that is attached to a micropeg is less likely to proliferate than ones on a flat surface, but this difference can be partially abrogated in the presence of drugs that inhibit cell contractility. The cells also show increased adhesion to the micropegs as opposed to flat surfaces, as demonstrated by measurements of the dynamics of deadhesion from the surface and changes in expression of specific mechanotransductive genes. Together, these data support a model in which microtopographical cues alter the local mechanical microenvironment of cells by modulating adhesion and adhesion-dependent mechanotransductive signaling, thereby leading to a reduction in proliferation capability.   The research focus then shifts to the use of microtopographical cues to control cardiomyocyte adhesion and organization. Cardiomyocytes cluster around and interact with the full length of the micropegs, exhibiting three-dimensional organization on a two-dimensional surface. By controlling the diameter and spatial arrangement of the micropegs, the degree of clustering can be regulated. The expression of functional markers N-cadherin and connexin 43 also exhibit a dependence on the spatial arrangement of the micropegs. The preference of cardiomyocytes for three-dimensional adhesion is further investigated in the final part of the thesis. By isolating cardiomyocytes in PDMS microwells, the cells are presented with the option of attaching to a vertical wall or a flat space. The cells demonstrate a preferential attachment to the side walls and corners of the microwell. Introduction of the myosin inhibitor blebbistatin reduces the percentage of cells attached to these side walls. Cells attached to a side wall also are less likely to proliferate, similar to the behavior of fibroblasts attached to micropegs. Taken together, these data indicate that incorporation of microtopographical features into cardiac tissue engineering scaffolds can be used to control the adhesion and organization of cardiomyocytes while simultaneously limiting the formation of scar tissue.",ucb,,https://escholarship.org/uc/item/1c78p3zh,,,eng,REGULAR,0,0
13,1449,"Grocery Stores: Neighborhood Retail or Urban Panacea? Exploring the Intersections of Federal Policy, Community Health, and Revitalization in Bayview Hunters Point and West Oakland, California","Elias, Renee Roy","Corburn, Jason;",2013,"Throughout the nation, grocery retailers are reentering underserved communities amidst growing public awareness of food deserts and the rise of federal, state, and local programs incentivizing urban grocery stores. And yet, even with expanding research on food deserts and their public health impacts, there is still a lack of consensus on whether grocery stores truly offer the best solution. Furthermore, scholars and policymakers alike have limited understandings of the broader neighborhood implications of grocery stores newly introduced into underserved urban communities.This dissertation analyzes how local organizations and agencies pursue grocery development in order to understand the conditions for success implementation. To do this, I examine the historical drivers, planning processes, and outcomes of two extreme cases of urban grocery development: a Fresh and Easy Neighborhood Market (a chain value store) in San Francisco's Bayview Hunters Point and the Mandela Foods Cooperative (a worker-owned cooperative) in Oakland's West Oakland districts. Through a comparative institutional analysis, I find that both Fresh and Easy and Mandela Foods reflect distinctive neighborhood revitalization legacies, critical moments of institutional capacity building, localized versions of national policy narratives, and the role of charismatic leaders in grocery store implementation. While national narratives shape the rhetoric of urban grocery development, ultimately local context dictates how food access issues are defined, who addresses them, and how. These findings suggest that federal grocery incentive programs should: 1) maintain a broad framework that enables local communities to define food access problems and their solutions on a case-by-case basis, 2) encourage diverse solutions not limited to grocery stores and supermarkets, and 3) emphasize community reinvestment goals.",ucb,,https://escholarship.org/uc/item/1d45296j,,,eng,REGULAR,0,0
14,1450,Wavefront metrology for high resolution optical systems,"Miyakawa, Ryan","Attwood, David;",2011,"Next generation extreme ultraviolet (EUV) optical systems are moving to higher resolution optics to accommodate smaller length scales targeted by the semiconductor industry.  As the numerical apertures (NA) of the optics become larger, it becomes increasingly difficult to characterize aberrations due to experimental challenges associated with high-resolution spatial filters and geometrical effects caused by large incident angles of the test wavefront.  This dissertation focuses on two methods of wavefront metrology for high resolution optical systems.  The first method, lateral shearing interferometry (LSI), is a self-referencing interferometry where the test wavefront is incident on a low spatial frequency grating, and the resulting interference between the diffracted orders is used to reconstruct the wavefront aberrations.  LSI has many advantages over other interferometric tests such as phase-shifting point diffraction interferometry (PS/PDI) due to its experimental simplicity, stability, relaxed coherence requirements, and its ability to scale to high numerical apertures. While LSI has historically been a qualitative test, this dissertation presents a novel quantitative investigation of the LSI interferogram.   The analysis reveals the existence of systematic aberrations due to the nonlinear angular response from the diffraction grating that compromises the accuracy of LSI at medium to high NAs. In the medium NA regime (0.15 < NA < 0.35), a holographic model is presented that derives the systematic aberrations in closed form, which demonstrates an astigmatism term that scales as the square of the grating defocus.  In the high NA regime (0.35 < NA), a geometrical model is introduced that describes the aberrations as a system of transcendental equations that can be solved numerically.  The characterization and removal of these systematic errors is a necessary step that unlocks LSI as a viable candidate for high NA EUV optical testing.The second method is a novel image-based reconstruction  that characterizes the aberrations of an optical system with arbitrary numerical aperture and illumination coherence.   In this method a known pattern is imaged by the test optic at several planes through focus.  A computer model is created that iterates through possible sets of wavefront aberrations until the through-focus series of aerial images matches the aerial images from the experiment.  Although the sample space of Zernike coefficients is non-convex, a hybrid algorithm consisting of pattern search and simulated annealing methods is used to search for the global minimum.The computation of aerial images from a partially coherent optical system is expedited with a novel decomposition of the Hopkins equation known as the Reduced Optical Coherent Sum (ROCS).  In this method, the Hopkins integral is described by an operator S which maps the space of pupil aberrations directly to the space of aerial images.  This operator is shown to be semipositive definite and well-approximated by a truncated sum of its spectral components.  The ROCS decomposition has a customizable error bound allowing one to tradeoff aerial image fidelity for significant speed improvements.  For aerial image errors of 1-3%, the ROCS algorithm can compute aerial images up to 15 times faster than the Hopkins integration.  The ROCS-based wavefront test is extremely versatile since it is applicable in nearly all optical systems that measure aerial image intensity regardless of numerical aperture or illumination coherence and requires little or no experimental modifications.  This test is used to characterize the field-dependent aberrations of the SEMATECH Berkeley Actinic Inspection Tool (AIT), and the results match an independent analysis of the astigmatism aberrations to within lambda/20 rms.",ucb,,https://escholarship.org/uc/item/1dd5j7ss,,,eng,REGULAR,0,0
15,1451,Investigating Innovation Practice: Cross-Disciplinary Studies in International Development,"Gordon, Pierce Edward Cornelius","Agogino, Alice M;",2018,"Innovation practice is a transdisciplinary field that aims to create a better world out of an existing one by pooling methods and mindsets of inquiry and creation. The field observes design contexts, assimilates the collected knowledge into problems to be addressed, ideates solutions to those problems, and iteratively tests those solutions in real environments to determine how they address these problems. Over the past decade, the field has become more accessible to a much broader collection of amateur designers. They utilize the field to understand more diverse contexts, to include and adapt more disciplines, and to address a wide variety of complex and seemingly intractable issues. Due to the evolution of the fieldsâ€™ popularity, debates began to arise about the fieldsâ€™ utility and place in society. Development professionals treated design thinking and related fields as a silver bullet that could easily address issues of global poverty. Critics asked if the field was different from existing disciplines, whether the field delivers demonstrable impact, and if the democratization of design practice to â€˜amateurâ€™ designers is even worthwhile. However, these debates revealed how little knowledge is collected about how practitioners conduct innovation practice in the first place. To learn about the activities, benefits, methods, and obstacles of beneficial development-focused design practice, I detail three studies that apply lenses of analysis to innovation narratives to see how various collectives of self-determined innovators actually practice their craft. The first study outlines a systematic literature review of human-centered design for development. By applying design principles to a population of researcher-designers and their narratives, we learn if these designers actually practice innovation with these principles of human-centeredness in mind. I outline three previously conducted studies about the nature of this field, which describe the population, location, history, and methods these projects use across various contexts. and detail an analysis of the participatory nature of human-centered design for development. In so doing, I describe statistics about the prevalence of participatory design practice, reveal how the studies report the complexities of participation, and collect insights about the stakeholders who are allowed to design. The study then sums up the importance of investigative analysis methods across populations of design narratives, so that researchers can learn more about how â€˜good practiceâ€™ is perceived. The second study describes an ethnographic evaluation study of notable actors in the Botswana innovation community. This study begins with a reflection on epistemological frictions between the popular fields of innovation practice and impact evaluation. After revealing the theoretical and practical gaps in how innovators evaluate, I introduce the Botswana history, policies, and institutions that support innovation practice on the national level, while describing their activities and how innovation actors perceive them. I then detail the creation of a grassroots innovation community that practices participatory co-design of locally beneficial technologies by outlining the history of its indigenous stakeholders and describing an ethnographic narrative of two formative innovation workshops. I then describe the methods, approaches, purpose, and stakeholders involved in the evaluation of innovation in the local and national institutions. This analysis reveals evaluation tools applicable to many innovation contexts, and insights about how these evaluation approaches are aligned and misaligned with each other. Finally, I describe insights on the practice and facilitation of innovation in the country, to clarify cultural, institutional, and practical barriers and qualities that hinder the potential benefit of innovation. The final study is a reflection on the inadequacies of ethics systems in Botswana to support beneficial innovation practice. While investigating the previous chapter, I happened upon narratives with no simple solutions, and few resources for development-centric designers to effectively navigate this ethics space. Moreover, while facing the countryâ€™s institutional review board system, I gained first-hand experience with the goals, dynamics, and limitations of the Botswana research system of ethics. This chapter unpacks how the ethical system fails to align with the needs of beneficial innovation practice and suggests theoretical alternatives to draw upon for future use. This dissertation describes the complex possibilities of participatory design practice, the various goals, activities, and perceptions of the evolving Botswana innovation ecosystem, and details the frictions between the understudied field of ethics in design for development and existing institutions. These studies reveal how â€˜goodâ€™ innovation practice is wholly based on the context it is applied: on its practitioners, their tools, their goals, the environment where it is used, and the stakeholders with whom the designers interact. Though these studies outline how the methods and mindsets of innovation practice are accessible to more communities than ever, it does not mean that innovation practice itself becomes simpler. These lenses of cross-contextual analysis, participation, evaluation, and ethics reveal how the amorphous, evolving field requires innovators who are responsive and respectful of the contexts in which they are situated. These studies outline a few of many approaches that reveal the unique dynamics in development-centered innovation practice but are essential for any designersâ€™ toolbox to ensure we collectively create a better world.",ucb,,https://escholarship.org/uc/item/1f20709j,,,eng,REGULAR,0,0
16,1452,Optimal Reconstruction of Cosmological Density Fields,"Horowitz, Benjamin Aaron","Seljak, Uros;",2019,"A key objective of modern cosmology is to determine the composition and distribution of matter in the universe. While current observations seem to match the standard cosmological model with remarkable precision, there remains tensions between observations as well as mysteries relating to the true nature of dark matter and dark energy. Despite the recent increased availability of cosmological data across a wide redshift, these tensions have remained or been further worsened. With the explosion of astronomical data in the coming decade, it has become increasingly critical to extract the maximum possible amount of information available across all available scales. As the available volume for analysis increases, we are no longer sample variance limited and existing summary statistics (as well as related estimators) need to be re-examined. Fortunately, parallel with the construction of these surveys there is significant development in the computational techniques used to analyze that data. Algorithmic developments over the past decade and expansion of computational resources allow large cosmological simulations to be run with relative simplicity and parallel theoretical developments motivate increased interest in recovering the underlying large scale structure of the universe beyond the power spectra.The detailed study of this large scale structure has the potential to shed light on various unanswered questions and under-constrained physical models for the dark sector and the nature of gravity. As we reach higher redshifts with statistically significant samples, the large scale structure can serve as a link between local observations and the cosmic microwave background. These surveys rely on a variety of biased probes, including the lensing and distribution of galaxies, imprints of large scale structure in secondary anisotropies of the CMB, and absorption lines in the spectra of high redshift quasars. These observations are complementary; they probe different scales, have different sources of astrophysical and observational uncertainties, have unique degenercies in parameter space, and require their own methods to extract cosmological parameters from.In this thesis, I discuss a number of new developments in the analysis of these diverse cosmological datasets. After introductory material, I discuss work re-examining the lensing of the Cosmic Microwave Background by cluster-sized objects and implement techniques for accurate mass estimation. I demonstrate that this analysis is optimal in the low noise, small scale limit. In the second part, I develop a maximum likelihood formalism for linear density fields, applicable for reconstructing underlying signal from a variety of cosmological probes including projected galaxy fields and cosmic shear, showing that effects of anisotropic noise and masking can be mitigated. Finally, I extend this work to nonlinear observables by using a forward modeling approach for Lyman Alpha forest tomography, finding more accurate cosmic web reconstruction verses existing techniques. The unifying theme of all these works is revisiting existing matter density reconstruction techniques with a critical eye and using new statistical and computational techniques to efficiently perform an unbiased, lower variance, estimate. Included is discussion of the possible impacts of these methods to improve constraints of cosmological parameters and/or astrophysical processes.",ucb,,https://escholarship.org/uc/item/1fk125d0,,,eng,REGULAR,0,0
17,1453,Effects of shape and surfaces on fluid-dynamic performance of organisms at intermediate Re,"Dolinajec, Trevor Hendry","Koehl, Mimi A.R.;",2015,"An organism's performance in relation to the fluid it lives and operates in is importantacross size and time scales, but the effects on performance of body shape andproximity to a surface become particularly nuanced at intermediate Re. This physicalregime in which both viscosity and inertia play important roles has not been studiedas extensively as that of macroscopic animals in which inertia dominates or that ofmicroscopic animals in which viscosity dominates. However, many ecologically importantanimals such as the copepod occupy these intermediate flow conditions, as doboth airborne and aquatic propagules such as the sporocarps of fungi and the larvae ofbenthic animals. Through recorded observation and modeling this dissertation arrivesat biological implication regarding these organisms' habitats and life cycles. This workalso creates a fuller understanding of general principles that govern intermediate Re.Zooplankton contain a range of morphologies, and life cycles that bring them in contactwith surfaces that act as crucibles. The purpose of this study was to determinehow the morphology and orientation of a variety of ecologically-important microscopicmarine animals (adult copepod, snail veliger larva, barnacle nauplius and cyprid larvae)affect the forces they experience while swimming in the water column, and whileon surfaces (e.g. prey captured on tentacles of benthic predators, larvae settled ontobenthic substrata). Drag, lift, and side forces as well as moments were measuredabout three axes for dynamically-scaled physical models of each animal. These forcesand moments can transport and reorient swimming animals, and can push, lift, peel,or shear animals o surfaces, and thus affect important ecological processes such asdispersal, predation, and larval settlement. The Reynolds numbers (Re, the ratio ofinertial to viscous forces) for the zooplankton and the models was in the range of 10^2to 10^3. Body shape and orientation of small animals were found to have significant effects on the magnitudes of fluid dynamic forces and moments at Reynolds numbersof order 10^3, but were less important at lower Re's. The magnitude and direction ofthe net force on an organism was found to change drastically as an organism nears,and then lands on a surface. The shear stress on the attachment of a small animal toa surface that is caused by drag pushing the animal downstream is greater than theshear stress due to rotation of the organism by flow-induced spinning, thus zooplanktonon surfaces are more likely to be pushed than twisted o the surfaces by water currents.For phytopathogenic fungi in the order Erysiphales, the cause of the diseases calledpowdery mildew, reinfection or dispersal to a new host plant is contingent on sporocarpsescaping a fluttering leaf, but the mechanisms that allow for this liberation arelargely unknown and unquantied. The genus Phyllactinia, unlike other members ofthe order, has specialized and upwardly bent radial appendages that allow the body ofthe sporocarp to extend down from the bottom of the host leaf. This causes the tipsof the appendages to be the only physical connection between the sporocarp and theleaf with a gap of up to 300 microns, thus creating an arrangement where fluid flow maycontribute to liberation. To test the importance of ambient fluid flow on sporocarp liberationforces and moments were measured and fluid flow around dynamically-scaledphysical models was observed at Re of 60 - 360. Flow velocities, boundary layer heights,and sporocarp morphologies were varied to match unsteady flow conditions and sporocarpmaturation. To test the importance of aeroelastically induced inertial forces the kinematics of fluttering leaves in a wind tunnel were recorded at a range of wind speeds, and samples of sporocarps were weighed. Physically modeled aerodynamic forces and moments alongside recorded inertial forces were compared to measured adhesive forces. The comparative forces strongly suggest that steady wind flow and realistic turbulent wind flow do not exert force necessary for liberation in magnitude or direction, but that unsteady flow can lead to significant pitching moments. The accelerations of fluttering leaves and the resulting inertial forces on sporocarps varied greatly among leaves, with forces large enough to liberate sporocarps occurring in a small subset of leaves with a characteristic flutter frequency of 25 Hz. Pitch-induced overturning of sporocarps can explain the removal of sporocarps observed on wind-exposed leaves, with more sporocarps liberated at greater wind speeds and towards the tips of leaves.Terminal velocity is an important parameter in the wind dispersal of propagules (seeds,pollen grains, spores). Aerial righting and aerodynamic stability is common amongvertebrate and invertebrate animals, and some propagules. Fungal sporocarps of thepowdery mildew Phyllactinia have shapes that aect their terminal velocity and aerodynamicstability while operating at Re 1.0 - 3.3, thus Phyllactinia represents a modelorganism for aerodynamic performance at near-unity Re. The reproductive success ofthese mildew species is dependent on stability during aerial transport so that a particularorientation is achieved upon deposition. High speed videography was used tomeasure terminal velocity, angular velocities, and angular accelerations of free-fallingsporocarps during aerial righting. Physical models allowed for quantification of forcesand moments acting on sporocarps falling at terminal velocity, as well as providing fine-scaleflow visualization. The morphology of sporocarps is dependent on their maturity,and experiments carried out with collected sporocarps showed that terminal velocityis partially a function of morphological parameters. Terminal velocities of sporocarpsranged from 8 to 28 cm/s. Flow visualizations showed that both the width and lengthof the wake formed around a falling sporocarp were dependent on the spread of thecharacteristic radial appendages of the genus. Sporocarps were recorded rotating whilefalling prior to reaching stability, and angular velocity and angular accelerations decreasedas sporocarps approached zero angle of attack. Models conrmed that a stablexed point existed at an angle of attack of zero for all tested morphologies of Phyllactiniasporocarps. However, naturally occurring morphologies that were the most likelyto have smaller terminal velocities also displayed smaller aerial-righting moments, andsporocarps most likely to have larger terminal velocities displayed larger aerial-rightingmoments. This suggests a potential trade-o between sporocarps that are more stable(larger aerial-righting moments) and those that can disperse longer horizontal distances(smaller terminal velocity).",ucb,,https://escholarship.org/uc/item/0jd1746s,,,eng,REGULAR,0,0
18,1454,Fabrication and Optimization of Nano-Structured Composites for Energy Storage,"Carrington, Kenneth Russell","Mao, Samuel S;Carey, Van P;",2009,"This dissertation is focused on the development and characterization of a novel class of solid-state nano-structured composites for hydrogen storage based on silica aerogel. It is organized sequentially around experiments conducted to fabricate, optimize and characterize silica aerogel and the composites for hydrogen storage. First, the basics of nano-structured media, silica aerogel technology and solid-state hydrogen storage are introduced. Next, the fabrication and optimization of silica aerogel for hydrogen storage is described in detail. The key result is that varying fabrication parameters can improve the physical properties of the resultant silica aerogel in the context of hydrogen storage. The fabrication of solid-state nano-structured composites using chemical vapor infiltration is then discussed. A series of experiments is used to parameterize the fabrication process, which results in a collection of parameters that minimize variation and structural damage in the composites. Silica aerogel and the composites are then physically characterized using transmission electron microscopy, X-ray diffraction and porosimetry in order to investigate their nano-structuring.An overview of hydrogen storage characterization and two innovations that improve the accuracy and efficiency of hydrogen storage characterization of low-bulk density media like silica aerogel and the composites are then presented. Finally, the innovations are applied to silica aerogel and the composites to characterize their hydrogen storage performance. Silica aerogel and the composites are found to outperform the most common benchmark in physisorption media, and one composite in particular shows unique hydrogen storage performance.",ucb,,https://escholarship.org/uc/item/0nc0p6fm,,,eng,REGULAR,0,0
19,1455,Worlds of Desire: Gender and Sexuality in Classical Tamil Poetry,"Segran, Elizabeth Rani","Hart, George L;",2011,"This dissertation contributes to the nascent study of the Tamil Cankam corpus, a collection of poetic anthologies produced in the first three centuries CE. The Cankam poems are constructed around the two complementary themes of the ""inner world"" relating to emotions, romance and family life, and the ""outer world"" relating to kingship, warfare and public life. This dissertation argues that the thematic division within the corpus is gendered, as the ""inner world"" is associated with the feminine while the ""outer world"" is associated with the masculine. Each chapter explores the way that the poets establish the boundaries of femininity and masculinity through both the form and content of their verses. This dissertation focuses closely on the moments of rupture in the poets' system of gender construction, for these moments suggest that the poets acknowledged that gender is more fluid and complex than it initially appears. To better understand the workings of gender and sexuality in these poems, this study juxtaposes recent theoretical frameworks with these poems from the distant past. Methodologically, this dissertation collapses traditional historical time, bringing the ancient Cankam anthologies into conversation with ideas that are circulating now. In doing so, it seeks to elucidate both the poems and the theory, while also opening up new questions in both fields.",ucb,,https://escholarship.org/uc/item/0nk260ck,,,eng,REGULAR,0,0
20,1456,Cellodextrin Transporters of Neurospora crassa and their Utility in Saccharomyces cerevisiae During a Biofuel Production Process,"Galazka, Jonathan Matthew","Cate, Jamie H. D.;",2011,"The filamentous fungus, Neurospora crassa, is capable of depolymerizing and metabolizing plant cell walls. When grown on plant cell walls or pure cellulose, N. crassa upregulates an overlapping set of 114 genes. Amongst this set are 10 major facilitator superfamily transporters. I have shown that two of these, CDT-1 and CDT-2, transport cellodextrins, which are the major degradative product of fungal cellulases. Deletion of cdt-2 affects the growth of N. crassa on crystalline cellulose. Furthermore, diverse fungi transcriptionally upregulate orthologs of cdt-1 and cdt-2 when in contact with plant cell walls, suggesting that cellodextrin transporters are important to fungal interactions with plants. Engineering the cellodextrin transport pathway into Saccharomyces cerevisiae allows this yeast to ferment cellodextrins to ethanol with high yields, and facilitates the simultaneous saccharification and fermentation of cellulose to ethanol. Cellodextrin transport can be coupled to downstream hydrolysis or phosphorolysis of cellodextrins by a cellodextrin hydrolase or cellobiose phosphorylase, respectively. Cellodextrin transport circumvents a major limitation of yeast in fuel production: the inability to simultaneously transport and ferment pentose sugars and glucose to ethanol. S. cerevisiae did not evolve to co-ferment cellobiose and xylose and unintended consequences are likely.  For example, we speculate that S. cerevisiae may not sense cellobiose as a fermentable carbon source.",ucb,,https://escholarship.org/uc/item/0nr9n5zw,,,eng,REGULAR,0,0
21,1457,Dynamics of Viral Packaging: Single-Molecule Observations in Multiple Dimensions,"Hetherington, Craig Lee","Bustamante, Carlos J;",2011,"During the self-assembly of bacteriophage phi29, the viral genome is packaged into a pre-formed capsid by a molecular motor. The packaging motor is a complex of several oligomers including a dodecameric connector ring and a pentameric ATPase ring. These rings coordinate with each other, generating high forces in order to compact the dsDNA genome into the capsid at high pressures.The connector was proposed to engage and directly perform work on the DNA during packaging. Consideration of the symmetry of the connector and the capsid predicts that the connector must rotate relative to the capsid as part of the mechanism for translocating the DNA. An experiment designed to directly measure rotation of the connector is discussed in Chapter 2 of this dissertation. A combination magnetic tweezers and total internal reflection microscope is used to track the polarization of single fluorescent dyes attached to the connector. No evidence of polarization changes were found, indicating that the connector does not rotate at the expected rates. This further suggests that the connector does not directly perform mechanical work on the DNA during packaging.Viral packaging can be observed in optical tweezers by monitoring the length of the DNA as it is drawn into the capsid. Past studies have revealed many details of the packaging mechanism by following the dependence of the packaging velocity on factors like ATP concentration and applied load. In Chapter 3, I propose an experimental design intended to measure the effect of the packaging motor on the angle of the DNA in addition to its length and thus recover the full three-dimensional trajectory of the DNA as it passes into the capsid. In addition, this scheme can be used to apply torque and thus provides an additional tool with which to probe the packaging mechanism.In Chapter 4, we find that during packaging the downstream DNA is twisted in an underwinding direction, with a magnitude that depends on the extent to which the capsid is filled. The change in twisting can be attributed to cumulative looping of the DNA within the capsid, and the data predicts that the loops formed in the last kilobasepair of packaging are as small as 4 nm in radius. In addition, a non-lethal method of rupturing the viral capsids prior to packaging was discovered. Observations of DNA twisting by those packaging complexes revealed that, in the absence of internal pressure, the DNA is twisted by -1.2 Â°/bp. This number suggests that one of the packaging motor's five subunits makes contact with the DNA every ten basepairs, and that the cycle repeats with that subunit performing the same function every time. Such a strict functional segregation, in addition to the catalytic segregation revealed in previous high-resolution optical tweezers experiments, is an important part of the mechanism by which the motor packages DNA against high forces.",ucb,,https://escholarship.org/uc/item/0pw8703c,,,eng,REGULAR,0,0
22,1458,Viral Politics: Sex Worker Activism and HIV/AIDS Programs from Bangalore to Nairobi,"Vijayakumar, Srigowri","Ray, Raka;",2016,"This dissertation studies the international success story of Indiaâ€™s HIV/AIDS response and the activism of sex workers and sexual minorities that produced it. A number of recent ethnographies have turned their attention to the workings of state programs in middle-income countries (e.g. Baiocchi 2005; Sharma 2008; A. Gupta 2012; Auyero 2012), demonstrating both the micro-effects of state strategies for managing poverty on poor people and the ways in which state programs are produced outside the visible boundaries of â€œthe stateâ€â€”through NGOs and social movement organizations as well as transnational donors and research institutes.  Yet, even as state programs are constituted through struggles over resources and representations within and outside the official agencies of the state, states also derive legitimacy from projecting themselves as cohesive rather than disaggregated, and as autonomous from society rather than anchored within it (Abrams 1988; Mitchell 1991b; Mitchell 1999; A. Gupta 2012). The representation of state programs as cohesive, pre-constituted, exportable â€œmodelsâ€ serves as a new way of consolidating state legitimacy within a global, hierarchical order of development â€œsuccess.â€ However, this dissertation argues that the traveling policies disseminated through transnational expert communities are a selective codification of hard-fought struggles among institutions within the state, between the state and organizations, among organizations, and among groups within organizations over the aims and strategies of social policies and programs.  These struggles shape what travels in traveling policies and what is left out.  Drawing on over 150 in-depth interviews and a year of participant observation with sex workers involved in implementing policy in community-based organizations, NGOs, and activist groups, I show how the material and social conditions of men, women, and transgender women in sex work, mediated through community-based organizations, constituted the successful approaches to HIV prevention that were later, sometimes selectively, translated around the world.",ucb,,https://escholarship.org/uc/item/0qs1n4fh,,,eng,REGULAR,0,0
23,1459,Optodynamical Measurement and Coupling of Atomic Motion and Spin,"Kohler, Jonathan","Stamper-Kurn, Dan M;",2018,"The quantum nature of light makes it a basic component for models of quantum measurement and information exchange between disparate quantum modes, pioneered in the field of cavity quantum electrodynamics. The interaction of atomic ensembles with the mode of an optical cavity provides a flexible platform for exploring the coherent interaction of light with diverse macroscopic dynamics, such as collective motion and spin. This dissertation presents experimental results and theoretical models for continuous measurement and control of the center of mass motion and collective spin precession of an atomic ensemble, mediated by coupling to a high-finesse optical cavity. First, the theory of dispersive coupling between the cavity mode and the collective motion and spin of an atomic ensemble is derived, and then a general time-domain formalism is developed for theoretical analysis of multi-mode optodynamical systems. Single-mode optodynamical effects are introduced through experimental demonstrations of measurement and control of the collective atomic spin, providing a close analogy to cavity optomechanics.Next, multiple collective atomic modes are considered within a single cavity, in order to assemble optically mediated interactions within multi-mode optodynamical systems. A demonstration of optodynamical interactions between the center of mass motion of two atomic ensembles is presented, coupled through an optical spring mediated by the cavity mode. Then simultaneous coupling of the center of mass motion and total spin precession of a single ensemble of atoms is described, yielding an experimental realization of a negative-mass instability, facilitated by the novel resource of the spin ensembles inverted state. A theoretical analysis of the negative-mass instability is presented, which indicates the possibility of generating two-mode squeezed states in the absence of excess incoherent noise. Finally, linear state retrodiction from the optodynamical signals is discussed, providing background and supplemental material for a forthcoming manuscript.",ucb,,https://escholarship.org/uc/item/0hn835xt,,,eng,REGULAR,0,0
24,1460,"Searching for Sustainable Utopia: Art, Political Culture, and Historical Practice in Germany, 1980-2000","Allen, Jennifer Leigh","Jay, Martin E;",2015,"At the end of the twentieth century, the gradual triumph of liberal democracy and capitalism over â€œreally existing socialismâ€ brought to many West Germans not relief but melancholy. Facing what they interpreted as the dissipation of radical social and political alternatives, academics and public intellectuals pronounced the death of ideology, of history, and of utopian ambitions. This dissertation asks how West Germans nevertheless found ways to challenge this resignation by giving voice to new, radical hopes for Germanyâ€™s future. For their broad popularity and sustained impact, this study traces the grassroots efforts of three groups. First, it follows the Berlin History Workshop, a collection of amateur and professional historians, as they attempted to liberate the process of researching and writing history from the rigid confines German universities. This group sought, instead, to bring the historianâ€™s craft into Berlinâ€™s local neighborhoods in order to enable ordinary Germans to narrate their own histories. Second, this dissertation analyzes the Green Party, which practiced localized plebiscitary policy making in an effort to endow German citizens with greater political agency. The Greens brought this political practice to numerous cultural projects in an effort to democratize German society by democratizing the cultural encounters of its citizens. Third, this project investigates a loosely-connected group of artists who echoed the investments of the historians and politicians by creating art installations with ordinary objects in ordinary spaces that prompted passersby to reevaluate their relationships to the topography of their daily lives.This dissertation argues that, through these groups, everyday Germans adopted a set of cultural practices in the 1980s and â€˜90s that not only critiqued established institutions but also crafted new institutions in their place. Their critical practices followed three conventions. First, they championed radical grassroots democracy by giving citizens opportunities to create socially-significant cultural products like museums and monuments. Second, they decentralized the creative process, locating it in the spaces of everyday life in order to make it widely accessible. Finally, they borrowed from the environmental movement the concept of sustainability, which demanded that any alternative to existing society be both enduring and adaptable. These practices put culture to work in realizing a more democratic, more socially-integrated Germany. In doing so, they permitted their practitioners to reclaim utopian hope from the dustbin of historical ideas.These three case studies span Germanyâ€™s academic, political, and aesthetic terrain. As such, together they offer evidence that efforts to battle twentieth-century apathy signaled a broad shift in German cultural sensibilities, not an isolated phenomenon. The first three chapters of the dissertation treat each of these groups individually as they began to advocate for new, more democratic geographies of cultural engagement, or â€œalternative public spheres,â€ in the early 1980s. Their focus on Germanyâ€™s cultural environment made them particularly receptive to the idea of sustainability popularized after the convening of the World Commission on Environment and Development in the middle of the decade. The next three chapters trace their pursuit of sustainability in culture. A sustainable culture, they came to realize, had to regard its projects as part of an ongoing process rather than as static goals: theirs was a renewable, future-oriented cultural movement in the present, or a â€œsustainable utopia.â€ Faced with radical changes to the international political landscape and the rapid expansion of their constituencies to include sixteen million East Germans alongside more pedestrian concerns like funding difficulties and interpersonal conflicts, these groups weathered the last decade of the twentieth century with varying success. The study concludes by underscoring the irony that the most durable component of their cultural programs in the wake of German political reunification was their push for cultural decentralization.",ucb,,https://escholarship.org/uc/item/0j67m8xp,,,eng,REGULAR,0,0
25,1461,"Conservation, Economics, and Management of Hunting on Private Land: An International, National and California Analysis","Macaulay, Luke Thomas","Huntsinger, Lynn;Barrett, Reginald;",2015,"Privately owned land accounts for significant areas of land internationally, nationally, and in California. In the U.S. and elsewhere, private land tends to support high levels of biodiversity because land with more productive natural resources was settled and privatized first. These lands, which are integral to conservation goals, are often the most vulnerable to habitat degradation and loss through changes in land-use and fragmentation. In 1930, Aldo Leopold encouraged the development of an incentive scheme to better conserve private lands in the U.S. where hunters would pay landowners for access to conserved wildlife habitat and game populations that could be sustainably harvested. Although a wide body of literature has discussed this approach, much of the research is either theoretical or limited to particular regions and these studies have rarely tested for an explicit connection to whether conservation is ultimately improved as a result of paid hunting. The goal of this dissertation is to evaluate the economic, conservation and management aspects of hunting on private land internationally, nationally, and in California. The first chapter uses a case study approach to explore the environmental and economic issues surrounding hunting in the context of Spain and California. The study found that increased game management rights in Spain appears to yield improved economic return, but at an environmental cost. The second chapter evaluates the scale, distribution and conservation aspects of spending to access private land for fishing, hunting, and wildlife-watching in the United States. This study found that that approximately 440 million acres of private land, an estimated 22% of the contiguous land area of the U.S. and 33% of all private land in the U.S., are either leased or owned for wildlife-associated recreation. Much of these lands are private rangelands and forestlands.  Land utilized for hunting accounted for 81% of that total, while land utilized for fishing and wildlife-watching, although comparatively small, likely includes riparian zones and areas with high environmental or amenity values. Hunters own or lease properties of larger size classes than anglers or wildlife-watchers, providing a possible economic incentive for maintaining large unfragmented properties that provide a variety of conservation benefits. Results show that Americans annually spend an estimated $814 million in day-use fees, $1.48 billion for long-term leases, and $14.8 billion to own private land primarily for wildlife-associated recreation. Hunting, in particular big game hunting, comprises some of the largest contributions to payments for wildlife-associated recreational use on private land. This finding suggests that hunting may be an important market-based mechanism to maintain large unfragmented parcels of wildlife habitat.  Chapter 3 utilizes interviews with a random sample of landowners in California to evaluate conservation practices associated with hunting enterprises. This study found relatively low adoption of hunting enterprises among landowners, and that there were mixed conservation outcomes associated with hunting. Landowners who enrolled in the California Department of Fish & Wildlifeâ€™s Private Land Management program, which provides enhanced game management rights to landowners in exchange for habitat improvement practices, performed the most comprehensive habitat improvement practices, including riparian zone restoration and adjustment of grazing practices to enhance cover and forage resources for wildlife. Many other landowners, however, earned some income from hunting, but either did not implement additional conservation practices to enhance wildlife habitat or performed practices that could cause some ecological problems, such as planting of feed crops that can create openings for invasive noxious weeds to be established on a property. This study found significant opportunities in California to not only increase adoption of hunting enterprises, but to engage in educational efforts to encourage ecologically-friendly wildlife management practices as a way to enhance both revenue from hunting enterprises and conservation outcomes. The final chapter focuses on the development of methods to better understand the population characteristics of the Columbian black-tailed deer (Odocoileus hemionus columbianus) in order to improve harvest recommendations. Using camera-traps on a 2,500 acre private ranch in San Benito County, California, the study estimates the density and sex ratios of deer by using a Bayesian spatial mark-resight model. It also evaluates the effect of using bait in developing population estimates. The study found that deer densities on the property are estimated to be 9.9 (SE 0.91) individuals/km2, and that antlered bucks make up only 11% (SE 1%) of the population. Bait increased encounter rates of deer by a factor of 3.7, showing that the use of bait can help reduce the length of time that cameras must be operational and may create more precise population estimates due to increased detectability of deer.In conclusion, this dissertation found the game management rights for hunting were important for economic return from hunting on private land, but without regulation may result in negative environmental impacts. Across the United States, hunting contributes significantly to landowner income, especially to properties of larger size classes in rangeland and forestry habitats, which suggests that hunting provides an economic incentive to maintain large unfragmented properties. In the context of California, programs that give landowners greater game management rights in exchange for habitat improvement practices resulted in benefits for landowners and the environment. Finally, this dissertation has developed a statistical model that can be utilized to evaluate population parameters for one of the most economically important game species in California. In sum, recreational hunting can provide income to the private landowner and with the appropriate regulations, education and management, can incentivize the enhancement and maintenance of wildlife habitat on private land.",ucb,,https://escholarship.org/uc/item/0jm6t2jx,,,eng,REGULAR,0,0
26,1462,Quantum Trajectories of a Superconducting Qubit,"Weber, Steven Joseph","Siddiqi, Irfan;",2014,"In quantum mechanics, the process of measurement is intrinsically probabilistic.  As a result, continuously monitoring a quantum system will randomly perturb its natural unitary evolution.  An accurate measurement record documents this stochastic evolution and can be used to reconstruct the quantum trajectory of the system state in a single experimental iteration.   We use weak measurements to track the individual quantum trajectories of a superconducting qubit that evolves under the competing influences of continuous weak measurement and Rabi drive. We analyze large ensembles of such trajectories to examine their characteristics and determine their statistical properties.   For example, by considering only the subset of trajectories that evolve between any chosen initial and final states, we can deduce the most probable path through quantum state space.  Our investigation reveals the rich interplay between measurement dynamics, typically associated with wavefunction collapse, and unitary evolution.  Our results provide insight into the dynamics of open quantum systems and may enable new methods of quantum state tomography, quantum state steering through measurement, and active quantum control.",ucb,,https://escholarship.org/uc/item/0k0687ns,,,eng,REGULAR,0,0
27,1463,Project Planning Algorithms: Lowering Cost and Improving Delivery Time in Capital Projects,"Jabbari, Arman","Kaminsky, Philip;",2020,"With the goal of developing models and approaches leading to better operation of large-scale project delivery supply chains, we interviewed a variety of consultants and project and supply chain managers (with a particular emphasis on oil and gas major capital project delivery) and asked them a set of questions so that we could better understand current capital project delivery views of supply chain management, inventory, risk management tools, and related topics (Appendix A). Our interviewees expressed surprisingly diverse opinions, particularly regarding the future of mega-project delivery and the need for more closely coordinating supplier deliveries with onsite needs.The work in the dissertation is particularly motivated by mega-projects in the oil and gas industry, and our goal is to build models that lead to better operation of capital project delivery supply chains. The characteristics of this industry, and of these projects, place specific requirements on project scheduling models, and many of the existing models in the literature do not meet these requirements. Our focus in this dissertation is to formulate models and develop approaches that are particularly useful for mega-projects in the oil and gas industry, and that enable the concurrent determination of the project schedule and inventory delivery times in order to efficiently manage the project supply chain, and to effectively control project delivery time and cost.We consider the Stochastic Resource-Constrained Project Scheduling Problem with inventory, where the objective is to minimize a weighted combination of the expected project makespan and the expected inventory holding costs. Motivated by the requirements of major oil and gas industry projects, we introduce a class of proactive policies for the problem. We develop several effective heuristics for this problem, as well as deterministic and probabilistic lower bounds on the optimal solution. In computational testing, we demonstrate the effectiveness of these heuristics and develop insights into the value of explicitly considering inventory in this setting.A related problem that arises is the scheduling of oil field drilling operations, where the goal is to maximize the expected revenue generated by oil extraction. For this problem, we build a model and propose a heuristic approach. We confirm the effectiveness of our heuristic approach by analyzing its performance compared to the current practice in a real-world case study. Our results demonstrate the potential to increase the efficiency and productivity of drilling operations significantly and to boost profitability by decreasing the time until wells start the extraction.Through our interviews, and through analysis in subsequent models, it is clear that suppliers, and the timely delivery of supplies, plays a critical role in the successful implementation of large-scale projects. In the context of oil and gas projects, our focus is on the suppliers that provide customized materials. While the bulk of this dissertation focuses on projects from the project plannerâ€™s point of view, we were also motivated to consider these problems from the perspective of the supplier, and hence, to consider scheduling models with due dates. We present a stylized model, where we consider sequencing decisions on a single processor, here representing a supplier, in an online setting where no data about the future incoming opportunities is available. With the goal of minimizing total weighted (modified) earliness and tardiness cost, we introduce a new scheduling policy, which we refer to as the list-based delayed shortest processing time policy, and develop lower and upper bounds on the performance of this policy.Finally, we consider an alternative view of managing construction in projects, a location-based method known as the Work Density Method for takt planning. Given a work space and the number of zones in which to divide that space, the so-called WoLZo problem is to identify the shape and dimensions of each zone while minimizing the peak in the tradesâ€™ workloads per zone. We model this problem and develop an optimization algorithm to divide a work space into zones while leveling work densities across trades in a process.The tools presented in this dissertation are useful for managing different elements of mega-projects and significantly advance the state-of-the-art in those areas. We confirm the effectiveness of these tools by analyzing their performance compared to current practice in real-world case studies as well as their performance over the benchmark test problems that are available in the literature.",ucb,,https://escholarship.org/uc/item/0xt4s766,,,eng,REGULAR,0,0
28,1464,Risk Management and Combinatorial Optimization for Large-Scale Demand Response and Renewable Energy Integration,"Yang, Insoon","Tomlin, Claire J;",2015,"To decarbonize the electric power grid, there have been increased efforts to utilize clean renewable energy sources, as well as demand-side resources such as electric loads. This utilization is challenging because of uncertain renewable generation and inelastic demand. Furthermore, the interdependencies between system states of power networks or interconnected loads complicate several decision-making problems. Growing interactions between power and energy systems and human agents with advances in sensing, computing and communication technologies also increase the need for personalized operations.In this dissertation, we present three control and optimization tools to help to overcome these challenges and improve the sustainability of electric power systems. The first tool is a new dynamic contract approach for direct load control that can manage the financial risks of utilities and customers, where the risks are generated by uncertain renewable generation. The key feature of the proposed contract method is its risk-limiting capability, which is achieved by formulating the contract design problem as mean-variance constrained risk-sensitive control. To design a globally optimal contract, we develop a dynamic programming solution method based on a novel dynamical system approach to track and limit risks. The performance of the proposed contract framework is demonstrated using data from the Electricity Reliability Council of Texas. The second tool is developed for combinatorial decision-making under system interdependencies, which are inherent in interconnected loads and power networks. For such decision-making problems, which can be formulated as optimization of combinatorial dynamical systems, we develop a linear approximation method that is scalable and has a provable suboptimality bound. The performance of the approximation algorithm is illustrated in ON/OFF control of interconnected supermarket refrigeration systems. The last tool seeks to provide a personalized control mechanism for electric loads, which can play an important role in demand-side management. We integrate Gaussian progress regression into a model predictive control framework to learn the customer's preference online and automatically customize the controller of electric loads that directly affect the customer's comfort. Finally, we discuss several future research directions in the operation of sustainable cyber-physical systems, including a unified risk management framework for electricity markets, a selective optimal control mechanism for resilient power grids, and contract-based modular management of cyber-physical infrastructure networks.",ucb,,https://escholarship.org/uc/item/0zh6b007,,,eng,REGULAR,0,0
29,1465,"Ecological Restoration for Community Benefit: People and Landscapes in Northern California, 1840-2010","Diekmann, Lucy Ontario","Huntsinger, Lynn;",2011,"Restoration has important ecological work to do, particularly maintaining biological diversity and repairing impaired ecological functions. In addition, many people anticipate and hope that restoration will also produce changes in and provide benefits to human communities. Although these expectations are widespread, relatively little is known about how well restoration projects achieve their goals generally, and even less about the social and cultural consequences of restoration work.This dissertation draws on the experiences of two communities in northwestern California--the American Indians and non-Indians who are part of the United Indian Health Services (UIHS) and the resource managers, scientists, and landowners who work together to implement restoration projects throughout Humboldt County--to explore the impact of ecological restoration on human communities that undertake, use, or are home to restoration projects. I used qualitative interviews along with a review of historical and contemporary documents to develop an understanding of restoration goals and outcomes that is grounded in the experiences of UIHS community members and members of the broader Humboldt County restoration community.UIHS community members share a vision of restoration that is rooted in cultural understandings of the relationship between people and the environment and in historical changes to the local landscape and American Indian communities that have affected their ability to enact this relationship and to apply key cultural values. In the contemporary cultural landscapes of northwestern California, UIHS community members' access to culturally significant places and natural resources is restricted. Restoration offers one way to restore a role for American Indians in the landscape through active management, traditional activities, and applications of cultural knowledge. I find that the process of restoring and using the Ku' wah-dah-wilth Restoration Area has had at least six outcomes that contribute to community wellbeing. These are: encouraging healthy behaviors; offering opportunities for cultural and environmental education; serving as a source of inspiration; facilitating community interaction; providing a culturally meaningful place that produces a range of positive emotional responses; and acting as positive symbol of living American Indian cultures. However, the Restoration Area's potential for meaningful change is constrained at present by the limited number of people who access the site or receive information about it and the relatively small number of opportunities to actively engage with the site.Members of the Humboldt County restoration community are also motivated by the hope that restoration will benefit communities culturally and economically. Although restoration contributes significantly to the county's economy and has led to relationship building and improved knowledge about local ecosystems, general uncertainty about restoration's community impacts suggests that restoration goals are not necessarily reflected in restoration outcomes. Taken together the experiences of these two communities indicate that restoration has a range of social and cultural outcomes. They also suggest that more effectively realizing cultural and social goals will take active planning, engagement with the broader political and social forces that have contributed to current conditions, ongoing involvement with restored sites to create opportunities for education and use, monitoring and evaluation of social outcomes, and attention to who is and who is not benefitting from restoration.",ucb,,https://escholarship.org/uc/item/1365d8cr,,,eng,REGULAR,0,0
30,1466,Development and Application of Oxidative Coupling Bioconjugation Reactions with ortho-Aminophenols,"Obermeyer, Allie","Francis, Matthew B;",2013,"The synthetic modification of proteins plays an important role in the fields of chemical biology and biomaterials science. As applications of protein-based materials continue to become more complex, improved methods for the covalent modification of proteins are needed. Although many methods for the modification of native and artificial amino acids exist, they often require long reaction times or lengthy syntheses of reactive substrates. This work describes the development and application of a suite of bioconjugation reactions that utilize ortho-aminophenols. The oxidative coupling of aniline residues with o-aminophenol substrates was optimized. Potassium ferricyanide was identified as an alternative, mild oxidant for this coupling. These new conditions enabled the use of the oxidative coupling reaction in the presence of free cysteines and glycoslated substrates. Aminophenols were also discovered to react with native residues on protein substrates in addition to artificial aniline moieties. Cysteine and the N-terminus were identified as the reactive residues. The oxidative coupling of o-aminophenols with the N-terminus was optimized to achieve high levels of modification on peptide and protein substrates. The oxidative coupling of anilines and o-aminophenols was applied to the synthesis of a targeted, virus-like particle and to the detection of protein tyrosine-nitration. Overall, these updated and novel oxidative coupling methods expand the utility ortho-aminophenols for the modification of proteins.",ucb,,https://escholarship.org/uc/item/15p1m5m0,,,eng,REGULAR,0,0
31,1467,Effects of Market Approaches to Green Technologies for the Poor: The Case of Improved Cookstoves,"Booker, Kayje Merrea","Huntsinger, Lynn;",2011,"""Sustainable"" or ""green"" technologies for the global poor have been proposed as solutions to the difficult problem of how to improve the lives of the world's poorest without contributing to climate change or other environmental catastrophes. While such technologies were once the domain of non-profit and government funded initiatives, theyare now increasingly developed and deployed through market mechanisms. Using improved biomass cookstoves as a representative technology, this dissertation seeks to assess the social and technological effects of this shift to market-based approaches for development and dissemination of sustainable technologies for the poor.Chapter 2 uses a Science and Technology Studies theoretical framework to follow the coproduction of the material form of improved biomass cookstoves and the cookstove movement from the 1960s to the present. The chapter shows that during the 1980s, particular conceptions and articulations of the problem that cookstoves were meant to solve led to a definition of technological ""improvement"" that included fuel efficiency, consistency of performance, and ability to scale quickly. This particular type of cookstove was much more compatible with mass-production than traditional artisanal production, creating social organizations that could mass-produce cookstoves, which then encouraged commercial approaches in order to recover costs. The move to a market-based approach was in part driven by and in part the cause of a particular kind of technology, demonstrating the mutual coproduction of the social and technological.Chapter 3 takes one market-based tool, intellectual property, and analyses the effect of deploying it in the realm of green technologies for the poor. Using the contrasting cases of UV Waterworks and the Berkeley-Darfur Stove the chapter identifies some of the salient social and technical characteristics that determine whether such effect is positive. The complex social arrangements involved in developing technologies for the poor mean that tools such as intellectual property can be useful but must be compatible with the organizations involved at the level at which the tool is targeted, each of which may have different orientations and incentives. The type of funding at each level, donor versus investor, appears to be a particularly important variable in predicting positive or negative outcomes.Chapter 4 examines one specific environmental policy market mechanism, the carbon market, and its role in stimulating technological change, invention, innovation, and dissemination (Schumpeter, 1942) in biomass cookstoves. It shows that carbon credits are thus far improving diffusion of current cookstoves but failing to stimulate innovation in cookstoves with stronger health and environmental impacts. Additionally, the chapter shows that the carbon market is influencing the selection of cookstoves for dissemination. The characteristics selected for are most compatible with centralized, mass production, which is likely to strengthen the shift towards these approaches.",ucb,,https://escholarship.org/uc/item/16v148bg,,,eng,REGULAR,0,0
32,1468,Seiberg-Witten and Gromov invariants for self-dual harmonic 2-forms,"Gerig, Chris","Hutchings, Michael;",2018,"For a closed oriented smooth 4-manifold X with $b^2_+(X)>0$, the Seiberg-Witten invariants are well-defined. Taubes' ""SW=Gr"" theorem asserts that if X carries a symplectic form then these invariants are equal to well-defined counts of pseudoholomorphic curves, Taubes' Gromov invariants. In the absence of a symplectic form there are still nontrivial closed self-dual 2-forms which vanish along a disjoint union of circles and are symplectic elsewhere. This thesis describes well-defined counts of pseudoholomorphic curves in the complement of the zero set of such near-symplectic 2-forms, and it is shown that they recover the Seiberg-Witten invariants (modulo 2). This is an extension of Taubes' ""SW=Gr"" theorem to non-symplectic 4-manifolds.The main results are the following. Given a suitable near-symplectic form w and tubular neighborhood N of its zero set, there are well-defined counts of pseudoholomorphic curves in a completion of the symplectic cobordism (X-N, w) which are asymptotic to certain Reeb orbits on the ends. They can be packaged together to form ""near-symplectic"" Gromov invariants as a map on the set of spin-c structures of X. They are furthermore equal to the Seiberg-Witten invariants with mod 2 coefficients, where w determines the ""chamber"" for defining the latter invariants when $b^2_+(X)=1$.In the final chapter, as a non sequitur, a new proof of the Fredholm index formula for punctured pseudoholomorphic curves is sketched. This generalizes Taubes' proof of the Riemann-Roch theorem for compact Riemann surfaces.",ucb,,https://escholarship.org/uc/item/1730608x,,,eng,REGULAR,0,0
33,1469,Pathway and organelle engineering for production of useful chemicals in yeast,"Grewal, Parbir","Dueber, John E;Clark, Douglas S;",2020,"Researchers in the field of metabolic engineering aim to develop processes to produceuseful chemicals, sustainably and responsibly, using biotechnology. These processes areoften designed to replace products derived from fossil fuels, which are unsustainable andcontribute to climate change, or plant-based products, which compete with foodproduction for scarce land and are subject to supply uncertainty due to weather, cropdisease, and climate change. Here, we present two research projects in metabolicengineering. First, we demonstrate microbial production of the red food dye betanin byengineering the betalain biosynthesis pathway into yeast. Betanin is currentlymanufactured through extraction from red beets specifically grown for dye production.We achieved betanin production levels of 17 mg/L, which is equivalent to the amount ofbetanin found in 10 g/L of beet extract. With further production improvements, thisbioprocess may become cost-competitive with agricultural production and is likely tolead to a purer product. We also demonstrate the synthesis of a suite of non-naturalbetalain dyes achieved through feeding of diverse amines to a yeast production host,including several which have never been reported. In the second research project, wediscover that an enzyme that limits production levels of a drug family is toxic to the yeastproduction host. This enzyme, norcoclaurine synthase, is critical to the production ofbenzylisoquinoline alkaloids, an important family of medicines that are extracted fromplants like the opium poppy. We devised a novel subcellular compartmentalizationstrategy, sequestering norcoclaurine synthase in the peroxisome to alleviate cytotoxicitywhile maintaining access to the enzymeâ€™s substrates. By targeting norcoclaurine synthasefor organellar compartmentalization, we achieved improved cell growth, final titer, andculture productivity. These projects highlight the potential of engineering complex plantpathways into microbial hosts for economical and sustainable chemical production.",ucb,,https://escholarship.org/uc/item/17p341hd,,,eng,REGULAR,0,0
34,1470,Slow Photoelectron Velocity-Map Imaging of Transient Species and Infrared Multiple Photon Dissociation of Atmospherically Relevant Anion Clusters,"Yacovitch, Tara Irene","Neumark, Daniel M;",2012,"Two different types of vibrationally resolved spectroscopies are used in the experimental study of reactive species: slow-electron velocity map imaging (SEVI) and infrared multiple photon dissociation (IRMPD).SEVI spectroscopy is used to study the series of vinoxy and substituted vinoxy radicals: vinoxy (H2C=CH-O), i-methylvinoxy (H2C=C(-O)-CH3) and n-methylvinoxy (H3C-HC=CH-O). Vibrational resolution of their ground and first excited electronic states is achieved, leading to accurate measurement of electron affinities, term energies and vibrational frequencies. Radical geometries are deduced and conformational isomers for the larger species are identified. The i-methylvinoxy radical is found to be most stable when the methyl substituent is eclipsed in the ground-state radical and staggered in the excited state radical and ground state anion. Both cis and trans isomers of the n-methylvinoxy radical are observed, with the lower-energy cis isomer contributing to most of the spectral peaks. The SEVI experiment is also used to study the transition state region of the F + H2 and F + CH4 reactions. The F + H2 results improve on previous spectra, resolving narrow features and suggesting that additional theoretical treatment is necessary to fully describe and assign the experimental results. The entrance valley of the F + CH4 reaction coordinate is measured, showing extended structure attributed to bending or hindered rotation of the methane moiety. The significance of these results in terms of reactive resonances is discussed.The final SEVI experiments involve a series of alkoxy radicals and their sulfur-substituted analogs: methoxy (CH3O), thiomethoxy (CH3S), ethoxy (CH3CH2O),  thioethoxy (CH3CH2S), i-propoxy ((CH3)2CHO) and n-propoxy (CH3CH2CH2O). The two lowest electronic states are close in energy (or formally degenerate) leading to a slew of nonadiabatic effects such as vibronic coupling from the Jahn-Teller or pseudo-Jahn-Teller effect and spin-orbit splitting. Precise determinations for the electron affinities and splittings between the electronic states are made. Variation of the size, symmetry and O/S atoms significantly affects the potential energy landscape of these radicals, leading to drastically altered spectra governed by differing contributions of the various nonadiabatic effects. IRMPD spectra of negatively charged cluster species containing inorganic acids and water are studied, revealing structural information and size-dependent trends. The small bisulfate-water clusters, HSO4-(H2O)n, show lengthening of the acidic bond in the bisulfate anion, H-OSO3-. This is observed through the characteristic SOH bending vibration. The small mixed clusters of sulfuric and nitric acid, HSO4-(HNO3), NO3-(H2SO4)(HNO3) and HSO4-(H2SO4)(HNO3), show charge localization effects that in some cases counter the structural assumptions made based on the gas phase acidities of the molecular acids. Finally, the clusters containing bisulfate, sulfuric acid and water, HSO4-(H2SO4)m(H2O)n show the recurrence of the triply hydrogen-bound HSO4-(H2SO4) configuration for n = 0, while incorporation of water disrupts this stable motif for clusters with m > 1.",ucb,,https://escholarship.org/uc/item/1862c66d,,,eng,REGULAR,0,0
35,1471,Variability Modeling and Statistical Parameter Extraction for CMOS Devices,"Qian, Kun","Spanos, Costas J;",2015,"Semiconductor technology has been scaling down at an exponential rate for many decades, yielding dramatic improvements in power, performance and cost, year after year. Todayâ€™s advanced CMOS transistors have critical dimensions well below 24nm. This means that controlling the manufacturing process is increasingly difficult. Process and material fluctuations cause device and circuit characteristics to deviate from design goals, and introduce significant device-to-device variability due to spatial variations across silicon wafers. Accurate modeling of these spatial process variations has become critical to both foundries and circuit designers that seek optimal power/speed/area balance. To understand the nature of spatial process variations, we first carried out a comprehensive variability analysis of data measured from thousands of variability-sensitized test structures, including ring oscillators, SRAM bit cells and their internal transistors. We manufactured these test chips using early stage 90nm and 45nm commercial semiconductor processes. We proposed a hierarchical variability model to capture the systematic and random components of device parameter variations across silicon wafers, and across chips. The detailed decomposition of the process variation profile reveals significant across-wafer systematic component for the delay and leakage of ring oscillators, and across-chip systematic component for the read/write margins of SRAM bit cells, as well as their internal transistors. The proper modeling of each hierarchical component proved to be crucial for the accurate estimation of the statistics of device performance distribution and its parametric yield.The knowledge gained about process variation from carefully designed test structures was leveraged into estimating the variation and parametric yield of new devices and circuits. This was accomplished by improved the statistical compact model parameter extraction methodology, and by proposing a stepwise parameter selection method. We used a normalized notional confidence interval and, and the sum of squares of fitting residuals as extraction and fitting quality criteria. This allowed us to determine the essential model parameters for accurate fitting over a large number of transistors. We applied this methodology to EKV and PSP with both simulated and experimental data, demonstrating its effectiveness. Finally, we combined the results from statistical parameter extraction with the hierarchical spatial variability model. This, compared to traditional methods, produced much-improved estimates of device performance and manufacturing yield.",ucb,,https://escholarship.org/uc/item/19x656kn,,,eng,REGULAR,0,0
36,1472,"Gravure-printed electronics: Devices, technology development and design","Grau, Gerd Fritz Milan Nino","Subramanian, Vivek;",2016,"Printed electronics is a novel microfabrication paradigm that is particularly well suited for fabrication of low-cost, large-area electronics on flexible substrates. Applications include flexible displays, solar cells, RFID tags or sensor networks. Gravure printing is a particularly promising printing technique because it combines high print speed with high resolution patterning. In this thesis, gravure printing for printed electronics is advanced on multiple levels. The gravure process is advanced in terms of tooling and understanding of printing physics as well as its application to substrate preparation and device fabrication.Gravure printing is applied to transform paper into a viable substrate for printed electronics. Paper is very attractive for printed electronics because it is low-cost, biodegradable, lightweight and ubiquitous. However, printing of high-performance electronic devices onto paper has been limited by the large surface roughness and ink absorption of paper. This is overcome here by gravure printing a local smoothing layer and printed organic thin-film transistors (OTFTs) are demonstrated to exhibit performance on-par with device on plastic substrates.If highly-scaled features are to be printed by gravure, traditional gravure roll making techniques are limited in terms of pattern definition and surface finish. Here, a novel fabrication process for gravure rolls is demonstrated utilizing silicon microfabrication. Sub-3Î¼m features are printed at 1m/s. Proximity effects are demonstrated for more complex highly-scaled features. The fluid mechanics of this effect is studied and it is suggested how it can be used to enhance feature quality by employing assist features.Finally, advancements are made to printed organic thin-film transistors as an important technology driver and demonstrator for printed electronics. First, a novel scanned thermal annealing technique is presented that significantly improves the crystallization of an organic semiconductor and electrical performance. Second, transistors are fully gravure printed at a high print speed of 1m/s. By scaling both lateral and thickness dimensions and optimizing the printing processes, good electrical performance, low-voltage operation and low variability is demonstrated.",ucb,,https://escholarship.org/uc/item/1bn3t372,,,eng,REGULAR,0,0
37,1473,Actomyosin mediated tension orchestrates thermogenic programs in adipocytes,"Tharp, Kevin Menard","Stahl, Andreas;",2017,"Innovative approaches to shift energy balance are urgently needed to combat metabolic disorders such as obesity and diabetes. One promising approach has been the expansion or activation of thermogenic adipose tissues to improve metabolic homeostasis. My doctoral studies presented in the following text have identified novel approaches to translate adipose based metabolic therapeutics and the underlying mechanisms by which thermogenic adipocytes establish their therapeutically applicable metabolic capacity.In chapter I, I present a novel biomaterial technology optimized to expand metabolically beneficial thermogenic adipose depots in vivo. This system enabled me to determine the degree of metabolic enhancement possible with the exogenous expansion of thermogenic adipose depots. To generate therapeutic adipose implants I modified hyaluronic acid-based hydrogels to support the differentiation of white fat derived multipotent stem cells (ADMSCs) into lipid accumulating, uncoupling protein 1 (UCP1) expressing thermogenic adipocytes. Subcutaneous implantation of the synthetic tissues successfully attracted host vasculature and persisted for several weeks and the implant recipients demonstrated elevated core body temperature during cold challenges, enhanced respiration rates, improved glucose homeostasis, and reduced weight gain demonstrating the therapeutic merit of this highly translatable approach.In chapter II, I outline the experimentation leading to the discoveries presented in chapter III as well as thoroughly review pertinent tissue engineering strategies. Specifically, I sought to define the mechanism by which synthetic ECM components identified in chapter I could alter differentiation outcomes of preadipocytes to yield greater thermogenic capacity. In chapter III, I demonstrate that actomyosin-based mechanical responses provide a critical differentiation cue for the development of thermogenic adipocytes. Since I had determined that the hydrogel optimization techniques described in chapter I were likely acting through cytoskeletal-mediated processes I examined the role of cytoskeletal structure and tension in thermogenic adipose development. I identified that the muscle-like gene expression patterns of UCP1+ adipocytes are critical for the acute induction of oxidative metabolism and uncoupled respiration and regulate mechanosensitive transcriptional co-activators, YAP/TAZ, that control thermogenic gene expression.This dissertation establishes the role of physical mechanics in the development and function of thermogenic adipocytes which may engender future metabolic therapeutics.",ucb,,https://escholarship.org/uc/item/1g26d365,,,eng,REGULAR,0,0
38,1474,"""Other Lovings"": Abjection, Love Bonds, and the Queering of Race","Lee, Seulghee","JanMohamed, Abdul;",2014,"This dissertation discusses the intersection of racial abjection and love bonds in late 20th-century and 21st-century African-American and Asian-American literature and culture. The manuscript deploys affect studies and queer theory to discuss works by Audre Lorde, Amiri Baraka, David Henry Hwang, Adrian Tomine, and Gayl Jones, in addition to the cultural phenomena of ""Linsanity"" and ""afro-pessimism."" Whereas most critical readings of failed love in minority literature have emphasized the tragic interpersonal consequences of internalized racism, this dissertation argues that these writers narrate love's apparent failure in order to explore the positive content emergent in the felt rupture of breakups. Through readings of dissolved love relationships in these authors' works, I inquire into love's operation as an affect that always desires more and better sociality. The appearance of love's failure is precisely what illuminates the ineluctably positive content of love, and I situate this content in the context of recent theoretical discussions of love as narcissistic, not-yet-here, oppressive, or antisocial. The project ultimately argues that blackness, yellowness, and queerness share a privileged access to and familiarity with love's affective positivity.",ucb,,https://escholarship.org/uc/item/0mw1p6xm,,,eng,REGULAR,0,0
39,1475,Phonological Encoding and Phonetic Duration,"Fricke, Melinda",,2013,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0q71t6xk,,,eng,REGULAR,0,0
40,1476,A Convenient Partnership: The Ribosome and the Nascent Chain Interact to Modulate Protein Synthesis and Folding,"Goldman, Daniel Hershel","Bustamante, Carlos J;",2015,"During translation, the ribosome reads the genetic code of the messenger RNA, adding one amino acid at a time to the nascent protein. The sequence of the polypeptide determines the three dimensional structure of the natively folded protein, and thus encodes its biological activity. Because folding rates are often fast compared to translation, many proteins likely undergo folding transitions during synthesis, with folding potentially modulated by the sequential appearance of the polypeptide and the chemical environment of the ribosome. Traditional experimental approaches to study protein folding employ chemical, temperature or pH-induced denaturation and would irreversibly destroy the ribosome; thus, such techniques cannot be used to probe folding on the ribosome. In this work, we implement a novel single-molecule optical tweezers assay to probe folding transitions of the nascent polypeptide as it emerges from the ribosome. We demonstrate that the ribosome can modulate the kinetics of folding through interactions between the nascent chain and the charged ribosomal surface. Additionally, the ribosome can prevent misfolding of incompletely synthesized protein fragments. These observations point to a chaperone-like role for the ribosome in guiding the nascent protein to its native state.In addition to interacting with the exterior of the ribosome, some nascent chain sequences can form specific contacts with the ribosome exit tunnel. These contacts lead to conformational changes of the ribosome, and reduced translation rates. The Secretion Monitor protein stalls the ribosome upon translation of a 17 amino acid motif. Arrest release requires targeting of the stalled ribosome-nascent chain complex to the translocon; thus, it has been hypothesized that arrest is released by a mechanical pulling force generated as the polypeptide is translocated across the membrane. By applying force to the nascent polypeptide of stalled ribosomes, we demonstrate that translation arrest at SecM is released by mechanical force. Additionally, we show that the force needed to release stalling can be generated by a protein folding in close proximity to the ribosome tunnel exit. Our results demonstrate the feasibility of a feedback mechanism, whereby a folding protein can modulate its synthesis through the generation of force. More generally, since the nascent polypeptide in the cell can undergo a number of potentially force-generating eventsâ€“chaperone binding, protein or ligand binding, translocation and membrane insertionâ€“force applied to the nascent chain may be an important modulator of protein synthesis.",ucb,,https://escholarship.org/uc/item/0rh988bb,,,eng,REGULAR,0,0
41,1477,"An Athenian Commentary on Plato's Republic: Poetry, science and textual engagement in Proclus' In Rem.","Pass, David Blair","Long, Anthony;Ferrari, G.R.F.;",2013,"Proclus' Commentary on Plato's Republic is the only extant ancient Greek commentary on Plato's Republic.  Despite the fact that it includes discussions of most of the major parts of the book, it has received very little scholarly attention.  This dissertation introduces the work in its entirety and tries to identify some of the most important contributions it can make to philosophical and philological scholarship on the Republic.  I am particularly attentive to ways in which Proclus' concerns--such as responding to Epicurean critiques of Platonic myth or defending Homer--may help us see Plato's work in its cultural context.The first chapter focuses on introducing the work and answering basic questions about the place of the Republic in late antique Platonism, the extent of Proclus' sources and what portions of the Republic Proclus discusses.  I consider the form of the commentary, arranged as various essays, in comparison with Proclus' other commentaries which proceed in a line by line manner.  I respond to arguments that have claimed that the commentary is not a unified work by considering the form and extent of the essays relative to the content of the Republic.The second chapter argues that Proclus' commentary is not trimming the Platonic tradition to fit into the religious orthodoxy of late antiquity but rather stressing arguments and interpretative approaches that became most influential in the Renaissance.  I consider several examples such as Proclus' interest in the Orphic and Pythagorean tradition, his emphasis on gender equality and the scientific aspects of his approach to natural philosophy.The third chapter considers some important aspects of Proclus' hermeneutics.  I consider how and why Proclus sometimes disagrees with Plato.  In particular, I focus on some portions of the commentary that demonstrate Proclus' approach to the dramatic aspects of the dialogues and discuss why Proclus' defence of Homer includes some observations about his Platonic hermeneutics.  I consider also his responses to Aristotle's idea of catharsis and his approach to Glaucon's role in the Republic.The fourth chapter translates and discusses a particular portion of the sixth essay in which Proclus argues, contrary to the view Socrates expresses in the Republic, that Homer is a text which teaches the political virtue of sophrosune.  I consider the historical origins of allegorizing interpretations and then distinguish between Proclus' use of allegory and his use of other interpretative methodologies.  I consider in particular Proclus' defence of the idea of euphrosune and compare his approach with earlier philosophical discussions which responded to the same passage of Homer (Odyssey 9.6-10) and interrogated the passage along the lines suggested in the Republic.",ucb,,https://escholarship.org/uc/item/0sn280zv,,,eng,REGULAR,0,0
42,1478,Impacts of Cloud Microphysics on Extreme Precipitation and Lightning,"Charn, Alexander Benedict","Collins, William D;",2020,"The microphysical processes involving water droplets and ice crystals in clouds are too small to be explicitly simulated by climate and weather models. Nevertheless, they play a critical role in the large-scale energy balance of the Earth and its atmosphere, as well as smaller-scale phenomena such as storms. This dissertation examines the impact of microphysics on the latter, specifically extreme precipitation and lightning. Climate change threatens to exacerbate such events, making the understanding of such extremes crucial.We focus primarily on the effects of microphysical processes as they are simulated in a superparameterized climate model, which is better suited to studying clouds and the associated extreme weather events than conventional models. We find statistically significant differences in extreme precipitation rates via two separate mechanisms when replacing one commonly used microphysics parameterization with another. We also find that the sign of changes in lightning flash rates with global warming depends on the microphysics representation used. Finally, we employ observations to address a longstanding question about the necessity of ice as a precursor of lightning. With the data available it is concluded that there is insufficient evidence to suggest that thunderstorm electrification can occur in the absence of ice.",ucb,,https://escholarship.org/uc/item/0sw3x7qx,,,eng,REGULAR,0,0
43,1479,"Cities on the Periphery: Urbanization in Bithynia, Pontus, and Paphlagonia under the Roman Empire","PITT, ERIN MIKAEL","NoreÃ±a, Carlos;",2016,"This dissertation, entitled â€œCities on the Periphery: Urbanization in Bithynia, Pontus, and Paphlagonia under the Roman Empire,â€ seeks to provide the first comprehensive urban history of the region during the period of Roman rule. Modern scholarship on this region has focused on cultural and political topics, including Greek reactions to Roman rule; provincial elites and euergetism; and urban life. This scholarship has ignored dramatic increases in the number of new settlements in north central Anatolia, urban and rural, as well as consistent vitality and even growth during the turbulent 3rd century CE. I address these lacunae and investigate the factors behind this growth and stability. I analyze the complexities of this development across four frameworks: the construction and finance of civic monuments, shifting settlement patterns, the extent of bulk and prestige goods networks, and integration into networks of administration, military affairs, and imperial ideology.The introductory first chapter documents the dramatic increases in the number of urban and rural settlements in the region and poses a set of key questions regarding urbanization, imperial intervention, and local stability. I then set out the methodology of my dissertation. I briefly review and critique previous scholarship on this region, which has focused mainly on cultural and political topics of urban and imperial life. I then indicate the advantages of shifting the focus to consider the diachronic nature of urbanization over the long term, the archaeological record, integration and connectivity, and interpretive questions that address the uniqueness of the region. My approach is highly interdisciplinary, making heavy use of evidence from archaeological surveys, epigraphic finds, and network theory, as well as ancient literary and historical accounts.The second chapter examines how local preferences and financial resources influenced the construction and use of civic monuments. The emphasis on Graeco-Roman cities as lived environments, not synchronic monumental landscapes, plays a critical role in this analysis. My discussion qualifies recent assertions that cities in the eastern empire expressed their Greek identity by building democratic monuments with public money. Monuments such as theaters and temples are clearly prioritized, yet cities also enthusiastically adopted monuments marked as Roman, such as baths, or used democratic structures for Roman entertainment. Though civic funds remained a consistent resource, the patronage of local elites and the emperor were essential in the 1st and later 3rd and 4th centuries, respectively. The third chapter synthesizes five decades of archaeological survey. I identify broad trends in expansion, size, and continuity from the Iron Age to the Late Roman period and assess the extent of Roman influence behind these fluctuations. Administrative, economic, and military priorities guided the efficient management of this region. This was achieved by the creation of a few new cites and by an extensive road network. Both constituted unique developments and indirectly encouraged the proliferation of small towns and villages, which benefitted from the demands of regional capitals and access to roads. This produced a balanced urban system that fashioned a robust administrative hierarchy, but that was relatively moderate in overall urban density. The fourth and fifth chapters discuss connectivity across a range of landscapes: city and hinterland, the Black Sea area, and the Mediterranean basin as a whole. The third chapter focuses on the circulation of staple goods and luxury items. This area was remarkably well integrated and even self-sufficient at the local and regional levels. Its position on the periphery of the Roman empire limited intensive contact with the broader Mediterranean, but encouraged intensive commercial relationships with the Black Sea, Armenia, and Syria. The fourth chapter also examines connectivity, but in the context of imperial administration, communication, and military activity.This project ultimately seeks to provide the first comprehensive synthesis of the urban history of north central Anatolia in the Roman period. Roman intervention and traditional urban ideals were early stimuli; as I argue, however, regional preferences, a geographical position on the Mediterranean periphery, and heightened imperial interests in the 3rd century were the most prominent influences on urban development and stability in north central Anatolia. The region occupied a unique geographical, political, and economic position within the Roman empire and it represents a compelling contrast to the urban character of other Roman provinces. I conclude by stressing the complexity of the urban development of this region as well as the strong role that local traditions and geographical position played in negotiating imperial interaction.",ucb,,https://escholarship.org/uc/item/0t88x6h9,,,eng,REGULAR,0,0
44,1480,Syntactic Agreement in Bilingual Corpora,"Burkett, David","Klein, Dan;",2012,"The task of automatic machine translation (MT) is the focus of a huge variety of active research efforts, both because of the intrinsic utility of this difficult task, and the theoretical and linguistic insights that arise from modeling relationships between natural languages. However, MT systems that leverage syntactic information are only recently becoming practical, and in a typical system of this sort, syntactic information is generated by monolingual parsers; the task of explicitly modeling syntactic relationships between target and source languages is yet to be fully explored.This thesis investigates the problem of finding syntactic parse trees of target and/or source sentences that are more appropriate for use in a syntactic MT system. Two basic methodologies are explored.First, we present a sequence of two statistical models that leverage bilingual information to improve the linguistic quality of syntactic parses, as measured by their ability to replicate human-generated gold-standard annotations. The first model uses word to word alignments as an external source of information, while the second models the alignments jointly. These models are both quite effective at improving the intrinsic quality of the parse trees, and the second model additionally improves word alignment performance. However, while the two models achieve similar parsing improvements, we find that improving parses in conjunction with word alignments is much more helpful for the downstream machine translation task.In the next part of the thesis, we explore this finding further by investigating the effects on MT performance of agreement between parse trees and word alignments. We present a simple method for transforming input trees in a way that ignores gold-standard annotations, concentrating instead on improving syntactic agreement directly. In experiments, we find that though we obviously lose fidelity to more linguistically informed treebank annotation guidelines, this transformation-based approach yields the strongest improvements in syntactic machine translation.",ucb,,https://escholarship.org/uc/item/0k07m9zb,,,eng,REGULAR,0,0
45,1481,Wetland water flows and interfacial gas exchange,"Poindexter, Cristina Maria","Variano, Evan A;",2014,"The flow of water in wetlands may exert significant influence on wetland biogeochemistry, and specifically interfacial gas exchange.  Measuring currents in wetlands requires caution.  The acoustic Doppler velocimeter (ADV) is widely used for the characterization of water flow and turbulence.  However, deployment of ADVs in low-ï¬‚ow environments is hampered by a unique source of bias related to the ADV's mode of operation.  The extent of this bias is revealed by Particle image velocimetry (PIV) measurements of an ADV operating in quiescent fluid.   Image-based flow measurement techniques such as PIV may provide improved accuracy in low-flow environments like wetlands.  Such techniques were applied to observe wind-driven flows in a wetland with emergent vegetation and investigate the effects of the wind shear on gas transfer across the air-water interface.  Wind speed is the parameter most often used to model interfacial gas exchange in other aquatic environments.  In wetlands with emergent vegetation, the emergent vegetation will attenuate wind speed above the water surface, modify fluid shear at the water surface, and influence stirring beneath the water surface.  Direct measurements of gas transfer in a model wetland in the laboratory indicated that unless wind speeds are extreme, interfacial gas transfer in wetlands is typically dominated by another physical force: surface cooling-induced thermal convection.  In an application of these lab results, gas transfer across the air-water interface due to thermal convection in the water column is shown to account for a sizable portion of total methane fluxes from a restored marsh in California's Sacramento-San Joaquin Delta.",ucb,,https://escholarship.org/uc/item/1191m1hx,,,eng,REGULAR,0,0
46,1482,"Nanoscale Optical Devices: Force, Torque and Modulator","Liu, Ming","Zhang, Xiang;",2010,"Manipulating and utilizing light in nanoscale are becoming tasks of not only scientific interest, but also industrial importance. My research includes two major topics in nanoscale optics: 1. Nanoscale optical motor. 2. Optical modulator based on novel materials.Light carries both linear and angular momentum, and therefore generating force or torque with light is feasible. The ability to provide torque in nano-meter scale opens up a new realm of applications in physics, biology and chemistry, ranging from DNA unfolding and sequencing, to active Nano-Electro-Mechanical Systems (NEMS). In the first part of this dissertation, I demonstrate a nano-scale plasmonic structure generating a significantly large rotational force when illuminated with linearly-polarized light. I show that a metallic particle with size of 1/10 of the wavelength is capable of rotating a silica microdisk, 4,000 times larger in volume. Furthermore, the rotation velocity and direction can be controlled by merely varying the wavelength of the incident light, thereby inducing different plasmonic modes which possess different torque directions. The tiny dimensions along with the tremendous torque may have a profound impact over a broad range of applications such as energy conversion, and in-vivo biological manipulation and detection.Compared with the interaction between particles and photons, the optical force between particles is of fundamentally important as well. In the end of the first part, I propose a new technique to measure the optical binding forces between two plasmonic particles. By using localization technique on a build-in cantilever, I prove the potential to measure the force with accuracy up to sub-pico-Newton.The second part of this dissertation is about the modulation of light, an optical modulator. Integrated optical modulator with high modulation speed, small footprint and large optical bandwidth is poised to be the enabling device for on-chip optical interconnects. However, present devices suffer from intrinsic narrow-bandwidth aside from their sophisticated optical design, stringent fabrication, temperature tolerances and large foot print. By using graphene, a monolayer of carbon atoms, I experimentally demonstrated a broadband, high-speed, waveguide-integrated electroabsorption modulator. The extremely strong interaction between light and relativistic electrons in graphene allows us to integrate an optical modulator within an ultra-small footprint while operating at a high speed with broad bandwidth under ambient conditions. Even monolayer of being less than 1 nm in thickness, its modulation effectiveness is comparable with the best materials like Ge and SiGe with tens nanometers. In addition, the athermal optoelectronic properties of graphene make the device immune to harsh operation environments, in sharp contrast to all existing semiconductor approaches.",ucb,,https://escholarship.org/uc/item/12f6b8rg,,,eng,REGULAR,0,0
47,1483,"Total Synthesis of (+)-Spectinabilin, Taiwaniaquinoids, Synthetic Progress toward Aspergillin PZ, and Synthesis of a Photoswitchable Agonist of Glutamate Receptor-dMAG","Xu, Yue","Trauner, Dirk;",2010,"A new and highly enantioselective synthetic route to Î³-methoxypyranone addressed the long unsolved racemization problem in literature. A concise total synthesis of (+)-Spectinabilin was achieved with this method in 10 linear steps. Concept of kinetic resolution with temporary stereocenter was used to improve the enantiomeric excess.A new variant of Nazarov reaction, aromatic Nazarov triflation was discovered which allowed rapid access of polycyclic ring skeleton. The triflation product, indene triflate, was further elaborated with modern palladium cross coupling methods in the total syntheses of many taiwaniaquinoid natural products. Also, the triflation method worked well with electron rich and neutral substrates and was not compatible with electron deficient substrates.Effort toward total synthesis of Aspergillin PZ was described. The biomimetic synthetic hypothesis was pursued. Synthesis of all components was achieved. Future work would be focused on mild reaction condition to bring all components together to for the key intermediate for Aspergillin PZ.Finally, based on the similar principle of previous work in our group, a photoswitchable agonist for metabotropic glutamate receptors was designed and synthesized. Preliminary results confirmed the agonist activity and reversible isomerization with radiation of light of different wavelengths. Further studies are under investigation under collaboration.",ucb,,https://escholarship.org/uc/item/1365h97h,,,eng,REGULAR,0,0
48,1484,"Low Power, Crystal-Free Design for Monolithic Receivers","Wheeler, Bradley","Pister, Kristofer S.J.;",2019,"Predictions of the proliferation of hundreds of billions of connected wireless devices have yet to come true.The economics of such deployments becoming feasible require that current wireless modules become smaller, cheaper, and use less power.A typical wireless device combines a RF System-on-Chip with multiple frequency references, passive components, an antenna, and a battery on a printed circuit board.The Single Chip Mote project aims to reduce the size, weight, power, and cost of these devices by eliminating the off-chip frequency references and passives.The ultimate goal being to form a 2.4 GHz wireless node by attaching only an antenna and energy source to a single CMOS die.Of particular interest is the range of applications this could enable where the size and weight of current wireless devices has prohibited their use.This work implements a crystal-free IEEE 802.15.4 receiver that covers the data path from RF to bits.The receiver utilizes a passive front-end to reduce power and quadrature down-conversion followed by on-chip filtering and digitization.Integrated digital baseband is included for demodulation and clock recovery as well as built-in estimation of the errors in the RF channel frequency and data rate.Initial frequency calibration is performed simultaneously with bootloading using contact-less optical programming.Operation across the 0 - 70 C commercial temperature range has been demonstrated while inter-operating with commercial off the shelf IEEE 802.15.4 devices.The analog portion of the receiver, including the free-running LO, consumes 1.03 mW from a 1.5 V battery while achieving a sensitivity of -83 dBm.",ucb,,https://escholarship.org/uc/item/13f3h6hd,,,eng,REGULAR,0,0
49,1485,"Living Taiwanese Opera: Improvisation, Performance of Gender, and Selection of Tradition","Hsu, Pattie","Wade, Bonnie C.;",2010,"This dissertation investigates the culture and cultural production of itinerant, professional Taiwanese opera performers in Taipei's temple circuit.  I argue that the community of actors and musicians and their occupational and lifestyle practices constitute a subculture that is central to both maintaining and transforming Taiwanese opera.  Drawing on ethnographic research, I characterize the opera subculture's idiosyncratic and fluid features, examine the major ways in which they are manifested--namely in improvisation, performance of gender, and selection of tradition--and discuss the cultural work they perform.Full-time, for-profit troupes--the focus of my research--primarily work for temple patrons in privately contracted performances and occasionally in government-sponsored events.  Performances in the former venue are improvised or, as the performers describe it, ""alive,"" whereas the latter type privileges written practices and marginalizes oral conventions.  I assert that improvisation, a distinctive and crucial attribute in the temple-contracted context, is an imperative performance skill for producing unscripted stories and a professional strategy for adapting to new circumstances.  My analyses of improvisation as a performance skill highlight actor-musician interactions in song performance that shows spontaneous musical processes in opera production.  Improvisation, or the ability to be flexible, is a professional strategy with which performers operate enabling them to maintain the appeal of a traditional art in a rapidly changing cosmopolitan society.  In particular, I argue that the socioeconomic situation in recent decades and the developing hybrid opera style in the temple context opened a space for an alternative model of gender performance, one that expresses female masculinity.  Moreover, improvisation as a professional strategy enables performers to adapt to the demands of recently developed government-sponsored events and participate in a hegemonically-constructed process for selecting a dominant version of the Taiwanese opera tradition.  Through three case studies, I posit that the performers' flexible approach in this process constructs multiple versions of the opera tradition, thereby disrupting authoritative attempts at claiming a singular mode of production.Through these analyses, I suggest that Taiwanese opera is a living tradition with continually shifting conventions and cultural meanings.  The performers rapidly adjust to different and new ways of performance in order to capitalize on opportunities, ensure the cultural relevancy of their creative production, and secure their livelihood.",ucb,,https://escholarship.org/uc/item/1471d868,,,eng,REGULAR,0,0
50,1486,A role for host activation-induced cytidine deaminase in innate immune defense against herpesviruses,"Bekerman, Elena","Coscoy, Laurent;",2013,"Activation-induced cytidine deaminase (AID) is specifically induced in germinal center B cells to carry out somatic hypermutation and class-switch recombination, two processes responsible for antibody diversification. Because of its mutagenic potential, AID expression and activity are tightly regulated to minimize unwanted DNA damage. Surprisingly, AID expression has been observed ectopically during pathogenic infections. However, the function of AID outside of the germinal centers remains largely uncharacterized. This dissertation demonstrates that infection of human primary naive B cells with Kaposi's sarcoma-associated herpesvirus (KSHV) rapidly induces AID expression in a cell intrinsic manner. We find that infected cells are marked for elimination by Natural Killer cells through upregulation of NKG2D ligands via the DNA-damage pathway, a pathway triggered by AID. Moreover, AID impinges directly on the viral fitness by inhibiting lytic reactivation without having a measurable effect on KSHV latency. We extend this analysis to the murine homologue of KSHV, MHV68 and find that AID mutates the viral genome at a rate that exceeds normal somatic mutation by several orders of magnitude. The tremendous mutational load accumulated by sequential passaging of MHV68 through AID-expressing cells leads to the eventual inactivation of the virus. Importantly, we uncover two KSHV-encoded microRNAs that directly regulate AID abundance, further reinforcing the value of AID in the antiviral response. Together our findings reveal an additional role for AID in innate immune defense against herpesviruses with implications for a broader role in innate immunity to other pathogens.",ucb,,https://escholarship.org/uc/item/15150708,,,eng,REGULAR,0,0
51,1487,A Study of Electrolytic Processes in Micro-Electroporation and Electroporation,"Meir, Arie","Rubinsky, Boris;",2015,to be completed,ucb,,https://escholarship.org/uc/item/15p9j6dx,,,eng,REGULAR,0,0
52,1488,Genetically Tuning Cellular Mechanobiology,"MacKay, Joanna Lynn","Kumar, Sanjay;",2013,"The recognition that cells can sense physical cues has inspired numerous investigations into the roles that mechanical forces play in both healthy and diseased cells.  This rapidly growing area of research, often called cellular mechanobiology, has shown that physical interactions between cells and the surrounding extracellular matrix can regulate a number of fundamental cell behaviors.  This insight has prompted the development of new methods to systematically engineer the mechanical properties of extracellular matrices (e.g., rigidity and geometry), both as a way to study the mechanisms behind cellular mechanosensing and as a way to directly control cell behavior in tissue engineering applications.  In contrast, an equally powerful approach for manipulating cell-matrix interactions could be to directly engineer the mechanical properties of the cells themselves by modifying the intracellular signaling pathways that regulate how cells sense and respond to physical cues.  This type of ""inside-out"" strategy would be useful for controlling cell behavior independently from extracellular matrix properties and would allow investigation into how changes in cellular mechanics such as cell shape, stiffness, and contractility can directly alter cell behavior.In these dissertation studies, a genetic strategy was developed to precisely control the mechanical properties and motility of cells by manipulating the activity of cytoskeletal signaling proteins.  Genetic mutants of the signaling proteins RhoA GTPase, Rac1 GTPase, or myosin light chain kinase (MLCK) were introduced into human glioblastoma cells under the control of conditional promoters, thereby enabling graded and dynamic control over their expression through addition and withdrawal of the transcriptional inducers.  Increasing the activity of RhoA or MLCK by inducibly expressing constitutively active (CA) mutants increased both the stiffness and the contractility of cells in a graded manner, which had an inhibitory effect on cell migration.  Interestingly, decreasing RhoA activity through expression of a dominant negative (DN) mutant also produced a graded decrease in cell migration speed, indicating that cell motility varies biphasically with RhoA activity levels.  A similar biphasic dependence was discovered upon varying the activity of Rac1.  These results demonstrate the importance of using quantitative methods to reveal potentially nonlinear relationships between protein activity and cell behavior.Expanding upon this strategy, two orthogonal promoter systems were combined to provide simultaneous control over the activity of two proteins in the same cell.  RhoA and Rac1 are known to suppress each other through crosstalk between their signaling pathways, suggesting that cells can normally have high activity of only one protein or the other.  To investigate the effects of forcing high activation of both proteins, CA RhoA and CA Rac1 were introduced into the same cells under different conditional promoters (either doxycycline-inducible or cumate-inducible).  Expression of CA RhoA did not alter Rac1 activity and vice versa, demonstrating that the activity levels of RhoA and Rac1 can be independently varied with this strategy.  Notably, expressing both CA mutants had a greater inhibitory effect on cell migration than either mutant alone, indicating that the effects of these mutants were additive rather than suppressive.Finally, these orthogonal promoters were used to dynamically control the motility of multiple cell populations in a three-dimensional matrix, providing a new way to spatially pattern cells.  When cells were cultured as multicellular spheroids within a collagen matrix, CA Rac1 expression stimulated cell migration, while DN Rac1 expression strongly inhibited it.  Thus the ability to switch these two phenotypes on and off by adding and withdrawing the transcriptional inducers provided a way to control both the timing and the extent of cell migration.  To exploit this as a patterning method, cells expressing DN Rac1 from either the doxycycline-inducible promoter or the cumate-inducible promoter were mixed together as multicellular spheroids and then subjected to alternating administration of the two inducers.  When the inducer was switched (e.g., doxycycline removed and replaced with cumate), one population was stimulated to migrate while the other was inhibited, and this created radially symmetric patterns of cells over time.The strategies developed in these dissertation studies represent a novel method for tuning cellular mechanical properties and behaviors, which we expect will be useful in a number of tissue engineering applications.  In addition, by enabling graded control over the activity of multiple proteins, these methods provide a unique opportunity to investigate the quantitative relationships describing how protein activity levels influence cell behavior.  Given that cell and tissue mechanics have been discovered to play critical roles in a number of human diseases, a better understanding of these relationships may lead to new therapeutic targets for disease treatments.",ucb,,https://escholarship.org/uc/item/16m4157x,,,eng,REGULAR,0,0
53,1489,Topics in Evidence Synthesis,"Pozzi, Luca","Jewell, Nicholas P.;Hubbard, Alan E.;",2014,"This dissertation considers three different topics related to extracting and merging evidence from heterogeneous sources. This problem is addressed from different angles, from the field of design of experiment to machine learning.Within this dissertation, we add to the existing literature in each area by developing novel methodology and software. Adaptive trial designs can considerably improve upon traditional designs,by modifying design aspects of the ongoing trial, like early stopping,adding or dropping doses, or changing the sample size. We propose a two-stage Bayesian adaptive design for a Phase IIb study aimed at selecting the lowest effective dose for Phase III. In this setting, efficacy has been proved for a high dose in a Phase IIa proof-of-concept study, but the existence of alower but still effective dose is investigated before the scheduled Phase III starts.In the first stage patients are randomized to placebo, maximaltolerated dose, and one or more additional doses within the doserange. Based on an interim analysis, the study is either stopped forfutility or success, or enters the second stage, where newly recruitedpatients are allocated to placebo, some fairly high dose, and oneadditional dose chosen based on interim data. At the interim analysiscriteria based on the predictive probability of success are used todecide on whether to stop or to continue the trial, and, in the lattercase, which dose to select for the second stage.Finally, a dose will be selected as lowest effective dose for Phase IIIeither at the end of the first or at the end of the second stage. The operating characteristics of the procedure are evaluated viasimulations and results are presented for several scenarios comparingthe performance of the proposed procedure to those of the non adaptivedesign.The development of novel therapies in multiple sclerosis (MS) is one area where a range of surrogateoutcomes are used in various stages of clinical research. While the aim of treatments in MS is to preventdisability, a clinical trial for evaluating a drugs effect on disability progression would require a largesample of patients with many years of follow-up. The early stage of MS is characterized by relapses. Toreduce study size and duration, clinical relapses are accepted as primary endpoints in phase III trials. Forphase II studies, the primary outcomes are typically lesion counts based on Magnetic Resonance Imaging(MRI), as these are considerably more sensitive than clinical measures for detecting MS activity.Recently, Sormani and colleagues \cite{sormani2010surrogate} provided a systematic review, andused weighted regression analyses to examine the role of either MRI lesions or relapses as trial levelsurrogate outcomes for disability. We build on this work by developing a Bayesian three-level model,accommodating the two surrogates and the disability endpoint, and properly taking into account thattreatment effects are estimated with errors. Specifically, a combination of treatment effects based onMRI lesion count outcomes and clinical relapse, both expressed on the log risk ratio scale, were used todevelop a study level surrogate outcome model for the corresponding treatment effects based ondisability progression. While the primary aim for developing this model was to support decision makingin drug development, the proposed model may also be considered for future validation.In Genomics and Epidemiology we deal with a high number of features for each observation. Many well known approaches to drawing inferences in this kind of settings use the topology of the feature space, induced by an appropriate metric, to group observations and summarize their main characteristics to get rid of the noise and to predict an outcome of interest. In the present work we generalize this approach in the context of Loss-Based Estimation. We propose an alternative method for constructing a nonparametric multidimensional regression function. This approach is based on the simple idea of clustering data points in the feature space and then fitting a constant to the outcome. HOPACH-PAM is used for partition. This approach results in the choice of a small number of distinct regions easy to interpret. This is specifically illustrated by simulations from which we can see immediately the superiority of this method on CART. Pre-screening and feature selections methods are also developed to improve the performances and reduce the noise. Software is also available in the R package HOPSLAM (HOpach-Pam Supervised Learning AlgorithM) to make this methodology easily accessible.",ucb,,https://escholarship.org/uc/item/1b61431k,,,eng,REGULAR,0,0
54,1490,Evaluating a Telenovela: The Safety of Latino Construction Workers,"Castaneda, Diego Emiliano","Syme, S. Leonard;",2011,"Latino-Hispanic construction workers in the United States are at significantly higher risks for injuries and fatalities at construction worksites than their White and African-American counterparts. Currently the main mode of dissemination of workplace safety information is through direct translation of work safety material delivered at the worksite. Current research, however, suggests that even when translated into Spanish, many of these materials are not culturally or linguistically effective modes of preventable risk education and persuasion. One promising method for far-reaching, cost-effective, and culturally relevant education may be found in the Entertainment-Education (EE) health communication strategy. EE leverages popular entertainment media - such as movies, television shows, music, theater and/or radio - by embedding specific health messages within a storyline and using the power of narrative to stimulate positive health choices. Spanish-language soap operas (telenovelas) are an entertainment media format culturally embraced by Latino Spanish-speaking audiences and have been effectively utilized by health educators and public health officials to promote changes in knowledge, attitudes, and behaviors for a variety of health issues. The Centers for Disease Control/National Institute of Occupational Safety and Health (CDC/NIOSH) worked with two public health partners and the Spanish language TV chain Telemundo to develop and implement an entertainment education intervention that utilized a telenovela embedded with construction worksite safety information. A statistical analysis of audience survey data collected both before and after the airing of the workplace storyline showed improvements in knowledge outcomes but not in changes in perceptions or behavioral intention outcomes. Detailed analyses revealed that survey respondents who reported recognition of the telenovela workplace storyline were more likely to identify key safety messages embedded within the storyline than respondents who did not recognize the storyline. In addition to the quantitative data, semi-structured key informant interviews were conducted with eight (8) individuals associated with the intervention project. The objective of these interviews was to explore how the partnership between public health institutions and media organizations affected the development, implementation, and evaluation of this project. Project stakeholders voiced challenges which stemmed from the chaotic nature of network television, tensions between developing entertaining vs. accurate educational messaging, and difficulty in communicating actionable messages that would be effective in changing workplace behaviors. Despite these challenges partners felt confident that future endeavors using an EE strategy should be made in communicating other workplace safety issues to Latino and other vulnerable populations. Improved collaboration between entertainment media writers/producers and public health experts is needed to create interventions with the power to change viewers behaviors over time. In addition, more refined research methods are needed to examine EE intervention development and outcomes.",ucb,,https://escholarship.org/uc/item/1cj6d90r,,,eng,REGULAR,0,0
55,1491,Statistical Problems in DNA Microarray Data Analysis,"Wang, Nancy Naichao","Speed, Terence P;",2009,"DNA microarrays are powerful tools for functional genomics studies.  Each array contains thousands of microscopic spots of DNA oligonucleotides with specific sequences, which can hybridize with their complementary DNA sequences.  Thus each microarray experiment consists of parallel assays about thousands of genomic fragments.  This thesis concerns some statistical issues in the analysis of DNA microarray data.One common usage of DNA microarrays is to monitor the dynamic levels of gene expression in response to a stimulus.  This is often achieved through a time course experiment, in which RNA samples are extracted at various time points after exposing the organism to the stimulus.  A particularly interesting type of time course experiments involve replicated series of longitudinal samples.  In 2006, Tai and Speed proposed a multivariate empirical Bayes model for analyzing this type of data.  The MB-statistic derived from this model was shown useful for ranking the genes according to changes in their temporal expression profiles.  In the first part of this thesis, we propose an empirical Bayes false discovery rate (FDR)-controlling procedure for multiple hypothesis testing using the MB-statistic.  A null distribution is obtained using the parametric bootstrap.  Critical values are determined according to the empirical Bayes FDR procedure.  This method was compared, through simulations, to the frequentist FDR procedure, which requires a theoretical null distribution for calculating the nominal p-values.  Although our method is slightly anti-conservative, it is more robust to the variability in the estimates of the hyperparameters, when the degree of moderation is small.Another common usage of DNA microarrays is to detect genomic locations that are associated with DNA-binding proteins.  This is often achieved through ChIP-chip experiments that combine chromatin immunoprecipitation with the microarray technology.  Traditional DNA microarrays designed for gene expression studies contain only a few probes for each gene.  A special type of DNA microarrays, called tiling arrays, are often used in ChIP-chip experiments.  They typically contain probes that are placed densely along the chromosomes to cover either the entire genome or contigs of the genome.  A couple of challenges in the analysis of ChIP-chip tiling array data have not been met satisfactorily in the literature.  When large scale genomic studies are carried over a long period of time, tiling arrays with different probe designs are often used for practical reasons.  The first challenge is the integration of replicate experiments performed using different tiling array designs.  When the biological process of interest involves a large protein complex, the investigators often perform ChIP-chip experiments on each component DNA-binding protein individually.  DNA targets that are shared by the individual proteins are thought to be the localization sites of the protein complex.  The second challenge is the joint analysis of multiple DNA-binding proteins, aimed at identifying their shared targets.  In the second part of this thesis, we propose a nonhomogeneous hidden Markov model (HMM) for addressing these two challenges.  The nonhomogeneous time axis represents the genomic positions of the probes.  The hidden states represent the binding statuses of the proteins.  The state-conditional emission distributions of the tiling array data are protein-specific and design-specific.  We derived a modified Baum-Welch algorithm for fitting the model parameters.  We also developed a procedure that converts the probe level summaries into peaks, which represent the putative binding sites, based on both signal strength and peak shape.  To compare our method with existing methods, we curated a set of positive and negative genomic regions from a C. elegans dataset, and performed some receiver operating characteristics (ROC) analyses.  When applied to each experiment separately, our method performs similarly as the three best existing methods.  When applied to the combined data set, which consists of tiling arrays with different probe designs, our method shows a drastic improvement in performance.  A generalization of the nonhomogeneous HMM enables the joint analysis of the ChIP-chip data of multiple proteins.  We present an application of this method to identify the shared localization sites of two DNA-binding proteins, under two different conditions.",ucb,,https://escholarship.org/uc/item/1dm0z29w,,,eng,REGULAR,0,0
56,1492,Employer Preparedness for Pandemic Influenza: Shifting the Conversation from Insurance to Investment,"Lachance, Jennifer Alice","Reingold, Arthur;",2010,"Pandemic influenza is currently one of the most visible public health threats of concern to the general public, and private businesses are an important part of pandemic preparedness. The health of communities is affected by the local economy, which is driven by the businesses in that economy. To date, public health authorities' efforts to engage businesses in pandemic influenza preparedness efforts have justified preparedness based on potential losses due to future, uncertain threats. However, this approach has not successfully engaged businesses on a broad scale. This dissertation proposes that a more effective way to engage the private sector may be to shift the conversation away from justifying preparedness only as a long-term insurance strategy and toward justifying it as an investment strategy with short-term benefits such as improved employee health during interpandemic cold and influenza seasons. The viability and acceptability of this new approach are explored here via three distinct but complementary studies using both quantitative and qualitative methods.       The first study, a prospective observational cohort study, examined the individual characteristics and situations that predicted changes in hand and respiratory hygiene and social distancing behaviors among university students during an interpandemic cold and influenza season. This analysis reveals that individuals have higher adherence to behaviors in situations such as when they are ill. Additionally, some individual characteristics predict higher behavior adherence. In particular, individuals who perceive peer expectations concerning adherence to hygiene behaviors tend to have better adherence to those behaviors over the course of a cold and influenza season.       The second study, a cost-effectiveness analysis of a hand and respiratory hygiene intervention among university students, assessed whether an intervention could be cost-effective in reducing influenza-like illness and associated time lost from productive activities during an interpandemic cold and influenza season. This analysis finds that hand and respiratory hygiene interventions can be cost-effective and may even become cost-saving during a severe cold and influenza season, especially using group-level interventions that may create peer expectations to influence behaviors.      Finally, the third study, an exploratory analysis based on key informant interviews with private sector business continuity managers, consultants, and public sector planners, examined private sector preparedness for pandemic influenza. This analysis assessed the key components of employer pandemic influenza preparedness plans, including whether short-term benefits are a consideration in business planning. The results indicate that the most important components of private sector pandemic influenza plans before and during the 2009 H1N1 influenza pandemic included communications and employee education around hygiene behaviors. Participants further identified that implementation of these initiatives during interpandemic cold and influenza seasons is of interest to organizations due to potential short-term and long-term benefits.      These results together provide evidence that education and provision of materials for hygiene behaviors at a group level can be cost-effective in reducing influenza-like illnesses during interpandemic cold and influenza seasons and are an acceptable strategy to the private sector. This provides a basis for the hypothesis that employer preparedness for public health events such as pandemic influenza can be justified as a short-term business investment strategy rather than only as a long-term insurance strategy.",ucb,,https://escholarship.org/uc/item/0p05b5qx,,,eng,REGULAR,0,0
57,1493,The Role of Distribution Infrastructure and Equipment in the Life-cycle Air Emissions of Liquid Transportation Fuels,"Strogen, Bret","Horvath, Arpad;",2012,"Production of fuel ethanol in the United States has increased ten-fold since 1993, largely as a result of government programs motivated by goals to improve domestic energy security, economic development, and environmental impacts.  Over the next decade, the growth of and eventually the total production of second generation cellulosic biofuels is projected to exceed first generation (e.g., corn-based) biofuels, which will require continued expansion of infrastructure for producing and distributing ethanol and perhaps other biofuels.  In addition to identifying potential differences in tailpipe emissions from vehicles operating with ethanol-blended or ethanol-free gasoline, environmental comparison of ethanol to petroleum fuels requires a comprehensive accounting of life-cycle environmental effects.  Hundreds of published studies evaluate the life-cycle emissions from biofuels and petroleum, but the operation and maintenance of storage, handling, and distribution infrastructure and equipment for fuels and fuel feedstocks had not been adequately addressed.  Little attention has been paid to estimating and minimizing emissions from these complex systems, presumably because they are believed to contribute a small fraction of total emissions for petroleum and first generation biofuels.  This research aims to quantify the environmental impacts associated with the major components of fuel distribution infrastructure, and the impacts that will be introduced by expanding the parallel infrastructure needed to accommodate more biofuels in our existing systems.  First, the components used in handling, storing, and transporting feedstocks and fuels are physically characterized by typical operating throughput, utilization, and lifespan.  US-specific life-cycle GHG emission and water withdrawal factors are developed for each major distribution chain activity by applying a hybrid life-cycle assessment methodology to the manufacturing, construction, maintenance and operation of each component.  Emissions from activities at the end of life of equipment and infrastructure are not included, as these activities have previously been shown to contribute negligibly to life-cycle emissions.  Life-cycle transportation mode GHG emission factors per tonne-kilometer (t-km) are presented for long distance pipelines (5-20 g CO2-e/t-km), ocean tankers (5-17 g/t-km), fuel-carrying barges (31 g/t-km), fuel-carrying unit trains (25 g/t-km), tanker trucks (140-180 g/t-km), and bale-transporting flatbed trucks (200 g/t-km).  Life-cycle emission factors are also presented per tonne of material throughput for several types of agricultural equipment (600-19,000 g CO2-e/t handled), fuel conversion facilities (9,000-98,000 g/t), fuel storage and dispensing facilities (2,000-12,000 g/t), and the portion of passenger vehicle operations dedicated to refueling errands (2,000-200,000 g/t).  The emissions intensity ranges reported for specific transportation modes are largely due to the greater energy efficiency of larger vehicles and pipelines, and the emissions intensity ranges within stationary storage and handling equipment is often due to differences in utilization of capital equipment and/or material losses during storage and handling activities.  Consistent with existing literature, the contribution of non-operation stages to life-cycle GHG emissions ranges from 20% to 40% for most of the components modeled.  Criteria air pollutant (NOx, PM2.5, SOx, VOC, CO) emission factors are also presented for the operation stage (e.g., tailpipe only) of each transportation mode.  In order to apply the new emission factors to policy-relevant scenarios, a projection is made for the fleet inventory of infrastructure components necessary to distribute 21 billion gallons of ethanol (the 2022 federal mandate for advanced biofuels under the Energy Independence and Security Act of 2007) derived entirely from Miscanthus grass, for comparison to the baseline petroleum system.  Due to geographic, physical and chemical properties of biomass and alcohols, the distribution system for Miscanthus-based ethanol is more capital- and energy-intensive than petroleum per unit of fuel energy delivered.  Assuming steady-state annual turnover, operation, and maintenance of infrastructure to supply the projected quantities of ethanol and petroleum fuels, ethanol is estimated to be approximately five times more GHG and water intensive than petroleum (i.e., GHG emissions of more than 17 g CO2-e/MJ versus 3 g/MJ, and water withdrawals of 380 L/MJ vs. 77 L/MJ of consumed fuel, neglecting feedstock production and conversion).  Embodied GHG emissions from manufacturing and maintaining infrastructure, equipment, and vehicles make up less than half of these emissions, at approximately 1 g CO2-e/MJ of petroleum fuel and 8 g CO2-e/MJ of ethanol.  Although petroleum fuels are projected to supply twenty times the energy content of ethanol in 2022, the annual GHG and water withdrawal footprint of petroleum's liquid fuel infrastructure and distribution system is slightly less than four times that of ethanol (i.e., 110 vs. 30 million tonnes of CO2-e and 2,500 vs. 640 billion liters of water).  Opportunities to significantly reduce emissions include shifting transportation to more efficient modes, consuming products closer to producers, and converting biorefineries to produce fuel with higher energy density than ethanol.  Minimizing fuel transportation distance is believed to be the most feasible and cost-effective opportunity to reduce emissions in the near term.The transportation of biofuels away from producer regions poses environmental, health, and economic trade-offs that are herein evaluated using a simplified national distribution network model.  In just the last ten years, ethanol transportation within the contiguous United States is estimated to have increased more than ten-fold in total t-km as ethanol has increasingly been transported away from Midwest producers due to air quality regulations pertaining to gasoline, renewable fuel mandates, and the 10% blending limit (i.e., the E10 blend wall).  From 2004 to 2009, approximately 10 billion t-km of ethanol transportation are estimated to have taken place annually for reasons other than the E10 blend wall, leading to annual freight costs greater than $240 million and more than 300,000 tonnes of CO2-e emissions and significant emissions of criteria air pollutants from the combustion of more than 90 million liters of diesel.  Although emissions from distribution activities are small when normalized to each unit of fuel, they are large in scale.Archetypal fuel distribution routes by rail and by truck are created to evaluate the significance of mode choice and route location on the severity of public health impacts from locomotive and truck emissions, by calculating the average PM2.5 pollution intake fraction along each route.  Exposure to pollution resulting from trucking is found to be approximately twice as harmful as rail (while trucking is five times more energy intensive).  Transporting fuel from the Midwest to California would result in slightly lower human health impacts than transportation to New Jersey, even though California is more than 50% farther from the Midwest than most coastal Northeast states.In summary, this dissertation integrated concepts from infrastructure management, climate and renewable fuel policy, fuel chemistry and combustion science, air pollution modeling, public health impact assessment, network optimization and geospatial analysis.  In identifying and quantifying opportunities to minimize damage to the global climate and regional air quality from fuel distribution, results in this dissertation provide credence to the urgency of harmonizing policies and programs that address national and global energy and environmental goals.  Under optimal future policy and economic conditions, infrastructure will be highly utilized and transportation minimized in order to reduce total economic, health, and environmental burdens associated with the entire supply and distribution chain for transportation fuels.",ucb,,https://escholarship.org/uc/item/0r60z01k,,,eng,REGULAR,0,0
58,1494,Prediction Methods for Astronomical Data Observed with Measurement Error,"Long, James Patrick","Rice, John A;El Karoui, Noureddine;",2013,"We study prediction when features are observed with measurement error. The research is motivated by classification challenges in astronomy.In Chapter 1 we introduce the periodic variable star classification problem. Periodic variable stars are periodic functions which belong to a particular physical class. These functions are often sparsely sampled, which introduces measurement error when attempting to estimate period, amplitude, and other function features. We discuss how measurement error can impact performance of periodic variable star classifiers. We introduce two general strategies, noisification and denoisification, for addressing measurement error in prediction problems.In Chapter 2 we study density estimation with Berkson error. In this problem, one observes a sample from the density $f_X$ and seeks to estimate $f_Y$, the convolution of $f_X$ with a known error distribution. We derive asymptotic results for the behavior of the mean integrated squared error for kernel density estimates of $f_Y$. The presence of error generally increases convergence rates of estimators and optimal smoothing parameters. We briefly discuss some potential applications for this work, including classification tasks involving measurement error.In Chapter 3 we study prediction of a continuous response for an observation with measurement error in its features. Using Nadaraya Watson type estimators we derive limit theorems for convergence of the mean squared error as a function of the smoothing parameters.In Chapter 4 we study the effects of measurement error on classifier performance using data from the Optical Gravitational Lensing Experiment (OGLE) and the Hipparcos satellite. We illustrate some challenges in constructing statistical classifiers when the training data is collected by one astronomical survey and the unlabeled data is collected by a different survey. We use noisification to construct classifiers that are robust to some sources of measurement error and training--unlabeled data set differences.",ucb,,https://escholarship.org/uc/item/0s79z3hk,,,eng,REGULAR,0,0
59,1495,"Social Work Delivered Intervention for Persons with Mild Traumatic Brain Injury: Implementation and Evaluation in an Urban, Public, Trauma Center Emergency Department","Moore, Megan","Segal, Steven P;",2012,"Mild traumatic brain injury (mTBI) is a prevalent and costly public health problem with potentially disabling consequences.  Interventions aimed at alleviating cognitive, emotional and behavioral sequelae are underdeveloped.  This prospective, quasi-experimental cohort study evaluated a brief social work delivered intervention (SWDI) for adults with mTBI discharged from the emergency department.  The SWDI included education, reassurance, coping strategies and community resource information.  Participants were recruited from consecutive admissions to the emergency department.  A total of 64 persons with confirmed mTBI diagnoses were assessed 3 months post-injury.  Participants in the Usual Care group (N=32) were identified via medical record; confirmation of mTBI was based on World Health Organization definition.  Participants in the SWDI group (N=32) were identified and mTBI diagnosis confirmed by emergency department medical staff.  Both groups completed standardized assessments of post-concussion symptoms, depression, anxiety, Posttraumatic Stress Disorder, alcohol use, and community functioning three months after injury.  To assess change in alcohol use and community functioning, participants were asked to recall pre-injury drinking levels and functioning and then asked about current status three months post injury.  The SWDI group also completed an open-ended Patient Experience Survey following their ED service.    The paired sample t test was used to assess community functioning outcomes.  For all other standardized measures, non-parametric Mann Whitney or Wilcoxon Signed Rank tests were used to compare groups.  Qualitative themes from the Patient Experience Survey were identified through systematic review of all survey responses.Three months post injury, both groups reported pre-injury drinking in the ""hazardous"" range.  The SWDI group reported significantly reduced alcohol use from pre-injury to post-intervention (p < 0.05).  The Usual Care group maintained their pre-injury level of drinking.  Analysis of the community functioning measure revealed the SWDI group maintained pre-injury levels of community functioning, while the Usual Care group reported significant decline in functioning (p = 0.05).  All other analyses of standardized measures (anxiety, depression, PTSD, post-concussive symptoms) trended in favor of the intervention group, but were not statistically significant.  Results from the SWDI Patient Experience Survey indicate that 96% of participants who remembered receiving the intervention (N=25) found it helpful.  In response to an open ended question about the most helpful aspects of the intervention, 60% reported it was most helpful to learn about symptoms to expect because this decreased anxiety about symptoms, 28% reported that the recovery tips were most helpful and 24% reported that education about ceasing alcohol use was most helpful. The study provides support for the use of the SWDI in the emergency department.  Decrease in alcohol use and maintenance of community functioning are clinically and functionally significant outcomes.  Alcohol use is a risk factor for re-injury and poor outcome, and the measure of community functioning includes probes about work, school and social activity attendance as well as ability to complete household and daily living activities.  In addition, the SWDI group overwhelmingly found the intervention helpful.  Education about symptoms to expect and decreasing alcohol use was particularly salient for participants.  Future studies should consider survey themes and ways to enhance the intervention in order to increase the impact on additional outcomes of interest.",ucb,,https://escholarship.org/uc/item/0sw9w8t7,,,eng,REGULAR,0,0
60,1496,Understanding and Engineering Cellulase Binding to Biomass Components,"Strobel, Kathryn Lynn","Clark, Douglas S;",2015,"Lignocellulosic biomass is an abundant, low-cost resource for the renewable production of fuels and chemicals. To unlock the potential of lignocellulosic biomass, the cellulose must be broken down into sugars before fermentation to produce ethanol, butanol, or other bio-based products. Unfortunately, lignocellulose is highly resistant to enzymatic degradation, necessitating high enzyme loadings that increase the cost of biofuels. The recalcitrance of biomass stems in part from the presence of lignin, a major component of lignocellulosic biomass. Lignin impedes enzymatic hydrolysis by non-productively binding cellulases and contributing to cellulase denaturation. Despite numerous studies documenting cellulase adsorption to lignin, the structural basis has not been fully elucidated and few attempts have been made to engineer enzymes for reduced lignin affinity. In this work, we investigate and engineer cellulase adsorption to lignin and the resulting effect on hydrolysis of cellulose. The lignin inhibition of two homologous cellulases, T. reesei Cel7A and T. emersonii Cel7A, was found to differ significantly. In Chapter 2, we propose that differences in surface charge, stability, and glycosylation patterns may be the driving force/s behind the observed differences in lignin inhibition and we suggest engineering strategies for improving lignin tolerance of Cel7A catalytic domains.  Chapters 3 and 4 detail our efforts to investigate the mechanisms of cellulase lignin adsorption and engineer an enzyme with reduced lignin affinity using site directed mutagenesis of the T. reesei Cel7A carbohydrate binding module (CBM) and linker. Mutation of aromatic and polar residues on the planar face of the CBM greatly decreased binding to both cellulose and lignin, supporting the hypothesis that the cellulose-binding face is also responsible for the majority of lignin affinity. Cellulose and lignin affinity of the alanine mutants were highly correlated, indicating similar binding mechanisms for cellulose and lignin. CBM mutations that added hydrophobic or positively charged residues decreased the selectivity toward cellulose, while mutations that added negatively charged residues increased the selectivity.  Mutating the linker to alter predicted glycosylation patterns greatly impacted lignin affinity but did not affect cellulose affinity. Beneficial mutations were combined to generate a mutant with 2.5 fold less lignin affinity and fully retained cellulose affinity. This mutant was not inhibited by added lignin during hydrolysis of Avicel and generated 40% more glucose than the wild type enzyme from dilute acid-pretreated Miscanthus. The mutations studied here inform engineering efforts of other homologous CBMs and will hopefully contribute to reducing the cost of biofuels. The final chapter details the development of a high-throughput selection platform for engineering protease enzymes with new sequence specificity. Proteases are commonly used in research, industry, and medicine, and there is considerable promise for new proteases that could cleave at a user-specified sequence. Positive selection and counter-selection were combined to select a tobacco etch virus protease mutant with new substrate compatibility.",ucb,,https://escholarship.org/uc/item/0tw1f2tz,,,eng,REGULAR,0,0
61,1497,"The Therapeutic Turn in International Humanitarian Law: War Crimes Tribunals as Sites of ""Healing""?","Anders, Diana Elizabeth","Butler, Judith;Cohen, David;",2012,"AbstractThe Therapeutic Turn in International Humanitarian Law: War Crimes Tribunals as Sites of ""Healing""?  by  Diana Elizabeth Anders  Doctor of Philosophy in Rhetoric Designate Designated Emphasis in Women, Gender, and SexualityUniversity of California, Berkeley  Professor Judith Butler, Co-Chair  Professor David Cohen, Co-Chair This dissertation examines the growing tendency to figure international war crimes tribunals in terms of their therapeutic value for their victims. My project documents and questions how the discourse of juridical healing emerged from what I term ""the therapeutic turn"" in international humanitarian law (hereafter, IHL). I analyze this phenomenon in terms of its key features, conditions of possibility, modes of legitimization, and effects, focusing on legal institutions designed to adjudicate crimes such as genocide, mass rape, and torture. My central argument is that the rhetoric of juridical healing, despite its commendable achievements, comes at an important cost, in that the appeal to law can invite new forms of regulation and domination. In short, this novel form of justice produces and authorizes its own forms of violence. It does so in part by obscuring the political effects of the law's promise to heal. To bring this uncomfortable fact into relief is but a first step towards countering such ill effects.  This project focuses on the first two international ad hoc tribunals -- the International Criminal Tribunals for the former Yugoslavia and the International Tribunal for Rwanda -- as well as the International Criminal Court (hereafter, the ICTY, the ICTR, and the ICC). All were established in the 1990s in the beginning of what has been called the ""tribunal era,""  which has ushered in an unprecedented emphasis on victims of atrocity. Primarily by means of discourse analysis, I examine court documents and trial transcripts, as well as relevant statements made by diplomats, politicians, court officials, scholars, and non-governmental-organizations. Such analysis aims to chart the expansion of a new norm of justice as healing that has so far largely gone unrecognized.   Chapter One outlines the general problem of the dissertation, introducing the phenomenon of juridical healing and situating it historically. Although such healing has become a powerful, even normative trope in humanitarian discourse, it has not been well defined. The chapter raises questions concerning what juridical healing can realistically achieve, and how it might constitute a new mode of power that paternalistically regulates the very subjects it pledges to heal. It also examines how the special status of healing discourse as ""above reproach"" has shielded it from critical scrutiny.     Chapter Two surveys the growing scholarly discourse on juridical healing, arguing that such inquiries tend to uncritically accept the core terms of the therapeutic turn. This work can thus serve to reify the problematic notion of healing promulgated elsewhere. Such thinking holds that tribunals can occasion forms of ""catharsis"" and ""closure,"" both for individual victim-witnesses and more broadly. I argue that this belief in ""disclosure for closure"" forecloses critical reflection on the effects of juridical healing, or on alternatives to this conception.  In Chapter Three, I develop a genealogy of juridical healing in relation to new legal institutions. I analyze how the promise of healing has served as a means of legitimization, even as it has led courts into uncharted legal territory. Even as the tribunals of the 1990s derived their credibility from the Nuremberg and Tokyo tribunals that followed World War II, they also had to distance themselves from the accusation that the latter had only dispensed ""victors' justice."" In an uncanny echo, recent tribunals can be said to have produced forms of ""victims' justice."" I examine how such rhetoric threatens to undermine the same credibility that it otherwise means to establish, even at the cost of the victims it purportedly champions.  Chapter Four considers the tribunals' adjudication of sexual violence as a war crime. Here I use individual case studies to show the unforeseen costs of such procedures. I examine how the female victim of sexual violence is effectively condemned to victimhood by the very discourse that promises to heal her, but denies her meaningful agency. At the same time, ""other"" victims of wartime sexual violence--such as men, boys, or women from the ""enemy camp""--are marginalized. My analyses of these cases explore how therapeutic-juridical interventions can undermine their avowed aims, while concealing the power relations on which they rely and which they perpetuate.   The final chapter is based on fieldwork that I carried out in 2009 in The Hague, Netherlands, and examines the depoliticizing effects of juridical healing.  Drawing on interviews with ICTY and ICC officials, the chapter outlines the temporal and spatial coordinates of the rhetoric of healing. I focus on the ways in which such rhetoric enacts movements of deferral and displacement, and thus neutralizes potential forms of political activity. As an alternative, I examine Hannah Arendt's account of politics, which is centered on collective, participatory action and antagonistic debate. Such a view allows us to imagine a more capable subject of politics, one with the potential to recover, resist, and revolt.  The Epilogue evaluates the current and future implications of the rhetoric of healing, exploring alternative responses to extreme violence. I claim that juridical healing can be understood as the latest ""last utopia""  or the least ""lesser evil""  in a time when ""human rights"" and ""humanitarianism"" have become increasingly wed to military interventions. I proceed to trace additional contradictions in the discourse of juridical healing, in that contemporary IHL also identifies with the ideology of militarized humanitarianism in its endorsement of the UN doctrine of ""Responsibility to Protect."" I close by suggesting that juridical healing presents the international community with an aporia that might ultimately be generative, insofar as it produces conditions under which the very politics it stifles might also be aroused. By rethinking and reframing this rhetoric, I hope to indicate avenues for differently imagining and producing the future--a future not destined to repeat or be dictated by the violence, injustice, and pain of the past.",ucb,,https://escholarship.org/uc/item/0vc1f4gc,,,eng,REGULAR,0,0
62,1498,Biodiversity and Ecosystem Services in Agriculture: Evaluating the Influence of Floral Resource Provisioning on Biological Control of Erythroneura Leafhoppers (Hemiptera: Cicadellidae) and Planococcus Mealy Bugs (Hemiptera: Pseudococcidae) in California Vineyards,"Miles, Albie Felix","Altieri, Miguel A;",2013,"The research tested the natural enemies hypothesis in an attempt to explain why lower pest densities are observed in some diversified farming systems. The research evaluated the influence of floral resource provisioning (FRP) and chemical ecology strategies on biological control of Erythroneura leafhoppers (Hemiptera: Cicadellidae) and Planococcus mealybug (Hemiptera: Pseudococcidae) in California vineyards. Field and laboratory studies quantified the impacts on crop damage, pest and natural enemy abundance, and natural enemies fitness theorized to be enhanced through floral resource provisioning in agroecosystems. Multiple two-year studies measured the impact of intercropping three flowering ground covers, lacy phacelia (Phacelia tanacetifolia), bishop's weed (Ammi majus), and common carrot (Daucus carota) on biological control of leafhoppers and vine mealybug by the parasitoids Anagrus spp. (Hymenoptera: Mymaridae) and Anagyrus pseudococci (Hymenoptera: Encyrtidae). Using identical intercropping treatments, the research included three large scale and fully replicated research designs located in the central San Joaquin, the northern San Joaquin, and the Napa Valley of California. Laboratory studies quantified the impacts of FRP on the fitness of Anagyrus pseudococci, a key parasitoid natural enemy of vine mealybug. The central San Joaquin Valley field study measured the impact of FRP and pheromone based mating disruption on biological control of vine mealybug. The northern San Joaquin Valley field study measured the impact of FRP and methyl salicylate on biological control of Erythroneura leafhoppers. The Napa Valley field study measured the effect of methyl salicylate alone on biological control of Erythroneura leafhoppers.",ucb,,https://escholarship.org/uc/item/0vt4z0fd,,,eng,REGULAR,0,0
63,1499,Seeing in the Dark: Weak Lensing from the Sloan Digital Sky Survey,"Huff, Eric Michael","Schlegel, David J;Selak, Uros;",2012,"Statistical weak lensing by large-scale structure { cosmic shear { is a promising cosmological tool, which has motivated the design of several large upcoming astronomical surveys. This Thesis presents a measurement of cosmic shear using coadded Sloan Digital Sky Survey (SDSS) imaging in 168 square degrees of the equatorial region, with r < 23:5 and i < 22:5, a source number density of 2.2 per arcmin2 and median redshift of zmed = 0.52. These coadds were generated using a new rounding kernel method that was intended to minimize systematic errors in the lensing measurement due to coherent PSF anisotropies that are otherwise prevalent in the SDSS imaging data. Measurements of cosmic shear out to angular separations of 2 degrees are presented, along with systematics tests of the catalog generation and shear measurement steps that demonstrate that these results are dominated by statistical rather than systematic errors. Assuming a cosmological model corresponding to WMAP7(Komatsu et al., 2011) and allowing only the amplitude of matter fluctuations &sigma8 to vary, the best-t value of the amplitude of matter fluctuations is &sigma8=0.636+0.109-0.154 (1&sigma); without systematic errors this would be  &sigma8=0.636+0.099-0.137 (1&sigma). Assuming a flat &LambdaCDM model, the combined constraints with WMAP7 are &sigma8=0.784super>+0.028-0.026  (1&sigma). The 2&sigma error range is 14 percent smaller than WMAP7 alone. Aside from the intrinsic value of such cosmological constraints from the growth of structure, some important lessons are identied for upcoming surveys that may face similar issues when combining multi-epoch data to measure cosmic shear. Motivated by the challenges faced in the cosmic shear measurement, two new lensing probes are suggested for increasing the available weak lensing signal. Both use galaxy scaling relations to control for scatter in lensing observables.The first employs a version of the well-known fundamental plane relation for early type galaxies. This modified ""photometric fundamental plane"" replaces velocity dispersions with photometric galaxy properties, thus obviating the need for spectroscopic data. We present the first detection of magnication using this method by applying it to photometric catalogs from the Sloan Digital Sky Survey. This analysis shows that the derived magnication signalis comparable to that available from conventional methods using gravitational shear. We suppress the dominant sources of systematic error and discuss modest improvements that may allow this method to equal or even surpass the signal-to-noise achievable with shear. Moreover, some of the dominant sources of systematic error are substantially different from those of shear-based techniques. The second outlines an idea for using the optical Tully-Fisher relation to dramatically improve the signal-to-noise and systematic error control for shear measurements. The expectederror properties and potential advantages of such a measurement are proposed, and a pilot study is suggested in order to test the viability of Tully-Fisher weak lensing in the context of the forthcoming generation of large spectroscopic surveys.",ucb,,https://escholarship.org/uc/item/0z22n08t,,,eng,REGULAR,0,0
64,1500,Exploring Landscapes of Naturalness with Lifshitz Field Theories,"Grosvenor, Kevin Torres","Horava, Petr;",2015,"In this thesis, we examine the question of technical naturalness from the point of view of nonrelativistic quantum field theories of Lifshitz type. Lifshitz field theories are distinguished from standard relativistic quantum field theories by the spacetime scaling symmetries that they enjoy in the vicinity of their renormalization group fixed points. These scaling symmetries are parametrized by the dynamical critical exponent, which measures the degree of scaling of time relative to spatial coordinates. Whereas in relativistic theories, space and time scale equally with each other, in Lifshitz field theories, the dynamical critical exponent may differ from unity. Furthermore, Lifshitz field theories live in spacetimes that possess a foliation structure by spatial leaves of constant time. Therefore, in contrast to relativistic theories, Lifshitz field theories are not invariant under the full diffeomorphism group of spacetime, but rather only those diffeomorphisms that preserve the built-in foliation structure. In the flat spacetime, this would simply exclude the usual spacetime boost symmetries. Since time is treated on a fundamentally different footing as is space, these theories are often referred to as anisotropic. In addition, these theories often require the tuning of multiple parameters in order to approach their fixed points under renormalization group flow, and are consequently called multicritical.We will explore some new and interesting lessons that nonrelativistic theories have to teach us about technical naturalness. We begin this study at the modest level of Lifshitz scalar field theories. We examine the nature of Nambu-Goldstone (NG) bosons that arise from spontaneous symmetry breaking in multricritical systems described by Lifshitz scalar field theories. The NG modes in such nonrelativistic theories were previously classified into two types: (1) Type-A, which disperses linearly, and which derives its kinetic energy from a term which is quadratic in time derivatives; and (2) Type-B, which disperses quadratically, and which is described by a pair of fields with kinetic terms linear in time derivatives. In principle, Type-A modes dispersing by a power different from unity, and Type-B modes dispersing by a power different from two, can exist. However, the naive expectation from relativistic quantum field theory is that these would require fine tuning and are therefore technically unnatural. We discover that this is not the case. Instead of fine-tuning, all one needs is a new type of symmetry by which the fields transform by a polynomial function of some appropriate degree in the spatial coordinates. This polynomial shift symmetry protects the naturalness of the corresponding NG bosons. This leads to a refinement of the classification of technically natural NG modes in nonrelativistic theories.Having discovered these polynomial shift symmetries, we turn our attention to the classification of Lagrangians that are invariant (up to total derivatives) with respect to these symmetries. We develop a novel graph-theoretical technique in order to address this problem. In this language, the invariants display beautiful patterns that otherwise remain obscured. For example, linear-shift invariants are presented as equal-weight sums over all labeled trees with some fixed number of vertices. Furthermore, we develop a graph-theoretical method for constructing invariants under polynomial shifts of high degree from invariants under polynomial shifts of lower degree. In this way, one no longer needs to repeat the entire classification process for each degree of the polynomial shift symmetry.The third part of the thesis uses some of the theories, which are built out of invariants constructed in the second part of the thesis, to study a novel feature that these Lifshitz theories possess as one changes the energy scale at which the systems are examined. We find that these systems can flow from one fixed point described by one value of the dynamical exponent, to another fixed point described by a different value of the dynamical exponent. Furthermore, the system can explore any number of fixed points between the extreme high energy regime and the extreme low energy regime. We refer to this behavior as a cascade. Not only can Type-A modes cascade into other Type-A modes (or similarly for Type-B modes), but Type-A modes can flow towards Type-B at low energies as well. Both mechanisms are protected by symmetries. The purely Type-A or Type-B cascade is protected by the polynomial shift symmetries in space. The Type-A to Type-B cascade can be protected by various symmetries, including a linear shift symmetry in time. Furthermore, we re-examine the Coleman-Hohenberg-Mermin-Wagner (CHMW) theorem, which prohibits the spontaneous breaking of global internal continuous symmetries in relativistic theories in two spacetime dimensions. Naively, this theorem would prevent the existence of a Type-A mode unless its dynamical exponent is strictly less than the spatial dimension, which is when the theory is in its lower critical dimension. The cascade represents a mechanism by which this result can be circumvented.Next, we examine the renormalization group flow of one particular Lifshitz scalar field theory. We perform the analysis explicitly using three different standard techniques of renormalization and show that they are all mutually consistent. Furthermore, we demonstrate that the RG flow of Lifshitz theories can be interpreted physically in many different, but consistent, ways due to the additional freedom of renormalizing the dynamical exponent.The lessons in Lifshitz field theories discussed in this thesis are most readily applied in the area of condensed matter physics, where systems often display a richer spectrum of behavior than is described by relativistic physics. We perform a preliminary study of the effects of coupling these Lifshitz theories with other systems. In particular, we study the naturalness problem of the linear dependence on temperature of the resistivity of so-called strange metals, which are high-temperature superconductors above their critical temperature. We show that this behavior is reproduced by the standard electron-phonon interaction picture of superconductivity, if the phonons are allowed to be multicritical and at their lower critical dimension. We also examine the impact that this model has on the heat capacity of the system.",ucb,,https://escholarship.org/uc/item/0073j1dx,,,eng,REGULAR,0,0
65,1501,Global Innovation Bridges: A new policy instrument to support global entrepreneurship in peripheral regions,"Martinez de Velasco Aguirre, Emilio","Chapple, Karen;",2012,"This dissertation analyzes a new set of policy instruments that several national and regional governments have recently implemented to help their home-grown innovative companies gain access to global technology markets. These initiatives, which in this dissertation are referred to as Global Innovation Bridges (GIBs), introduce a novel spatial approach to supporting global entrepreneurship in peripheral regions. Establishing a physical presence in the most dynamic regions of technological innovation around the world, and having deep ties with organizations in their home country, GIBs have effectively instituted a cross-national business support structure with the capacity to mobilize knowledge, talent, technology and capital across borders. These initiatives are based on the premise that facilitating innovative companies' access to global markets will accelerate their growth at home, generating new jobs and income. But in addition to a quantitative increase in economic activity, governments are implementing GIBs in an attempt to foster a transition towards high-growth, high value-added economic activities. Despite their potential to stimulate economic development and to foster a qualitative transformation in the economic structure of countries and regions, the literature on entrepreneurship and global entrepreneurship policies remains completely silent about GIBs. This dissertation is the first academic contribution to reveal the workings of this emerging economic development tool. The research achieves two main objectives. First, it provides an initial characterization of GIBs, describing their main features and the factors that are driving national and regional governments to implement them. Based on a multiple case-study of six GIBs with operations in Silicon Valley, California, this characterization also introduces a taxonomy that clearly differentiates GIBs from similar organizations supporting entrepreneurship. Second, it develops an in-depth analysis of the Mexican GIB, the Technology Business Accelerator (TechBA) program, in order to explain how GIBs work. This in-depth study reveals the diversity of actors supporting the mission of the TechBA program as well as the learning processes involved in turning a local company into a global player.Applying the concept of `communities of practice'  (Lave 1991; Brown and Duguid 1991; Wenger 1998; Brown and Duguid 2001) to the analysis of the TechBA program, this dissertation advances the following arguments:* The TechBA program articulates a community of practice that involves individuals in various organizations linked together by shared experience, expertise, and commitment to a joint enterprise: supporting the global expansion of Mexican companies. These are individuals whose work is related to the many technological, commercial, financial, and legal aspects of launching a new global venture. While all these individuals work for organizations that have their own agendas and goals, they all contribute in one way or another to advancing the mission of the TechBA program.* TechBA sustains a `distributed' community of practice (Hildreth et al., 2000) that transcends national borders. Through formal partnerships but primarily through informal collaborations with actors in both Mexico and in foreign markets, TechBA articulates a community of practice that operates across distant regions in different countries. The staff and individuals more closely involved in the operation of the TechBA program serve as a `brokers,' mediating among various technical and business communities in distant regions.* Supporting the global expansion of innovative companies involves a transformation in the views and practices of the entrepreneurs leading the global expansion effort as much as it involves adaptations in the strategy, structure, and organization of a firm. Parallel to the activities to support firm-level adaptations, TechBA facilitates a process of enculturation in which Mexican entrepreneurs develop the values and practices of a foreign business community. Through formal training, but primarily through numerous experience-based learning opportunities, Mexican entrepreneurs develop a new language and codes of communication, new know-how in the form of foreign business practices, new know-who or the knowledge to participate in professional networks in foreign markets, as well as new values and views in line with those of a foreign business community. * Rather than simply bridging the geographical distance to markets, the cross-national community of practice built around the TechBA program provides the social context for developing the knowledge, skills, practices, and views that are time- and context-specific and difficult to transmit over long distances. The TechBA community of practice serves as a ""living curriculum"" (Wenger 2006) in which Mexican entrepreneurs can develop a new identity and learn how to be a global entrepreneur.",ucb,,https://escholarship.org/uc/item/1f29r1dw,,,eng,REGULAR,0,0
66,1502,Generalized Arrows,"Joseph, Adam Megacz","Wawrzynek, John;",2014,"Multi-level languages and arrows both facilitate metaprogramming, the act of writing a program which generates a program. The arr function required of all arrows turns arbitrary metalanguage expressions into object language expressions; because of this, arrows may be used for metaprogramming only when the object language is a superset of the metalanguage.This thesis introduces generalized arrows, which are less restrictive than arrows in that they impose no containment relationship between the object language and metalanguage; this allows generalized arrows to be used for heterogeneous metaprogramming. This thesis also establishes a correspondence between two-level programs and one-level programs which take a generalized arrow instance as a distinguished parameter. A translation across this correspondence is possible, and is called a flattening transformation.The flattening translation is not specific to any particular object language; this means that it needs to be implemented only once for a given metalanguage compiler. Support for various object languages can then be added by implementing instances of the generalized arrow type class; this does not require knowledge of compiler internals. Because of the flattening transformation the users of these object languages are able to program using convenient multi-level types and syntax; the conversion to one-level terms manipulating generalized arrow instances is handled by the flattening transformation.A modified version of the Glasgow Haskell Compiler (GHC) with multi-level types and expressions has been produced as a proof of concept. The Haskell extraction of the Coq formalization in this thesis have been compiled into this modified GHC as a new flattening pass.",ucb,,https://escholarship.org/uc/item/1gg3m11q,,,eng,REGULAR,0,0
67,1503,Essays in China's Anti-corruption Campaign,"Lu, Xi","Wright, Brian;",2017,"China's unique system of hiring and promoting talented people within the state, under the supervision of the Communist Party, has been held up as an important institutional factor supporting its remarkably rapid and sustained economic growth. Jointly with Professor Peter L. Lorentzen, we explore this meritocracy argument in the context of Chinese leader Xi Jinping's ongoing anti-corruption campaign. Some question the sincerity of the campaign, arguing that it is nothing but a cover for intra-elite struggle and a purge of Xi's opponents. In the first chapter of my thesis, we use a dataset I have created to identify accused officials and map their connections. Our evidence supports the Party's claim that the crackdown is primarily a sincere effort to cut down on the widespread corruption that was undermining its efforts to develop an effective meritocratic governing system. First, we visualize the ""patron-client'' network of all probed officials announced by the central government and identify the core targets of the anti-corruption campaign. Second, we use a recursive selection model to analyze who the campaign has targeted, providing evidence that even personal ties to top leaders have provided little protection. Finally, we show that, in the years leading up to the crackdown, the provinces later targeted had departed from the growth-oriented meritocratic selection procedures evident in other provinces. 	In addition to its motivation, I also discuss the campaign's effects on economic efficiency. The second chapter of my thesis tests the ""greasing-the-wheels'' hypothesis in the context of China's residential land market. We show that China's anti-corruption campaign, aimed at removing corruption in China's monopoly land market, caused a decrease in land transaction volumes. Furthermore, not removing any form of corruption would also lead to a similar decrease. It is only necessary to remove corruption that enables real estate developers to circumvent red tape and reduce trading costs. Our findings support the ""greasing-the-wheels'' hypothesis hypothesis: when an economy has a low outcome owing to some preexisting distortions, corruption could be a positive factor in that it offers a ""second-best world.''",ucb,,https://escholarship.org/uc/item/16g527r2,,,eng,REGULAR,0,0
68,1504,Efficient Multi-Level Modeling and Monitoring of End-use Energy Profile in Commercial Buildings,"Kang, Zhaoyi","Spanos, Costas J;",2015,"In this work, modeling and monitoring of end-use power consumption in commercial buildings are investigated through both Top-Down and Bottom-Up approaches. In the Top-Down approach, an adaptive support vector regression (ASVR) model is developed to accommodate the nonlinearity and nonstationarity of the macro-level time series, thus providing a framework for the modeling and diagnosis of end-use power consumption. In the Bottom-Up approach, an appliance-data-driven stochastic model is built to predict each end-use sector of a commercial building. Power disaggregation is studied as a technique to facilitate Bottom-Up prediction. In Bottom-Up monitoring and diagnostic detection, a new dimensionality reduction technique is explored to facilitate the analysis of multivariate binary behavioral signals in building end-uses.",ucb,,https://escholarship.org/uc/item/1867c6vm,,,eng,REGULAR,0,0
69,1505,Nations of Retailers: The Comparative Political Economy of Retail Trade,"Watson, Bartholomew Clark","Levy, Jonah;",2011,"This dissertation analyzes the development of the retail sector in the United States and Western Europe.  The predominant literature on the service sector in both economics and political science has argued that the only way to create jobs in services such as retailing is through the low-wage, high-inequality route epitomized by the United States.  Nevertheless, in the retail trade sector, a number of European countries have matched or exceeded American productivity and employment growth, despite considerably higher wage levels.  How do we explain the puzzling combination of rapid job growth and high wages in European retail?This dissertation resolves this puzzle through a cross-national comparison of the retailing strategy in three countries: the United States, Denmark, and France.  It identifies three modes of competition in retail, labeled ""lean retailing"" (US), ""relational contracting"" (Denmark), and ""vertical integration"" (France).  It shows that each approach has strengths and limitations, with none inherently more successful than the others.The first model, American lean retailing, is a cost-squeezing strategy built around scale, turnover, and low margins.  It uses dominating relationships with suppliers and workers to strip costs and retailer control over logistics to improve efficiency.  The second model, Danish relational contracting, illustrates that more collaborative relationships can produce equally efficient retailing outcomes.  Danish retailers work with workers and suppliers, finding ways to share and reduce long-term costs through worker training, improved productivity, and reduced costs from confrontation.  Finally, a third model, French vertical integration, seeks to add and capture as much value as possible throughout the distribution supply chain.  French retailers have built their own partners and brands, developing services and private label products that allow them to add value.Most explanations of retail business strategies emphasize either the imperatives of the technology available to retailers or the vicissitudes of consumer preferences or markets.  This dissertation argues, by contrast, that contemporary retailer strategies are rooted in a series of political battles fought in the 1960s.  The key to this political explanation is the embedded nature of the retail sector.  Retailers are amongst the most connected actors in political economy, with ties to numerous economic and political players, including consumers, suppliers, workers, and both local and national governments.  These connections were activated in the 1960s as a new crop of large-scale retail entrants began shifting the retail sector from shopkeepers to supermarkets.The political coalitions that emerged were a function of national structural factors, notably the power resources of the retail sector, electoral institutions, avenues of interest aggregation, and the economic organization of small shops.  These coalitions set in motion longer economic trajectories of firm management and policymaking.  Where retailers could defend their interests unilaterally, without coalition partners (US), the lean retail model took hold; where retailers forged multiple coalitions with other stakeholders (Denmark), the relational contracting model developed; and where coalitions were partial and unstable (France), vertical integration predominated.In the United States, the fragmented and decentralized political environment meant that the opposition to retailers was local and diffuse.  Consequently retailers were able to counter with unilateral political action.  This unilateral political approach set the stage for a confrontational, and ultimately dominating   lean retail strategy that uses market power and digital information as a club against suppliers and workers.In Denmark, weak retailers confronted powerful small shops, producers, and workers.  Needing support, retailers reached a broad compromise involving all of these groups that divided the gains from consumer distribution in a more stable, long-term and equitable fashion.  Under this relational contracting approach, retailers were forced to work with partners, but eventually found numerous benefits from these alliances, and have continued their cooperation long after the initial political crisis has past.  Policymaking has also continued its history of concertation, and new challenges are still tackled jointly by a broad coalition of groups.Finally, in France, retailers faced vocal and national, but disjointed opposition movements of shopkeepers and workers.  Retailers responded by forging uneasy, partial alliances, but these were constantly contested.  In this unstable political environment, retailers pursued a vertical integration strategy, seeking to maximize value by controlling and internalizing much of the production process.Analysts of the retail sector suggest that new developments, such as trans-national retailing and e-commerce (sales over the Internet) are undermining national models of retailing.  This dissertation shows, however, that retailers are largely integrating these new developments into their existing, nationally distinctive business strategies.  In the United States, retailers are using market power and fragmented politics to dominate where possible.  In Denmark, numerous partners are working collaboratively to share the gains of new opportunities.  Finally, in France, national political fights over digital commerce are reopening unresolved policy questions from the pre-digital era.  Like the previous set of political challenges facing retailers, therefore, national institutions and politics continue to mediate economic transformations and drive divergence in firm strategy, competitive advantage, and national variation in secondary outcomes such as wage equality, price levels, and patterns of technology implementation.",ucb,,https://escholarship.org/uc/item/18z1138t,,,eng,REGULAR,0,0
70,1506,"Inheritance and Inflectional Morphology: Old High German, Latin, Early New High German, and Koine Greek","LeBlanc, MaryEllen","Rauch, Irmengard;",2014,"The inheritance framework originates in the field of artificial intelligence.  It was incorporated first into theories of computational linguistics, and in the last two decades, it has been applied to theoretical linguistics.  Inheritance refers to the sharing of properties: when a group of items have a common property, each item is said to inherit this property.  The properties may be mapped in tree format with nodes arranged vertically.  The most general (i.e. the most widely shared, unmarked) properties are found at the highest nodes, and the most specific (marked) information is found at the lowest nodes.Inheritance is particularly useful when applied to inflectional morphology due to its focus on the generalizations within and across paradigms.  As such, it serves as an alternative to traditional paradigms, which may simplify the translation process; and provides a visual representation of the structure of the language's morphology.  Such a mapping also enables cross-linguistic morphological comparison. In this dissertation, I apply the inheritance framework to the nominal inflectional morphology of Old High German, Latin, Early New High German, and Koine Greek.  The corpus consists of parallel biblical passages in each language which will serve as the basis for comparison.  The trees may be used as a translation aid to those reading these texts as an accompaniment to or substitute for traditional paradigms.  Moreover, I aim to shed light on the structural similarities and differences between the four languages by means of the inheritance trees.",ucb,,https://escholarship.org/uc/item/19n1x8hk,,,eng,REGULAR,0,0
71,1507,An Integrative Approach to Data-Driven Monitoring and Control of Electric Distribution Networks,"Dobbe, Roel Ignatius Jacobus","Tomlin, Claire J.;Callaway, Duncan;",2018,"The commodification of computing, sensors, actuators, data storage and algorithms has unleashed a new wave of automation throughout society. Motivated by the promise of new capabilities, quality improvements, or efficiency gains, data-driven technologies have captured the attention and imagination of the public and many domain experts. Though opportunities are ample, the rapid introduction of data-driven functionality also triggers well-founded concerns about safeguarding critical values, such as safety, privacy and justice.In the context of operating electric distribution networks, the need for data-driven monitoring and control is explained by the irreversible transition from fossil to renewable generation and the accompanied electrification of our economy in areas like transportation and heating.The traditional fit-and-forget paradigm of designing networks conservatively for the projected peak loads assumed unidirectional power flow, predictable future demand and monotonic voltage drops, and allowed for operating at near-100\% reliability with minimal requirement for sensing and actuation. The intermittent nature of Distributed Generation (DG), its ability to feed power back to the grid and cause bidirectional power flow, and the diversifying and nonlinear behavior of electric loads are all eating away at the robustness of this approach, causing Distribution System Operators (DSOs) to put caps on the allowable DG and revisit their design and operating practice. Rather than making traditional expensive network reinforcements in often aging physical infrastructures, DSOs are trying to increase the observability and controllability of their networks by leveraging new sensing and actuation technologies and exploring the ability to use data-driven algorithms to help with the integration of more DG in a more distributed (in space and time) and cost-effective way. This dissertation works towards this vision by formulating a systematic control-theoretic approach for integrating data-driven monitoring and control in the operation of electric distribution networks. Firstly, a Bayesian approach to state estimation overcomes the constraint of limited available real-time sensors by integrating voltage forecasting. A second class of tools discussed is the use of machine learning to decentralize Optimal Power Flow (OPF) methods, by utilizing inverter-interfaced Distributed Energy Resources (DERs). The Decentralized OPF method lets each DER learn a policy that contributes to network objectives from its local historical data and measurements alone. This approach is formulated as a compression and reconstruction problem through an information-theoretic lens, providing fundamental limits of reconstruction and a strategy for optimal communication to improve learning-based reconstruction of optimal policies throughout a network.Lastly, the ambition to control networks in a distributed fashion triggers concerns about privacy-sensitive information that may be inferred from an agent's shared data. For a general class of algorithms, a new notion of local differential privacy is integrated that allows each agent to customize the protection of local information captured in constraints and objective functions.The ultimate goal of the work presented in this dissertation is to contribute to a framework for the integral and value-sensitive design and implementation of data-driven methodologies in critical infrastructure. To address the inherent cross-disciplinary nature of this larger goal, the final chapter explains how each automated decision-making tool reflects and affects values important to its stakeholders. The chapter argues that in order to enable beneficial integration of such tools, practitioners need to reflect on their epistemology and situate the design of automated decision-making in its inherently dynamic and human context.",ucb,,https://escholarship.org/uc/item/1bw112wx,,,eng,REGULAR,0,0
72,1508,"Fine-Grained Soil Liquefaction Effects in Christchurch, New Zealand","Beyzaei, Christine Zahra","Bray, Jonathan D;",2017,"Liquefaction damage from the 2010-2011 Canterbury earthquake sequence devastated parts of Christchurch, New Zealand. There were many sites where state-of-practice liquefaction assessment procedures indicated liquefaction would be expected to occur, and surface manifestations of liquefaction were observed. However, there were also numerous sites, which were predominantly silty soil sites, where state-of-practice liquefaction assessment procedures indicated that liquefaction would be expected to occur, but no surface manifestations of liquefaction were observed. This discrepancy between state-of-practice liquefaction assessments and post-earthquake liquefaction observations led to the development of the research program presented in this dissertation. Several silty soil sites were selected for investigation to further our understanding of fine-grained soil liquefaction response and to evaluate potential limitations in the current state-of-practice liquefaction assessment procedures, which are based primarily on case histories and laboratory testing of sands. This dissertation investigates the liquefaction response of silty soil sites through no-liquefaction case histories from the Canterbury earthquake sequence, evaluating depositional environment effects on observed liquefaction performance, site characterization of silty soil deposits, and laboratory testing to characterize element-scale cyclic response.       Depositional environment effects are evaluated through regional CPT-based analyses and site-specific comparisons. Stratified silty soil swamp deposits are shown to have mitigating effects on the manifestation of liquefaction beyond what can be captured by simplified liquefaction assessment procedures in Christchurch. Differing surficial geology and depositional environments are found through examining historical documents to explain in part the limitations of current liquefaction assessment procedures in the swamps of southwest Christchurch, which contain stratified silt/sand deposits or thick silt layers. Consideration of depositional environment distinguishes between liquefaction performances that could not be differentiated through the CPT-based assessment alone. CPT resolution is not sufficient to capture the thin layering at these stratified sites, and the simplified liquefaction assessment methods do not take into account the effects of the stratification on pore water pressure movement within a soil profile. Continuous sampling and careful logging of high-quality samples provides important insights on in-situ stratification at these silty soil swamp sites, discerning differences in stratigraphy resulting from differences in depositional environment.       Site investigation techniques are evaluated at the silty soil case history sites to discern their capability to characterize thin layers and groundwater table fluctuation, two potential causes for the discrepancies between state-of-practice liquefaction assessments and post-earthquake liquefaction observations. CPT, mini-CPT, sonic borings, and high-quality sampling are critiqued in terms of their ability to capture thin layer stratigraphy, which is of importance for liquefaction assessment. Piezometers, sonic borings, high-quality sampling, crosshole testing, and regional groundwater maps are evaluated to assess their ability to capture groundwater table fluctuation. CPT, mini-CPT, and conventional sonic borings offer important information for site characterization, but they do not capture full details of thin layering at silty soil sites. Detailed logging of high-quality samples captures the actual in-situ layering that helps explain limitations of simplified liquefaction assessment procedures. Use of multiple groundwater measurement methods more fully illuminate fluctuating groundwater conditions. Subsurface investigation programs should utilize tools that characterize features impacting liquefaction potential in adequate detail for the intended engineering purpose. Use of multiple, complementary investigation techniques provides the most robust assessment.       A field investigation and advanced laboratory testing program was conducted in Christchurch. High-quality samples were obtained using a Dames & Moore hydraulic fixed-piston thin-walled sampler for cyclic triaxial testing to characterize the liquefaction response of silty soils at the no-liquefaction sites in southwest Christchurch. These natural silty soil specimens contained heterogeneity and variability that should be considered and is difficult, if not impossible, to replicated with laboratory-prepared specimens. Test results for stress-strain response and axial strain accumulation indicate a nuanced range of transitional responses for these intermediate soils. Post-liquefaction reconsolidation testing shows clear differences in specimen response, ranging from ""sand-like"" immediate reconsolidation to time-dependent reconsolidation. Simplified liquefaction assessment procedures estimate significant liquefaction at these case history sites and yet no liquefaction manifestations were observed during the Canterbury earthquake sequence. Laboratory estimates of cyclic resistance (CRR) are consistent with estimates from the simplified procedures, and both estimates of CRR are well below simplified procedure estimates of seismic demand (CSR). Depositional characteristics such as thin-layering of fine sand and silt may be why manifestations of liquefaction were not observed at these sites. Post-liquefaction reconsolidation testing provides insight that water and ejecta may not accumulate in these stratified silty soils as they would accumulate in thick deposits of liquefiable clean sands. Additional mitigating factors may also contribute to the discrepancy between simplified procedure estimates of liquefaction and the lack of liquefaction observed at these sites. The interaction of several factors contributing to observed liquefaction response at these silty soil sites indicates that in-situ â€œsystemâ€ response should be considered and that further research on silty soils is warranted.",ucb,,https://escholarship.org/uc/item/0s06z6gh,,,eng,REGULAR,0,0
73,1509,Multiexponential T2 and Diffusion Magnetic Resonance Measurements of Glioma Cells,"Jackson, Pamela","McKnight, Tracy R;",2012,"Using monoexponential models, magnetic resonance (MR) parameters transverse relaxation time (T2) and apparent diffusion coefficient (ADC) have been used to nonâ€“invasively assess cell density, which characteristically increases with brain tumor malignancy. Multiexponential models of T2 and ADC may allow for a more accurate evaluation of cell density than the traditionally used monoexponential model. To better understand how cell density affects multicomponent T2 and ADC, tumor models with a range of cell densities were created by suspending astrocytoma cells in agarose at different densities. T2 was measured using a Carrâ€“Purcellâ€“Meiboomâ€“Gill (CPMG) sequence with 64 echo times. ADC was measured using a diffusionâ€“weighted sequence with 32 bâ€“values. Three models were used to fit the data and determine the T2 values, ADC values, and associated fractions: monoexponential, biexponential, nonâ€“negative least squares (NNLS). Spearman Rank was used to test the correlation with cell density. Both the monoexponential T2 and ADC were significantly negatively correlated with cell density. The biexponential model identified two T2 components, short and long. Both T2 componentsâ€™ values and fractions were significantly correlated to cell density. The biexponential model identified two ADC components, slow and fast. Both ADC componentsâ€™ values were significantly correlated with cell density, but not the fractions. The NNLS model identified up to three T2 and three ADC components. For the NNLS identified T2s, the components were labeled short, medium, and long. Both the NNLS short and medium T2 values and fractions were significantly correlated with cell density. For the NNLS identified ADCs, the components were labeled slow, intermediate, and fast. Only the fast fraction was significantly correlated with cell density.The NNLS components obtained from samples of packed cells were further evaluated by washing the samples with a gadolinium contrast agent (Gdâ€“DTPA) to shorten the T2 associated with the extracellular space. Gdâ€“DTPA did not affect the short T2, which was considered to be associated with the intracellular space. The medium T2 component was no longer identified in samples with Gdâ€“DTPA and was considered to be associated with the extracellular space. Overall, the results suggest that separately measurable components exist that could be utilized for compartment specific information.",ucb,,https://escholarship.org/uc/item/00g6d7qg,,,eng,REGULAR,0,0
74,1510,Microfluidic Microbial Fuel Cells for Microstructure Interrogations,"Parra, Erika Andrea","Lin, Liwei;",2010,"The breakdown of organic substances to retrieve energy is a naturally occurring process in nature.  Catabolic microorganisms contain enzymes capable of accelerating the disintegration of simple sugars and alcohols to produce separated charge in the form of electrons and protons as byproducts that can be harvested extracellularly through an electrochemical cell to produce electrical energy directly.  Bioelectrochemical energy is then an appealing green alternative to other power sources. However, a number of fundamental questions must be addressed if the technology is to become economically feasible. Power densities are low, hence the electron flow through the system: bacteria-electrode connectivity, the volumetric limit of catalyst loading, and the rate-limiting step in the system must be understood and optimized.  This project investigated the miniaturization of microbial fuel cells to explore the scaling of the biocatalysis and generate a platform to study fundamental microstructure effects.  Ultra-micro-electrodes for single cell studies were developed within a microfluidic configuration to quantify these issues and provide insight on the output capacity of microbial fuel cells as well as commercial feasibility as power sources for electronic devices.Several devices were investigated in this work. The first prototype consisted of a gold array anode on a silicon dioxide passivation layer that intended to imitate yet simplify the complexity of a 3D carbon structure on a 2D plane. Using Geobacter sulfurreducens, an organism believed to utilize direct electron transfer to electrodes, the 1 mm2 electrode demonstrated a maximum current density of 1.4 Î¼A and 120 nW of power after 10 days. In addition, the transient current-voltage responses were analyzed over the bacterial colonization period. The results indicated that over a 6-day period, the bacteria increased the capacitance of the cell 5-orders-of-magnitude and decreased the resistance by 3X over the bare electrode. Furthermore, over short experimental scales (hours), the RC constant was maintained but capacitance and resistance were inversely related.  As the capacitance result coincides with expected biomass increase over the incubation period, it may be possible for an electrical spectroscopy (impedance) non-invasive technique to be developed to estimate biomass on the electrode. Similarly, the R and C relationship over short experimental scales could be explored further to provide insight on biolm morphology. Lastly, fluorescence and SEM microscopy were used to observe the biofilm development and demonstrated that, rather than growing at even density, the bacteria nucleated at points on the electrode, and dendritically divided, until joining to form the ""dense"" biofilm. In addition, viable microorganisms undergoing cell division were found dozens of microns from electrode surfaces without visible pili connections.To investigate single-cell catalysis or microstructure effects, a sub-micro-liter microfluidic single-channel MFC with an embedded reference electrode and solid-state nal electron acceptor was developed. The system allowed for parallel (16) working ultra-micro-electrodes and was microscopy compatible. With Geobacter sulfurreducens, the semiconducting ITO electrodes demonstrated forward bias behavior and suitability for anodic characterization.  The first prototype demonstrated, with 179 cells on the electrode, a per cell contribution of 223 fA at +400 mV (vs. SHE). The second prototype with a 7 Î¼m diameter electrode produced a current density of 3.9 pA/Î¼m2 (3.9 A/m2) at +200 mV (vs. SHE) and a signal-to-noise ratio (SNR) of 4.9 when inoculated at a seeding density of 109 cells/mL. However, diluting the sample by 10x produced an SNR of 0.5, suggesting that obtaining single cell electron transfer rates to an electrode over short experimental time scales may not be possible with the system as tested.  Nevertheless, the platform allows microstructure characterization and multiplexing within a single microfluidic chamber.",ucb,,https://escholarship.org/uc/item/04r1g2xs,,,eng,REGULAR,0,0
75,1511,Essays in Behavioral Economics and Environmental Policy,"Sexton, Steven E.","Zilberman, David;",2012,"Social planners have long relied upon non-coercive interventions in order to achieve social welfare improvements that are not obtained by markets or direct policy. Such policies are perhaps nowhere more relevant and common than in environmental economics. Environmental goods and services are typically not traded in markets because of the difficulties of property rights assignment. And yet efforts to create markets or correct market failures by coercive policy are fraught with controversy. Thus, in addition to coercive mechanisms, social planners use information provision campaigns, appeals for cooperation, and ""nudges"" to improve the efficiency of environmental resource allocations. Non-coercive interventions have grown in popularity among social planners as behavioral economics has gained acceptance within the mainstream of the field. Indeed, such policies typically affect market outcomes and achieve environmental goals only insofar as they can exploit or correct decision making that deviates from standard theory.In this dissertation, agent behavior is analyzed to assess the potential of non-coercive interventions to achieve socially preferred environmental outcomes. In a first essay, the concept of conspicuous conservation is introduced as a modern variant of conspicuous consumption that affords status for displays of austerity meant to signal environmental preferences rather than displays of ostentation meant to signal wealth. I identify conspicuous conservation in the automobile market and estimate a willingness to pay up to several thousand dollars for the ""green"" signal transmitted by ownership of the Toyota Prius. In a second essay, I demonstrate how automatic bill payment programs can induce excessive consumption of goods and services by boundedly rational consumers who exhibit inattention to prices. As automatic payment programs have spread throughout industries characterized by recurring payments, from utility and telecommunication services to insurance and loan markets, this essay is the first to consider their implications for consumer demand and welfare. It is also the first to test empirically whether enrollment in such programs increases demand, as price salience theory suggests. It is shown that residential electricity consumption increases on average 2-4.5% due to enrollment in automatic payment programs, while commercial electricity consumption grows much as 6%. Moreover, bill-smoothing programs that utilities offer to low-income households are shown to induce an 8-9% increase in electricity consumption.A final essay examines the extent to which free transit fares and appeals for car-trip avoidance reduce car pollution on smoggy days. With data on freeway traffic volumes and transit ridership, public appeals for cooperation are shown to have no significant effect on car trip demand. Free transit fares, however, do have a significant effect on car trip demand. But the effect is perverse in that it generates an increase in car trips and related pollution. Free fares also increase transit ridership. These results suggest that free transit rides do not induce motorists to substitute to transit, but instead subsidize regular transit rides and additional trips. Appeals for cooperation also have no affect on carpooling behavior.Viewed in their totality, these essays communicate the importance of behavioral theories in formulating environmental policies and predicting agents' responses to such policies. Policies formulated without due regard for agents' bounded rationality and multifaceted motivations are doomed to unintended consequeces. However, recognition of these behavioral responses and their incorporation in policy design can result in improved environmental outcomes and efficient policies.",ucb,,https://escholarship.org/uc/item/04x0f3cc,,,eng,REGULAR,0,0
76,1512,The Organizational Weapon: Ruling Parties in Authoritarian Regimes,"Meng, Anne","Arriola, Leonardo;Powell, Robert;",2016,"This project examines party building in authoritarian regimes. The overarching puzzle I seek to address is: why are some autocratic ruling parties stronger organizations than others? What explains variation in the institutional capacity of autocratic rule? The collection of three essays in this dissertation outline the strategic logic of party institutionalization, in addition to providing new and original ways in which to measure this key concept of authoritarian party strength. It tests previously untested hypotheses about the origins of strong autocratic parties and provides insights on the conditions under which leaders will be incentivized to rule through binding institutions. The first paper conceptualizes autocratic party strength as institutionalization and provides new ways of measuring this variable. The second paper describes the strategic logic of party institutionalization in autocracies and explains why and when some autocrats choose to tie their own hands. The third paper evaluates the thesis that parties emerging out of revolutions and independence wars tend to be more durable by examining parties that emerged out of independence struggles in Africa.",ucb,,https://escholarship.org/uc/item/04x1698k,,,eng,REGULAR,0,0
77,1513,Exploring Competing Orders in the High-Tc Cuprate Phase Diagram Using Angle Resolved Photoemission Spectroscopy,"Garcia, Daniel Robert","Lanzara, Alessandra;",2010,"With more than a quarter century of study, the high temperature superconducting cuprates still represent one of the most active areas of research in condensed matter physics. Its complex phase diagram  continues to present challenges to our understanding, stemming from its correlated electronic nature. Being able to tease out the effect of different lattice orderings and their effects on electronic states may be crucial to understanding the physics of the cuprates where such  orderings may be crucial to the phase diagram. Thus, because of its ability to directly probe electronic band structure, Angle Resolved  Photoemission Spectroscopy (ARPES) is an ideal probe to study the effects of competing orders on electronicstates near EF.  This thesis will be organized in the following way. Chapter 1 provides a broad introduction to the physics central to our work including concepts of band structure and Fermi liquid theory, as well as more exotic phenomena explored throughout the thesis. Chapter 2 introduces the ARPES technique, how it is physically understood via concepts like Green's functions, and traditional methods of data analysis. Chapter 3 explores magnetic ordering and its effect on both core level and valance band states in the iron oxypnictides. From the near-EF electronic states, we find that the magnetic physics of the parent compound may still be present even at superconducting dopings. Chapter 4 explores charge density wave (CDW) ordering by looking at the rare earth ditellurides. This ARPES work establishes LaTe2 as the first quasi-2D CDW system to behave like a true Peierls transition, with both Fermi surface nesting tied to a metal - to - insulating transition. Chapter 5 explores the effectof lattice strain on electronic states by studying the single layered Bi2201 cuprates with lanthanide substitution. The effect of this substitution competes with superconductivity and appears to enhance bosonic modes acting on the nodal point states which are otherwise unaffected. Chapter 6 takes the specific case of Nd-Bi2201 and finds evidence of a distinct crossover point in the electronic states near EF segregating the nodal point states. Finally, Chapter 7 provides a summary of our work and its conclusions.",ucb,,https://escholarship.org/uc/item/04x3m7kc,,,eng,REGULAR,0,0
78,1514,Ion Transport and Structure in Polymer Electrolytes with Applications in Lithium Batteries,"Chintapalli, Mahati","Balsara, Nitash P;Minor, Andrew M;",2016,"When mixed with lithium salts, polymers that contain more than one chemical group, such as block copolymers and endgroup-functionalized polymers, are promising electrolyte materials for next-generation lithium batteries.  One chemical group can provide good ion solvation and transport properties, while the other chemical group can provide secondary properties that improve the performance characteristics of the battery.  Secondary properties of interest include non-flammability for safer lithium ion batteries and high mechanical modulus for dendrite resistance in high energy density lithium metal batteries.  Block copolymers and other materials with multiple chemical groups tend to exhibit nanoscale heterogeneity and can undergo microphase separation, which impacts the ion transport properties.  In block copolymers that microphase separate, ordered self-assembled structures occur on longer length scales.  Understanding the interplay between structure at different length scales, salt concentration, and ion transport is important for improving the performance of multifunctional polymer electrolytes.  In this dissertation, two electrolyte materials are characterized: mixtures of endgroup-functionalized, short chain perfluoropolyethers (PFPEs) and lithium bis(trifluoromethanesulfonyl) imide (LiTFSI) salt, and mixtures of polystyrene-block-poly(ethylene oxide) (PS-b-PEO; SEO) and LiTFSI.  The PFPE/LiTFSI electrolytes are liquids in which the PFPE backbone provides non-flammability, and the endgroups resemble small molecules that solvate ions.  In these electrolytes, the ion transport properties and nanoscale heterogeneity (length scale ~1 nm) are characterized as a function of endgroup using electrochemical techniques, nuclear magnetic resonance spectroscopy, and wide angle X-ray scattering.  Endgroups, especially those containing PEO segments, have a large impact on ionic conductivity, in part because the salt distribution is not homogenous; we find that salt partitions preferentially into the endgroup-rich regions.  On the other hand, the SEO/LiTFSI electrolytes are fully microphase-separated, solid, lamellar materials in which the PS block provides mechanical rigidity and the PEO block solvates the ions.  In these electrolytes longer length scale structure (~10 nm â€“ 1 Î¼m) influences ion transport.  We study the relationships between the lamellar grain size, salt concentration, and ionic conductivity using ac impedance spectroscopy, small angle X-ray scattering, electron microscopy, and finite element simulations.  In experiments, decreasing grain size is found to correlate with increasing salt concentration and increasing ionic conductivity.   Studies on both of these polymer electrolytes illustrate that structure and ion transport are closely linked.",ucb,,https://escholarship.org/uc/item/04z7w363,,,eng,REGULAR,0,0
79,1515,Essays in Applied Microeconomics,"Severnini, Edson Roberto","Card, David;",2013,"This dissertation consists of three studies analyzing causes and consequences of location decisions by economic agents in the U.S. In Chapter 1, I address the longstanding question of the extent to which the geographic clustering of economic activity may be attributable to agglomeration spillovers as opposed to natural advantages. I present evidence on this question using data on the long-run effects of large scale hydroelectric dams built in the U.S. over the 20th century, obtained through a unique comparison between counties with or without dams but with similar hydropower potential. Until mid-century, the availability of cheap local power from hydroelectric dams conveyed an important advantage that attracted industry and population. By the 1950s, however, these advantages were attenuated by improvements in the efficiency of thermal power generation and the advent of high tension transmission lines. Using a novel combination of synthetic control methods and event-study techniques, I show that, on average, dams built before 1950 had substantial short run effects on local population and employment growth, whereas those built after 1950 had no such effects. Moreover, the impact of pre-1950 dams persisted and continued to grow after the advantages of cheap local hydroelectricity were attenuated, suggesting the presence of important agglomeration spillovers. Over a 50 year horizon, I estimate that at least one half of the long run effect of pre-1950 dams is due to spillovers. The estimated short and long run effects are highly robust to alternative procedures for selecting synthetic controls, to controls for confounding factors such as proximity to transportation networks, and to alternative sample restrictions, such as dropping dams built by the Tennessee Valley Authority or removing control counties with environmental regulations. I also find small local agglomeration effects from smaller dam projects, and small spillovers to nearby locations from large dams. Lastly, I find relatively small costs of environmental regulations associated with hydroelectric licensing rules. In Chapter 2, I study the joint choice of spouse and location made by individuals at the start of their adult lives. I assume that potential spouses meet in a marriage market and decide who to marry and where they will live, taking account of varying economic opportunities in different locations and inherent preferences for living near the families of both spouses. I develop a theoretical framework that incorporates a collective model of household allocation, conditional on the choice of spouse and location, with a forward-looking model of the marriage market that allows for the potential inability of spouses to commit to a particular intra-household sharing rule. I address the issue of unobserved heterogeneity in the tastes of husbands and wives using a control-function approach that assumes there is a one-to-one mapping between unobserved preferences of the two spouses and their labor supply choices. Estimation results for young dual-career households in the 2000 Census lead to three main findings. First, I find excess sensitivity of the sharing rule that governs the allocation of resources among couples to the conditions in the location they actually choose, implying that spouses cannot fully commit to a sharing rule. Second, I show that the lack of commitment has a relatively larger effect on the share of family resources received by women. Third, I find that the failure of full commitment can explain nearly all of the gap in the interstate migration rates of single and married people in the U.S. Finally, in Chapter 3, I examine unintended consequences of environmental regulations affecting the location of power plants. I present evidence that while hydroelectric licensing rules do conserve the wilderness and the wildlife by restricting the development of hydro projects in some counties, they lead to more greenhouse gas emissions in those same locations. Such environmental regulations aimed to preserve natural ecosystems do not seem to really protect nature. Basically, land conservation regulations give rise to a replacement of hydropower, which is a renewable, non-emitting source of energy, with conventional fossil-fuel power, which is highly pollutant. Restrictions imposed by hydroelectric licensing rules might be used as leverage by electric utilities to get permits to expand thermal power generation. Each megawatt of hydropower potential that is not developed because of those regulations induces the production of the average emissions of carbon dioxide per megawatt of U.S. coal-fired power plants. Environmental regulations focusing only on the preservation of ecosystems appears to stimulate dirty substitutions within electric utilities regarding electricity generation.",ucb,,https://escholarship.org/uc/item/0554c2gv,,,eng,REGULAR,0,0
80,1516,The Development and Evolution of Floral Symmetry in the Zingiberales and Interactive Tools for Teaching Evolution (ArborEd),"Bruenn, Riva Anne","Specht, Chelsea D;",2017,"AbstractThe Development and Evolution of Floral Symmetry in the Zingiberales and Interactive Tools for Teaching Evolution (ArborEd)byRiva Anne BruennDoctor of Philosophy in Plant BiologyUniversity of California, BerkeleyProfessor Chelsea D. Specht, ChairFloral symmetry is a key innovation in the evolution of flowering plants. Zygomorphy, or single-planed symmetry, is associated with the diversification of many flowering plant lineages. The model system for floral symmetry is the snapdragon (Antirrhinum majus). In A. majus flowers, a set of TCP and MYB-related transcription factors form a core gene regulatory network necessary for zygomorphy. The genes involved in this network have been implicated in several independent transitions to zygomorphy from actinomorphy (many-planed symmetry). Although the TCP components of the symmetry network have been investigated across flowering plants, MYB-related transcription factors remain largely unstudied outside of the Asterid group containing A. majus and close relatives. Here we investigate the evolution of MYB-related genes DIVARICATA-like (DIV-like), RADIALIS-like (RAD-like), and DIVARICATA and RADIALIS INTERACTING FACTOR-like (DRIF-like) across flowering plants, and their expression patterns in the developing flowers of two zygomorphic species of the monocot order Zingiberales. We found that RAD-like and DIV-like are sister MYB-related genes which diverged before the diversification of flowering plants. Each gene contains one MYB-like domain that has been closely conserved throughout flowering plant evolution. Furthermore, we identified candidate homologs to A. majus RAD and DIV in several monocot taxa, with at least three copies of each in the Zingiberales. In the Zingiberales, RAD-like and DIV-like genes are expressed in Costus spicatus (Costaceae) and Musa basjoo (Musaceae) in patterns consistent with roles in floral symmetry. Using Reverse Transcription PCR and in situ hybridization we recovered asymmetric expression patterns for some RAD-like genes across the dorsal/ventral plane of developing flowers, and universal expression of DIV-like genes, consistent with the model known from Antirrhinum majus. We identified DRIF-like genes across flowering plants, recovering a previously undescribed duplication in eudicot DRIF Group 1 genes. Furthermore, we recovered candidate DRIF-like genes in Musa basjoo (Musaceae: Zingiberales) with expression patterns similar to those described in A. majus DRIF1 and DRIF2. Finally, we developed a tutorial for high school and college students to investigate a coevolutionary hypothesis in sharpshooters and their bacterial endosymbionts. This tool will help students understand how comparative evolutionary research is performed, and give them hands-on experience performing common analyses.",ucb,,https://escholarship.org/uc/item/01x4k6vh,,,eng,REGULAR,0,0
81,1517,"From Subjects to Citizens: American Colonial Education and Philippine Nation-Making, 1900-1934","Francisco, Adrianne","Candida Smith, Richard;",2015,"This dissertation examines the U.S. colonial state's efforts to promote Filipino national sentiment and patriotism through the public school system between 1900 and 1934. During the early years of American rule, U.S. colonial officials argued that Filipinos lacked a sense of nationality due to their linguistic and religious diversity, cultural heterogeneity, and regionalism. This perception shaped U.S. educational policy in the Philippines, leading to the creation of a curriculum that would attempt to homogenize and foster national affiliation among Filipinos. Using administrator files, Bureau of Education records, textbooks, and curricular materials collected in both the United States and the Philippines, this study reconstructs the colonial curriculum, paying special attention to English language instruction, history and civics, and vocational education. It shows that colonial education aimed to quell Filipino anti-colonial nationalism and facilitate obedience to the colonial state by casting good citizenship and â€œproperâ€ patriotism in terms of economic self-sufficiency and non-violence, and by defining national allegiance as loyalty to both the Philippines and the U.S. Its central contention is that American colonial education created a form of Philippine nationalism that would become the dominant strain of official nationalism among Filipino leaders and educators. Bringing local actors the fore, this study enlists Filipino studentsâ€™ and educatorsâ€™ writings, vernacular novels, newspapers, and Philippine education journals to examine how Filipinos, both in the colony and metropole, responded to colonial education. It finds that Filipinos reformulated colonial lessons to fit in with older strains of Filipino nationalism even as they saw their American education as a path to economic opportunity and Philippine independence. By looking at the U.S. colonial stateâ€™s promotion of a native national identity, this study contributes to and complicates current narratives of U.S. colonial education and Philippine nationalism.",ucb,,https://escholarship.org/uc/item/01x8n57g,,,eng,REGULAR,0,0
82,1518,"Mobiles, Media, and the Agency of Indian Youth","Kumar, Neha","Parikh, Tapan S;",2013,"Technologies have been and are being designed to address varied human needs. Of these, the need for physical and economic well-being is typically considered to trump the need for culture, leisure, fun, and entertainment. Research initiatives in the field of Information and Communication Technology and Development (ICTD) have been in motion to address agricultural, educational, and health care needs, among others. The need for entertainment is central even in the lives of the `have-less', my dissertation affirms. Affordable new media technologies play a critical role towards the procurement of entertainment content and the resulting production of culture. Individuals quickly learn to navigate their way around technology, also paving the way for development-friendly outcomes. It is this phenomenon that my dissertation analyzes, as it studies individual agency in the intertwining of culture (society) and new media (technology) within the larger discourse of development.I use ethnographic methods to investigate the leisure-driven appropriation of the mobile phone by youth from socioeconomically disadvantaged backgrounds in rural, small-town, and urban India. I first analyze the influx of new media and its resulting impact on folk music practices in rural Madhya Pradesh and Rajasthan. Shifting focus to the motivations that drive youth towards mobile consumption of folk and popular media, I examine the unique material affordances of new media technologies and their influence on emerging practices. I use the Actor-Network Theory (ANT) lens to draw particular attention to the notion of agency, both human and material, as I investigate the pirate media actor-network responsible for the widespread dissemination of digital media and technical skills. I then focus on the agency of urban Indian youth that leads them to build further on these skills as they negotiate various linguistic, social, and technological hurdles for engagement with social media towards a new, improved identity for themselves.",ucb,,https://escholarship.org/uc/item/01x9b6bb,,,eng,REGULAR,0,0
83,1519,A Fractured Society: The Socio-Legal Environment of Fracking in the United States,"Kluttz, Daniel N","Fligstein, Neil D;Haveman, Heather A;",2017,"This dissertation examines the relationships between law and society when encountering disruptive, risky economic activities. In doing so, it assesses how culture, politics, civil society, and powerful industry interests influence the laws and legal instruments intended to protect or benefit citizens exposed to such activities. My case is the domestic shale-energy boom brought about by â€œfracking,â€ which, over the past decade, has revolutionized the US energy economy and sparked controversy for its potentially detrimental effects on local communities and the environment. By examining sociological influences on statesâ€™ fracking regulations and revealing inequalities reinforced in individual mineral-rights leasing contracts, I span time and space to analyze the social forces that shape who wins and who loses when fracking comes to an area.The dissertation is organized around three empirical studies. In the first, I draw from theories of social movements, organizations, politics, and markets to examine how social movements, economic industries, and state institutional environments influence the decisions of states to issue new regulations governing fracking or ban it altogether. Analyzing a longitudinal dataset of 34 states at risk of fracking from 2009 to 2016, and consistent with findings from political sociology and social-movement literatures, I find that increased economic security and increased environmental movement organizational capacity in a state boost the likelihood that a state will regulate the fracking industry or even ban fracking entirely. I also find that higher potential profitability (and accordingly, potential environmental risk) for fracking in a state moderates the effects of state government ideology and resource dependence on industry. These findings support my argument that the effects of non-state actors and institutional context on the regulation of disruptive industrial activity in new markets depend, in part, on the extent of potential economic benefits and societal risks posed by the economic activity.In the second empirical chapter, I examine the political, economic, and cultural factors influencing how stringently states regulate fracking. Analyzing state fracking chemical disclosure requirements from 2009 through 2016, I find that a stateâ€™s expected chemical disclosure stringency is most positively influenced by how stringently its geographically proximate peer states regulate. Interacting economic hardship with fossil-fuel industry political influence is associated with less stringent regulation. I argue for a field theory-based approach to state-level regulation, which conceives of states as both constitutive of their own regulatory fields and embedded within broader fields, taking similarly situated states into account but susceptible to industry capture during particularly difficult economic times.Finally, in the last empirical chapter, I move from the state to the local level and investigate how social inequalities become reinforced in legal instruments. Specifically, I analyze economic disparities in a ubiquitous but understudied aspect of the fracking boom: mineral-rights lease contracts. Lease contracts represent an alternative, but no less important, way that socio-legal processes determine who stands to gain, and who stands to lose, when fracking comes to town. I analyze a unique proprietary dataset of nearly 90,000 leases in Texasâ€™s Barnett shale. I find that 1) local-community embeddedness yields expected higher payments to mineral-rights owners when compared to those who reside outside of the local community, and 2) people of color, in particular those of Hispanic/Latino ethnicity, receive significantly lower royalty terms when compared to whites, all else equal. The results hold when extended to a national-level analysis. These findings suggest that local ties can open pathways to locally sourced information and confer social capital, which can be beneficial during contract negotiations. They also support sociological theories of how social biases and categories affect economic transactions, resulting in patterned inequalities and discriminatory effects for socially disadvantaged groups. This chapter opens a new empirical domainâ€”subsurface property rightsâ€”for socio-legal studies of contracts, and it offers new theoretical directions into how social inequalities become reinforced in legal instruments.",ucb,,https://escholarship.org/uc/item/0264r1mc,,,eng,REGULAR,0,0
84,1520,Global Architects Meet the Place - Bridging the Gap through Information and Communication Technology,"Perez, Yael Valerie","Kalay, Yehuda E;Agogino, Alice M;",2013,"In this study, I examine the ability of Information and Communication Technologies (ICTs) to narrow the gap between architects, aspiring to meet the place, and local users that are part of the place. The overarching goal is to identify tools necessary for successful place-driven design, particularly in the extreme design conditions in marginalized places. International architects are often invited to design in difficult to access, marginalized places through aid-organizations or through international developers invested in these places. This scenario propagates the gap between the architect's conceptions of place and the local users' conceptions of place. The design literature provides a range of recommendations for comprehending place. Yet, as expressed by several of the architects interviewed, these commonly used design methods appear to be ineffective in marginalized places, too often leading to designs that are inappropriate. Addressing the gap with marginalized places is especially valuable given their limited resources and the impact that design projects have on human development, which I refer to as `design freedom'.In search for tools to comprehend place I take on Canter's 1977 definition of place as the overlap between physical attributes, activities, and conceptions. Through interviews with architects, designing in marginalized places, both within the non-profit and for-profit realms, I found that while Internet-based ICTs are currently used for capturing physical attributes of place they are underutilized in communicating the subjective conceptions of place. By compiling the recommended methods in the literature together with those used by architects I interviewed I identify five levels of depth of the experiences available for comprehending place: egocentric, passive, active, interactive, and immersive. My hypothesis is therefore that when designing in marginalized places, a set of technologies that communicates the breadth of place through deep experiences will equip designers with comprehensive information about the place, enabling more place-appropriate design.Participatory Action Research (PAR) methodologies were used in two case-studies of design with the Pinoleville Pomo Nation (PPN), a Native American nation located near Ukiah, California. The first case study is a reflection on action through which I evaluated both face-to-face and mediated techniques for meeting the PPN. Through this reflection I identify the most-appropriate ICTs, and assembled them to communicate the PPN's place. In the second study I assess and measure how these technologies are used in an actual design project through ParticiPlace, an international design competition that attracted 17 design teams from around the world, to work on the PPN's Living Culture Center.Through these studies I found that technologies which communicate all three elements of place - physical-attributes, activities, and conceptions - can bridge the gap between designers and place. More specifically, architects who visited the site produced, on average, the same levels of place-appropriate designs compared to those who were too far to visit it and relied solely on ICTs to experience place. I have identified social networks as a technology that enables immersion in the conceptions of place. Nevertheless, while social networks can immerse users in conceptions, several limitations, including privacy setting still hinder its professional design use in marginalized communities. Moreover, integration of social network with technologies to allow interaction with physical attributes and with activities of place is still required to make these more effective place-driven design tools. I conclude with recommendations for ICT attributes to support place-driven design with a focus on marginalized communities.",ucb,,https://escholarship.org/uc/item/02c496hg,,,eng,REGULAR,0,0
85,1521,The Impact of Sleep Deprivation on Anxiety and Affective Brain Function,"Goldstein, Andrea Nicole","Walker, Matthew P;",2014,"Recent evidence suggests there is an intimate and causal relationship between sleep and anxiety. Despite such progress, several unknowns remain. For example which factors predispose some individuals to be vulnerable to the anxiogenic impact of sleep loss while others appear to be resilient have yet to be identified. Furthermore, whether trait-anxiety, in turn, predicts the sensitivity of individuals to amplified emotional brain reactivity associated with a lack of sleep is similarly unknown. Moreover, little is currently known about the embodied interplay between peripheral and central nervous system mechanisms leading to such abnormalities of affective processing caused by sleep deprivation. Characterizing these mechanisms is necessary not only to gain a deeper understanding of the pathophysiological pathways underlying the condition of anxiety, but also for developing effective treatments for the amelioration of anxiety as well as guiding public health policy aimed at anxiety disorder prevention. Targeting these unanswered questions, this thesis combines functional and structural MRI techniques, together with high-density EEG recordings, to test the overarching hypothesis that sleep deprivation leads to dysregulation of the extended-limbic system contributing to, and interacting with, the state of anxiety. Three specific predictions emerged from this overarching hypothesis and were tested in separate experiments. First, experiment 1 confirms the hypothesis that structural brain morphology in a network of limbic brain regions predicts vulnerability to the anxiogenic impact of sleep deprivation, and that sex moderates this interaction. Second, experiment 2 provides evidence supporting the hypothesis that trait anxiety determines the degree to which sleep deprivation amplifies limbic brain reactivity during the anticipation of potentially aversive emotional experiences. Finally, results from experiment 3 affirms the prediction that, beyond central limbic brain changes, sleep deprivation additionally dysregulates peripheral, autonomic cardiac signaling in response to affective stimuli as well as decouples the normally inter-related association between central brain and peripheral nervous systems. Collectively, these results help characterize a framework in which sleep deprivation contributes to anxiety symptomology through impairments in affective processing by both the central (limbic) and peripheral (autonomic) nervous system functioning. Moreover, these data suggest that the co-morbid features of sleep disruption and altered limbic as well as autonomic function commonly reported across anxiety disorders may be causally related. Considering the continued decreases in sleep time across society and the high prevalence of anxiety disorders, these findings have significant therapeutic, clinical and public health ramifications.",ucb,,https://escholarship.org/uc/item/0569x2sd,,,eng,REGULAR,0,0
86,1522,Essays on Environmental Policy in Energy Markets,"Boomhower, Judson Paul","Borenstein, Severin;",2015,"Producing and consuming energy involves costly environmental externalities, which are addressed through a wide range of public policy interventions.  This dissertation examines three economic questions that are important to environmental regulation in energy.  The first chapter measures the effect of bankruptcy protection on industry structure and environmental outcomes in oil and gas extraction.  The second chapter measures additionality in an appliance replacement rebate program.  Finally, the third chapter focuses on the environmental impacts of subsidizing electricity production from forest-derived biomass fuels.   The first chapter measures the incentive effect of limited liability.  When liability is limited by bankruptcy, theory says that firms will take excessive environmental and public health risks.  In the long run, this ``judgment-proof problem'' may increase the share of small producers, even when there are economies of scale.  I use quasi-experimental variation in liability exposure to measure the effects of bankruptcy protection on industry structure and environmental outcomes in oil and gas extraction.  Using firm-level data on the universe of Texas oil and gas producers, I examine the introduction of an insurance mandate that reduced firms' ability to avoid liability through bankruptcy.  The policy was introduced via a quasi-randomized rollout, which allows me to cleanly identify its effects on industry structure. The insurance requirement pushed about 6% of producers out of the market immediately.  The exiting firms were primarily small and were more likely to have poor environmental records.  Among firms that remained in business, the bond requirement reduced oil production among the smallest 80% of firms by about 4% on average, which is consistent with increased internalization of environmental costs. Production by the largest 20% of firms, which account for the majority of total production, was unaffected.  Finally, environmental outcomes, including those related to groundwater contamination, also improved sharply.  These results suggest that incomplete internalization of environmental and safety costs due to bankruptcy protection is an important determinant of industry structure and safety effort in hazardous industries, with significant welfare consequences.The second chapter focuses on the importance of a regulator's inability to distinguish between households responding to a subsidy, and households doing what they would also have done in the absence of policy.  Economists have long argued that many recipients of energy-efficiency subsidies may be ``non-additional,'' getting paid to do what they would have done anyway.  Demonstrating this empirically has been difficult, however, because of endogeneity concerns and other challenges. In this paper we use a regression discontinuity analysis to examine participation in a large-scale residential energy-efficiency program. Comparing behavior just on either side of several eligibility thresholds, we find that program participation increases with larger subsidy amounts, but that most households would have participated even with much lower subsidy amounts. The large fraction of inframarginal participants means that the larger subsidy amounts are almost certainly not cost-effective. Moreover, the results imply that about half of all participants would have adopted the energy-efficient technology even with no subsidy whatsoever.Finally, the third chapter addresses consequences of renewable energy subsidies in other markets.  Electricity generated from logging residues provides a large and growing share of US renewable electricity generation.  Much of the low-value wood used by biomass power plants might otherwise be left in the field.  This increased harvest can negatively affect forest health. I investigate the supply of woody biomass fuel in Maine using a 15-year panel of prices and quantities for whole tree wood chips.  I find that doubling the price of woody biomass increases harvest by about 64%.  I also find that coal prices are a major determinant of woody biomass harvest.  This suggests that environmental policies that raise the price of coal will affect forest health.",ucb,,https://escholarship.org/uc/item/05f4r67b,,,eng,REGULAR,0,0
87,1523,"Machine Learning Techniques in Nuclear Material Detection, Drug Ranking and Video Tracking","Yang, Yan","Hochbaum, Dorit S.;",2013,"The main focus of this thesis is using machine learning and data mining techniques to solve challenging problems.  Three problems from different subject areas are discussed: nuclear material detection, drug ranking and target tracking in video sequences. The techniques of the three problems described are all based on an efficiently solvable variant of normalized cut, Normalized Cut Prime (NC').The first problem concerns detecting concealed illicit nuclear material, an important part of strategies preventing and deterring nuclear terrorism.  What makes this an extremely difficult task are physical limitations of nuclear radiation detectors (arising from energy resolutions and efficiency) and shielding materials terrorists would presumably use to surround the radioactive nuclear material and absorb some of the radiation, thereby reducing the strength of the detected signal. This means the central data analysis problem is identifying a potentially very weak signal, and distinguishing it from both background noise arising from the detector characteristics and naturally occurring environmental radiation.  We aim at enhancing the capabilities of detection with algorithmic methods specifically tailored to nuclear data. A novel graph-theory-based methodology based on NC' is used, called Supervised Normalized Cut (SNC).  This data mining method classifies measurements obtained from very low resolution plastic scintillation detectors.  The accompanying computational study, comparing SNC method with several alternative classification methods shows that in terms of accuracy, the SNC method is on par with alternative approaches, yet SNC is computationally more efficient.The second subject area is in the field of drug ranking.  This problem refers to placing in rank order, according to their effectiveness, several drugs treating the same disease, using data derived from cell images.   Current technologies use the recently developed high-throughput drug profiling (high content screening or HCS).  Despite the potential of HCS for accurate descriptions of drug profiles,   it produces a deluge of data of quantitative and multidimensional nature, posing analytical challenges in the data mining process.  Our new framework is designed to alleviate these difficulties, in the way of producing graph theoretic descriptors and automatically ordering the performance of drugs, called fractional adjusted bi-partitional score (FABS), a way of converting classification to scores.   We experimented with the FABS framework by implementing different algorithms and assessing the accuracy of results by a comparative study, which includes other four baseline methods.  The conclusion is encouraging: FABS implemented with NC' consistently outperforms other implementations of FABS and alternative methods currently used for ranking that are unrelated to FABS.   The third problem is target tracking in video sequences -- it can be framed as an unsupervised learning problem:  the goal is to delineate a target of interest in a video from background.  The tracking task is cast as a graph-cut, incorporating intensity and motion data into the formulation.  Tests on real-life benchmark videos show that the developed technique, NC-track, based on NC', is more efficient than many existing techniques, and that it delivers good quality results.",ucb,,https://escholarship.org/uc/item/05f838rg,,,eng,REGULAR,0,0
88,1524,ARTELIA GREENâ€™S & OLIVIA WILLIAMSâ€™ LEGACY: A STUDY ON THE PEDAGOGICAL PRACTICES THAT IMPROVE HEALTH FOR BLACK CHILDREN,"Johnson, Tiffani Marie","Perlstein, Daniel;",2019,"This dissertation examines the relationship between caring teaching practices and greater health outcomes for black children. Public health theory suggests that Black youth generally experienced greater levels of adversity compared to non-black youth (Schilling et al., 2007; Marie, 2016). Exposure to these frequent and/or sustained stressors without the buffering care of a supportive adult can change childrenâ€™s brains and bodies, including disrupting learning, behavior, immune systems, and even the way DNA is read and transcribed. My research examines the efficacy of critical classroom pedagogy (Duncan-Andrade & Morrell, 2008) and social design-based research (Gutierrez, 2016) as a framework to address and attenuate the impacts of toxic stressors that black youth embody.This study honors research principles grounded in care (Angelou, 1979; Noddings, 1988; Duncan-Andrade, 2006), to generate grounded theory for social transformation. This dissertation anchors data (field notes, classroom video, in-depth interviews) in order to integrate the fields of education and public health to produce ecologically valid findings that: 1) highlight and reproduce that types of teaching practices and conditions that mediate healthier children and 2) reframe our understandings of the possibilities of education.",ucb,,https://escholarship.org/uc/item/05g5p795,,,eng,REGULAR,0,0
89,1525,"The Grammaticalization of Grammatical Relations: A Typological and Historical Study Involving Kashaya Pomo, Old English, and Modern English","Gamon, David",,1997,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/05g8s71b,,,eng,REGULAR,0,0
90,1526,Applying riboswitches for novel sensing and chemistry,"Truong, Johnny","Hammond, Ming C;Francis, Matthew B;",2019,"Riboswitches are cis-regulatory structured RNA elements capable of controlling expression of downstream genes by binding to small molecule ligands. These naturally evolved RNA elements possess remarkable affinity and selectivity for their small molecule ligands, high folding efficiencies, and thermostability for functioning in cellular environments.  Due to these properties, a number of riboswitch-based technologies have emerged such as riboswitch reporters, aptazymes, and RNA-based fluorescent (RBF) biosensors which all have wide applications for detection, imaging, and regulatory circuits. While riboswitch reporters and aptazymes have been robustly studied to better understand how to improve their function, there are fewer studies that expand on RBF biosensor development.  Here, novel approaches towards expanding the functional repertoire of RBF biosensors and systematically probing their properties are described.First, we show that engineering circular permutations of the riboswitch aptamer domain yields functional biosensors for S-adenosyl-L-methionine (SAM), using the SAM-I riboswitch as our model. We reveal that this design can enhance fluorescence turn-on and ligand binding affinity compared to the non-permuted topology. Expanding upon these established design principles, novel biosensors for the ligand guanidine was developed. Two novel designs were added to our existing repertoire that generated functional RBF biosensors using the architecture of the guanidine-I riboswitch. A new base-pair mutation strategy was applied on these guanidine biosensors which, resulting in modest changes to biosensor activation speeds just from single base-pair mutations. Lastly, riboswitches were explored as potent scaffolds to generate a self-labeling ribozyme.  Various natural or engineered riboswitches for the electrophilic ligand, SAM, were screened for reactivity with an analog, Hey-SAM, as a proxy to measure ribozyme activity. In collaboration with Agilent Labs, a high-throughput method was developed for probing and screening latent ribozyme activity using a microarray platform. The efforts and strategies put forth here use riboswitches outside their native context for applications in detection and catalysis further showcasing the broad utility of riboswitch-based tools.",ucb,,https://escholarship.org/uc/item/05n8k3nq,,,eng,REGULAR,0,0
91,1527,Writing the Storyteller: Folklore and Literature from Nineteenth-Century France to the Francophone World,"Gipson, Jennifer","Paige, Nicholas;",2011,"Nineteenth-century modernity, according to Walter Benjamin and other critics, kills storytelling.  Instead of treating this as a real disappearance, I consider how writers continually reinvent this death to work through historically specific questions about tradition, memory, and cultural transmission.  In nineteenth-century France, for example, the recurrent belief in the end of tradition prompted movements for folklore collection--like NapolÃ©on III's decree for the preservation of poÃ©sies populaires--as well as broad reflections about the future of cultural expressivity.  Nodier, Nerval, MÃ©rimÃ©e, and Champfleury, all combined literary creation with folklore study for the eclipse of oral tradition was, paradoxically, the very foundation of modern literature as it was coming to be defined.  Thus, Nerval appends to his Sylvie a folklore collection so as to mark the distance between the written and the oral, reaffirming literary modernity while mourning tradition.  Though far-removed from folklore, Barbey d'Aurevilly's short stories and fragmented frames that impede narrative transmission also question the very possibility of storytelling.  In addition to such formal innovations, writers also revisited earlier storytelling topoi.  In the early eighteenth century, when Antoine Galland introduced Les Mille et une nuits to the West, SchÃ©hÃ©razade's life-or-death storytelling stood as commentary on the tyranny of audience demands as the patronage system was breaking down.  But nineteenth-century writers returned to this ancient storytelling sultana to think about the demands of newspaper editors, growing readerships, and the transformation of literature into mass market entertainment.  Finally, interest in folklore fades on the mainland, but debates about preservation of traditions and ownership of cultural goods--debates once linked to France's colonial projects--become central to post-colonial Francophone literature.  Patrick Chamoiseau and other proponents of crÃ©olitÃ© spotlight the paradox of preserving Creole storytellers' legacies in writing--or in French.  Assia Djebar even intersperses her bloody tale of a modern Algerian SchÃ©hÃ©razade with fragments from Galland's eighteenth-century text, foregrounding the question of what happens to the voice and to stories after a storyteller dies.  In short, folklore has a long history of being a reference point for thinking about the very notions of literature or modernity that supposedly spell its demise.  Literary depictions of storytelling tell a story less about oral culture than about literature itself. And concern about who is no longer telling stories often reveals a deeper cultural anxiety about who is now writing or reading stories.",ucb,,https://escholarship.org/uc/item/05p5h8zd,,,eng,REGULAR,0,0
92,1528,Quasi-Variational Inequalities for Source-Expanding Hele-Shaw Problems,"DeIonno, John Adrian","Evans, Lawrence C.;",2013,"We study nonlinear partial differential equations describing Hele-Shaw flows for which the source of the fluid, which classically is at a single point, instead expands as the moving boundary uncovers new sources. This problem is reformulated as a type of quasi- variational inequality, for which we show there exists a unique weak solution. Our procedure utilizes a Baiocchi integral transform. However, unlike in traditional applications, the inequality solved by the transformed variable continues to depend on the free boundary, and thus standard theory of variational inequalities cannot be immediately applied. Instead, we develop convergent time-stepping scheme. As a further application, we generalize and study a related problem with a growing density function obeying a hard constraint on the maximum density.",ucb,,https://escholarship.org/uc/item/05z2t7br,,,eng,REGULAR,0,0
93,1529,Modern Low-Complexity Capacity-Achieving Codes For Network Communication,"Goela, Naveen","Gastpar, Michael;",2013,"Communication over unreliable, interfering networks is one of the current challenges inengineering. For point-to-point channels, Shannon established capacity results in 1948, and it took more than forty years to find coded systems approaching the capacity limit with feasible complexity. Significant research efforts have gone into extending Shannon's capacity results to networks with many partial successes. By contrast, the development of low-complexity codes for networks has received limited attention to date. The focus of this thesis is the design of capacity-achieving network codes realizable by modern signal processing circuits. For classes of networks, the following codes have been invented on the foundation of algebraic structure and probability theory: i ) Broadcast codes which achieve multi-user rates on the capacity boundary of several types of broadcast channels. The codes utilize ArÃ½kan's polarization theory of random variables, providing insight into information-theoretic concepts such as random binning, superposition coding, and Marton's construction. Reproducible experiments over block lengths n = 512, 1024, 2048 corroborate the theory; ii ) A network code which achieves the computing capacities of a countably infinite class of simple noiseless interfering networks. The code separates a network into irreducible parallel sub-networks and applies a new vector-space function alignment scheme inspired by the concept of interference alignment for channel communications. New bounds are developed to tighten the standardcut-set bound for multi-casting functions. As an additional example of low-complexity codes, reduced-dimension linear transforms and convex optimization methods are proposed for the lossy transmission of correlated sources across noisy networks. Surprisingly, simple un-coded or one-shot strategies achieve a performance which is exactly optimal in certain networks, or close to optimal in the low signal-to-noise regime relevant for sensor networks.",ucb,,https://escholarship.org/uc/item/05z6v4zg,,,eng,REGULAR,0,0
94,1530,SQUID-Detected MRI in the Limit of Zero Static Field,"Kelso, Nathan","Clarke, John;",2009,"The magnetic gradient fields used in magnetic resonance imaging (MRI) have a component which is parallel to the uniform field B0 = B0z, as well as a component perpendicular to B0.  The component parallel to B0 is used in spatial encoding.  The component perpendicular to B0, called the ``concomitant gradient,"" causes image distortions (by altering the magnitude and direction of the total field) if its magnitude approaches B0 at any point in the field of view (FOV).  In a conventional imaging sequence, the presence of the concomitant gradients limits the maximum gradient that can be used with a given B0 field or, conversely, limits the minimum B0 field that can be used with a given gradient field.This thesis describes an implementation of the so-called ``zero-field MRI"" (ZFMRI) pulse sequence, which allows for imaging in an arbitrarily low B0 field.  The ZFMRI sequence created an effective unidirectional gradient field by using a train of Ï€ pulses to average out the concomitant gradient components during encoding.  The signals were acquired using a low-transition temperature dc Superconducting QUantum Interference Device (low-Tc dc SQUID) coupled to a first-order axial gradiometer.  The experiments were carried out in a liquid helium dewar which was magnetically shielded with a single-layer mu-metal can around the outside and a superconducting Pb can contained within the helium space.  We increased the filling factor of the custom-made, double-walled Pyrex insert by placing the liquid alcohol sample, at a temperature of approximately -50Â°C, at the center of one loop of the superconducting gradiometer, which was immersed in the helium bath.Using the aforementioned sequence and apparatus, images were acquired in the limit of zero static field, using gradients of up to 100 Î¼T/m over a 23 mm FOV.  The change in field magnitude over the FOV due to gradients was up to 10 times larger than the magnitude of any static field present in the dewar (static fields arose from residual magnetic fields and were 1 Î¼T or less).  These images were free of concomitant gradient distortions.  Images encoded using aconventional imaging sequence under similar conditions were also acquired; the conventional images were irreparably distorted.The limitations of the present ZFMRI sequence implementation are considered, as well as how the procedure could be made more practical with regard to imaging time.  The extension of the technique to unshielded operation in a uniform ambient field is discussed, as are other methods of mitigating or eliminating concomitant gradient distortions.",ucb,,https://escholarship.org/uc/item/02k9c28f,,,eng,REGULAR,0,0
95,1531,From Another Psyche: The Other Consciousness of A Speculative American Mystic (The Life and Work of Jane Roberts),"Skafish, Peter William","Pandolfo, Stefania;",2011,"This dissertation attempts to develop the beginnings of a new approach to understanding the significance of modes of thought marginal and/or external to those of the modern West. I call this approach an ""anthropology of concepts"" because it examines concepts and themes belonging to scriptural, ""philosophical,"" and poetic traditions as concepts rather than, as normally happens in anthropology, in the context of social practices, historical events, or everyday life. I also call it this because it accordingly involves the close reading and interpretation of the written or oral texts in which concepts are articulated. Concepts, when treated this way, retain their capacity to bring about novel understandings of the real, and to engender thereby theoretical perspectives not attainable through more conventional interpretive means. Such an approach may be necessary if the humanities and social sciences are to continue to hold a critical perspective on a world so enclosed that gaining any distance from its basic schemes of thought has become extremely difficult.The present dissertation undertakes such an ""anthropology of concepts"" in order to elaborate what I intend to be a new theory of the psyche and consciousness. Popularly regarded as one of the founders of the New Age spirituality of the United States, Jane Roberts (1925-1984) was a ""channel"" (a kind of spirit medium) and visionary mystic who published in the 1960's and 1970's over twenty books that she understood to have been dictated or written through her by different spiritual beings, including one she called ""Seth."" Although these texts were crucial to the popularization of Western occult ideas about reincarnation, magic, and health that were at the heart of the New Age, Roberts's intellectual curiosity and background as an author of science fiction give her writings a speculative, intellectually reflexive, and even manifestly ontological tone that is reminiscent of certain mystical thinkers and that sets them apart from popular religious discourse. My engagement with Roberts' writings focuses, first of all, on the concepts she and her cohort of personalities articulated in the course of addressing what was for her the most pressing question raised by the decades she spent channeling: how could her experience during her trances of being herself and another self in the same instant of time be possible? Her answer was that such an experience--what she called ""other-consciousness""--occurs not through language but when the subject sees itself in the non-sensory, mental images of dreams and the imagination. She was right in the sense that such images, as Jean-Paul Sartre makes clear in Psychology of the Imagination, allow two aesthetic figures or persons to appear as one. My argument is that her claim is significant for showing, surprisingly enough, that contrary to what French philosophy claimed for decades, the other can be brought into and made part of consciousness without being appropriated and consciousness therefore takes a radically altered form. The baseline consciousness of oneself, that is, changes from apperception to a consciousness of oneself as both oneself and another--and even of oneself as a plurality of selves. To make this point, I read concurrently with Jane Roberts' texts the work of Deleuze, showing that she raised in her own fashion some of the same questions about being, time, and the subject as he did, but that the strange context in which she thought led her to furnish significantly different--and now for us, novel--responses to them. Given that a subject that would be at once itself and another would also be both what it actually is and what it otherwise only could have been, I furthermore show how Roberts' work allows one to rethink the Deleuzean (and by implication deconstructive) understandings of the categories of actuality and possibility and another concept--time--to which they are integrally tied. The fact that her writings provide a basis for recasting the thought of such a comprehensive philosopher on matters this fundamental is an indication, I think, of the broad value an anthropology of concepts could hold for humanistic research.",ucb,,https://escholarship.org/uc/item/02m143dp,,,eng,REGULAR,0,0
96,1532,Characterization of Complex Genetic Component Contributing to the Susceptibility for Multiple Sclerosis and Rheumatoid Arthritis,"Briggs, Farren Basil Shaw","Barcellos, Lisa F;",2010,"Autoimmune diseases (ADs) are a major public health concern, as the third most common category of disease in the US, after cancer and heart disease. As a result, ADs has become one of the most active genetic and epidemiologic research areas, however, unraveling the etiological mechanisms of ADs has proven difficult. There is strong evidence suggesting a complex genetic component contributing to all ADs. For most ADs, the prominent genetic risk locus is within the major histocompatibility complex (MHC) on chromosome 6p21.3. Unfortunately, identifying non-MHC susceptibility loci has proven difficult in these complex ADs with multigenic patterns of inheritance. Recently, through concerted international efforts, several genome-wide association (GWA) studies and subsequent replication analyses have confirmed several other AD susceptibility loci of modest effects; much of the remaining genetic variants contributing to AD susceptibility are unknown. It is clear that current approaches will be limited to identify all the complex genetic component ADs, therefore this dissertation focuses using strong epidemiological approaches and robust analytical frameworks to identify additional non-MHC genetic risk factors in two complex ADs: multiple sclerosis (MS) and rheumatoid arthritis (RA).In Chapter 1, the relationship between variation in DNA repair pathways genes and risk for MS was investigated. Univariate association testing, epistatic tests of interactions, logistic regression modeling and non-parametric Random Forests analyses were performed using genotypes from 1,343 MS cases and 1,379 healthy controls of European ancestry. A total of 485 single nucleotide polymorphisms (SNPs) within 72 genes related to DNA repair pathways, including base excision repair, nucleotide excision repair, and double strand breaks repair, were investigated. A SNP variant within GTF2H4 on 6p21.33 was significantly associated with MS (odds ratio=0.7, p=3.5 x 10-5) after accounting for multiple testing, and was not due to linkage disequilibrium with HLA-DRB1*1501. Despite clear evidence for an association between GTF2H4and MS, collectively, these results, derived from a well-powered study, do not support a strong role for variation within DNA repair pathway genes in MS.In Chapter 2, the relationship between variation within 8 candidate hypothalamic-pituitary-adrenal (HPA) axis genes and susceptibility to MS were comprehensively investigated. A total of 326 SNPs were investigated in 1,343 MS cases and 1,379 healthy controls of European ancestry using a multi-analytical strategy. Random Forests identified 8 SNPs within the corticotropin releasing hormone receptor 1 or CRHR1 locus on 17q21.31 as important predictors of MS. Based on univariate analyses: five CRHR1 variants were associated with decreased risk for disease following a conservative correction for multiple tests. Independent replication was observed in a large meta-analysis comprised of 2,624 MS cases and 7,220 healthy controls of European ancestry. The results provide strong evidence for the involvement of CRHR1 (rs242936: p=9.7 x 10-5) in MS.In Chapter 3, epistatic interactions with a well-established genetic factor (PTPN22 1858T) in a RA was investigated. The analysis consisted of four principal stages: Stage I (data reduction) - identifying candidate chromosomal regions in 292 affected sibling pairs, by predicting PTPN22 concordance using multipoint identity-by-descent probabilities and Random Forests; Stage II (extension analysis) - testing detailed genetic data within candidate chromosomal regions for epistasis with PTPN22 1858T in 677 cases and 750 controls using logistic regression; Stage III (replication analysis) - confirmation of epistatic interactions in 947 cases and 1,756 controls; Stage IV (combined analysis) - a pooled analysis including all 1,624 RA cases and 2,506 control subjects for final estimates of effect size. A total of 7 replicating epistatic interactions were identified. A SNP variant (rs7200573) within CDH13 demonstrated significant evidence for interaction (p=1.5 x 10-4) with PTPN22. There was also evidence for epistasis between PTPN22 and SNP variants within MYO3A, CEP72 and near WFDC1.The research conducted in Chapters 1 through 3 describe analytical approaches that were based on strong hypotheses, multi-stage analyses, and the use of robust non-parametric methods in tandem with conventional association testing. These chapters are scientifically important, as they contribute to our understanding of the underlying genetic architecture in two debilitating ADs (MS and RA) and provide strong methodological frameworks for investigating other chronic diseases with a complex genetic component.",ucb,,https://escholarship.org/uc/item/02m2x4jz,,,eng,REGULAR,0,0
97,1533,Context in Constructions,"Lee-Goldman, Russell",,2011,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/02m6b3hx,,,eng,REGULAR,0,0
98,1534,Coping with Text Complexity in the Disciplines: Vulnerable Readers' Close Reading Practices,"Buffen, Leslie","Pearson, P. David;",2019,"Early reactions suggest that secondary teachers need support implementing the Common Core State Standards, specifically when teaching close reading strategies with complex disciplinary texts to vulnerable readers. This mixed-methods study conducted in a formative experiment paradigm (Reinking & Bradley, 2008) aimed to provide explanatory theories generated by applying a grounded theory approach to data analysis. An ethnographic case study design framed the grounded theory (Glaser & Strauss, 1967) and the formative experiment. I wanted to understand how two teachers adjusted their curriculum and instruction to create a more rigorous experience for vulnerable readers. I also sought to explore how such an approach affected the academic identities of these students. Teacher surveys, teacher interviews, student interviews, student surveys, student work, reading assessments, classroom observations, and teacher planning sessions provided evidence about how readers in two classrooms make sense of text and the strategies that support their comprehension and engagement during two curricular units that involve close reading of disciplinary texts. The units were used by students in two Special Day English classrooms at two different school sites and were co-designed by each high school teacher and myself. Reflections upon the first curricular unit informed the planning for the second curricular unit in each class. Also, I followed three students into at least one of their other disciplinary classes to understand their experiences with discipline-specific literacy instruction, both in the language arts and their disciplinary classes. Finally, I interviewed students about their in-school and out-of-school literacy practices, again to look for evidence of transfer from the units to their everyday repertoire of practices. Results indicate that both teachers, as they implemented the collaboratively planned lessons, asked predominantly open-ended questions that expected students to include textual evidence in their responses and did not have predetermined answers. This behavior was contrary to what I expected at the beginning of my study because I expected teachers might have predetermined answers. When responding to each teacherâ€™s instruction, students referenced the text in supporting the claims they developed on their own. In both classrooms, some of the studentsâ€™ academic identities improved during the study. All students reported rich relationships with text outside of school; however, only some students experienced success with reading complex disciplinary text in their English class. Overall findings suggest a positive effect of open-ended tasks that require students reference both the text and the knowledge that they bring to the task in forming arguments and applying understandings gained while reading. Future studies including a greater number of teachers from a range of disciplines who implement instruction with more students over a longer period of time would be beneficial in developing a more robust database about the power and influence of close reading practices.",ucb,,https://escholarship.org/uc/item/02w5k412,,,eng,REGULAR,0,0
99,1535,"Translating Sweetness: Type 2 Diabetes, Race, Research, and Outreach","Battle, James","Hayden, Cori;",2012,"Through the lens of Type 2 diabetes this dissertation considers race and problems of difference and risk with developments in treatment, genomic science, and the conduct of research and research priorities. Based primarily on fieldwork in New York and California, I interrogate public health notions of outreach with biotechnology and clinical research concepts of biomedical translation as synonymous practices. Institutional relationships and marketing drivers, I argue, reflect relatedness back onto the Type 2 diabetes patient through causal narratives of risk and inevitability. In effect, kinship--genetic, familial, racial, ethnic, and environmental--becomes the driver of both risk and emergent forms of bioliterary discipline.  I present a narrative of how diabetic risk became racialized over time and how African Americans became seen as a desirable research population. Arguing against biological race, I present an ethnographic example of how one such African American population, or community, emerged from particular social and political histories. Fieldwork uncovered lingering memories of the Tuskegee Experiment combined with cultural incompetency in both public health and biotechnology sectors. Further, I consider the bioethical challenges of African (American) participation in new genomic research aimed toward reducing health disparities. However, as a racial category, genetics researchers debate the precise genetic location and definition of ""Africa"" in the human genome. I suggest that the search for a pathological Africa in the human genome may deepen racial stigmatization as well as author new narratives of difference. I submit that social disparity, not biological disparity, presents the ultimate challenge to successful effective clinical translation and public health outreach.",ucb,,https://escholarship.org/uc/item/06f5s847,,,eng,REGULAR,0,0
100,1536,GB Virus Type C (HGV) and Human Immunodeficiency Virus (HIV) Co-Infection: Incidence and Impacts on Survival in a Cohort of HIV-Infected Transfusion Recipients,"Vahidnia, Farnaz","Reingold, Arthus L.;",2011,"GB virus C (GBV-C), an RNA virus closely related to hepatitis C virus (HCV), is transmitted through sexual, parenteral, and vertical routes. GBV-C is highly prevalent among patients receiving blood products and those at high risk of sexual or parenteral exposure. Unlike HCV, GBV-C replicates mainly in lymphocytes; many attempts to find an association between GBV-C infection and human disease have been unsuccessful. Therefore, donated blood is not routinely screened for GBV-C infection. In vitro and clinical studies have suggested that GBV-C co-infection may inhibit human immunodeficiency virus (HIV) replication by several different biological mechanisms. Some previous studies, but not all, have shown an association between GBV-C infection and both lower HIV viral load (VL) and better survival among HIV-infected patients.  Few studies describe predictors of acute GBV-C infection following transfusion in HIV-infected patients. Reports on survival benefits associated with co-infection after advent of highly active retroviral therapy (HAART) are inconclusive. An open question in many previous reports is the temporal relationship between GBV-C infection and HIV disease markers.  To address some of the currently unanswered questions concerning GBV-C and HIV co-infection, we used a limited access database obtained from the National Heart, Lung, and Blood Institute. The Viral Activation Transfusion Study (VATS) was a randomized controlled trial comparing leukoreduced (LR) vs. non-LR transfusions given to anemic HIV-infected transfusion-naÃ¯ve patients. Pre- and post-transfusion samples from 489 subjects were tested for GBV-C markers. We used the VATS dataset and the results of GBV-C testing to examine two hypotheses. First, we tested the hypothesis that GBV-C is transmitted to HIV-infected VATS subjects (n=294) via transfusion. We estimated the risk of acquiring GBV-C RNA per unit of blood transfused and examined the predictors of GBV-C acquisition. We found an incidence of 39 GBV-C infections per 100 person-years during follow-up in this population and an 8% increased risk of acquiring GBV-C associated with each additional unit of blood transfused, controlling for HAART status and baseline HIV VL. A lower HIV VL, use of HAART and white race were associated with an increased risk of subsequent GBV-C acquisition. Second, we examined the hypothesis that GBV-C co-infection is associated with lower mortality and lower HIV VL in 489 HIV-infected VATS subjects and in two VATS sub-cohorts. GBV-C viremia was associated with significantly lower mortality and HIV VL in unadjusted analyses. We found a non-significant trend towards lower mortality and lower HIV VL among HIV-infected VATS subjects, after adjusting for HIV risk behavior and time-updated E2 antibody, HAART status, HIV VL, and CD4 cell count. Acquisition of GBV-C was associated with lower mortality in the sub-cohort of individuals who were GBV-C RNA and antibody negative at baseline (n=294), adjusting for time-updated covariates (HR= 0.31, 95% CI 0.11, 0.86). Our results suggest high rates of GBV-C transmission by transfusion among HIV-infected subjects and an increased hazard of GBV-C acquisition with lower pre-transfusion HIV VL and current use of HAART. Our results also indicate that GBV-C viremia is associated with a trend towards lower mortality and lower HIV VL, and GBV-C acquisition via transfusion is associated with a significant reduction in mortality in HIV-infected individuals, after adjusting for HIV disease markers. These findings confirm previous reports that GBV-C infection inhibits HIV replication in vitro and in vivo.",ucb,,https://escholarship.org/uc/item/00p0412d,,,eng,REGULAR,0,0
101,1537,Introns and alternative splicing in choanoflagellates,"WESTBROOK, MARJORIE WRIGHT","King, Nicole;",2011,"The first organisms to evolve were unicellular, and the vast majority of life has remained so for billions of years. Complex forms of multicellularity, requiring increased levels of cell adhesion, cell signaling and gene regulation, have evolved in only a few eukaryotic lineages. The comparison of genomes from choanoflagellates, the closest relatives of metazoans, with genomes from metazoans may reveal genomic changes underlying metazoan origins. I used this approach to investigate the evolution of introns during the origin of metazoans.By analyzing the genome of the first choanoflagellate to be sequenced, Monosiga brevicollis, I found that its intron density rivals that of genes in intron-rich metazoans. Many intron positions are conserved between choanoflagellates and metazoans, implying that their shared unicellular ancestor was also intron-rich. In my analysis of the M. brevicollis genome, I made the unexpected discovery that, unlike most choanoflagellate genes, the longest genes contain relatively few introns. Indeed, one M. brevicollis gene contains the longest stretch of intron-free coding sequence known to date. I also found a similar trend in the genome of a basal metazoan, the sponge A. queenslandica. However, most long genes in other metazoans are not depleted of introns, revealing a difference in gene structure between eumetazoans and their closest relatives that may have implications for how these genes are regulated. The results of these analyses led me to investigate the evolution of alternative splicing during the emergence of metazoans. Intron-rich metazoan genes undergo complex patterns of developmentally regulated alternative splicing. My analysis of intron evolution revealed that the unicellular ancestor of metazoans was also intron-rich, raising the possibility that alternative splicing was common before the transition to multicellularity. To test this, I used transcriptome sequencing to detect alternative splicing in choanoflagellates and the early branching metazoan, Hydra magnipapillata. I found that alternative splicing, especially the skipping of entire exons, occurs less frequently in choanoflagellates than in H. magnipapillata. Increased alternative splicing of already intron-rich genes may thus represent an augmentation of gene regulation that evolved during the origin of metazoans. My analyses suggest that metazoans evolved from an intron-rich unicellular ancestor, setting the stage for complex patterns of alternative splicing to evolve during the transition to multicellularity. The connection between gene structure and alternative splicing provides an example of how non-coding features of eukaryotic genomes can impact the evolution of regulatory and morphological complexity.",ucb,,https://escholarship.org/uc/item/00w3t04k,,,eng,REGULAR,0,0
102,1538,"Bimetallic Nanoparticles: Synthesis, Characterization, and Applications in Catalysis","Landry, Alexandra Marcela","Iglesia, Enrique;",2016,"Bimetallic nanoparticles can lead to catalysts with improved turnover rates and selectivities, but many synthetic protocols, such as impregnation or precipitation, typically form particles of non-uniform size and composition. Colloidal methods may be able to improve their uniformity, but often require reagents that poison catalytic surfaces (ex. S, B, P). Such compositional non-uniformity and ubiquitous impurities have prevented rigorous conclusions about the consequences of alloying on reactivity and selectivity. Herein, we describe a sequential galvanic displacement-reduction (GDR) colloidal synthesis method using precursors containing only C, H, O, and N, that leads to bimetallic AuPt and PtPd nanoparticles narrowly distributed in size and composition. Au3+ or Pt4+ precursors were added to monometallic Pt or Pd clusters, respectively, whose surface atoms are thermodynamically driven to reduce and deposit the solvated cations onto cluster surfaces due to the lower reduction potentials (E0) of the seed metal relative to the precursor cations (E0Au > E0Pt > E0Pd); oxidized Pt and Pd surface atoms subsequently return to cluster surfaces upon reduction by the solvent, a reductant (ethanol or ethylene glycol, respectively). Such methods have been previously used to synthesize AuPd clusters from Pd seed clusters. TEM micrographs confirm that initial seed cluster sizes increase monotonically with increasing Au3+ or Pt4+ content, with final bimetallic cluster dispersity values near unity indicating a narrow size distribution. UV visible spectroscopy of AuPt cluster suspensions show no plasmon resonance features characteristic of Au nano-sized surfaces, indicating the presence of Pt atoms at bimetallic surfaces, as expected for GDR processes. Elemental analysis by EDS confirmed the formation of strictly bimetallic particles with the mean composition of the synthesis mixture.The GDR model requires that bimetallic growth be proportional to the initial seed surface area, with the number of precursor atoms deposited per surface metal atom of the seed constant and independent of seed metal size. Elemental analysis using EDS supports this hypothesis for thermodynamically favorable alloys such as PtPd and AuPd, but not for AuPt, an unfavorable alloy. These differences appear to reflect the segregation of metals within AuPt clusters during synthesis, placing the metal with the lower surface energy, Au, at cluster surfaces, and decreasing the availability of Pt0 surface atoms for GDR. Consequently, autocatalytic Au3+ reduction on Au0 sites becomes a competitive Au3+ reduction pathway during the synthesis of AuPt clusters.Polymers such as polyvinylpyrrolidone (PVP)â€”which bind to metal surfaces during synthesis via charge-transfer interactionsâ€”were required in colloidal suspensions to prevent particle agglomeration in solution, but must be removed prior to catalysis. We show that after depositing clusters on SiO2, PVP can be removed from particle surfaces by post-synthetic treatments at mild temperatures (â‰¤ 423 K) in reductants such as H2 and/or EtOH without significant particle agglomerations. Reductants compete with the polymer at the metal surface, thus breaking the polymer-metal bond. The absence of surface residues was confirmed by the similar cluster sizes derived from O2 chemisorption and TEM measurements. Larger cluster sizes and surfaces that chemisorb oxygen more weaklyâ€”such as Pt relative to Pdâ€”were found to facilitate the removal of PVP from metal particles due to weaker metal-polymer bonds. The model catalytic materials prepared in this study are of both fundamental and practical interest to probe the effects of alloying. Using AuPd and AuPt, we investigate the consequences of alloying with Au on the reactivity of catalyst surfaces saturated with either chemisorbed CO* (CO oxidation) or O* (H2 oxidation) that bind strongly to Pt and Pd surfaces and inhibit rates. Singleton Pt-CO* bond energies, reflected in vibrational CO* stretches, were decoupled from dipole-dipole coupling effects using isotopic dilution methods, and were shown to decrease with increasing catalyst Au content. Despite lower CO* binding energies, CO oxidation turnover rates (normalized per metal surface atom) on AuPt catalysts decreased with increasing Au content. These results show that CO oxidation rates depend weakly on CO* binding energyâ€”consistent with the reported structed insensivity of this reactionâ€”and that Au acts primarily as an inert diluent of the active Pt ensembles required for catalysis. In contrast, H2 oxidation turnover rates (normalized per metal surface atom) on AuPt and AuPd catalysts increase with increasing Au content (up to 11 % at. Au content on AuPt and up to 67 % at. Au content on AuPd), indicating that the reactivity of O* saturated surfaces is more sensitive to changes in adsorbate binding energy than surfaces saturated in CO*, consisted with the reported structure sensitivity of reactions on O* saturated surfaces. Reconstruction of CO* adlayers is facile due to highly mobile CO* molecules, thus allowing CO* adlayers to access configurations that help mitigate strong CO* binding and introduce vacancies. O* adlayers, meanwhile, are more strongly bound to Pt and Pd metal surfaces and less mobile. H2 oxidation rates thus depend more strongly on adsorbate binding energy than CO oxidation rates.",ucb,,https://escholarship.org/uc/item/00w6r4tn,,,eng,REGULAR,0,0
103,1539,Regulation of isoprenoid precursor pathways in Listeria monocytogenes,"Lee, Eric David","Portnoy, Daniel A;",2019,"Listeria monocytogenes is a facultative, Gram-positive intracellular pathogen that is also a model organism for studying bacterial pathogenesis. Much of the appeal of using L. monocytogenes as a model organism derives from the fact that much of the knowledge gained from studying L. monocytogenes pathogenesis or metabolism can be applied to other pathogens that are more difficult to work with or manipulate. However, the unique aspects of L. monocytogenes biology are equally fascinating and still shed light on the host cell processes that help protect against infections.Isoprenoids are a diverse class of compounds produced by all forms of life and are synthesized from essential precursors derived from either the mevalonate pathway or the nonmevalonate pathway. Most organisms have one pathway or the other, but L. monocytogenes is unique because it encodes all of the genes from both pathways. Other scientists reported that the mevalonate pathway was essential for growth, but here we report that the nonmevalonate pathway was sufficient for growth anaerobically. Deleting the mevalonate pathway gene hmgR (âˆ†hmgR) impaired bacterial growth in all L. monocytogenes strains studied, but the laboratory strain 10403S grew significantly slower than two lineage I strains, FSL N1-017 and HPB2262 Aureli 1997. The faster anaerobic growth of the lineage I strains was initially traced to a difference in the nonmevalonate pathway enzyme GcpE, and then chimeric proteins were constructed to precisely identify the molecular basis of this phenotype. Three amino acid residues, K291T, E293K, and V294A were shown to be necessary and sufficient to increase the anaerobic growth rate of 10403S âˆ†hmgR. However, these mutations did not map to GcpE in a way that provided any obvious mechanistic insights to how the enzyme function was altered. Even though the nonmevalonate pathway was found to function anaerobically, we were unable to identify conditions where deleting the nonmevalonate pathway impaired L. monocytogenes growth of any strain. Given the fitness cost of maintaining all seven genes in the nonmevalonate pathway, it is likely that there are yet unidentified anaerobic growth conditions that require the nonmevalonate pathway. This work showed that, contrary to previous reports, the mevalonate pathway is not essential for L. monocytogenes growth anaerobically, although it is still essential for growth aerobically.After finding specific mutations in GcpE that altered growth, we were also interested in understanding the broader networks regulating the nonmevalonate pathway in L. monocytogenes. The difference in anaerobic growth rates between 10403S âˆ†hmgR and lineage I âˆ†hmgR mutants indicated that it could be possible to find suppressor mutations that increased 10403S âˆ†hmgR growth rate. 10403S âˆ†hmgR cultures were passaged anaerobically, and fast-growing suppressor mutants began arising after three passages, which was noted by the observation that cultures initially required four or five days to reach stationary phase, but later only required two days to reach stationary phase. Whole genome sequencing of these cultures identified mutations in several genes, but the most common mutations were in the histidine kinase of a two-component system LisRK. We demonstrated that the suppressor mutations altered rather than disrupted LisRK signaling, but were unable to identify specific genes in the regulon that were responsible for the increased rate of anaerobic growth. This work showed that multiple mutations can be made that alter the rate of L. monocytogenes growth using the nonmevalonate pathway. The previously identified mutations in GcpE likely directly change enzyme kinetics, allowing for increased flux through the nonmevalonate pathway, but the additional mutations identified here likely change gene expression levels or alter metabolite concentrations at points upstream or downstream of the nonmevalonate pathway itself.",ucb,,https://escholarship.org/uc/item/00w7f1kg,,,eng,REGULAR,0,0
104,1540,The Complexity of Optimal Auction Design,"Pierrakos, Georgios","Papadimitriou, Christos H;",2013,"This dissertation provides a complexity-theoretic critique of Myerson's theorem, one of Mechanism Design's crown jewels, for which Myerson was awarded the 2007 Nobel Memorial Prize in Economic Sciences. This theorem gives a remarkably crisp solution to the problem faced by a monopolist wishing to sell a single item to a number of interested, rational bidders, whose valuations for the item are distributed independently according to some given distributions; the monopolist's goal is to design an auction that will maximize her expected revenue, while at the same time incentivizing the bidders to bid their true value for the item. Myerson solves this problem of designing the revenue-maximizing auction, through an elegant transformation of the valuation space, and a reduction to the problem of designing the social welfare-maximizing auction (i.e. allocating the item to the bidder who values it the most). This latter problem is well understood, and it admits a deterministic (i.e. the auctioneer does not have to flip any coins) and simple solution: the Vickrey (or second-price) auction. In the present dissertation we explore the trade-offs between the plausibility of this result and its tractability:First, we consider what happens as we shift away from the simple setting of Myerson to more complex settings, and, in particular, to the case of bidders with arbitrarily correlated valuations. Is a characterization as crisp and elegant as Myerson's still possible? In Chapter 2 we provide a negative answer: we show that, for three or more bidders, the problem of computing a deterministic, ex-post incentive compatible and individually rational auction that maximizes revenue is NP-complete --in fact, inapproximable. Even for the case of two bidders, where, as we show, the revenue-maximizing auction is easy to compute, it admits nonetheless no obvious natural interpretation a-la Myerson.Then, motivated by the subtle interplay between social welfare- and revenue-maximizing auctions implied by Myerson's theorem, we study the trade-off between those two objectives for various types of auctions. We show that, as one moves from the least plausible auction format to the most plausible one, the problem of reconciling revenue and welfare becomes less and less tractable. Indeed, if one is willing to settle for randomized solutions, then auctions that fare well with respect to both objectives simultaneously are possible, as shown by Myerson and Satterthwaite. For deterministic auctions on the other hand, we show in Chapter 3 that it is NP-hard to exactly compute the optimal trade-off (Pareto) curve between those two objectives. On the positive side, we show how this curve can be approximated within arbitrary precision for some settings of interest. Finally, when one is only allowed to use variants of the simple Vickrey auction, we show in Chapter 4 that there exist auctions that  achieve constant factor approximations of the optimal revenue and social welfare simultaneously.",ucb,,https://escholarship.org/uc/item/0132p6nt,,,eng,REGULAR,0,0
105,1541,Mercury Sensing with Optically Responsive Gold Nanoparticles,"James, Jay Zachary","Koshland, Catherine P;Fernandez-Pello, Carlos;",2012,"Mercury, a potent neurotoxin, is a global environmental problem. New mercury sensing technologies are needed to meet the expanding demands on the mercury observation network. We demonstrate the utility of gold nanoparticles as a stand-alone, inexpensive, and sensitive mercury monitor. Gold nanoparticles display a peak in the visible range of their UV-vis absorbance spectra due to localized surface plasmon resonance (LSPR). The energy of this resonance is affected by adsorption of mercury. Isolated individual gold nanorods with an average length and diameter of 60 and 20 nm saturate after adsorption of 4 attograms of elemental mercury, and produce a 3 nm blue shift in their LSPR, with the shift dependent on the surface-area-to-volume ratio and aspect ratio. A modified Gans theory model predicts the shift at saturation given the particle dimensions if saturation occurs with 45% monolayer coverage. Nanoparticle films on a transparent substrate show potential as a practical and robust method for sensing low levels of mercury vapor. The adsorption of 15 atoms of Hg causes a 1 nm shift in the LSPR wavelength of 5 nm gold spheres. The rate of shift in the peak absorbance is linear with mercury concentrations from 1 to 825 ÂµgHg/mair3. The speed of the sensor response is limited by diffusive mass transfer and can be enhanced by optimizing the sample flow characteristics. We modeled the diffusive mass transfer and optimized the sample delivery accordingly. Regeneration of the sensing films, done by heating to 160Â°C, allows for repeatable measurements on the same film.",ucb,,https://escholarship.org/uc/item/0327r0g6,,,eng,REGULAR,0,0
106,1542,C0Rigidity in Hofer Geometry and Floer Theory,"Seyfaddini, Sobhan","Weinstein, Alan D;",2012,"This dissertation explores two instances of C0 rigidity in symplectic geometry: First, we prove that continuous Hamiltonian flows as defined by Oh and M\""uller have unique  generators.  Second, we study the behavior of certain Floer theoretic invariants of Hamiltonian flows, called spectral invariants, under  C0 perturbations of Hamiltonian flows.",ucb,,https://escholarship.org/uc/item/0385r0bk,,,eng,REGULAR,0,0
107,1543,Essays in International and Development Economics,"Li, Nicholas","Gourinchas, Pierre-Olivier;",2011,"Standard analysis of consumer behavior uses theory and evidence on quantities and prices to measure consumer welfare, particularly through the use of price indexes that capture the cost-of-living. In three essays I go beyond this standard analysis to analyze the impact of household variety, store variety, and energy requirements on food demand, consumption patterns and consumer welfare, with implications for several important topics in development and international economics. In the first essay I develop theoretical and empirical tools for analyzing variety-seeking behavior by households in a non-homothetic setting, showing how changes in the slope and intercept of ``variety Engel curves'' map into the level and distribution of consumer welfare in a way that is not captured by standard analysis. The ``cost'' of variety is introduced as an important concept for analysis that is distinct from price, e.g. the ``cost'' of quantity. The application to Indian food consumption indicates that increases in food variety over time and greater urban food variety are not attributable solely to income growth, with significant implications for both the size and distribution of welfare gains from variety. In the second essay, co-authored with Shari Eli, we continue to explore empirical patterns in Indian food consumption and draw attention to the decline in calorie intake at all expenditure levels. Once again the Engel curve is used as a guiding framework for the empirical analysis and once again there are changes in consumption patterns that are not explained by movements in prices and income alone. In this case the ``cost'' of calories is represented by the role of calories (as opposed to other food attributes) in satisfying household caloric requirements. Using data on time-use to impute caloric requirements and combining this with data on ownership of labor-saving durables, we find support for the hypothesis that a decline in energy requirements has had an important impact on variation in calorie intake over time and space, explaining most of the rural-urban discrepancy in calorie intake but only a modest share of the decline in calorie intake over time. In the third essay I examine product variety from the point of view of retailers rather than consumers, showing that differences in variety across different stores of a North American grocery retailer have large impacts on consumer welfare. I argue that these differences in variety can be largely attributed to differences in store size and international borders, and provide a stylized partial equilibrium model to analyze what factors influence the retailer's choice of store size in different locations. I find empirical support for the predictions of the model, with larger markets (higher income per capita, higher population density) leading to larger store size and variety while greater competition reduces store size and variety, with significant impacts on consumer welfare across different geographic areas.",ucb,,https://escholarship.org/uc/item/03j674h7,,,eng,REGULAR,0,0
108,1544,Spatial Frustration: Using Pharmacological and Mechanical Disruption of Membrane Receptor Transport to Probe Cell Signaling Dynamics on Supported Lipid Bilayers,"Petit, Rebecca","Groves, John T;",2013,"Micron-scale spatial reorganization of cell membrane receptor protein clusters is emerging as an important early modulator of inter- and intracellular signaling input. Detailed within is an in-vitro investigative technique utilizing cultured cells, nanolithograpically-patterned supports for fluid biomolecule-functionalized phospholipid bilayer membranes and inverted optical microscopy. This method allows the user to visualize and, if desired, restrict the formation and transport of these transient macroclusters as they coalesce on the membrane of a stimulated cell. Multi-color fluorescence imaging allows the simultaneous tracking of multiple molecules, permitting real-time analysis of coupling between surface receptors and intracellular signaling and structural proteins. The two systems examined are the T cell immunological synapse and ephrinA1-EphA2 interactions in highly invasive breast cancer cells. In both cases, it is demonstrated that induced restrictions of receptor-mediated cytoskeletal reorganization is sufficient to alter downstream signaling outcomes, and the molecular mechanisms of feedback coupling are investigated.",ucb,,https://escholarship.org/uc/item/06w9d9n5,,,eng,REGULAR,0,0
109,1545,Fragment-Based Identification of Phosphatase Inhibitors,"Rawls, Katherine Anne","Ellman, Jonathan A;",2010,"AbstractFragment-Based Identification of Phosphatase InhibitorsbyKatherine Anne RawlsDoctor of Philosophy in ChemistryUniversity of California, BerkeleyProfessor Jonathan A. Ellman, ChairChapter 1.  A new fragment-based method for the identification of phosphatase inhibitors, Substrate Activity Screening, is described.  Application of the method to Mycobacterium tuberculosis protein tyrosine phosphatase PtpB resulted in the identification of novel, nonpeptidic substrate scaffolds that were optimized by rapid analog synthesis and evaluation.  These substrate scaffolds were then converted to low molecular weight inhibitors for PtpB by incorporation of a variety of established phosphate mimetics, resulting in nanomolar affinity inhibitors that were highly selective for PtpB over mycobacterial and human phosphatases.Chapter 2.  The design and synthesis of new inhibitor analogs based on the scaffold identified in Chapter 1 is described.  The synthesis of more challenging inhibitor scaffolds was achieved, resulting in a panel of low molecular weight, nanomolar to micromolar affinity inhibitors for the Mycobacterium tuberculosis protein tyrosine phosphatase PtpB.  These compounds represent chemical tools for further dissection of the biochemical role of PtpB in tuberculosis infection.Chapter 3.  Application of the method described in Chapter 1 to Mycobacterium tuberculosis protein tyrosine phosphatase PtpA is described.  Inhibitors incorporating a well established phosphate mimetic were explored, resulting in compounds with low micromolar affinity for PtpA.  Modeling studies provided a rationale for the observed structure-activity relationships and guided further compound optimization.  The most potent compound was additionally shown to be selective for PtpA over a variety of human enzymes as well as the other Mycobacterium tuberculosis phosphatase PtpB.  This inhibitor represents a chemical tool that can be used in conjunction with the inhibitors described in Chapters 1-2 to further probe the role of PtpA and PtpB in tuberculosis infection, and to examine potential synergistic effects.Chapter 4.  The application of inhibitors developed in Chapters 1-3 to the pathogenic target Staphylococcus aureus, the causative agent of Staph infection, is described.  Several of the inhibitors described in Chapter 1 were found to have activity versus Staphylococcus aureus bacteria, prompting further probing of structure-activity relationships.  The synthesis of new analogs was realized by developing a new synthetic strategy to allow for rapid analog synthesis and evaluation.  The cellular target is postulated to be Staphylococcus aureus protein tyrosine phosphatases SaPtpA and SaPtpB, newly discovered enzymes which may play a role in pathogenesis.  Compounds were evaluated directly in cell assays, and the mechanism of action of these compounds, which show activity in Staphylococcus aureus strains that are resistant to traditional beta-lactam antibiotics, is under investigation.",ucb,,https://escholarship.org/uc/item/0723n24v,,,eng,REGULAR,0,0
110,1546,Measurement and Causal Inference in Patent Strategy,"Kuhn, Jeffrey Michael","de Figueiredo, Rui;",2017,"Determining the effect of patents on firms has long presented significant challenges to innovation scholars due to the specific and idiosyncratic nature of patent protection. This thesis develops new techniques for measuring the impact, technological relatedness, and extent of patents. It then describes two new approaches to performing causal inference on patent protection. Finally, it applies these new measures and causal instruments to investigate the effect of receiving a broader patent on patent sales and the effect of winning a patent race on follow-on innovation.Patent scope is central to the sale of ideas, which can spur economic growth and provide significant gains from trade. Awarding an inventor a patent on a new idea partially solves a commitment problem that would otherwise prevent the inventor from selling the idea. (Arrow, 1962). In the absence of a patent, a prospective buyer cannot credibly promise not to steal the idea should the inventor reveal it, while the inventor cannot credibly promise to reveal the idea should the prospective buyer pay for it. A firm's ability to use a particular patent to overcome this transactional hurdle derives from two factors: (1) the scope of the patent's legal right to exclude and (2) the effectiveness of that legal right in providing market exclusivity. This thesis first shows that a broader patent is more likely to be sold by employing a causal instrument that provides a plausibly exogenous shock to the scope of a patent's legal right to exclude, holding fixed the underlying idea. It then examines variation in the effectiveness of the right by interacting the instrument with endogenous firm, industry, and market characteristics. These results shed light on how firms profit from innovation and also connect the important but understudied market for patents, widely believed to be illiquid and inefficient, with fundamental research about how markets function in other contexts.Competition between firms to invent and patent an idea, or ""patent racing,""' has been much discussed in theory, but seldom analyzed empirically. This thesis introduces an empirical way to identify patent races, and provides the first broad-based view of them in the real world. It reveals that patent races are common, particularly in information-technology fields. The analysis is then extended to identify the causal impact of winning a patent race, using a regression-discontinuity approach. It shows that both the winners and losers of patent races typically receive patent protection, but that the winners receive much broader patent scope. It also shows that patent race winners do significantly more follow-on innovation, and the follow-on research that they do is more similar to what was covered by the patent.Underlying both of these analysis is a new measure of patent-to-patent similarity. Current measures of patent similarity rely on the manual classification of patents into taxonomies. This thesis describes and validates a machine-automated measure of patent-to-patent similarity developed by leveraging information retrieval theory and Big Data methods. It also demonstrates that the measure significantly improves upon existing patent classification systems.",ucb,,https://escholarship.org/uc/item/0791g94s,,,eng,REGULAR,0,0
111,1547,Peace Agreements as Counterinsurgency,"Brandt, Caroline M.","Mattes, Michaela;",2020,"Peace agreements are heralded as tools for ending civil war. However, exclusive peace agreements, accords that include only a subset of a conflict's warring parties, are unlikely to bring an end to a civil conflict. I argue that exclusive peace agreements serve a purpose beyond conflict resolution. Exclusive peace agreements are a counterinsurgency strategy.Combatting more than one rebel group strains governmentsâ€™ military abilities by dividing resources across multiple wars. Governments that would otherwise be able to defeat a rebel group may be unable to do so when tied down by multiple insurgencies. Based on this logic, and in contrast to the literature on spoilers in peace processes, I argue that the threat posed by other insurgent groups increases the likelihood that a government and rebel group sign an exclusive negotiated settlement. Exclusive peace agreements allow governments to consolidate military resources into the fight against the remaining insurgency. Exclusive peace agreements can further strengthen a government's counterinsurgency capabilities by including provisions for military power-sharing that transform conflict adversaries into war-fighting allies. To test this hypothesis, I analyze all multiparty civil wars from 1975-2013. In support of the theory, I find that the threat posed by other rebel groups is positively correlated with the likelihood that a government and rebel group sign an exclusive peace accord. I then use a case study of civil war in the Southern Philippines to bring to light the mechanisms that undergird these correlations. In line with the theory, the threat posed by other rebel groups jump-started stalled peace talks with Moro insurgent groups. I also find that exclusive peace agreements were successful mechanisms for incorporating rebel soldiers into the government's fight against other rebel groups.",ucb,,https://escholarship.org/uc/item/0798055m,,,eng,REGULAR,0,0
112,1548,"Low Molecular Weight Organic Contaminants in Advanced Treatment: Occurrence, Treatment and Implications to Desalination and Water Reuse Systems","Agus, Eva","Sedlak, David L;",2011,"Water reuse and desalination are increasingly considered as viable sources of potable water because improvements in materials and designs have decreased the cost of reverse osmosis (RO) membranes and their operation. Although most contaminants are efficiently rejected by reverse osmosis membranes, compounds with neutral charge and low molecular weight have proven to be difficult to remove. Depending on the characteristics of the membrane and the feed water, some contaminants may be present in reverse osmosis permeate at concentrations that are high enough to compromise water quality. 	When chemical disinfection is applied in desalination systems, compounds that pose potential risks to the human health and aquatic ecosystems or impact the aesthetic quality of drinking water may be formed. In particular, several compounds of concern are produced when chlorine is used as pretreatment. The formation and speciation of chlorination byproducts in desalination systems is affected by the elevated concentrations of bromide and iodide in seawater and desalinated product water. To gain insight into byproducts most likely to be formed in desalination systems, disinfection byproduct formation studies conducted in saline source waters, coastal power stations and existing desalination systems were reviewed. These prior studies suggested that chlorine, chloramine and chlorine dioxide all pose potential risks in desalinated water systems. Chlorination of seawater intakes to prevent membrane fouling and disinfection of blended product water both pose potential risks to water quality.	To assess the formation and fate of chlorination byproducts under different conditions likely to be encountered in desalination systems, trihalomethanes, dihaloacetonitriles, haloacetic acids, and bromophenols were analyzed in water samples from a pilot-scale seawater desalination plant. In the pilot plant, the rejection of neutral, low-molecular-weight byproducts ranged from 45% to 92%, while charged species of similar molecular weights ranged between 77% to 97% rejection. Bench-scale chlorination experiments conducted on seawater from various locations indicated significant temporal and spatial variability for chlorination byproduct formation that could not be explained by bulk measurements of dissolved organic carbon concentration and UV absorbance.  	When desalinated water was blended with freshwater, elevated concentrations of bromide in the blended water enhanced dihaloacetonitrile formation through a shift in the active disinfecting agent from hypochlorous acid (HOCl) to hypobromous acid (HOBr). In most situations, data from the pilot plant and bench-scale studies indicated that chlorination byproducts formed from continuous chlorination of seawater or blending of desalinated water and freshwater will not compromise water quality or pose significant risks to aquatic ecosystems. However, blends of desalinated seawater with water rich in humic substances could lead to higher-than-expected production of haloacetonitriles and other chlorination byproducts.	When reverse osmosis is used for the treatment of municipal wastewater effluent, compounds that exhibit low taste and odor thresholds could compromise water quality. To assess potential for odors in wastewater effluent to compromise potable water reuse schemes, we evaluated odors in secondary effluent using flavor profile analysis and gas chromatography with olfactometry detection (GC/Olfactometry or GC/Olf). The primary odor reported in secondary effluent samples was classified as earthy/musty and was typically present at an intensity well above the odor threshold. Using GC/Olfactometry on samples prior to reverse osmosis, we identified sixteen peaks present at high intensity in more than 80% of the wastewater effluent samples. Odor descriptors reported in GC/Olfactometry analysis of secondary effluent were categorized as fragrant, sulfide, rancid, and hydrocarbon/chemical. Potential odorants associated with olfactometry peaks were identified by comparing the odorant with sensory descriptors and gas chromatography and mass spectrometry (GC/MS) analysis of an authentic standard of the putative compound. Other than organosulfide, aldehydes and volatile acid odorants previously identified in wastewater treatment, compounds including 2-pyrrolidone, lactones, chlorophenol, and vanillins were also identified as odorants associated with olfactometry peaks.	Potent odorous compounds were detected in secondary effluent by quantitative GC/MS. The most prominent compounds were 2,4,6-trichloroanisole (median concentration 9.5 ng/L) and geosmin (median 7 ng/L). Both compounds exhibited earthy/musty sensory profiles at these concentrations. During advanced treatment, olfactometry peaks exhibited variable fate depending on their abundance, molecular structures and odor thresholds. Reverse osmosis significantly decreased the concentrations of low molecular weight odorous compounds in wastewater, but did not eliminate all odors. Odor peaks were typically reduced to below their odor thresholds during advanced oxidation processes (i.e., UV/H2O2Â¬) but also in a system employing biologically activated carbon (BAC) with ozone pretreatment. Odors can be removed from secondary effluent by applying multiple barrier treatment trains that combine reverse osmosis or another physical treatment method with chemical oxidation.",ucb,,https://escholarship.org/uc/item/014004j9,,,eng,REGULAR,0,0
113,1549,Using morphology and structure to tune solid-state thermal properties,"Hippalgaonkar, Kedar","Grigoropoulos, Costas;",2013,"Diffusive phonon transport in nanostructured materials has been a subject of intense interest and micro-fabricated platforms have been used to measure the thermal conductivity of nanowires. In this work, we demonstrate how the limits of heat transport can be tested in three novel material systems by extending this platform to probe material structure and provide a direct correlation to their thermal properties.Phonons are lattice vibrations and their scattering in solids has largely been explained like collision of particles. Since the development of nanostructures, diffusive boundary scattering from large surface-to-volume ratio materials has been studied in nanowires and superlattices. To beat this diffusive scattering limit, we designed integrated silicon nanowires with rough surfaces with 30% reduction in thermal conductivity. Subsequently, we took a significant step further by making nanostructures with broadband roughness close to the dominant phonon wavelength (1-10 nm) at room temperature. The decrease in thermal conductivity of intrinsic silicon by a factor of ~30 from 140 W/m-K to 5 W/m-K in this sub-diffusive regime might be due to multiple scattering stemming from coherent phonon wave effects. Transmission Electron Microscopy (TEM) based techniques including three- dimensional tomography were then used to map out the morphology and find that we can reduce the thermal conductivity to as low as 1 W/m-K, while preserving the single-crystalline core, which is as low as that amorphous silicon or silica. Correlating the surface roughness and porosity to the measured thermal conductivity opens up a new paradigm to observing wave physics in thermal phonons at room temperature in nanomaterials.Secondly, the platform developed previously was extended to be compatible with TEM, allowing us to characterize the crystal structure of measured nanowires. While phonon optics experiments in the 1970s showed a crystallographic direction dependent thermal conductivity, we performed the first 1-1 mapping of nanowire growth direction and thermal conductivity in Bismuth Nanowires. In the boundary scattering regime with diameter 100 nm, a nanowire in the [-102] direction had k = 8.5 W/m-K, ~6 times higher than a nanowire in the [110] direction with k = 1.5 W/m-K.Finally, this thesis also studies tapered Vanadium Oxide beams to study asymmetric phonon physics that manifest in temperature dependent thermal rectification. The interplay between electrons and phonons and the possibility of asymmetric scattering rates prompted us to look closely at the existence of Metal-Insulator interfaces that could result in thermal rectification. Between 150K and 340K, the Vn O2n-1 phases could be either metallic or insulating with nanoscale domains. We performed high resolution Auger spectroscopy on single-crystal Vanadium Oxide beams that show a stoichiometry variation and measured thermal rectification as high as 22%. The rectification behavior turns off (<4%) once the whole beam reaches the insulating phase, higher than 340K.Our platform thus couples materials characterization, especially TEM, with thermal property measurement to enhance understanding of thermal phonons.",ucb,,https://escholarship.org/uc/item/01h2v1jw,,,eng,REGULAR,0,0
114,1550,Multiscale multichroic focal planes for measurements of the cosmic microwave background,"Cukierman, Ariel Jozef","Lee, Adrian T;",2018,"We report on the development of multiscale multichroic focal planes for measurements of the cosmic microwave background (CMB). A multichroic focal plane, i.e., one that consists of pixels that are simultaneously sensitive in multiple frequency bands, is an efficient architecture for increasing the sensitivity of an experiment as well as for disentangling the contamination due to galactic foregrounds, which is increasingly becoming the limiting factor in extracting cosmological information from CMB measurements. To achieve these goals, it is necessary to observe across a broad frequency range spanning roughly 30-350 GHz. Depending on the foreground complexity, it may become necessary to observe across an even larger range. For this purpose, the Berkeley CMB group has been developing multichroic pixels consisting of planar superconducting sinuous antennas coupled to extended hemispherical lenslets, which operate at sub-Kelvin temperatures. The sinuous antennas, microwave circuitry and the transition-edge-sensor (TES) bolometers to which they are coupled are integrated in a single lithographed wafer. We describe the design, fabrication, testing and performance of multichroic pixels with bandwidths of 3:1 and 4:1 across the entire frequency range of interest. Additionally, we report on a demonstration of multiscale pixels, i.e., pixels whose effective size changes as a function of frequency. This property keeps the beam width approximately constant across all frequencies, which in turn allows the sensitivity of the experiment to be optimal in every frequency band. We achieve this by creating phased arrays from neighboring lenslet-coupled sinuous antennas, where the size of each phased array is chosen independently for each frequency band. We describe the microwave circuitry in detail as well as the additional benefits of a multiscale architecture, e.g., mitigation of beam non-idealities, reduced readout requirements and polarization-wobble cancellation. Finally, we discuss the design and fabrication of the detector modules and focal-plane structures including cryogenic readout components, which enable the integration of our devices in current and future CMB experiments.",ucb,,https://escholarship.org/uc/item/01p6c0bx,,,eng,REGULAR,0,0
115,1551,How Can Child Labor Lead to an Increase in Human Capital of Child Laborers and What Are Policy Implications?,"Luong, Quoc Viet","Rausser, Gordon C.;",2011,"This dissertation attempts to answer three critical questions that have remained largely misunderstood in the literature of child labor. The first question is whether child labor can help child laborers gain more human capital, including both formal education and health status.  The second question focuses on the mechanisms through which child labor impacts human capital. It asks how a positive causal impact from child labor to human capital can possibly take place. The third question discusses policy implications. Given the gain in human capital of child laborers due to child labor, what are the unintended consequences of current policies and what can we do to effectively combat child labor and at the same time help child laborers acquire more human capital? Because these three questions are intrinsically related I find it more productive to present them in form of one major study rather than in three separate papers. To provide empirical evidence to the first question of whether child labor can help child laborers gain more human capital, I exploit a quasi-controlled experiment that took place between 2004-2009 in a poor rural area in Vietnam. Most children in this area were so poor that they dropped out of school prematurely. In order to help these children sustain their education, a non-governmental organization (NGO) decided to provide a cow to each poor household with school-aged children so that the children could spend time tending the cows, earn some income to pay for their schooling. Practically this intervention provided the children a means to convert their time into income.Due to limited resources, the NGO could provide cows to only a subset of the poor children, effectively creating a controlled experiment in which some of the children had work (the treatment group) and the others did not (the control group). Since the children were not randomized into the treatment and control groups, the main concern was the selection bias. An examination of the bias shows that the children were selected into the treatment group on the basis of most urgent needs - which means those determined to be more likely to drop out of school in absence of the treatment were selected to receive the cows. The data collected verified that at baseline those in the treatment had indeed acquired less education, had higher dropout ratio, were poorer, had less land, lived further away from school, and their parents had lower levels of education. All of these socio-economic indicators suggest that the selected children would have been more likely to drop out of school if the status-quo had continued. Since the selection bias (being more likely to drop out) works against the treatment effect (acquiring more education), estimates of the impact are likely to be the lower bounds of the true effect and should be valid. I find that the poor children who worked gained a significant average of 0.59 years of education over a period of 5 years compared to those who did not have any work opportunity. While the finding of a positive causal relationship between child labor and education is striking, this outcome per se is not very useful in terms of proposing new policy interventions because it does not explain how child labor results in more human capital. Imagine even if we have the luxury of running a perfect randomized controlled trial and the experiment shows that child labor leads to an increase in human capital, there remains a ""black box"". We still cannot explain how the positive impact takes place. Clearly unless we can explain what happen in the black box - unless we can explain with economic theories how child labor can positively affect human capital, we cannot construct informed policy interventions.This immediately leads to the second question: what are the mechanisms through which child labor can result in more human capital? To answer this question, I construct a theoretical model which examines how a household would choose optimal levels of human capital under the treatment (where children can work) and under the control (where children are not allowed to work). This framework shows that child labor affects child laborers' human capital through a positive income effect and a negative time cost. On one hand, child work brings home more income to acquire more education and consumption (a positive impact on health status). On the other hand, child labor takes away time, a necessary input for schooling. The most important finding is that while child labor always generates a positive income effect, its opportunity cost of time in terms of the education forgone can be zero, leading to a positive net effect. To see this, consider a household's choices of child labor and human capital as in a controlled experiment. Note that under the control when child labor is not allowed, the optimal level of schooling can be zero. For example, a hungry family that can afford only one meal per day would choose zero schooling, a costly expenditure in poor countries, in order to spend all income on food. In this case, the time cost of child labor in terms of forgone schooling is zero because in the absence of work children stay home anyway. When these hungry children can work, there are only two possibilities. First, they might choose to work full-time and spend all additional income on food. This case would lead to an increase in health status of the child laborers with no change in their education. Second, they might choose to work part-time and go to school part-time, using their additional income to pay for more food and more schooling. This case would lead to an increase in both their health status and education. The model shows that the income effect of child labor can dominate the time cost (because it can be zero), resulting in a positive net effect on human capital. The critical point that separates this research from the literature is that I use the amount of school time that would be chosen in the absence of work as the benchmark, not school time endowment, to measure the time cost of child labor.New answers to question 1 and 2 immediately bring up question 3: what are the unintended consequences of current policies and what can we do to effectively combat child labor and promote human capital? I find that current interventions such as trade sanctions, consumer boycotts, legal penalties or an outright ban against child labor, which would diminish or eliminate child work opportunity, would unambiguously reduce the human capital of the poorest child laborers. Note that child labor restricting policies are grounded on the belief that a loss in household welfare due to the loss of child labor income can be offset by an increase in child schooling due to reduced child labor. However, this study shows that by restricting child labor, these policies would reduce not only household welfare but also the schooling and physiological capital of the poorest children. Such instruments will have unintended consequences on children's human capital, the very point that they advocate for. Instead I find policies that make schooling more affordable such as such as reducing school fees, providing free meals and textbooks, providing cash transfer conditional on schooling, improving teacher/student ratios, improving curriculum, would simultaneously increase education and reduce child labor. In addition, I suggest new market-based policies that can enhance child laborers' education and health status without consuming additional public resources more than the status-quo. For example, encouraging the private sector to provide work to children with unemployed non-school time conditional on their school attendance would maximize education gain by capturing the income effect while excluding the substitution effect.My research adds to the literature in a number of ways. This is the first study to provide empirical evidence that child labor can lead to an increase in the human capital of child laborers. Moreover, this research is also the first to provide a theoretical framework that explains the mechanism at work - that is child laborers can gain more human capital from working because child labor always generates a positive income effect while its opportunity cost of time in terms of forgone education can be zero. Most importantly, my work suggests a need for a major overhaul of current policies that are adversely affecting hundreds of million poor children around the globe. Reducing child labor by enforcing interventions that restrict child work opportunities will have the exact unintended consequences of reducing the human capital of the poorest child laborers. The best way to get the more than 100 million hungry children worldwide out of work is to subsidize schooling (i.e. even pay them to go to school). Such a subsidized education in poorest countries, however, more often than not is practically out of the question. In this situation, the hungry children need, not less, but more work opportunities to buy more food, and at times also buy more education.",ucb,,https://escholarship.org/uc/item/07m2v2db,,,eng,REGULAR,0,0
116,1552,Inverse Modeling of Geological Heterogeneity for Goal-Oriented Aquifer Characterization,"Savoy, Heather Marie","Rubin, Yoram;",2017,"Characterizing the spatial heterogeneity of aquifer properties, particularly hydraulic conductivity, is paramount in groundwater modeling when the transport and fate of contaminants need to be predicted. The field of geostatistics has focused on describing this heterogeneity with spatial random functions. The field of stochastic hydrogeology uses these functions to incorporate uncertainty about the subsurface in groundwater modeling predictions. Bayesian inference can update prior knowledge about the spatial patterns of the subsurface (e.g. plausible ranges of values) with a variety of information (e.g. direct measurements of hydraulic conductivity as well as indirect measurements such as water table drawdown at an observation well) in order to yield posterior knowledge. This dissertation focuses on expanding the tools for Bayesian inference of these spatial random functions.First, the development of open-source software tools for guiding users through the Bayesian inference process are described. There is an desktop application that implements the Method of Anchored Distributions and is referred to as MAD#. It is built in a modular fashion such that it can be coupled with any geostatistical software and any numerical modeling software. This modularity allows for a wide variety of spatial random functions and subsurface processes to be incorporated in the Bayesian inference process. There is also a R package, called anchoredDistr, that supplements the MAD# software. While the MAD# software handles the communication between the geostatistical software and the numerical modeling software, the anchoredDistr package provides more flexibility in analyzing the results from MAD#. Since R is an open-source statistical computing language, the anchoredDistr package allows users to take advantage of the plethora of statistical tools in R to calculate the posterior knowledge in the Bayesian process. Although MAD# provides a post-processing module to calculate this posterior knowledge, it does not provide all of the options that the R community can provide for modifications. Second, the expansion of which kinds of data and knowledge can be incorporated into the Bayesian process is explored. Incorporating time series (e.g. the drawdown of a water table from pumping over time) as indirect data in Bayesian inference poses a computational problem referred to as the `curse of dimensionality'. Since each additional measurement in time is correlated with the measurements before and after it, the calculation of probability distributions of these data become multi-dimensional. A synthetic case study incorporating drawdown time series in the Bayesian inference process is explored. A second form of information, conceptual models of geology, is also explored with a synthetic case study. Conceptual models of geology (e.g. a graphical representation of assumed geologic layering) can be described with images. There is a geostatistical technique called Multipoint Statistics that uses images as its input. The synthetic case study provides a proof-of-concept example in which the Bayesian inference process can infer conceptual models of geology using Multipoint statistics. Third, the issue of devising spatial models with realistic geology while constraining the complexity of the model is explored. An aquifer analog is used as the basis for an example. An aquifer analog is a data set with data of hydraulic properties at high spatial resolution, i.e. much higher than expected for ordinary field measurements. The aquifer analog used in this dissertation has ten soil types distributed in three-dimensional space. The objective posed is to predict the early arrival time of a contaminant traveling through the analog. Given this prediction goal, the task is to simplify the analog into a simplified structure without changing the prediction outcome. The purpose of this exercise is to take a goal-oriented approach to defining a parsimonious spatial model for describing this complex aquifer analog such that a geostatistical model can be inferred for this kind of geology in a computationally efficient manner.   Ultimately, any uncertainty quantification regarding the spatial heterogeneity of subsurface properties has the goal of improving groundwater modeling prediction efforts. With the addition of freely available software tools, the ability to integrate more forms of information, and methodology for translating complex geological structures into parsimonious spatial models, the characterization of our groundwater resources improves.",ucb,,https://escholarship.org/uc/item/07n5t8pw,,,eng,REGULAR,0,0
117,1553,The Mechanism of Processivity and Directionality of Cytoplasmic Dynein,"Cleary, Frank Banta","Yildiz, Ahmet;",2014,"Cytoplasmic dynein is a dimeric motor that transports intracellular cargoes towards the minus-end of microtubules (MTs). In contrast to other processive motors, microtubule release of the dynein heads is not precisely coordinated and the mechanism of dynein processivity remains unclear. By engineering the mechanical and catalytic properties of the motor we show that dynein processivity minimally requires a single active motor domain (head) and a second inert MT binding domain. The AAA+ ring and the linker of the other head are dispensable, suggesting that processivity arises from a high ratio of MT bound to unbound time, and not from interhead communication. Additionally, nucleotide-dependent microtubule release is gated by tension on the linker domain.We find that dynein releases rapidly from the MT when force is applied in the forward direction (towards the minus-end), but remains bound under backward forces. This finding suggests that the asymmetric release properties of the microtubule binding domain (MTBD) control dynein directionality. Consistent with this hypothesis, replacing the MTBD with a catalytically inactive kinesin motor domain, which favors release towards the plus end, results in reversal of motor directionality. Furthermore, a dynein dimer maintains its minus-end directed motility when the released monomer is allowed to orient freely during the search for a new binding site. The results rule out directionality models based on a swinging mechanism of the linker domain. We propose that the mechanism of dynein directionality is fundamentally distinct from kinesin and myosin motors and determined by asymmetric release and binding properties of its MT binding interface. We develop a quantitative model for dynein stepping that reproduces the velocity and stepping characteristics of dynein motors and their response to chemical and mechanical perturbation.",ucb,,https://escholarship.org/uc/item/07t3x6xg,,,eng,REGULAR,0,0
118,1554,Symplectic approaches in geometric representation theory,"Jin, Xin","Nadler, David;",2015,"We study various topics lying in the crossroads of symplectic topology and geometric representation theory, with an emphasis on understanding central objects in geometric representation theory via approaches using Lagrangian branes and symplectomorphism groups. The first part of the dissertation focuses on a natural link between perverse sheaves and holomorphic Lagrangian branes. For a compact complex manifold $X$,  let $D_c^b(X)$ be the bounded derived category of constructible sheaves on $X$, and $Fuk(T^*X)$ be the Fukaya category of $T^*X$. A Lagrangian brane in $Fuk(T^*X)$ is holomorphic if the underlying Lagrangian submanifold is complex analytic in $T^*X_{\mathbb{C}}$, the holomorphic cotangent bundle of $X$. We prove that under the quasi-equivalence between $D^b_c(X)$ and $DFuk(T^*X)$ established by Nadler and Zaslow, holomorphic Lagrangian branes with appropriate grading correspond to perverse sheaves.The second part is motivated from general features of the braid group actions on derived category of constructible sheaves. For a semisimple Lie group $G_\mathbb{C}$ over $\mathbb{C}$, we study the homotopy type of the symplectomorphism group of the cotangent bundle of the flag variety and its relation to the braid group. We prove a homotopy equivalence between the two groups in the case of $G_\mathbb{C}=SL_3(\mathbb{C})$, under the $SU(3)$-equivariancy condition on symplectomorphisms.",ucb,,https://escholarship.org/uc/item/07t5s31b,,,eng,REGULAR,0,0
119,1555,Live Imaging of Segmentation Clock Dynamics in Zebrafish,"Shih, Nathan Pui-Yin","Amacher, Sharon L;",2013,"Segmentation is a developmental program in animals that generates semi-repetitive structures along the body axis. In vertebrates, somites are formed sequentially from the mesoderm of the extending tailbud, eventually giving rise to structures such as axial muscles and vertebrae. Somites form with great regularity, every thirty minutes in zebrafish. The periodicity of somite formation is controlled by a set of oscillating genes known as the segmentation clock. To better understand the dynamics of the clock and its role in patterning somites, I have explored its behavior through an in vivo clock reporter, her1:her1-venus. I show that individual cells in the presomitic mesoderm express oscillating clock expression, consistent with predictions made by mathematical modeling and analysis in fixed embryos. I am able to rapidly track a large number of PSM cells using novel semi-automated cell tracking and fluorescence quantification programs. Through these methods, I find that the clock oscillations are coordinated through the Notch pathway, and a loss of Notch function causes slower and desynchonized clock oscillations. I also explore the interaction of mitosis and the segmentation clock, and find the two processes are connected. Finally, I investigate the slowing of the segmentation clock in real-time, and find that oscillations in the anterior PSM are about twice the periodicity of somitogenesis. This has interesting implications for the role of the segmentation clock in patterning somites, including the potential to polarize somites. By studying the segmentation clock in real-time, I am able to better investigate and understand the mechanisms driving somitogenesis.",ucb,,https://escholarship.org/uc/item/0813z4j6,,,eng,REGULAR,0,0
120,1556,"Design, Synthesis, and Evaluation of Next Generation Technologies in Stimulus-Responsive Materials and Organic Electronics","Unruh, Jr, David Allen","Frechet, Jean M. J.;",2011,"Future advancements in technology are fundamentally limited by materials research and development.  New materials are most often found by exploring new chemistries, but the nature of these chemistries can vary widely.  This dissertation is a compilation of some of the chemical insights that can lead to the development of new stimulus-responsive materials and organic electronics.  For example, a tool that can consistently display specific, precise types of reactivity can have dramatic effects on the ability to make never-before seen materials.  Chapter 1 discusses a multifunctional mixed monolayer resist for scanning probe nanolithography that can respond to two different electrochemical stimuli to produce two chemically different products, each with nanoscale resolution.  The synthesis of a novel reductively active monolayer precursor, the preparation of this mixed monolayer surface, and its use in a proof-of-principle bottom-up assembly of complementary semiconductor components are described.  A combination of older chemistries can result in the development of new materials as well.  In Chapter 2, the synthesis of a thermally-triggered single component epoxy monomer is described.  A small library of these monomers is prepared, and structure-property relationships are established between monomer molecular structure and cure temperature, enthalpy of cure, and glass transition temperature.  In addition, effects of cure on the production of voids in the fully cured thermosetting polymer are investigated.  New materials can also be made by altering the way other materials pack together, as commonly found in supramolecular chemistry.  Chapter 3 discusses the pre-organization of small molecules into nanoscale crystalline domains as a means to establishing long range order in organic electronics.  A small molecule previously used in bulk heterojunciton organic photovoltaics is shown to make high aspect ratio nanowires through solution-phase nanocrystal synthesis, and the degree of crystallinity and device performance in organic field effect transistors are compared to the same molecule cast as a thin film.  In addition, different derivatives of the same molecule are evaluated for their ability to pack differently in the solid state.  Finally, new materials can be inspired by the limitations of existing materials and the challenges of an emerging technology.  Chapter 4 introduces a new thienooxypyrroline acceptor building block for donor-acceptor materials in organic photovoltaics and organic field effect transistors.  The synthetic sequence to access this chromophore is described, and a small library of thienooxypyrroline monomers are prepared.   Small molecules and polymers using these monomers are synthesized and shown to have promising device performance characteristics in preliminary testing of both OPVs and OFETs.  Based upon the device data, possible next steps are presented for use of this building block in future materials for organic electronics.",ucb,,https://escholarship.org/uc/item/081604mr,,,eng,REGULAR,0,0
121,1557,Kinetic isotope and trace element partitioning during calcite precipitation from aqueous solution,"Nielsen, Laura Christina","DePaolo, Donald J;",2012,"Precipitation of carbonate minerals is ubiquitous in the near-surface environment, and the isotopic and trace element composition of carbonates may be used to reconstruct the conditions of growth. Little is known about the mechanisms controlling isotope and trace element distribution into carbonates. Proposed growth mechanisms are typically inferred from the supersaturation dependence of CaCO3 precipitation rates. As different experimental techniques often generate different apparent reaction orders, numerous hypothesized mechanisms can be found in the literature. These descriptions of crystallization pathway cannot be used to identify processes controlling trace element and isotope partitioning. Recent advances in experimental methodology allow us to observe microscopic to nano-scopic structures at the mineral surface during growth. The mechanisms controlling calcite growth have been directly determined by using fluid flow cells placed in an atomic force microscope (AFM; chapter 3). Calcite growth below the threshold oversaturation for amorphous calcium carbonate (ACC) formation &ndash typical of seawater and most terrestrial fluids &ndash occurs primarily through the attachment of ions to kink sites on the surface. The net flux of ions to kink sites governs both overall growth rate and mineral composition. In this thesis, I present a self-consistent model based on observed calcite growth mechanisms that may be used to predict growth rate (chapter 1), and isotopic (chapter 2; Nielsen et al., 2012) and trace element (chapter 5; Nielsen et al., in prep.) partitioning as a function of solution composition. I apply this model to calcium isotope fractionation during the precipitation of synthetic calcite, which I grew and analyzed using novel secondary ion mass spectrometry (SIMS) methods (chapter 3). In chapter chapter 4 (Nielsen and DePaolo, in review), I model calcium isotope fractionation in carbonate minerals that I collected from the highly alkaline Mono Lake, CA. The mechanistic framework developed here may be extended to multicomponent systems, and may be adapted for use in reactive transport models. When interpreted through the lens of this model, trace element and isotope signatures preserved in carbonates may eventually be used to reconstruct the chemistry of natural aqueous fluids.",ucb,,https://escholarship.org/uc/item/03t4933z,,,eng,REGULAR,0,0
122,1558,Resonant Interactions of Surface and Internal Waves with Seabed Topography,"Couston, Louis-Alexandre","Alam, Mohammad-Reza;",2016,"This dissertation provides a fundamental understanding of water-wave transformations over seabed corrugations in the homogeneous as well as in the stratified ocean. Contrary to a flat or mildly sloped seabed, over which water waves can travel long distances undisturbed, a seabed with small periodic variations can strongly affect the propagation of water waves due to resonant wave-seabed interactions--a phenomenon with many potential applications.  Here, we investigate theoretically and with direct simulations several new types of wave transformations within the context of inviscid fluid theory, which are different than the classical wave Bragg reflection. Specifically, we show that surface waves traveling over seabed corrugations can become trapped and amplified, or deflected at a large angle ($\sim 90\degree$) relative to the incident direction of propagation. Wave trapping is obtained between two sets of parallel corrugations, and we demonstrate that the amplification mechanism is akin to the Fabry-Perot resonance of light waves in optics. Wave deflection requires three-dimensional and bi-chromatic corrugations and is obtained when the surface and corrugation wavenumber vectors satisfy a newly found class I$_2$ Bragg resonance condition. Internal waves propagating over seabed topography in a stratified fluid can exhibit similar wave trapping and deflection behaviors, but more surprising and intricate internal wave dynamics can also be obtained. Unlike surface waves, internal waves interacting with monochromatic seabed corrugations can simultaneously generate many new waves with different wavenumbers and directions of propagation--a phenomenon which we call chain resonance. Here, we demonstrate that the chain resonance leads to significant energy transfer from long internal waves to short internal waves for almost all angles of the incident waves relative to the orientation of the oblique seabed corrugations. Since short internal waves are prone to breaking, the resonance mechanism, therefore, may have important consequences on the spatial variability of ocean mixing and energy dissipation. Potential applications of the theory of resonant wave-seabed interactions for wave energy extraction and coastal protection are also discussed.",ucb,,https://escholarship.org/uc/item/04077138,,,eng,REGULAR,0,0
123,1559,On the Biosynthesis of Triacsins,"Twigg, Frederick Fairbank","Zhang, Wenjun;",2020,"Natural products are a source of engineering innovation and design for small molecules due to their relevance to wide swaths of the chemical sector including medical, agricultural, food and fragrance, and commodity chemical fields. Their structural complexity comprising of numerous chiral centers and an abundance of heteroatoms makes organic synthesis challenging, expensive, and generally infeasible. While combinatorial chemistry hoped to address these obstacles by enabling rapid diversification and screening methods, it is still limited by access to an initial scaffold on which to act upon. As such there is great potential for leveraging biosynthesis in combination with synthetic strategies to facilitate sustainable production of new bioactive compounds. In this manuscript we present our findings on the biosynthesis of NN bonds in the context of the triacsin natural product family. By elucidating the biosynthesis of a compound with multiple NN chemical bonds, we have discovered multiple enzymatic strategies employed by nature to create a linkage which is synthetically challenging due to the inherent nucleophilicity of nitrogen atoms. As such this research provides insight into the biogenesis of NN bonds and addresses the aforesaid synthetic challenges in chemical access to new bioactive compounds.Triacsins are notable for the conserved N-hydroxytriazene moiety that all members of the family bear. In addition to two sequential NN bonds, the terminal nitrogen itself is a member of an additional heteroatom-heteroatom linkage in the form of a nitrogen-oxygen bond. As with many stories in natural product biosynthesis, this manuscript begins with the genomic sequencing of the originally reported native producer of triacsins. Mutagenesis and isotopically labeled precursor feeding led to the identification of the essential genes required for triacsin biosynthesis and led to the discovery of another native triacsin-producing organism. Cultivation of mutant strains and analysis of organic extracts from said strains led to the structural characterization of a key late-stage chemical intermediate that informed the biochemical timing of N-hydroxytriazene biosynthesis. Subsequent in vitro reconstitution of several encoded enzymes has advanced our knowledge of biochemical strategies for NN bond biogenesis. Finally, nascent work on the detailed characterization of an NN bond-forming enzyme will provide a full mechanistic understanding of this catalytic transformation. Collectively, this work contributes to the biocatalytic formation of NN bonds.",ucb,,https://escholarship.org/uc/item/0414x28r,,,eng,REGULAR,0,0
124,1560,Inertial and Aerodynamic Tail Steering of a Meso-scale Legged Robot,"Kohut, Nicholas Jospeh","Fearing, Ronald S;O'Reilly, Oliver M;",2013,"Legged robots have excellent potential for all-terrain mobility, and are capable of many behaviors wheeled and tracked robots are unable to perform. However, legged robots have difficulty turning rapidly, or turning while running forward. For maximum maneuverability, terrestrial robots need to be able to turn precisely, quickly, and with a small radius. Previous efforts at turning in legged robots primarily have used leg force or velocity modulation. This work presents two novel methods of legged robot turning. The first of these methods is inertial tail turning, or using a rapidly actuated, weighted tail to cause a sudden change in angular momentum, turning the body of the robot. The tailed robot presented here is able to make rapid, precise turns. By rapidly rotating the tail as the robot runs forward, the robot was able to make sudden 90 degree turns at 360 degrees per second, making it the fastest turning legged robot known to the author at the time of this publication. Unlike other turning methods, this turn is performed with almost no change in forward running speed. The dynamics of this maneuver have also been modeled, to examine how features, such as tail length and mass, affect the robot's turning ability. This approach has produced turns with a radius of 0.4 body lengths at 3.2 body lengths per second running speed. Using a nonlinear feedback controller, turns with an accuracy of  5 degrees for a 60 degree turn have been achieved.The second method of turning presented here allows a legged robot to turn continuously while running at high speeds. This remains a difficult task for legged robots, but is crucial for maneuvering quickly in a real-world environment. SailRoACH is the first running robot that uses aerodynamic forces to turn. A flat plate serves as an aerodynamic surface, and depending on its position, can be used to impart positive or negative aerodynamic yaw torques on the robot as it runs forward, causing turns of up to 70 degrees per second at a radius of 1.2 meters and a running speed of 1.8 meters per second. A scale analysis of aerodynamic steering is also presented, showing this method is most effective for small robots. Comparisons to other steering methods are made, showing that inertial and aerodynamic steering are superior for high speed turns at high forward velocity, compared to existing methods.",ucb,,https://escholarship.org/uc/item/048207h4,,,eng,REGULAR,0,0
125,1561,Context in Constructions,"Lee-Goldman, Russell Rafael","Sweetser, Eve E.;",2011,"Traditional associations between syntax and grammar include the notions of anaphora, deixis, ellipsis, speech acts, and information structure. However, there exist numerous other layers of communicative organization, including the structure of conversation and turn-taking, quasi-ritualized interactions, and genre and register. Long recognized as analytically important categories in the fields of Conversation Analysis, contrastive pragmatics, Interactional Sociolinguistics, and others, they have been largely ignored in linguistic and especially syntactic theory. This study aims to begin an integration of formal and social/interactional approaches to linguistics from the perspective of a flexible and precise grammatical framework: Sign-based Construction Grammar.Through a series of close studies of grammatical constructions in English and Japanese, it is shown that grammatical structure and the interactional contexts in which language is used have a far closer and more integrated relationship than is usually assumed. I introduce the script as a way to capture the fact that not only can language reflect context, but context can exert a significance force on speakers' linguistic choices. The case studies proceed from relatively low-level contextual features to higher levels of organization, showing at each point the necessity to recognize grammatical constructions sensitive to that level of interaction.Chapter 1 introduces the question of the syntax-context interface. Chapters 2 and 3 present a syntactic representation for lexically- and constructionally-licensed argument omission (null instantiation). Aside from contextual features normally associated with ellipsis, it is seen that to fully account for argument omission it is necessary to incorporate into grammatical constructions references to a fine-grained categorization of speech acts and attitudinal and epistemic stances. Chapters 4 and 5 show that broader regions of conversational context are crucial to licensing constructions. Chapter 4 examines means of providing identification in situations where there is no visual contact (on the phone, at the front door). Chapter 5 illustrates the existence of Japanese and English constructions which function to project future linguistic actions. Chapter 6 extends the framework to consideration of persistent contextual features, namely kinship relations, and how these determine or influence linguistic choices in referring to other family members. Chapter 7 concludes and discusses directions for future work.",ucb,,https://escholarship.org/uc/item/04g389dr,,,eng,REGULAR,0,0
126,1562,Toward the Systematic Design of Complex Materials from Structural Motifs,"SMIDT, TESS Eleonora","Neaton, Jeffrey B;",2018,"With first-principles calculations based on density functional theory, we can predict with good accuracy the electronic ground state properties of a fixed arrangement of nuclei in a molecule or crystal. However, the potential of this formalism and approach is not fully utilized; most calculations are performed on experimentally determined structures and stoichiometric substitutions of those systems. This in part stems from the difficulty of systematically generating 3D geometries that are chemically valid under the complex interactions existing in materials. Designing materials is a bottleneck for computational materials exploration; there is a need for systematic design tools that can keep up with our calculation capacity. Identifying a higher level language to articulate designs at the atomic scale rather than simply points in 3D space can aid in developing these tools.Constituent atoms of materials tend to arrange in recognizable patterns with defined symmetry such as coordination polyhedra in transition metal oxides or subgroups of organic molecules; we call these structural motifs. In this thesis, we advance a variety of systematic strategies for understanding complex materials from structural motifs on the atomic scale with an eye towards future design.In collaboration with experiment, we introduce the harmonic honeycomb iridates with frustrated, spin-anisotropic magnetism. At the atomic level, the harmonic honeycomb iridates have identical local geometry where each iridium atom octahedrally coordinated by oxygen hosts a $J_{eff}=1/2$ spin state that experiences interactions in orthogonal spin directions from three neighboring iridium atoms. A homologous series of harmonic honeycomb can be constructed by changing the connectivity of their basic structural units.Also in collaboration with experiment, we investigate the metal-organic chalcogenide assembly [AgSePh]$_\infty$ that hosts 2D physics in a bulk 3D crystal. In this material, inorganic AgSe layers are scaffolded by organic phenyl ligands preventing the inorganic layers from strongly interacting. While bulk Ag$_2$Se is an indirect band gap semiconductor, [AgSePh]$_\infty$ has a direct band gap and photoluminesces blue. We propose that these hybrid systems present a promising alternative approach to exploring and controlling low-dimensional physics due to their ease of synthesis and robustness to the ambient environment, contrasting sharply with the difficulty of isolating and maintaining traditional low-dimensional materials such as graphene and MoS$_2$.Automated density functional theory via high throughput approaches are a promising means of identifying new materials with a given property. We automate a search for ferroelectric materials by integrating density functional theory calculations, crystal structure databases, symmetry tools, workflow software, and a custom analysis toolkit. Structural distortions that occur in the structural motifs of ferroelectrics give rise to a switchable spontaneous polarization. In ferroelectrics lattice, spin, and electronic degrees of freedom couple leading to exotic physical phenomena and making them technologically useful (e.g. non-volatile RAM). We also propose a new neural network architecture that encodes the symmetries of 3D Euclidean space for learning the structural motifs of atomic systems. We describe how these networks can be used to speed up important components of the computational materials discovery pipeline and generate hypothetical stable atomic structures.Finally, we conclude with a discussion of the materials design tools deep learning may enable and how these tools could be guided by the intuition of materials scientists.",ucb,,https://escholarship.org/uc/item/04g5p8ph,,,eng,REGULAR,0,0
127,1563,A Grammar of Chilliwack Halkomelem,"Galloway, Brent",,1977,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/08q2g527,,,eng,REGULAR,0,0
128,1564,Matrix Factorization and Matrix Concentration,"Mackey, Lester","Jordan, Michael I;",2012,"Motivated by the constrained factorization problems of sparse principal components analysis (PCA) for gene expression modeling, low-rank matrix completion for recommender systems, and robust matrix factorization for video surveillance, this dissertation explores the modeling, methodology, and theory of matrix factorization.We begin by exposing the theoretical and empirical shortcomings of standard deflation techniques for sparse PCA and developing alternative methodology more suitable for deflation with sparse ""pseudo-eigenvectors."" We then explicitly reformulate the sparse PCA optimization problem and derive a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.We next develop a fully Bayesian matrix completion framework for integrating the complementary approaches of discrete mixed membership modeling and continuous matrix factorization. We introduce two Mixed Membership Matrix Factorization (M3F) models, develop highly parallelizable Gibbs sampling inference procedures, and find that M3F is both more parsimonious and more accurate than state-of-the-art baselines on real-world collaborative filtering datasets.Our third contribution is Divide-Factor-Combine (DFC), a parallel divide-and-conquer framework for boosting the scalability of a matrix completion or robust matrix factorization algorithm while retaining its theoretical guarantees. Our experiments demonstrate the near-linear to super-linear speed-ups attainable with this approach, and our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.Finally, inspired by the analyses of matrix completion and randomized factorization procedures, we show how Stein's method of exchangeable pairs can be used to derive concentration inequalities for matrix-valued random elements. As an immediate consequence, we obtain analogues of classical moment inequalities and exponential tail inequalities for independent and dependent sums of random matrices. We moreover derive comparable concentration inequalities for self-bounding matrix functions of dependent random elements.",ucb,,https://escholarship.org/uc/item/08q2q9vw,,,eng,REGULAR,0,0
129,1565,Learning to Learn with Gradients,"Finn, Chelsea","Levine, Sergey;Abbeel, Pieter;",2018,"Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches.",ucb,,https://escholarship.org/uc/item/0987d4n3,,,eng,REGULAR,0,0
130,1566,Three Essays on Legitimacy and Organizational Outcomes,"Shin, Shoonchul","Haveman, Heather;",2016,"This research demonstrates that CEO dismissal can be one form of activism utilized by a board of directors against CEOsâ€™ prior records of violating institutional logics (i.e., cultural beliefs, norms, and assumptions about appropriate conduct) especially during poor performance. To this end, it examines two organizational outcomes of CEO dismissal and post-dismissal strategic changes in large U.S. Fortune companies between 1984 and 2007. During this period, the field of large firms was characterized by the rise of a shareholder-value logic, in which maximizing shareholder returns was the ultimate goal of the firm and corporate refocusing and employment downsizing were appropriate and necessary means to that end. According to event-history analyses, poor performance generally led to an increase in the CEO dismissal rate, but this effect was even stronger for CEOs who were reluctant to refocus and downsize during their tenures in the position. Moreover, when their predecessors were dismissed, new CEOs were more inclined to refocus and downsize, especially during their early tenures. These results indicate that when firms deviating from logics perform poorly, directors attribute the performance problem to the deviation itself, and thus seek the removal of the CEO as a means of strategically reorienting towards prevailing logics. This study contributes to institutional theory and upper echelon research.",ucb,,https://escholarship.org/uc/item/0993h3dz,,,eng,REGULAR,0,0
131,1567,Unsupervised Models of Entity Reference Resolution,"Haghighi, Aria Delier","Klein, Dan;",2010,"A central aspect of natural language understanding consists of linking information across multiple sentences and even combining multiple sources (for example: articles, conversations, blogs and tweets). Understanding this global information structure requires identifying the people, objects, and events as they evolve over a discourse. While natural language processing (NLP) has made great progress on sentence-level tasks such as parsing and machine translation, far less progress has been made on the processing and understanding of large units of language such as a document or a conversation.The initial step in understanding discourse structure is to recognize the entities (people, artifacts, locations, and organizations) being discussed and track their refer- ences throughout. Entities are referred to in many ways: with proper names (""Barack Obama""), nominal descriptions (""the president""), and pronouns (""he"" or ""him""). Entity reference resolution is the task of deciding to which entity a textual mention refers.Entity reference resolution is influenced by a variety of constraints, including syntactic, discourse, and semantic constraints. Even some of the earliest work (Hobbs, 1977, 1979), has recognized that while syntactic and discourse constraints can be declaratively specified, semantic constraints are more elusive. While past work has successfully learned many of the syntactic and discourse cues, there has yet to be an entity reference resolution system that exploits semantic cues and operationalizes these observations into a coherent model.This dissertation presents unified statistical models for entity reference resolu- tion that can be learned in an unsupervised way (without labeled data) and models soft semantic constraints probabilistically along with hard grammatical constraints. While the linguistic insights which underlie this model have been observed in some of the earliest anaphora resolution literature (Hobbs, 1977, 1979), the machine learning techniques which allow these cues to be used collectively and effectively are relatively recent (Blei et al., 2003; Teh et al., 2006; Blei and Frazier, 2009). In particular, our models use recent insights into Bayesian non-parametric modeling (Teh et al., 2006) to effectively learn entity partition structure when the number of entities is not known ahead of time. The primary contribution of this dissertation is combining the linguistic observations of past researchers with modern structured machine learning techniques. The models presented herein yield state-of-the-art reference resolution results against other systems, supervised or unsupervised.",ucb,,https://escholarship.org/uc/item/09h6j8kp,,,eng,REGULAR,0,0
132,1568,"Relations Among Neighborhood, Parenting, and Effortful Control in Chinese American Children","Lee, Erica","Zhou, Qing;",2014,"Although an extensive literature links children's self-regulation abilities with both distal (e.g., neighborhood) and proximal (e.g., parenting) contexts, scarce attention has been paid to identifying potential cultural factors (e.g., ethnic density) in this association. Methodologically, most research has utilized cross-sectional data, with few studies examining the cumulative impact of multiple contexts on children's development and adjustment. Furthermore, it is unknown how these contexts are linked with different self-regulation outcomes among Asian American children. To address these gaps in the literature, I studied 258 first- and second- generation Chinese American immigrant children, using a combination of structured interviews, questionnaire measures, and behavioral tasks completed by children, parents, and teachers. Across two waves of data collection, I investigated the mediated and interactive relations of neighborhood (disadvantage, ethnic density) and parenting style (authoritarian, authoritative) to children's effortful control outcomes. Contrary to expectations, parenting style did not mediate the relation between neighborhood disadvantage and children's effortful control. However, children of families residing in neighborhoods with a higher Asian concentration were more likely to rate their parents as using an authoritarian parenting style, which subsequently predicted lower levels of effortful control. I also found that authoritative parenting for these children had a weaker benefit on their effortful control compared to children residing in less ethnically dense neighborhoods, as rated by their teachers. Taken together, these findings suggest that use of an authoritarian parenting style is more culturally normative in ethnically dense Asian American neighborhoods and may serve as a risk factor for poorer effortful control in this population. Additionally, residing in ethnically dense neighborhoods may confer some risk to Chinese American immigrant children's development of effortful control, regardless of their parents' parenting style. As children reach middle childhood, their perception of their parents' parenting style appears to play a role in determining their self-regulation outcomes, and this extends to both home and school contexts (i.e., both child and teacher reports of effortful control).",ucb,,https://escholarship.org/uc/item/09p297zj,,,eng,REGULAR,0,0
133,1569,"I. Seismic Moment Tensor Analysis of Micro-Earthquakes in an Evolving Fluid-Dominated System, II. Ambient Noise Cross-Correlation for Evaluating Velocity Structure and Instrument Orientations in a Geothermal Environment","Nayak, Avinash","Dreger, Douglas S;",2017,"This dissertation presents a detailed analysis of recorded seismic waves in terms of their source and their propagation through the Earth in multiple scenarios. First, I investigate the source mechanisms of some highly unusual seismic events associated with the formation of a large sinkhole at Napoleonville salt dome, Assumption Parish, Louisiana in August 2012. I implemented a grid-search approach for automatic detection, location and moment tensor inversion of these events. First, the effectiveness of this technique is demonstrated using low frequency (0.1-0.2 Hz) displacement waveforms and two simple 1D velocity models for the salt dome and the surrounding sedimentary strata for computation of Greenâ€™s functions in the preliminary analysis. In the revised, and more detailed analysis, I use Greenâ€™s functions computed using a finite-difference wave propagation method and a 3D velocity model that incorporates the currently known approximate geometry of the salt dome and the overlying anhydrite-gypsum cap rock, and features a large velocity contrast between the high velocity salt dome and low velocity sediments overlying and surrounding it. I developed a method for source-type-specific inversion of moment tensors utilizing long-period complete waveforms and first-motion polarities, which is useful for assessing confidence and uncertainties in the source-type characterization of seismic events. I also established an empirical method to rigorously assess uncertainties in the centroid location, MW and the source type of the events at the Napoleonville salt dome through changing network geometry, using the results of synthetic tests with real seismic noise. During 24-31 July 2012, the events with the best waveform fits are primarily located at the western edge of the salt dome at most probable depths of ~0.3-0.85 km, close to the horizontal positions of the cavern and the future sinkhole. The data are fit nearly equally well by opening crack moment tensors in the high velocity salt medium or by isotropic volume-increase moment tensors in the low velocity sediment layers. The addition of more stations further constrains the events to slightly shallower depths and to the lower velocity media just outside the salt dome with preferred isotropic volume-increase moment tensor solutions. I find that Greenâ€™s functions computed with the 3D velocity model generally result in better fit to the data than Greenâ€™s functions computed with the 1D velocity models, especially for the smaller amplitude tangential and vertical components, and result in better resolution of event locations and event source type. The dominant seismicity during 24- 31 July 2012 is characterized by the steady occurrence of seismic events with similar locations and moment tensor solutions at a near-characteristic inter-event time. The steady activity is sometimes interrupted by tremor-like sequences of multiple events in rapid succession, followed by quiet periods of little of no seismic activity, in turn followed by the resumption of seismicity with a reduced seismic moment-release rate. The dominant volume- increase moment tensor solutions and the steady features of the seismicity indicate a crack- valve-type source mechanism possibly driven by pressurized natural gas.Accurate and properly calibrated velocity models are essential for the recovery of correct seismic source mechanisms. I retrieved empirical Greenâ€™s functions in the frequency range ~ 0.2â€“0.9 Hz for interstation distances ranging from ~1 to ~30 km (~0.22 to ~6.5 times the wavelength) at The Geysers geothermal field, northern California, from cross-correlation of ambient seismic noise recorded by a wide variety of sensors. I directly compared noise- derived Greenâ€™s functions with normalized displacement waveforms of complete single-force synthetic Greenâ€™s functions computed with various 1D and 3D velocity models using the frequency-wavenumber integration method, and a 3D finite-difference wave propagation method, respectively. These comparisons provide an effective means of evaluating the suitability of different velocity models to different regions of The Geysers, and assessing the quality of the sensors and the noise cross-correlations. In the T-Tangential, R-Radial, Z- Vertical reference frame, the TT, RR, RZ, ZR and ZZ components (first component: force direction, second component: response direction) of noise-derived Greenâ€™s functions show clear surface-waves and even body-wave phases for many station pairs. They are also broadly consistent in phase and relative inter-component amplitudes with the synthetic Greenâ€™s functions for the known local seismic velocity structure that was derived primarily from body wave travel-time tomography, even at interstation distances less than one wavelength. I also found anomalous large amplitudes in TR, TZ, RT and ZT components of noise-derived Greenâ€™s functions at small interstation distances (â‰²4 km) that can be attributed to ~10Â°-30Â° sensor misalignments at many stations inferred from analysis of longer period teleseismic waveforms. After correcting for sensor misalignments, significant residual amplitudes in these components for some longer interstation distance (â‰³ 8 km) paths are better reproduced by the 3D velocity model than by the 1D models incorporating known values and fast axis directions of crack-induced shear-wave anisotropy in the geothermal field. I also analyzed the decay of Fourier spectral amplitudes of the TT component of the noise-derived Greenâ€™s functions at 0.72 Hz with distance in terms of geometrical spreading and attenuation. While there is considerable scatter in the amplitudes of noise-derived Greenâ€™s functions, the average decay is consistent with the decay expected from the amplitudes of synthetic Greenâ€™s functions and with the decay of tangential component local-earthquake ground-motion amplitudes with distance at the same frequency.",ucb,,https://escholarship.org/uc/item/09p8w3bj,,,eng,REGULAR,0,0
134,1570,Mousike and Mythos: The Role of Choral Performance in Later Euripidean Tragedy,"Weiss, Naomi Alison","Griffith, Mark;Kurke, Leslie;",2014,"This dissertation takes a new approach to the study of Greek theater by examining the dramatic function of mousike (music, song, dance) in the plays of Euripides. Previous scholarship has tended to see the many references to mousike in his later work only in connection with the ""New Music"" (the changes in musical style, language, and instruments in fifth-century Athens), and to disregard their place within the plays themselves, often deeming especially meta-musical choral odes to be irrelevant to the surrounding drama. In contrast, I explore the dynamics of choreia (choral song and dance) and the sociocultural meanings of different musical images in four plays to show how mousike plays a vital role in directing and complementing the movement of the plot. I demonstrate how Euripides uses traditional as well as new images of mousike, and argue that this combination of musical motifs is essential to an understanding of each play's dramatic structure.The dissertation is divided into four studies of individual plays, which span roughly the last fifteen years of Euripides' career. The first chapter focuses on Electra, the earliest extant tragedy to include multiple, extended descriptions of mousike. I argue that choreia both frames our understanding of Electra and has a generative power, anticipating and even enacting pivotal moments of the plot. In Chapter Two I examine how Hecuba and the chorus in Troades create the illusion of an absence of choreia, even while they sing and dance on stage, and liken this to the concept of ""embodied absence"" within Performance Studies. I also argue that the chorus' proclamation in the first stasimon that they will sing ""new songs"" refers not only to Euripides' experimentation at this point in his career, but to musical change within the drama itself. Chapter Three explores patterns of mousike and choreia in Helen, showing how the dominance of such imagery in the play's choral odes shapes the audience's understanding of Helen's relationship with the chorus. I suggest that the play's mousike creates an aetiology not only of Helen's cult in Sparta, but also of the Dionysiac performance of the chorus of Athenian citizens in the theater. Chapter Four examines the dynamics of chorality and monody in Iphigenia in Aulis, showing how, through the performance of mousike, the audience's attention is directed away from the panhellenic choreia of the parodos and toward the sacrifice of Iphigenia. I also explore how representations of instrumental mimesis provide a poignantly vivid impression of pastoral calm before the beginning of the Trojan War, and argue for the authenticity of contested lines at the end of the tragedy on the basis of their style of musical performance. Throughout the dissertation, my methodology centers on the idea that a complex interaction between described and performed mousike encourages the audience to see and hear a performance in a particular way--a form of aesthetic suggestion through choreia.",ucb,,https://escholarship.org/uc/item/09q2x7nq,,,eng,REGULAR,0,0
135,1571,Energy-Efficient 60GHz Phased-Array Design for Multi-Gb/s Communication Systems,"Kong, Lingkai","Alon, Elad;",2012,"Recent advance in wireless technologies has enabled rapid growth of mobile devices. Consequently, emerging applications for mobile devices have begun demanding data rates up to multiple Gb/s. Although advanced WiFi systems are approaching such data rates, the narrow bandwidth at ISM band fundamentally limits the achievable data-rate. Therefore, the unlicensed 7GHz of bandwidth at 60GHz band provides an opportunity to efficiently implement these communication systems with a potential to achieve $>$10Gb/s throughput. Besides the wider bandwidth, operating at higher frequency theoretically has higher achievable signal-to-noise ratio in area limited applications. This is because the maximum achievable antenna gain within limited aperture increases with frequency and it can be achieved using phased-array technique. This thesis therefore focuses on the design of 60GHz phased-array transceivers to support energy-efficient high data-rate communication systems.Despite the advantages of 60GHz, mobile applications often require low power consumption as well as low cost implementation, making the design of 60GHz phased-array systems challenging. Taking into account the limited power budget, this research investigates the design choices of the number of elements in phased-array transceivers, and identifies that the overhead power is the bottleneck of energy efficiency. In order to reduce the overhead power in the transmitter, a new architecture using a fast start-up oscillator is proposed, which eliminates the need of explicit modulator and 60GHz LO delivery. Measurements has shown that the transmitter efficiency is boosted by more than 2X. More importantly, the overhead power is significantly reduced down to 2mW, making this architecture a good candidate for large number phased-array. On the other hand, suffering from the similar overhead problem, the receiver unfortunately could not share the same architecture. A different architecture that stacks the mixer on top of LO generation is thus proposed to reduce the power consumption in the receiver. This approach demonstrated a 2X power reduction in receiver overhead, and the resulted optimum number of receiver elements is close to 4.Besides using CMOS technologies, on-chip antenna is also studied in order to further reduce the system cost. Slot-loop antenna is identified as a good candidate because that its intrinsic ground plane eases the integration with the rest of circuitry. Although the simulation shows an efficiency as high as $30\%$, the planar nature of the on-chip antenna limits its coverage in end-fire directions. Antenna diversity is thus proposed to overcome this limitation by utilizing multiple drive points on the same antenna. Because the antenna is fully integrated on-chip, antenna diversity can be implemented without extra high frequency I/Os, eliminating the loss that would be introduced otherwise.Using the proposed transceiver architectures, a 4-element phased-array with on-chip antennas was fabricated on TSMC's 65nm CMOS technology as a test vehicle. Consuming 50mW in the transmitter and 65mW in the receiver, this 10.4Gb/s phased-array covers a range larger than 45cm in all directions. This achieves a state-of-art energy-efficiency of 11pJ/bit. The 29mW/element power consumption also demonstrates the lowest power of a single phased-array element.",ucb,,https://escholarship.org/uc/item/09z098bd,,,eng,REGULAR,0,0
136,1572,"The Effects of Complex Optical Environments on the Development, Progression and Control of Myopia","Liu, Yue","Wildsoet, Christine;",2011,"Myopia or nearsightedness is a condition in which the axial length of the eye is too long relative to its optical focal length. This condition is reaching epidemic levels worldwide, and has become a tremendous public health burden. Myopia is one of the leading causes of vision loss and high myopia significantly increases the risk of permanent blindness. Consequently, myopia cannot be considered as a benign condition and early interventions aimed at slowing down or even stopping the progression of myopia rather than merely correcting the associated optical focusing error is of great importance. The consistent evidence from animal model studies showing that imposed hyperopic defocus, if sustained, comprises an effective myopogenic stimulus, accelerating eye growth, while imposed myopic defocus slows ocular elongation, has motivated clinical studies using bifocal and progressive addition spectacles for myopia control. While these studies, reviewed in Chapter 2 of this dissertation, failed to provide convincing evidence for a clinically significant treatment effect, i.e., slowed myopia progression; smaller scale, non-randomized studies using multifocal soft contact lenses and orthokeratology lenses have shown much more promising effects in terms of slowing myopia progression. However, the mechanism(s) underlying the latter anti-myopia treatment effects are poorly understood, thereby limiting their further refinement. In addition to the aforementioned systematic review of relevant clinical evidence for optical interventions for myopia control, this dissertation described 3 other studies using chick model, representing efforts to further understand how multifocal optical environments affect normal ocular development, specifically the process of emmetropization. These manipulations have also been used as a tool to investigate the mechanism underlying the myopia controlling effect of the novel contact lens applications referred above. Chapter 2 describes a systematic review and meta-analysis performed on randomized controlled trials investigating the effects of three traditional optical interventions, bifocal and progressive addition spectacles, and rigid contact lenses, all believed to control myopia progression, based on anecdotal evidence. The overall treatment effect was estimated to be only small and clinically insignificant. A number of reasons were proposed to explain the discrepancy between the results of related animal studies and these clinical trials - strong support for optical control of myopia from the former studies and inconclusive results from the latter studies. Poor compliance to the spectacle corrections in clinical studies, inaccurate measurement of adherence to the treatments, inadequate and potentially, ambiguous classification systems for clinical myopia, and potential flaws in the optical designs are likely to have contributed to the discouraging results from the clinical trials. Chapter 3 describes the first of three studies in which the chick model was used to test the effects on ocular growth of a series of custom-designed 2-zone multifocal ""spectacle"" lenses. This and the two follow-up studies using this model have the following merits; they made use of standardized experimental paradigms and objective methods of measurement to track ocular changes. The first study applied the lenses in a simple experimental paradigm, in which monocular lenses were attached to the normal eyes of young chickens. The 2-zone lenses included a plano zone, either in the center or peripheral surround, with either positive or negative power (+5 or -5 D) incorporated into the other zone. The size of the central zone was also allowed to vary, to control the size of the unifocal central and peripheral retinal areas, and intermediate multifocal zone, as was the placement of the powered zone, i.e. in the center or periphery of the lenses. Single vision lenses were included as a control treatment. Two important observations from this study were that 1) peripheral optical defocus can influence both peripheral (off-axis) and central (on-axis) refractive error development and 2) the inhibitory effect on axial ocular growth of myopic defocus imposed using 2-zone lenses (positive zones) could exceed that induced by single vision lenses of the same power. These results suggested complex interactions between adjacent retinal regions, with the peripheral retina apparently able to decode optical defocus, as well as complex interactions between the eyes own optical aberrations and those of the 2-zone lenses, which introduced large amounts of spherical aberration.  Chapter 4 described a closely related study in which subsets of the same 2-zone lenses were tested on eyes that had undergone surgical manipulations (sectioning of the ciliary nerve, CNX or iridectomy, ID) to investigate the influence of the pupil size of the eye as well as accommodation on the effects of the 2-zone lenses. Both were uncontrolled in the first study; yet create a dynamic optical system on which the multifocal optical environment was imposed. The ID surgery produced a fixed dilated pupil without any effect on accommodation (confirmed in another study, reported in Appendix I), while the CNX surgery produced a similarly enlarged pupil while also eliminating accommodation. This study revealed pupil size to be a critical factor in the treatment effects of 2-zone lenses, likely to reflect at least in part, its influence on the optical experience of various retinal zones (center to periphery), and also suggested a significant role of accommodation in the decoding of imposed optical defocus stimuli, in this case, complex multifocal optical stimuli. Chapter 5 described a third study which attempted to develop a clinically more relevant scenario; specifically, 2-zone lenses that incorporated two different negative powers (-5 & -10 D) in two optical zones (center & periphery or vice versa), were tested on both normal eyes and eyes made myopic before being fitted with one of the 2-zone lenses. The latter combination was intended to simulate the ocular conditions created when concentric multifocal contact lenses, i.e. with a near addition, are prescribed to human myopes, one of the novel, myopia control treatments currently being explored. When the 2-zone lenses were fitted to normal eyes, they induced myopia, of a magnitude falling between the values expected, had single vision lenses of the same powers been used. However, on myopic eyes, the lenses had a strong myopia inhibiting effect with the already induced myopia undergoing substantial regression. This result supports the further investigation of appropriately designed concentric multifocal contact lenses for the control of myopia progression. In summary, the studies reported in this dissertation indicate complex interactions between central and peripheral retinal regions in decoding and responding to complex defocus signals as well as critical influences of pupil size and accommodation on these processes. The strong and consistent inhibitory effects on ocular growth of concentric 2-zone lenses incorporating a zone of either positive power or a near addition, 2-zone designs lend plausibility to the notion of using custom-designed novel optical treatments for the control of myopia progression.",ucb,,https://escholarship.org/uc/item/0b6310p2,,,eng,REGULAR,0,0
137,1573,Exploring the Reliability and Validity of Pilot Teacher Ratings in a Large California School District,"Makkonen, Reino","Fuller, Bruce;",2013,"Many states and school districts have recently instituted revamped teacher evaluation policies in response to incentives from the federal government as well as a changing political climate favoring holding teachers accountable for the performance of their students. Many of these overhauls have mandated the incorporation of multiple performance indicators -- often including rubric-based classroom observation scores, estimated contributions to student test score outcomes, and surveys of students and parents -- into teacher evaluations. This three-paper dissertation explores the pilot implementation of a new standards-based multiple-measure teacher evaluation system in a large California school district in 2011/12. It examines both participants' views about the new system (particularly the challenges they faced and the early outcomes they felt were achieved), as well as the reliability and validity of the teacher observation ratings that resulted during pilot implementation. Results indicated that this self-selected group of pilot teachers and administrators generally appreciated the district's new teaching framework and pre/post-observation conferencing process, and participants also tended to report that certain key early outcomes were achieved, including increased reflection by teachers about their performance against the new teaching framework and better understandings of teachers' individual needs for instructional support (although a higher proportion of administrators than teachers reported that this latter outcome was achieved). Time constraints, staffing shortfalls and technology problems were all key challenges cited by both teachers and administrators during the pilot year. Analyses of the ratings for the small sample of participating teachers who received a complete set of observational focus element (item) scores from both of their raters across both observation cycles indicated that these teachers tended to be scored higher during the second cycle -- although such improvement wasn't universal -- and that across cycles the scores from second raters (who typically did not work at the school site) tended to be slightly lower than those awarded by the teachers' supervising site administrator. But ultimately, good agreement was evident between the primary and second raters who scored common teachers. Generalizability analyses indicated that approximately two-thirds of the variation in participating pilot teachers' total scores was attributable to systematic differences among teachers, while the variability associated with the observation cycle (approximately 25 percent) was larger than that associated with rater group (approximately 6 percent). These results were then used to forecast reliability coefficients based on different combinations of rater groups and observed lessons (cycles), and suggested that, based solely on pilot implementation and results from this particular analysis sample, varying the number of observations influenced reliability estimates far more than varying the number of observers. Finally, the group of participating pilot teachers who completed end-of-year surveys generally felt that the observations of their practice conducted during the pilot year represented a valid measure of their effectiveness, and pilot teachers' classroom observation-based ratings were not related to their ethnicity or the grade span they taught (factors that should theoretically be unrelated to performance). Low to moderate correlations were evident between pilot teachers' classroom observation-based ratings and their student survey ratings and value-added scores for the 2011/12 year. The uniqueness of this pilot context restricts the generalizability of these findings, however. The pilot consisted primarily of volunteers, and there was attrition during the pilot year -- approximately one-third of the teachers trained in fall 2011 never had any ratings entered online by an observer. In turn, the final pilot sample was comprised of a self selected group of experienced, mostly elementary school teachers who administrators from case study sites tended to characterize as particularly hard working and high performing. Moreover, our research team's limited capacity for qualitative data collection in spring 2012 (we were only able to visit five participating schools) and our low survey response rates (52 percent for teachers and 54 percent for administrators) also limit our ability to generalize findings more broadly. We did not hear the perspectives of those who dropped out of the pilot. Finally, the tools and processes under study were still being revised and fine-tuned by the district during the pilot year; observers were still learning the tools and teachers and administrators were just becoming familiar with the processes and measures. All told, these results likely do not reflect what will be found in any eventual full-scale roll out.",ucb,,https://escholarship.org/uc/item/0b63n98b,,,eng,REGULAR,0,0
138,1574,Media Aesthetics: Pergolesi's Stabat mater and its Circulation in the Long Eighteenth Century,"Meci, Jonathan Patrick","Mathew, Nicholas;",2019,"This dissertation takes as its starting point Clifford Siskin and William Warnerâ€™s recent contention that the Enlightenment is best understood as ""an event in the history of mediation,"" specifically the â€œproliferationâ€ of ""new and newly important"" media that ""establish the conditions for the possibility of Enlightenment.""  Media for the circulation of music likewise proliferated during this period, resulting in the unprecedented mobility and iterability of musical works.  Giovanni Battista Pergolesiâ€™s Stabat mater, as one of the most celebrated and copiously mediated works of the eighteenth century, offers a unique entry point into this changing mediascape and the effects of media proliferation on music discourse.  By following Pergolesi's Stabat on its global peregrinations, this dissertation surveys society's evolving relationship with new musical media, how these media became naturalized in different places, how different media interfaced with each other and how media proliferation reshaped generations of listenersâ€™ musical experiences.   	 The first three chapters serve as archaeologies of three media particularly enmeshed in the reception of Pergolesiâ€™s Stabat.  Chapter 1 focuses on the Neapolitan conservatory system, the medium through which the musical style that characterizes the Stabat was distilled and transmitted to Pergolesi along with his fellow students.  Not only did the conservatory system serve as a medium for the transmission of Neapolitan style, but it was also crucial in fostering a musical diaspora that enabled Neapolitan music to traverse the Alps and spread across Europe.  The second chapter examines Lenten public concerts and the new concert societies that reorganized local and supralocal sociabilities around musical performance.  Originating in a symbiotic relationship with the opera season, these concerts became a new media format in their own right.  By the end of the century, most cities in Europe had experimented with some form of â€œspiritual concertâ€ and many concert organizations had also experimented with at least semi-annual performances of Pergolesiâ€™s Stabat.  (A short intermezzo between this and the next chapter looks at the influence of Pergolesiâ€™s example on other Stabat settings, exploring how composers managed Pergolesiâ€™s legacy through reference and allusion).  The third chapter follows a parallel media development to that of the second chapter: the emergence of the new genre of composer biography.  The popularity of Pergolesiâ€™s music and the misfortune of his early death drove interest in his life.  Biography offered writers and readers an opportunity to use the character of Pergolesi to (re)imagine musical communication, musical labor and musical history in ways that addressed musicâ€™s increasing mobility and iterability.  	In the process of excavating these media forms, a reoccurring theme is the material underpinning of the Stabatâ€™s exceptional and novel fame.  Contemporary writers heaped praise on the expressivity of the Stabat.  Rather than demonstrating the origin of this unparalleled expressivity in the music itself, the reception of the Stabat strongly indicates that its vaunted ability to depict and illicit feeling was nothing less than the sum of the sentimental valences that accrued around the work in the process of its constant mediation.  The mediacy of the work, stemming from the peculiarities of Neapolitan style, allowed it to accumulate sentimental meaning, while allowing it to factor conspicuously in aesthetic debates concerning musicâ€™s new mobility and iterability.  Its early entrance into cosmopolitan circulation, though, ensured an ambiguous position in discourses surrounding notions of progress, emergent nationalism, ideas of canon formation as well as concerns over sacred musicâ€™s religious propriety and musicâ€™s gradual commercialization.  The final chapter investigates the aesthetics of media saturation, just as the first three explore those of media proliferation.  With the naturalization of once new media, the currency of the Stabatâ€™s mediacy became devalued, initiating a decline in prestige.  But even as the Stabatâ€™s European reputation waned, media proliferation into Europeâ€™s colonies brought the Stabat into contact with non-European music, prompting a clash between competing aesthetic values.",ucb,,https://escholarship.org/uc/item/0bd6m68n,,,eng,REGULAR,0,0
139,1575,Some Periodic Solutions of the Two-Dimensional Stokes-Oldroyd-B System with Stress Diffusion,"Isaacson, Erica Amy","Wilkening, Jon A;",2012,"We use a limited memory BFGS optimization method to seek time-periodic solutions of the Stokes-Oldroyd-B system of equations with a 4-roller forcing field and periodic boundary conditions. The gradient of the objective function for the optimization is found using a method which is based on the calculus of variations, and employs a pseudo-spectral implicit-explicit Runge-Kutta scheme.  Once solutions are found, their  asymptotic stability is calculated via an eigenvalue method.  A variety of stationary and periodic solutions are found, plotted and systematized in a manner that suggests a global structure of periodic solutions.",ucb,,https://escholarship.org/uc/item/0bp939gf,,,eng,REGULAR,0,0
140,1576,Queer Kinship: An Exploration of the Rewards and Challenges of Planned Parenting among Gay Fathers,"Barr, Ben-David","Gambrill, Eileen;",2011,"Gay fathers are creating family forms and parenting practices that reach beyond the nuclear family model. Analysis suggests that fathers in this study are developing unique and queer versions of kinship. Fathers' desire for emotional connection leads to the creative assemblage of paid caregivers, friends, children's non-legal biological kin, and gay men's families of origin into kinship networks. These creative mixtures may be perceived as unusual family formations, but they assist gay fathers in creating social support and connected lives for themselves and their children. These findings are based on a qualitative research project that consisted of interviews with 15 gay fathers who resided in 8 households and who were raising 13 children. The participants were all self-identified gay men who had formed planned families outside of heterosexual relationships. Research aims included: To explore the lived experience of gay men raising children; To explore how gay fathers adapt concepts of kinship; To describe the role of social support in the lives of gay fathers; To present emergent and unanticipated topics.  Data-collection methods included development of a genogram that described each family and their social support networks. In-depth interviews based on a semi-structured interview guide were then conducted with participants. Data analysis techniques were based within a grounded theory framework. Analysis resulted in development of 47 repeating ideas, which were then organized into nine themes: 1) Methods of family formation and Desire to parent; 2) Challenges of parenting; 3) Rewards of parenting; 4) Kinship is about connection; 5) Biology is less and more important than I thought; 6) Importance of non-kin social supports; 7) Changes in sense of connection to the gay community; 8) I always knew I would be a dad; and 9) Experiences with social welfare institutions. Implications and recommendations for future research and practice are included.",ucb,,https://escholarship.org/uc/item/0c52x7tp,,,eng,REGULAR,0,0
141,1577,"Self-presentation, Interpersonal Perception, and Partner Selection in Computer-mediated Relationship Formation","Fiore, Andrew Rocco Tresolini","Cheshire, Coye;",2010,"The use of social and technological intermediaries to seek intimate partners has a long history. Yet the affordances and limitations of modern computer-mediated communication (CMC) systems built for this purpose â€” specifically, online dating sites â€” present new challenges and opportunities for those who use them to initiate intimate relationships. The sheer number of potential mates available on such sites is tremendous, but accurately gauging their appeal and suitability for a relationship can be difficult through CMC.This dissertation presents a longitudinal survey of users of a major U.S. online dating service as they interact with potential dates online, meet them in person, and in some cases establish intimate relationships. The survey addresses two research questions: how interpersonal perceptions change when online daters meet in person for the first time, and how online and offline perceptions are associated with relationship duration, satisfaction, and intimacy.With respect to the first research question, I find that on average measures of liking and willingness to enter into a romantic relationship decline after participants meet their dates face-to-face for the first time. This result held for both inexperienced and experienced online daters. With regard to the second research question, I find that participants' perceptions of their dates before they have met in person generally do not predict the duration of the subsequent relationship, if any. However, their perceptions on many dimensions shortly after meeting in person are significantly associated with relationship duration. At the same time, among those who do begin dating, perceptions on numerous dimensions both pre-meeting and post-meeting are associated with intimacy and, to a lesser degree, relationship satisfaction in the weeks after the first date. That is, it appears that initial judgments from online interaction do not predict whether a couple will form a relationship, but these judgments do predict metrics of relationship quality if they choose to do so.",ucb,,https://escholarship.org/uc/item/0cd9r9ws,,,eng,REGULAR,0,0
142,1578,New Transcriptional Regulators of Non-shivering Thermogenesis,"Dempersmier, Jon Michael","Sul, Hei Sook;",2015,"Unlike white adipose tissue (WAT), which stores excess energy as triglycerides, brown adipose tissue (BAT) burns fatty acids and glucose to produce heat. The thermogenic ability of BAT is due to the specialized inner mitochondrial proton transporter named uncoupling protein 1 (UCP1), which dissipates the proton motive force generated by the electron transport chain to create heat instead of ATP.  Despite data suggesting that increasing BAT activity may be a promising antiobesity therapy, an inclusive model of the transcriptional regulation of thermogenic genes remain unclear. The aim of this dissertation work was to identify and characterize novel regulators of the UCP1 promoter and nonshivering thermogenesis. 	Chapter 1 reviews BAT in both mice and human, profiling the basic mechanism of uncoupled respiration and cold-induced nonshivering thermogenesis. Unlike classical BAT, which has constitutive UCP1 expression, brown adipocyte-like cells arise in WAT depots following prolonged cold exposure and contribute to whole body thermogenic capacity. While having similar functions, these cells arise from different precursor populations, having unique gene signatures and potentially depot specific regulation. Human BAT resembles either classical BAT or brown adipocyte-like cells in a depot specific manner, with differing levels of basal UCP1 expression and expression profiles. Finally, known transcriptional and hormonal regulators of BAT are discussed.	Chapter 2 profiles my screening efforts to identify novel transcriptional regulators of the UCP1. Briefly, a library of over 1100 transcription factors was screened for activation of the UCP1 promoter. Expression profiling of the positive factors identified 6 novel, brown fat enriched transcriptional activators of UCP1. The first such transcription factor identified was the previously uncharacterized C2H2 type zinc-finger protein, Zfp516.  Zfp516 is induced by cold where it binds and activates a brown fat gene program. Zfp516 ablation is embryonic lethal, but Zfp516 knockout embryos have little to no UCP1 expression and aberrant morphology. On the other hand, adipose specific transgenic overexpression in aP2-Zfp516 resulted in marked browning of inguinal WAT, increased body temperature and whole body energy expenditure, and prevention of diet-induced obesity.	Chapter 3 profiles a second transcription factor identified in my screening efforts, the CCCH-type zinc finger protein, Zc3h10.  Zc3h10, together with multiple cofactors, binds and activates the distal UCP1 promoter. Ablation of Zc3h10 results in defective BAT differentiation in cells, while adipose specific transgenic overexpression in aP2-Zc3h10 mice results in a lean phenotype.	Finally, chapter 4 concludes this work, discussing my findings in the context of the field of brown adipocyte biology and presents future directions and remaining questions.	This study has identified novel transcriptional regulators of UCP1, contributing significantly to the understanding of brown adipocyte biology and nonshivering thermogenesis, and providing new targets for future antiobesity therapeutics.",ucb,,https://escholarship.org/uc/item/0cf4h6j1,,,eng,REGULAR,0,0
143,1579,Integration Of Locational Decisions with the Household Activity Pattern Problem and Its Applications in Transportation Sustainability,"Kang, Jee E",,2013,"Examining the Cycle: How Perceived and Actual Bicycling Risk Influence Cylcing Frequency, Roadway Design Preferences, and Support for Cycling Among Bay Area Residents",ucb,,https://escholarship.org/uc/item/0cm3b1gq,,,eng,REGULAR,0,0
144,1580,Nanomagnetism research: benefit from reduced dimensionality and interfaces,"Wu, Jie","Qiu, Ziqiang;",2010,"Along the effort of integrating the spin degree of freedom in electronic devices, magnetic structures at the nanometer scale are intensely studied because of their importance in both fundamental research and technological applications. In this dissertation, I present my Ph.D research on several subjects to reflect the broad topics of nanomagnetism research. Single-crystalline, magnetic, ultrathin films are synthesized by Molecular Beam Epitaxy (MBE) and measured by state-of-art techniques such as Magneto-Optic Kerr Effect (MOKE), Photoemission Electron Microscopy (PEEM), X-ray Circular and Linear Dichriosm (XMCD and XMLD) Spectroscopy. First, I will present my work on the quantum well state in metallic thin films. Second, I will present my study on the magnetic long range order in two-dimensional magnetic systems, particularly on the observation of stripe and bubble magnetic phases and the universal laws governing the stripe-to-bubble phase transition. Third, I will present my result on a new type of magnetic anisotropy resulting from the spin frustration at ferromagnetic/antiferromagnetic interfaces. Fourth, I will present our studies on the mechanism of the abnormal interlayer coupling in ferromagnet/antiferomagnet/ferromagnet sandwiches structure. Fifth, I will show a new method to control the oxidation process to realize the control of exchange bias. Sixth, I will revisit the topic of exchange bias and show that the exchange bias actually takes place even before the antiferromagentic spins are frozen. In the last chapter, I will summarize my research and discuss the future of this exciting field.",ucb,,https://escholarship.org/uc/item/0cm863nn,,,eng,REGULAR,0,0
145,1581,"Metapopulations in miniature: connectivity, subpopulation extinction, and recovery in microbial microcosms","Kurkjian, Helen","Simms, Ellen L;",2018,"Metapopulations occupy spatially divided habitats and understanding how that fragmentation affects their survival, growth, dispersal, and persistence is critical to their conservation.  Researchers in many sub-fields of ecology and evolutionary biology test hypotheses relating to metapopulation dynamics and landscape spatial structure. Key aspects of these hypotheses are sometimes (a) large numbers of subpopulations and dispersal corridors and (b) their positions relative to each other.  Comparing such spatial hypotheses using traditional lab equipment and methods is impractical, unwieldy, expensive, or impossible.I invented the Metapopulation Microcosm Plate (MMP) to overcome these drawbacks. This device resembles a 96-well microtiter plate; the 96 wells represent habitat patches and they are connected by dispersal corridors that can be modified in their spatial position to create various artificial landscapes, with hundreds of non-intersecting dispersal corridors of varying lengths.  The device can be filled with nutrient broth and used to culture microbial metapopulations.In Chapter One, I first demonstrate that bacterial travel time is significantly faster through MMP dispersal corridors that are shorter, but is unaffected by corridor vertical position within the plate.  Thus, MMPs satisfy the necessary assumptions for use in metapopulation experiments.  Furthermore, travel time by bacteria with fully functional flagella was significantly faster than that of bacteria with disabled flagella, indicating that the bacteria actively swim through the corridors, rather than traveling by simple diffusion. Thus, MMPs can test hypotheses that account for behavioral responses.  MMPs can be used to test many spatial hypotheses that have previously been prohibitively difficult to test. Further, by incorporating individual behavioral responses to within-patch conditions, MMPs incorporate greater realism than do directed pipetting or other artificial dispersal methods.In Chapter Two, I used MMPs to explore how recolonization and recovery after subpopulation extinction differs in metapopulations in which the dispersal corridors have different spatial arrangements.  Some metapopulations have corridors spread relatively evenly through space in a homogeneous arrangement such that most subpopulations are connected to a few neighbors, while others have corridors clustered in a heterogeneous arrangement, creating a few highly connected subpopulations and leaving most subpopulations with only one or two neighbors.  Graph theory and empirical data from other biological and non-biological networks suggest that heterogeneous metapopulations should be the most robust to subpopulation extinction.  Here, I compared the recovery of metapopulations with homogeneous and heterogeneous corridor arrangements following small, medium, and large subpopulation extinction events.  I found that while metapopulations with heterogeneous corridor arrangements had the fastest rates of recovery following extinction events of all sizes and had the shortest absolute time to recovery following medium-sized extinction events, metapopulations with homogeneous corridor arrangements had the shortest time to recovery following the smallest extinction events.  Finally, for Chapter Three I conducted an experiment to test whether metapopulations with heterogeneous corridor arrangements recover more slowly from extinctions targeted at high connectivity subpopulations than random extinctions in low connectivity subpopulations.  Simulations of the World Wide Web and other heterogeneous networks have demonstrated that, while they are very robust to random loss of nodes, targeted attacks on highly connected nodes can lead to failure of the entire network.  Based on these simulations, I predicted that metapopulations with heterogeneous corridors would recover fastest when extinctions occurred in low connectivity wells, regardless of extinction event size.  Unlike in theoretical networks, however, the corridor arrangements of metapopulations cultured in MMPs cannot be completely homogeneous, because wells on the edge will be slightly less connected than those in the center.  However, I predicted that a small deviation in connectivity would be unimportant and that recovery in metapopulations with homogeneous corridors would not be affected by whether extinctions were in low connectivity or high connectivity wells.  Instead, I found that, at both low and medium levels of extinction targeted at highly connected subpopulations, both heterogeneous and homogeneous metapopulations recovered more quickly when those extinctions were targeted at high connectivity wells, but that when many subpopulations went extinct, all metapopulations recovered fastest when those extinctions were in low connectivity wells. This work demonstrates that MMPs can be used to test the assumptions of metapopulation theory, especially those involving large numbers of subpopulations and dispersal corridors.  I have shown that metapopulations with heterogeneous corridor arrangements have the fastest rates of recovery from subpopulation extinction, but that that faster rate only translates to a shorter absolute time of recovery after larger extinction events.  Furthermore, when smaller numbers of subpopulations go extinct, metapopulations recover more quickly when those extinctions are targeted at high connectivity subpopulations, but when large numbers of subpopulations go extinct, recovery is faster when low connectivity subpopulations are targeted.  This suggests that dispersal corridors that are clustered in space may help to alleviate the effects of habitat fragmentation in some circumstances, but exacerbate them in others.",ucb,,https://escholarship.org/uc/item/0d11302q,,,eng,REGULAR,0,0
146,1582,Exotic Species and Temporal Variation in Hawaiian Floral Visitation Networks,"Imamura, Jennifer Lynn","Roderick, George;",2019,"Many studies have documented the negative impact of invasive species on populations, communities, and ecosystems, although most have focused solely on antagonistic rather than mutualistic interactions.  For mutualistic interactions, such as pollination, a key to understanding their impacts is how invasive species interact with native species and alter interaction networks.  Chapter 1 explores the impacts of invasive species on islands, particularly in regard to plants, pollinators, and how these exotic species attach to existing pollination interaction networks.  Island pollination networks differ from mainland counterparts in several important characteristics, including fewer species, more connectance, and increased vulnerability to both invasion and extinction.  A progression of invasion has been previously proposed, through which supergeneralist native species facilitate the entry of new exotic species, then are eventually replaced by a few supergeneralist invader species that ultimately dominate the interaction networks.  As a result, highly-linked exotic supergeneralists become central nodes in the networks, thus altering network topology and community structure and functioning.  Here, I evaluate the evidence for (1) native supergeneralists that provide attachment points for exotic species, (2) exotic supergeneralists that are potentially replacing the function of native species, and (3) the consequences for the replacement of native species with exotics.  Both native and exotic supergeneralist species are found on islands, which may therefore represent different points along the invasion trajectory, with consequent concerns for future conservation. Chapter 2 utilizes a long-term series of observed floral visitations to break apart the potential differences between plants and pollinators as invaders of a community.  When plants are introduced into a new environment, their reproductive success can be limited by the lack of a suitable pollinator.  If there is no suitable native pollinator, the success of exotic plants may depend on the presence of exotic pollinators, a situation mirrored for exotic plant visitors.  Yet, rarely are the distinct roles for native and non-native species of both plants and pollinators examined in the same community. This study examines the role of exotic plants and insects in floral visitation networks in Hawaii, in simple ecological communities with a depauperate native pollinator fauna.  On the island of Hawaii, in sites that differed with respect to the presence of exotic plants, floral visitors were observed and quantified across multiple years and seasons.  Where exotic plants were present, exotic insects were observed to visit both native and exotic plant species, while native insects rarely utilized exotic plant resources.  Additionally, the majority of floral visitors comprised exotic bees and syrphid flies.  In contrast, where the vegetation was dominated by native plants, native bees were major visitors. Thus, the impact of exotic plants and insect visitors on visitation networks was non-symmetrical.  Exotic plants relied upon exotic insect taxa, while exotic insect taxa were able to utilize both native and exotic plants. This study demonstrates that the role of mutualistic interactions on the success and impact of invasive species cannot be predicted by looking at isolated interactions, but must also consider the context of the interactions.Chapter 3 evaluates how these floral visitation networks vary over time.  Pollination systems provide important ecosystem services in both natural and managed ecosystems, but their future ecological stability is uncertain as a result of global change, including the impacts of invasive species, habitat loss, and a changing climate.  Understanding how these systems vary naturally through time, including intra-annually, can provide critical context for evaluating future change, as well as elucidating the complexity of interspecific interactions in the community.   This study examines temporal variation in floral visitation networks in a tropical system in Hawaii characterized by both native and non-native pollinators and plants, and less seasonal variation than in temperate regions.  The three most common floral visitors exhibited unique seasonal visitation patterns.  In the presence of only native plant species, both the exotic honeybee Apis mellifera and the endemic Hylaeus bees had similar seasonal variation in floral foraging.  However, when the vegetation was a mix of native and exotic species, Apis visitation tracked the peak blooming of exotic plants while Hylaeus only visited native plants, leading to seasonal variation in resource partitioning.  In contrast, visitation by the invasive yellowjacket Vespula pensylvanica consistently peaked during the fall, unrelated to plant blooming cycles.  Thus, even in a system with minimal seasonal climate variation, there were marked differences in the patterns of pollination interactions between seasons, suggesting that intra-annual variation must be considered in predictions for stability of pollination networks in a changing world.Finally, Chapter 4 reviews and assesses the range of conservation threats to these Hawaiian pollination systems.  Pollination interactions worldwide are facing a wide variety of threats, including habitat loss/change, agricultural intensification, pesticide/herbicide use, invasive species, parasites/disease, and global climate change.  Pollination networks in Hawaii are of special concern, because of the unique nature of Hawaiiâ€™s terrestrial biota, including both plants and pollinators.  As the sites from this study were located within a protected national park, the most likely sources for their endangerment are exotic/invasive species, the introduction and spread of parasites/disease, and the slow but potentially devastating effects of climate change.  Hawaiian ecosystems, and these sites in particular, are additionally subject to the changes and hazards associated with a zone of active geologic activity.  In this chapter, I address specifically both the rising global threats of parasites/disease and climate change and the unique local dangers of active volcanoes for Hawaiian pollination interactions.  The variety and magnitude of potential effects provide a wealth of opportunities for future research utilizing existing network data to evaluate how these factors operate both independently and interactively to create change.",ucb,,https://escholarship.org/uc/item/0d1544kt,,,eng,REGULAR,0,0
147,1583,Consequences of Confinement in Zeolite Acid Catalysis,"Gounder, Rajamani Pachayappan","Iglesia, Enrique;",2011,"The catalytic consequences of confinement within zeolite voids were examined for several elimination (alkane cracking and dehydrogenation, alkene cracking, alkanol dehydration) and addition (alkene hydrogenation, alkylation and oligomerization) reactions catalyzed by BrÃ¸nsted solid acids. These reactions are mediated by cationic transition states that are confined within voids of molecular dimensions (0.4-1.3 nm) and proceed at rates that reflect the Gibbs free energies of late ion-pairs at transition states relative to those for the relevant reactants. Ion-pair stabilities depend on electrostatic interactions between organic cations and catalyst conjugate anions and on dispersion interactions between these cations and framework oxygen atoms. The former interactions are essentially unaffected by confinement, which influences weakly BrÃ¸nsted acid strength, while the latter depend strongly on the sizes and shapes of voids and the species confined within them. The catalytic effects of confinement in stabilizing ion-pairs are prevalent when transition states are measured relative to gaseous reactants, but are attenuated and in some cases become irrelevant when measured with respect to confined reactants that are similar in composition and size.Zeolite voids solvate confined species by van der Waals forces and mediate compromises in their enthalpic and entropic stabilities. Confinement is generally preferred within locations that benefit enthalpic stability over entropic freedom at low temperatures, in which free energies depend more strongly on enthalpic than entropic factors. For example, the carbonylation of dimethyl ether (400-500 K) occurs with high specificity within eight-membered (8-MR) zeolite voids, but at undetectable rates within larger voids. This specificity reflects the more effective van der Waals stabilization of carbonylation transition states within the former voids. In contrast, entropic consequences of confinement become preeminent in high temperature reactions. Alkane activation turnovers (700-800 K) are much faster on 8-MR than 12-MR protons of mordenite zeolites because the relevant ion-pairs are confined only partially within shallow 8-MR side pockets and to lesser extents than within 12-MR channels. The site requirements and confinement effects found initially for elimination reactions were also pertinent for addition reactions mediated by ion-pair transition states of similar size and structure. Ratios of rate constants for elimination and addition steps involved in the same mechanistic sequence (e.g., alkane dehydrogenation and alkene hydrogenation) reflected solely the thermodynamic equilibrium constant for the stoichiometric gas-phase reaction. These relations are consistent with the De Donder non-equilibrium thermodynamic treatments of chemical reaction rates, in spite of the different reactant pressures used to measure rates in forward and reverse directions. The De Donder relations remained relevant at these different reaction conditions because the same elementary step limited rates and surfaces remained predominantly unoccupied in both directions.Rate constants for elementary steps catalyzed by zeolitic BrÃ¸nsted acids reflect the combined effects of acid strength and solvation. Their individual catalytic consequences can be extricated using Born-Haber thermochemical cycles, which dissect activation energies and entropies into terms that depend on specific catalyst and reactant properties. This approach was used to show that thermal, chemical and cation-exchange treatments, which essentially change the sizes of faujasite supercage voids by addition or removal of extraframework aluminum species, influence solvation properties strongly but acid strength only weakly. These findings have clarified controversial interpretations that have persisted for decades regarding the origins of chemical reactivity and acid strength on faujasite zeolites. Born-Haber thermochemical relations, together with Marcus theory treatments of charge transfer reaction coordinates, provide a general framework to examine the effects of reactant and catalyst structure on ion-pair transition state enthalpy and entropy. The resulting structure-function relations lead to predictive insights that advance our understanding of confinement effects in zeolite acid catalysis beyond the largely phenomenological descriptions of shape selectivity and size exclusion. These findings also open new opportunities for the design and selection of microporous materials with active sites placed within desired void structures for reasons of catalytic rate or selectivity. The ability of zeolite voids to mimic biological catalysts in their selective stabilization of certain transition states by dispersion forces imparts catalytic diversity, all the more remarkable in light of the similar acid strengths among known aluminosilicates. This offers significant promise to expand the ranges of materials used and of reactions they catalyze.",ucb,,https://escholarship.org/uc/item/0d18z4sg,,,eng,REGULAR,0,0
148,1584,"Du Naturel, or Philippe de Champaigne Against Nature. Portraiture, artifice and the natural in seventeenth-century France","Douplitzky, Karine","Olson, Todd P.;",2020,"Philippe de Champaigne (1602-1674) is mostly remembered for his triple portrait of Richelieu and his hieratic series of Jansenist leaders' portraits but rarely considered for his rapport with nature despite his training as a Flemish landscape artist. By introducing the unexpected question of the natural in the context of his artistic practice, I reconsider Champaigne's rich corpus of portraits, which map his contemporary society and provide a new perspective on the evolving web of social identities.I explore how the concept of the natural, as opposed to artifice, is a shifting term that questions the ability of the painter to imitate nature, create a prototype, and give it ""life."" I successively qualify Champaigne's artistic praxis in relation to its contemporary reception within different communities â€“ the nobility, the Crown, the Jansenist community, and finally the Royal Academy of Painting and Sculpture. Champaigne's approach to portraiture raises the issues of exemplarity, resemblance, and presence of the model. These three problematics offer a chronological and thematic understanding of the painter as a multifaceted artist who leads portraiture into different paths â€“ decoration, diplomacy, and even the sacred.The position of the portraitist within the complex social and political agenda of the French Grand siÃ¨cle, provides a particularly interesting and underexamined insight into the intricate relations between power and religion under Louis XIII's reign and later, during the Regency's social unrest. By combining formal analysis with anthropologically rich archival evidence, I consider Champaigne's portraits as active agents in history, thus providing a conceptual framework to analyze the different actors' strategies of representation.",ucb,,https://escholarship.org/uc/item/0dk8j1th,,,eng,REGULAR,0,0
149,1585,Three Essays on Management and Organization,"Hong, Bryan","de Figueiredo, Rui;",2012,"This dissertation examines how managers influence firm behavior and performance. Managers play an important role in the performance and activities of firms, given their decision-making role within organizations. I conduct three separate empirical analyses examining specific factors that influence the impact that managers have on firm behavior and performance.The first chapter investigates the following question: How does the performance impact of supervisor changes differ across levels in a hierarchy? In my results, I find that supervisor changes at higher levels result in more severe performance declines relative to lower levels in the hierarchy, even when accounting for differences in span of control. The findings suggest that reassignment and turnover of managers at higher levels may be more costly for firms, independent of their ability and other individual characteristics.The second chapter examines the following: What is the effect of replacing experienced managers with rookie managers on firm performance? And, how does this change if they are instead replaced with experienced managers? At the individual store level, I observe the behavior and performance effects of management changes when successors are newly promoted store managers, and compare this to changes where successors are experienced store managers that are reassigned. In my results, newly promoted store managers systematically cut costs that briefly lead to profit increases, but ultimately result in profit declines in subsequent months. By contrast, successors that have prior experience as a manager do not make any changes observable in my data, and I find no evidence of performance changes. These findings suggest that inexperienced managers within firms may engage in well-intentioned behavior that may be costly for firms, at least in the short run. However, managerial experience may reduce the likelihood that the same costly behavior is repeated. The results shed additional insight into how managerial experience may matter for performance, and provide a tangible estimate of the performance costs of being a rookie manager.In the final essay, I investigate the influence of top managers on corporate social responsibility (CSR). A growing body of literature suggests that individual managers may play a critical role in determining corporate social responsibility (CSR) activities.  However, attempts to quantitatively measure the individual influence managers have on CSR face significant empirical challenges.  Estimation methods unable to adequately control for firm-specific factors influencing CSR are likely to overstate the importance of individual managers in their findings.  To address these concerns, I use an identification approach allowing for the simultaneous estimation of manager and firm fixed effects, and provide quantitative estimates of the degree to which individual managers might influence CSR.  The results suggest that managers do exert some degree of individual influence on CSR outside of firm-specific factors, but that the magnitude of their effect is relatively small.  Also, when managers switch firms, I find no evidence of a relationship between their influence on CSR in their first and second firm, suggesting that managers do not exert a persistent influence on CSR independent of the firm where they are employed.",ucb,,https://escholarship.org/uc/item/0dt4c8n2,,,eng,REGULAR,0,0
150,1586,Structure and Dynamics of Cu and Cu-Ag Nanocrystal Catalysts during Electrochemical CO2 Reduction,"Osowiecki, Wojciech Tomasz","Alivisatos, Paul;",2019,"In recent years, the CO2 reduction reaction (CO2RR) has been a popular topic in the field of electrocatalysis for its potential to help in mitigating climate change effects. As renewable energy sources such as solar and wind are rising in the market, there is a growing need for energy storage due to the intermittency of production. Renewable energy can be stored in batteries, but chemical bonds offer energy densities that are orders of magnitude higher. CO2 reduction is one of the electrocatalytic reactions that can store electrons in chemical bonds, simultaneously decreasing the amount of harmful greenhouse gas and creating useful fuels and chemical feedstocks. However, this vision is only possible if new catalysts for CO2 reduction are developed, because currently the efficiency and selectivity of the reaction are not high enough to allow for an industrially-viable technology.	Due to the fundamental restrictions between the bonding strengths of CO2RR intermediates, complex nano-engineered catalysts are particularly well suited for achieving substantially better catalytic selectivity as compared to state-of-the-art Cu bulk materials. Creating high-energy facets with low-coordination atoms as well as alloying Cu with other metals are among widely pursued strategies, and for such objectives, we find small (<10 nm) synthetically-tunable nanocrystals to be promising candidates. That said, industrially-viable catalysts must not only be efficient and selective but also stable. Ironically, the very properties that offer favorable catalytic performance also render the material prone to morphological restructuring, namely sintering, under the operating conditions. We believe that in order to prevent sintering in the future, it needs to be studied first, so we directly focus on this issue by systematically probing the reaction conditions during electrocatalysis. We hope that this topic will be further investigated as the complexity of the experimental parameters calls for efforts of the same magnitude as to what has been done to understand the CO2RR selectivity and reaction mechanism.  	Chapter 1 discusses the opportunities and challenges of electrocatalysis in the 21st century. We draw some analogies to other fields where an understanding of the theoretical limits as well as an ambitious pursuit of chemical reaction control were needed to create industrially-viable technologies. We then motivate the need of for a fundamental understanding of morphological changes occurring during electrocatalysis in light of these considerations.	Chapter 2 describes the synthesis, characterization, and thermodynamic understanding of different morphologies of Cu-Ag bimetallic nanocrystals. Cu and Ag do not alloy, so the bulk Cu-Ag materials used for catalysis possess monometallic domains of a specified size, but with nano-colloidal synthesis, it is possible to bring Cu and Ag into a more intimate contact which may give rise to a change in material properties. As such, we have synthesized a new structure, the nanocrescent, and its formation, based on thermodynamic principles, is explained.	Chapter 3 presents the catalytic performance of Cu-Ag bimetallic nanocrystals for CO2RR and compares it with that of physical mixtures of monometallic Cu and Ag particles. This chapter illustrates precisely why an understanding of sintering and its prevention are crucial, as the studied structures undergo a complete morphological restructuring and can no longer be confidently distinguished as distinct particles. Nevertheless, we observe a significant shift in catalytic selectivity as compared to pure Cu nanocrystals. Cu-Ag materials decrease the activity towards undesired H2 production and increase the efficiency of oxygenates formation.	Chapter 4 focuses directly on the issue of electrochemical sintering by studying the behavior of Cu nanocrystals under the standard conditions of CO2RR as well as a range of control experiments. We hypothesize some possible driving factors that could lead to the morphological restructuring and aim at distinguishing between them by changing variables such as the gas environment or pH. Ligand presence is probed with spectroscopic techniques, and morphological structures are imaged with electron microscopy. The presented set of evidence demonstrates that CO, a CO2RR intermediate, plays an important role in the sintering process by changing nanoparticle surface properties, leading to the formation of larger single-crystal facets.",ucb,,https://escholarship.org/uc/item/0f090711,,,eng,REGULAR,0,0
151,1587,"Hats off, Galileo: Early Richard Serra","Byrd, Anne Elizabeth","Wagner, Anne M;",2011,"This dissertation examines the first decade of Richard Serra's career, beginning with the European travels that followed his graduation with a Masters of Fine Arts from Yale and continuing through the mid-1970s. This period is especially interesting because it was during these years that Serra initiated the sculptural practice for which he is now best known, yet he was not so single-mindedly devoted to it as he would become - he was also very actively involved in the production of film, video, photo essays, conceptual proposals, and occasional ephemeral works. This dissertation studies these projects in conjunction with Serra's sculpture, arguing that they are in some respects parallel investigations, and arguing further that it therefore becomes necessary to find language that allows us to address the possibility that Serra's sculpture has some kind of content - whether psychological, political, or philosophical - despite the artist's assiduous avoidance of representation.I begin with a discussion of Serra's movement into ""process art."" Tracing a line through the visually very dissimilar sculptures that Serra made just prior to his process works, I argue that the tendency of Serra's earliest sculpture to privilege logical contradiction and perversity sets it apart from contemporary minimalist literalism, and opens it up to models of meaning found in the writings of the philosopher Alfred North Whitehead and the psychologically-minded art educator Anton Erhrenzweig, both of whom Serra was reading at the time. Then I turn to Serra's Props, lead sculptures propped up with no fixed joints that have often prompted viewers to focus on their threatening aspects. Tying these sculptures to works in other media that took the Vietnam War and Cold War technocratic theories as their materials, I argue that the Props did not simply (literally) enact violence but communicate about it. Finally I address the earliest of Serra's large-scale steel sculptures and landscape works, tying them to contemporaneous films, photo projects, and videos in order to argue that Serra's approach to sculpture here, while very much focused on embodiment, is more mediated by the mechanical image than has previously been acknowledged.",ucb,,https://escholarship.org/uc/item/0f0923k6,,,eng,REGULAR,0,0
152,1588,Progress in Xenon and Proton Relaxation Based Sensing,"Gomes, Muller De Matos","Pines, Alexander;",2017,"In this dissertation, the sensitivity of xenon relaxation to changes in its environment is used to both develop new types of biosensors and also to develop new techniques that make use of xenonâ€™s intrinsic interactions with its environment. A proton based relaxation experiment is also discussed due to its similarity to relaxation experiments done with xenon biosensors. Contrast agents are developed for xenon NMR. These agents consist of a cryptophane cage covalently attached to a DOTA chelating agent, allowing one to bring xenon close to chelated paramagnetic ions, enhancing the bulk relaxation of xenon. Both the T1 and T2 relaxivity of these contrast agents are tested. Adding paramagnetic metal ions seems to affect T1 more than T2 for most ions, possibly because the cage itself drastically affects the T2 of xenon because of the slow exchange rate and large chemical shift difference. In general, metal ions known to have long electronic relaxation times relax xenon more efficiently than ions with shorter electronic relaxation times. Gadolinium (III) and manganese (II) have the greatest effect on the T1 and T2 of xenon, with gadolinium (III) affecting T2 more and manganese (II) affecting T1 more. Adding gadolinium (III) increases the T1 relaxivity of M2 cages to 0.002 mM-1s-1 from 0.0009 mM-1s-1 and the T2 relaxivity to 92.5 mM-1s-1 from 26.1 mM-1s-1 . After testing the effect of these contrast agents, a relaxation based xenon biosensor is developed. This sensor consist of a cryptophane cage attached to a DOTA chelating agent and a biotin. The sensor works by binding to avidin, thereby increasing the rotational correlation time of the xenon inside the cage. This increases the relaxation rate of xenon inside the cage. Upon binding of a biotin-containing sensor to avidin at 1.5 ÂµM concentration, the free xenon T2 is reduced by a factor of 4. Changes in relaxation were more easily seen in T2 due to the strength of the field used in this experiment. At high magnetic fields, T1 hardly responds to changes in the rotational correlation time. A proton based relaxation agent, developed by the IBS institute from the Republic of Korea, is discussed in this dissertation. This group developed a sensor consisting of two parts: a super paramagnetic nanoparticle quencher and a paramagnetic metal ion enhancer. When the two are close together, the paramagnetic enhancer cannot efficiently relax water. Separating the two, done by either cleaving the bond keeping them together or by a conformational change in the linker binding them, prevents the super paramagnetic nanoparticle from quenching the enhancer, making water relaxation extremely rapid. Cleaving the bond between the quencher and enhancer increases the R1 of water by 1.5 s-1. This sensor was used to detect MMP2, an enzyme seen in certain tumors, both invitro and invivo. Concentrations as low as 15 ng per mL of MMP2 were detected invitro. This sensor is less sensitive invivo, with a lowest detected concentration of MMP2 being 450 ng per mL. After studying many varieties of sensors developed to functionalize xenon, the direct interactions between xenon and its target were studied. Xenon interacts with many substances, including proteins, leading to rapid relaxation of the entire xenon ensemble. This is due to both nonspecific interactions with the protein surface relaxing xenon and also because many proteins have hydrophobic pockets xenon can occupy. This leads to rapid xenon relaxation, which can be perturbed by the protein binding to another ligand. Adding a ligand to a solution of protein, such as a small molecule drug, alters the relaxation of xenon in that solution. This effect was exploited in order to develop a method for measuring the binding affinity of certain drugs for albumin by monitoring their effect on the relaxation of xenon. Of the drugs studied, warfarin, tenoxicam, and sodium salicylate had the strongest effects due to their high affinity for albumin, with warfarin lowering the T2 of xenon from 5 seconds to 2 seconds.",ucb,,https://escholarship.org/uc/item/0f13c7g6,,,eng,REGULAR,0,0
153,1589,"Structural, Biochemical Characterization, and Homology Based Modeling of Target Protein Interactions with Natural and Synthetic Indolecarbinol Compounds that Control Anti-Proliferative Signaling in Human Melanoma and Breast Cancer Cells","Quirit, Jeanne","Firestone, Gary L;",2016,"Cancer unforgivingly impinges upon millions of lives daily and is a predominant cause of death worldwide.  Conventional cancer treatment is comprised of surgery, radiation, chemotherapy, hormone, immune, and targeted therapy, however, with the deleterious side effects and eventual tumor resistance that invariably ensue, there is an urgency to develop safer, less invasive therapies.  Within the past decade, a significant transition in cancer therapeutics has unfurled as molecular targeted therapies have emanated as an alternative treatment for an array of cancers which include breast, colorectal, lung, pancreatic, lymphoma leukemia, and multiple myeloma.  Molecular targeted therapies suppress critical biochemical pathways or mutant proteins that are required for tumor cell growth and survival.  Within a molecularly defined cluster of patients, these drugs have the capacity to arrest tumor progression and can impel dramatic regressions.  Identifying novel classes of highly potent therapeutic agents that specifically act on molecular targets with diminished side effects after continued treatment has been a challenging obstacle to overcome in the treatment of cancer.  Among the array of molecular targeted agents employed, indole-3-carbinol (I3C), has emerged as a viable anti-cancer agent.  I3C is a bioactive component in cruciferous vegetables that is derived by hydrolysis from glycobrassicin in Brassica and exhibits multiple anticarcinogenic and antitumorigenic properties as well as chemo-preventative and strong anti-tumor properties in vivo in rodent model systems and in human cancer cell xenograft tumors.  A major headway in understanding the I3C anti-proliferative mechanism is our discovery that I3C triggers distinct and overlapping sets of anti-proliferative signaling events by direct interactions with specific target proteins.  Through a multi-faceted series of cellular and biochemical experiments, our lab has cemented three I3C target proteins which include human neutrophil elastase, E3 ubiquitin ligase NEDD4-1, and oncogenic B-RAF V600E serine/threonine kinase.  Because of I3Câ€™s off-target and nonspecific cytotoxic effects and its propensity to dimerize into its natural condensation product DIM, which triggers distinctly different antiproliferative cascades, there is a need to generate more potent, target-specific compounds this ultimately lead to a promising new compound, 1-benzyl-I3C, which is substantially more effective in suppressing enzymatic activity, inducing anti-proliferative effects in melanoma and breast cancer cells, and diminishing tumor size in mouse xenografts .  Here, we demonstrate that both I3C and 1-benzyl-I3C serve as molecular scaffold for creating a novel, robust enzymatic small molecule inhibitors aimed at disrupting specific target proteins in melanoma and breast cancer cells, particularly NEDD4-1 and elastase respectively.  By executing an in silico approach, we were able to make predictions about the mechanistic and structural nature of a set of five synthetic I3C and 1-benzyl-I3C derived analogs and employed a combination of in vitro protein thermostability assays and enzymatic assays to substantiate our predictions.  Notably, compounds 2242 and 2243, the two indolecarbinol analogues with added methyl groups that result in a more nucleophilic benzene ring Ï€ system, further inhibited NEDD4-1 enzymatic activity more dramatically with IC50s of 2.7 ÂµM and 7.6 ÂµM, respectively.  Interestingly, compounds 2242, 2160, and 2243 inhibited elastase activity much more significantly as well with and an IC50 of 30.4 Î¼M, 25.1 Î¼M, and 16.9 Î¼M respectively.  In both NEDD4-1 and elastase enzymatic studies, the potency of compound 2242, 2160, and 2243 appeared to be sensitive to structural changes on the phenyl ring, more notably when methyl substituents were added to the para and ortho positions.  The activity was abolished with the addition of bulky chemical groups like the thiophene substituent attached to the indole of 2163 and the electron donating methoxy group attached to the phenyl moiety of 2244 in the meta and ortho positions.    The quest for additional I3C target proteins has been facilitated through our understanding of homology modeling and identification of patterns in protein folds and ligand binding sites.  In order to make predictions about additional indolecarbinol target proteins, we obtained the crystal structures of the four I3C target proteins to date (human neutrophil elastase, ubiquitin E3 ligase NEDD4-1, oncogenic BRAF V600E serine/threonine kinase, WNT) and used them as the starting template structure for creating homologous protein models.  Homology models for various homologues of the target proteins were generated to ascertain whether or not similarities in potential indolecarbinol binding sites could be visibly discerned after performing a computational docking analysis of indolecarbinol compounds into their most feasible predicted binding sites.  After examining the indolecarbinol binding sites in the homology models, it is evident that the homologous proteins that have greater than 50% sequence identity share a distinct structural architecture that confers binding to the indolecarbinol compounds, but generally, homologous proteins that share 30% sequence identity or below tend to lose essential I3C contact residues.Examining the binding modes of I3C, 1-benzyl-I3C, and their corresponding synthetic derivatives in complex with their target proteins, namely, elastase, NEDD4-1, oncogenic BRAF V600E serine/threonine kinase, and wnt, can illuminate various patterns in binding sites and can hence provide valuable information for locating additional indolecarbinol target proteins.  Conceivably, information garnered from these studies will be useful in making predictions allowing identification of additional indolecarbinol compound target proteins.  The results presented here provide the fundamental framework for understanding the mode of inhibition of natural and synthetic indolecarbinol compounds and the proteins they effectively target.",ucb,,https://escholarship.org/uc/item/0f19g6f0,,,eng,REGULAR,0,0
154,1590,Prevention of Mother-to-Child HIV Transmission: Predictors of Utilization & Future Policy Implication,"Martz, Tyler Elizabeth","Bertozzi, Stefano M;",2015,"Despite the availability of highly efficacious antiretroviral drug regimens for the prevention of mother-to-child HIV transmission (PMTCT), transmission rates remain higher than those achieved in clinical trials. Access to these efficacious drug regimens continues to expand rapidly in countries most affected by HIV. Such expansion is an important first step in dramatically reducing mother-to-child HIV transmission rates. However, beyond access to drug regimens, programs must also identify and address individual and structural factors impeding the utilization of and adherence to PMTCT services by the women they are designed to serve. Additional research into factors both positively or negatively associated with PMTCT service utilization could help improve PMTCT programs to further reduce transmission rates. Each of the three papers included in this dissertation examined different factors of PMTCT service utilization. The first two papers analyze secondary data from a large-scale impact evaluation of Zimbabweâ€™s PMTCT program. Specifically, the first paper explores the association between costs (service costs, travel time, and transportation costs) and utilization of all recommended PMTCT services. The second paper explores the relationship between the timing of a pregnant womanâ€™s HIV-positive diagnosis, either prior to pregnancy or during antenatal care, and her utilization of PMTCT services. The hypotheses was that women who were diagnosed prior to pregnancy, having had more time to cope with their diagnosis, would complete more of the recommended PMTCT services. The final paper utilizes policy analysis methodology to examine two different implementation strategies Malawi could consider to improve lifelong adherence to antiretroviral therapy (ART) among women living with HIV who initiate treatment during pregnancy or breastfeeding. The two strategies proposed were: 1) for all pregnant/breastfeeding women to initiate and indefinitely receive treatment at maternal and child health clinics rather than be transferred to an ART specialty clinic, and 2) to expand access to lifelong ART, regardless of stage of disease, to any individual living with HIV in the pregnant/breastfeeding womanâ€™s household.",ucb,,https://escholarship.org/uc/item/0f83h1kd,,,eng,REGULAR,0,0
155,1591,Heat Pipe Performance Enhancement with Binary Mixture Fluids that Exhibit Strong Concentration Marangoni Effects,"Armijo, Kenneth Miguel","Carey, Van P;",2011,"This research investigates the impact of Marangoni phenomena, with low mixture concentrations of alcohol and water, to enhance thermal transport capability of gravity-assisted heat pipes. The use of binary mixture working fluids in gravity-assisted heat pipes are shown to improve the critical heat flux (CHF) and operating performance, more so than with pure fluids. The CHF is responsible for dryout when the pumping rate of a liquid flow structure is not sufficient to provide enough fluid to the evaporator section.     In the first study, heat pipe performance experiments were conducted for pure water and 2-propanol solutions with varying concentrations. Initial tests with pure water determined the optimal working fluid charge for the heat pipe; subsequent performance tests over a wide range of heat input levels were then conducted for each working fluid at this optimum value. The results indicated that some mixtures significantly enhance the heat transfer coefficient and heat flux capability of the heat pipe evaporator. For the best mixture tested, the maximum evaporator heat flux carried by the coolant without dryout was found to be 52% higher than the value for the same heat pipe using pure water as a coolant under comparable conditions. Peak evaporator heat flux values above 100 W/cm2 were achieved with some mixtures. Evaporator and condenser heat transfer coefficient data are presented and the trends are examined in the context of the expected effect of the Marangoni mechanisms on heat transfer.     Analytical modeling effort was also conducted investigating the impact of Marangoni phenomena for low concentrations of 2-propanol/water and methanol/water mixtures. In real systems the addition of small levels of surface-active contaminants can affect the surface tension of the liquid-vapor interface and thermodynamic conditions in this region. Analysis was performed for three widely accepted binary mixture correlations to predict heat flux and superheat values for subatmospheric experimental data using bulk fluid and film thermodynamic properties. Due to the non-ideal nature of these alcohol/water mixtures, this study employs an average pseudo single-component (PSC) coefficient in place of an ideal heat transfer coefficient (HTC) to improve the correlation predictions. This investigation evaluates the ability for these correlations to predict strong Marangoni effects of mixtures that have large surface tension variation with concentration under subatmospheric conditions. It is not always clear that evaluation of bulk fluid properties will satisfactorily account for Marangoni effects.  Analysis is also performed to assess correlation predictions for interfacial film properties rather than that of the bulk fluid. The results indicate that the use of film properties along with the PSC coefficient improves heat flux model predictions of  subatmospheric experimental data by as much as 59.3% for 0.015M 2-propanol and 49.1% for 0.04M methanol/water mixtures, where strong Marangoni effects are believed to be more evident.     A second experimental study was also performed of a 37Â° inclined, gravity-assisted, brass heat pipe with a 0.05M 2-Propanol/water binary mixture. The device design was developed from the first study by enlarging the evaporator and condenser surface areas. Strip heaters were also employed to provide larger input heat flux levels, for enhanced heat pipe performance testing. These experiments were carried out for varying liquid charge ratios between 30% and 70%, to determine an optimal value that would enhance heat transport performance by maximizing the critical heat flux (CHF) condition, while reducing the evaporator wall superheat. A 45% fill ratio was found to have the lowest overall superheat and highest thermal conductance by as much as 7.5W/K, as well as an enhanced CHF condition of 114.8W/cm2. A heat pipe analytical model, that characterizes binary mixture pool boiling is also presented, which was developed based on modeling efforts presented in studies 1 and 2. Model results with a 45% liquid charge ratio were found to provide good correspondence with the experimental data with an average rms evaporator vaporization heat flux deviation of 6.5%.     The final study of this investigation assesses the cooling of single and dual-junction solar cells with the inclined, gravity-assisted, brass heat pipe, with a 0.05M 2-propanol/water mixture. Thermal behavior of this heat pipe solar collector system was investigated theoretically and semi-empirically through experimentation of varying input heat loads from attached strip-heaters to simulate waste heat production of single-junction monocrystalline silicon (Si), and tandem multijunction GaInP/GaAs solar cells. It was also found that the 45% liquid charge was capable of achieving the lowest superheat levels and highest critical heat flux (CHF) condition of 114.8 W/cm2, at a predicted solar concentration of 162 suns. Solar cell semiconductor theory was employed to evaluate the effects of increasing temperature and solar concentration on solar cell performance. Results showed that a combined PV/heat pipe system had a 1.7% higher electrical efficiency, at a concentration ratio 132 suns higher than a stand-alone PV system. The dual-junction system also exhibited enhanced performance at elevated system temperatures with a 2.1% greater electrical efficiency, at an operational concentration level of 560 suns higher than a stand-alone PV system. Waste heat recovery analysis of the silicon solar cell, revealed respective thermal and system efficiencies as high as 56.3% and 66.3% as the incident solar radiation and corresponding condenser heat removal factor increased to 82 suns.",ucb,,https://escholarship.org/uc/item/0f86g1pf,,,eng,REGULAR,0,0
156,1592,Numerics and stability for orbifolds with applications to symplectic embeddings,"Wormleighton, Ben","Eisenbud, David;",2020,"This thesis studies the geometry of orbifolds - primarily via variation of GIT, derived category methods, and numerics - and develops connections of equivariant algebraic geometry with embedding problems in symplectic geometry, and with lattice point counting for rational polytopes. We also compile many aspects of the disparate toolkit required to rigorously study orbifolds.",ucb,,https://escholarship.org/uc/item/0fh5q00w,,,eng,REGULAR,0,0
157,1593,Lowness For Computational Speed,"Bayer, Robertson Edward","Slaman, Theodore;",2012,"From the original definition of a set whose jump is as simple as possible (A' =T 0'), to more recent definitions involving randomness, notions of lowness appear throughout recursion theory.  In that spirit, a non-recursive set A will be said to be low for speed if for any recursive set R and any computation of R from A, there is an oracle-free computation of R that is no more than polynomial-time slower than the A-computation.  We will construct such an r.e. set and discuss some properties of these sets.  We will show that promptly simple r.e. sets cannot be low for speed, and also that there are non-prompt sets that are not low for speed. We conclude by showing that generic sets are low for speed if and only if P = NP.",ucb,,https://escholarship.org/uc/item/0fj0k334,,,eng,REGULAR,0,0
158,1594,Constraining sources and sinks of atmospheric trace gases: Spectroscopy and kinetics of C1-C3 Criegee intermediates and the isotopic composition of lightning-produced N2O,"Smith, Mica","Boering, Kristie A.;",2016,"This dissertation presents a series of research projects designed and carried out to elucidate the physical chemistry and assess the atmospheric relevance of (1) carbonyl oxide radicals (i.e., Criegee intermediates) produced in alkene ozonolysis and (2) nitrous oxide (N2O) produced in lightning-induced corona discharges. The results provide UV absorption spectra and reaction rate coefficients for Criegee intermediates that will help constrain the formation and loss pathways of aerosol nucleation precursors such as H2SO4 and oxidized volatile organic compounds, and the isotopic signature of N2O formed in lightning that can help distinguish various N2O sources in atmospheric measurements.Criegee intermediates are byproducts of the reaction of alkenes with ozone. Bimolecular reactions of Criegee intermediates can lead to the production of low-volatility organic compounds and acids in the atmosphere, which in turn play a role in determining the concentration, size, and optical properties of aerosols. Recently, a novel method for producing measurable quantities of stabilized Criegee intermediates in the laboratory paved the way for the development of new experimental techniques to study their chemical properties and predict their importance in the atmosphere. For this dissertation, a unique apparatus combining time-resolved UV absorption in a flow cell with laser depletion in a molecular beam was adapted to obtain the absolute absorption spectrum of CH3CHOO with high resolution and accuracy relative to previous spectral measurements by other groups. The resulting absorption cross sections imply a photolysis lifetime of about seven seconds in the atmosphere, long enough for CH3CHOO to participate in unimolecular and bimolecular reactions. The broad absorption band with weak structure in the long-wavelength region of the spectrum represents a â€œspectral fingerprintâ€ for identifying CH3CHOO in future studies, and the cross sections provide valuable benchmarks for theory to characterize electronically excited states of CH3CHOO.The fast reaction of CH2OO with water dimer is thought to dominate CH2OO removal in the atmosphere. However, reaction rates can vary considerably under different conditions of temperature, humidity, and pressure. A temperature-controlled flow cell was designed to measure the transient absorption of CH2OO and obtain rate coefficients for its reaction with water dimer from 283 to 324 K. The rate of the reaction of CH2OO with water dimer was found to exhibit a strong negative temperature dependence, pointing to the participation of a hydrogen-bonded pre-reactive complex between CH2OO and two water molecules. Due to the strong temperature dependence, and shifting competition between water dimer and water monomer (which has a positive temperature dependence), the effective loss rate of CH2OO by reaction with water vapor is highly sensitive to atmospheric conditions. The role played by the stable pre-reactive complex suggests that similar complexes could form between water dimer and other larger Criegee intermediates, and that the stability and relative energy of these complexes control the reaction rate with water and its temperature dependence.Effective loss rates of Criegee intermediates due to bimolecular reactions in the atmosphere are limited by their rates of unimolecular decomposition. The rates of decomposition depend strongly on the molecular geometry, which affects the accessible isomerization pathways and dissociation products. (CH3)2COO is the main product of tetramethylethylene ozonolysis, and has been found to react slowly with water dimer and rapidly with SO2. While CH2OO decomposes slowly via isomerization to dioxirane, (CH3)2COO may decompose faster via intramolecular hydrogen transfer to form vinyl hydroperoxide. Fast (CH3)2COO decomposition could affect the significance of the Criegee intermediate H2SO4 source, as well as the non-photolytic production of OH radicals. In this dissertation, measurements of the transient absorption of (CH3)2COO to obtain thermal decomposition rate coefficients from 283 to 323 K by extrapolating the observed loss rate to zero concentration are reported. The rate of unimolecular decomposition is ~400 s-1 at 298 K and varies by nearly an order of magnitude within the studied temperature range. The effective loss rate of (CH3)2COO in the atmosphere due to thermal decomposition is thus competitive with its loss due to reaction with water vapor and with SO2, suggesting that the unimolecular decomposition pathway is a significant sink for (CH3)2COO and possibly other di-substituted Criegee intermediates, and should be included in models of Criegee chemistry in the atmosphere as well as in kinetic models of tetramethylethylene ozonolysis.N2O is the third most important greenhouse gas after CO2 and methane, and is mainly emitted to the atmosphere as a byproduct of microbial activity in soils. The expanding use of nitrogen-containing fertilizers in agriculture has led to an increase in N2O atmospheric concentrations since preindustrial times. Isotopic measurements are a valuable tool to distinguish the influence of different sources of N2O, but the isotopic composition of N2O formed from corona discharge in lightning has not previously been measured. Here, a corona discharge cell apparatus was used to generate a corona discharge in flowing or static zero air, and the N2O formed at discharge cell pressures from ~0.1 to 10 Torr and discharge voltages from 0.25 to 5 kV was collected and measured with isotope ratio mass spectrometry to determine its isotopic composition. The results show enrichments in 15N of N2O up to 32â€° relative to the reactant N2, and even larger enrichments in 15N of up to 77â€° at the central nitrogen atom. Large depletions in 18O as large as -71â€° relative to reactant O2 were also measured. The isotopic composition measured here may help to elucidate the chemical mechanisms leading to N2O formation and destruction in a corona discharge. Furthermore, the isotope-isotope relationships of the N2O produced in the corona discharge experiments are distinct from those of N2O from other sources, implying that isotopic measurements can be used to determine whether local variations in the atmospheric concentration of N2O â€“ e.g., the enhanced N2O levels recently measured in the upper tropical and subtropical troposphere â€“ are due to lightning activity, soil emissions, or biomass burning.",ucb,,https://escholarship.org/uc/item/0fj5x2w6,,,eng,REGULAR,0,0
159,1595,Influences in Voting and Growing Networks,"Racz, Miklos Zoltan","Mossel, Elchanan;",2015,"This thesis studies problems in applied probability using combinatorial techniques. The first part of the thesis focuses on voting, and studies the average-case behavior of voting systems with respect to manipulation of their outcome by voters. Many results in the field of voting are negative; in particular, Gibbard and Satterthwaite showed that no reasonable voting system can be strategyproof (a.k.a. nonmanipulable). We prove a quantitative version of this result, showing that the probability of manipulation is nonnegligible, unless the voting system is close to being a dictatorship. We also study manipulation by a coalition of voters, and show that the transition from being powerless to having absolute power is smooth. These results suggest that manipulation is easy on average for reasonable voting systems, and thus computational complexity cannot hide manipulations completely. The second part of the thesis focuses on statistical inference questions in growing random graph models. In particular, we study the influence of the seed in random trees grown according to preferential attachment and uniform attachment. While the seed has no effect from a weak local limit point of view in either model, different seeds lead to different distributions of limiting trees from a total variation point of view in both models. These results open up a host of new statistical inference questions regarding the temporal dynamics of growing networks.",ucb,,https://escholarship.org/uc/item/0fk1x7zx,,,eng,REGULAR,0,0
160,1596,"Global Mental Health Policy Diffusion, Institutionalization, and Innovation","Shen, Gordon Chit-Nga","Snowden, Lonnie R;",2013,"Mental health is an integral part of health and well-being. Mental health enables people to realize their potential, cope with the stressors of everyday life, and make contributions to society. Mental, neurological and substance use (MNS) disorders constitute 13% of the global burden of disease. And yet, across all countries, public investment in preventing and treating this cluster of disorders is disproportionately low relative to this disease burden. Health systems have not adequately or sufficiently responded to the burden of MNS disorders: the gap between the need and supply of treatment ranges from 76% to 85% in low- and middle-income countries, and from 35% to 50% in high-income countries. Mounting evidence underlines the inequitable distribution, poor quality, and inefficient use of scarce resources to address mental health needs. Globally, annual spending on mental health is less than US $2 per person in high-income countries and less than US $0.25 per person in low-income countries, with 67% of these financial resources allocated to stand-alone mental hospitals. Flagrant abuse of human rights and discrimination against people with mental disorders and psychosocial disabilities have been found in such psychiatric institutions. The redirecting of mental health budgets toward community-based services, including the integration of mental health into general health care settings, is needed. To address this state of affairs, this dissertation takes a fresh look at the actions taken to formulate a comprehensive, coordinated response from health and social sectors. It is founded at the nexus of new institutional, world culture, and diffusion of innovation theories.This dissertation employs a mixed methods approach, combining statistical and survey analyses. A mental health policy is an official statement of a government that defines its vision, values, principles, and objectives to improve the mental health of a population. It also outlines the areas of actions, strategies, timeframes, budgets, targets and indicators used to realize the vision and achieve the objectives of the policy. In the first study, I examine the coercive and emulative isomorphic effects on the diffusion of mental health policy across geopolitical borders. Using discrete-time data for 193 countries covering the period from 1950 to 2011, I conduct an event history analysis to examine the influence of WHO accession, foreign aid, and peer influence on mental health policy adoption. The results confirm that the act of adopting mental health policy is partly owed to membership in the World Health Organization, as well as influence of neighbors in the same World Bank and World Health Organization regions. National mental health policy adoption is trumpeted as a milestone for mental health reform. Is mental health policy limited to a rhetorical plane or taken up for pragmatic reasons? The effectiveness of this ""upstream"" factor could be realized based on examining ""downstream"" models of deinstitutionalized programming. While mental health policy adoption is treated as an outcome of interest in the first study, it is treated as a predictor in the second study. More specifically, I test the phase of policy adoption as a determinant of psychiatric bed rate changes using panel data for the same 193 countries between 2001 and 2011. The analysis finds that late-adopters of mental health policy are more likely to reduce psychiatric beds in mental hospitals and other biomedical settings than innovators, whereas they are less likely than non-adopters to reduce psychiatric beds in general hospitals. Deinstitutionalization is a much more complex and sophisticated process than reducing dehospitalization, or the reduction of psychiatric beds. It is also about improving the quality of care provided by inpatient facilities while increasing access to care through the development of mental health services in other medical and community settings. However, progress towards mental health reform is often stalled because it is an essentially contested issue in professional and advocacy circles and a highly politicized one among governments. For these reasons, the third study gathers contemporary perspectives on deinstitutionalization from 78 mental health experts. The survey administered assesses their knowledge, attitude, and practices of expanding community-based mental health services and/or downsizing institution-based care. The respondents also attested to the enabling, reinforcing, and constraining factors prevalent in the 42 countries they collectively represent. The qualitative evidence is complementary to the quantitative evidence in that it portrays the contemporary mental health system as being controlled by a nucleus of inpatient care. It further suggests that innovations are made in linking specialty services with primary and social services to support people with mental, neurological, and substance use disorders and their families as they (re)integrate into their communities. Mental health care has branched out in new directions at the turn of the 21st century. Time and again when governments are in the throes of strengthening their mental health systems, a closer look into the setup of infrastructure, essential medicines, human resources, and civil society involvement becomes necessary. This dissertation demonstrates that deinstitutionalization is a result of mental health policies imposed from the top down by the government. The experience with deinstitutionalizing mental health care also involves grassroots mobilization of social change by citizens, clients, families, and other advocates. In parallel with service reorganization, advances have been made in training lay personnel to offer services to people with MNS disorders. Research and development have made treatment more cost-effective and accessible. Cutting across temporal and geographic borders, tradition and modernity, this dissertation probes into the permeability of mental health policy and unpacks the complexity of deinstitutionalization.",ucb,,https://escholarship.org/uc/item/0fs1v90r,,,eng,REGULAR,0,0
161,1597,Design and Transient Analysis of Passive Safety Cooling Systems for Advanced Nuclear Reactors,"Galvez, Cristhian","Peterson, Per F;",2011,"The Pebble Bed Advanced High Temperature Reactor (PB-AHTR) is a pebble fueled, liquid salt cooled, high temperature nuclear reactor design that can be used for electricity generation or other applications requiring the availability of heat at elevated temperatures. A stage in the design evolution of this plant requires the analysis of the plant during a variety of potential transients to understand the primary and safety cooling system response. This study focuses on the performance of the passive safety cooling system with a dual purpose, to assess the capacity to maintain the core at safe temperatures and to assist the design process of this system to achieve this objective.  The analysis requires the use of complex computational tools for simulation and verification using analytical solutions and comparisons with experimental data. This investigation builds upon previous detailed design work for the PB-AHTR components, including the core, reactivity control mechanisms and the intermediate heat exchanger, developed in 2008. In addition the study of this reference plant design employs a wealth of auxiliary information including thermal-hydraulic physical phenomena correlations for multiple geometries and thermophysical properties for the constituents of the plant. Finally, the set of performance requirements and limitations imposed from physical constrains and safety considerations provide with a criteria and metrics for acceptability of the design. The passive safety cooling system concept is turned into a detailed design as a result from this study. A methodology for the design of air-cooled passive safety systems was developed and a transient analysis of the plant, evaluating a scrammed loss of forced cooling event was performed. Furthermore, a design optimization study of the passive safety system and an approach for the validation and verification of the analysis is presented. This study demonstrates that the resulting point design responds properly to the transient event and maintains the core and reactor components at acceptable temperatures within allowable safety margins. It is also demonstrated that the transition from steady full-power, forced-cooling mode to steady decay-heat, natural-circulation mode is stable, predictable and well characterized.",ucb,,https://escholarship.org/uc/item/0g2353c7,,,eng,REGULAR,0,0
162,1598,Efficient inference algorithms for near-deterministic systems,"Chatterjee, Shaunak","Russell, Stuart J;",2013,"This thesis addresses the problem of performing probabilistic inference in stochastic systems where the probability mass is far from uniformly distributed among all possible outcomes. Such near-deterministic systems arise in several real-world applications. For example, in human physiology, the widely varying evolution rates of physiological variables make certain trajectories much more likely than others; in natural language, a very small fraction of all possible word sequences accounts for a disproportionately high amount of probability under a language model. In such settings, it is often possible to obtain significant computational savings by focusing on the outcomes where the probability mass is concentrated. This contrasts with existing algorithms in probabilistic inference---such as junction tree, sum product, and belief propagation algorithms---which are well-tuned to exploit conditional independence relations.The first topic addressed in this thesis is thestructure of discrete-time temporal graphical models ofnear-deterministic stochastic processes.  We show how the structuredepends on the ratios between the size of the time step and theeffective rates of change of the variables. We also prove that accurateapproximations can often be obtained by sparse structures even for verylarge time steps. Besides providing an intuitive reason for causal sparsity in discrete temporal models, the sparsity also speeds up inference.The next contribution is an eigenvalue algorithm for a linear factored system (e.g., dynamic Bayesian network), where existing algorithms do not scale since the size of the system is exponential in the number of variables. Using a combination of graphical model inference algorithms and numerical methods for spectral analysis, we propose an approximate spectral algorithm which operates in the factored representation and is exponentially faster than previous algorithms.The third contribution is a temporally abstracted Viterbi (TAV) algorithm. Starting with a spatio-temporally abstracted coarse representation of the original problem, the TAV algorithm iteratively refines the search space for the Viterbi path via spatial and temporal refinements. The algorithm is guaranteed to converge to the optimal solution with the use of admissible heuristic costs in the abstract levels and is much faster than the Viterbi algorithm for near-deterministic systems.The fourth contribution is a hierarchical image/video segmentation algorithm, that shares some of the ideas used in the TAV algorithm. A supervoxel tree provides the abstraction hierarchy for this application. The algorithm starts working with the coarsest level supervoxels, and refines portions of the tree which are likely to have multiple labels. Several existing segmentation algorithms can be used to solve the energy minimization problem in each iteration, and admissible heuristic costs once again guarantee optimality. Since large contiguous patches exist in images and videos, this approach is more computationally efficient than solving the problem at the finest level of supervoxels.The final contribution is a family of Markov Chain Monte Carlo (MCMC) algorithms for near-deterministic systems when there exists an efficient algorithm to sample solutions for the corresponding deterministic problem. In such a case, a generic MCMC algorithm's performance worsens as the problem becomes more deterministic despite the existence of the efficient algorithm in the deterministic limit. MCMC algorithms designed using our methodology can bridge this gap.The computational speedups we obtain through the various new algorithms presented in this thesis show that it is indeed possible to exploit near-determinism in probabilistic systems. Near-determinism, much like conditional independence, is a potential (and promising) source of computational savings for both exact and approximate inference. It is a direction that warrants more understanding and better generalized algorithms.",ucb,,https://escholarship.org/uc/item/0g63029f,,,eng,REGULAR,0,0
163,1599,The Drama in Disguise: Dramatic Modes of Narration and Textual Structure in the Mid-Nineteenth-Century Russian Novel,"Wiggins, Kathleen Cameron","Paperno, Irina;",2011,"My dissertation investigates the generic interplay between the textual forms of drama and the novel during the 1850s, a fertile ""middle ground"" for the Russian novel, positioned between the works of Pushkin, Lermontov, and Gogol and the psychological realist novel of the 1860s and 70s. My study begins with Turgenev's Rudin (1856) and then considers Goncharov's Oblomov (1859) and Dostoevsky's Siberian novellas (1859), concluding with an examination of how the use of drama evolved in one of the ""great novels"" of the 1860s, Tolstoy's Voina i mir (War and Peace, 1865-69). Drawing upon both novel and drama theory, my dissertation seeks to identify the specific elements of the dramatic form employed by these nineteenth-century novelists, including dramatic dialogue and gesture, construction of enclosed stage-like spaces, patterns of movement and stasis, expository strategies, and character and plot construction. Each chapter examines a particular combination of these dramatic narrative strategies in order to pinpoint the distinct ways in which the form of the drama aided writers in their attempts to create a mature Russian novel. I also address the ways in which both characters and narrators discuss and make reference to drama and theatricality, revealing their ambivalence toward a genre and expressive mode in which they themselves participate. Finally, my dissertation traces a trajectory in the use of dramatic modes of narrative throughout the decade of the 1850s; while Turgenev, Goncharov, and Dostoevsky foreground their use of drama, Tolstoy strives to place his under disguise. As a whole, my dissertation seeks to add to our understanding of the enigmatic rise of the Russian novel in the nineteenth century by illuminating the importance of the dramatic form in this process.",ucb,,https://escholarship.org/uc/item/0g68t49h,,,eng,REGULAR,0,0
164,1600,New Approaches to the Asymmetric Traveling Salesman and Related Problems,"Ahmadipouranari, Nima","Rao, Satish B;",2015,"The Asymmetric Traveling Salesman Problem and its variants are optimization problems that are widely studied from the viewpoint of approximation algorithms as well as hardness of approximation. The natural LP relaxation for ATSP has been conjectured to have an $O(1)$ integrality gap. Recently the best known approximation factor for this problem was improved from the decades-old $O(\log(n))$ to $O(\log(n)/\log\log(n))$ using the connection between ATSP and Goddyn's Thin Tree conjecture.In this work we show that the integrality gap of the famous Held-Karp LP relaxation for ATSP is bounded by $\log\log(n)^{O(1)}$ which entails a polynomial time $\log\log(n)^{O(1)}$-estimation algorithm; that is we provide a polynomial time algorithm that finds the cost of the best possible solution within a $\log\log(n)^{O(1)}$ factor, but does not provide a solution with that cost. This is one of the very few instances of natural problems studied in approximation algorithms where the state of the art approximation and estimation algorithms do not match.We prove this by making progress on Goddyn's Thin Tree conjecture; we show that every $k$-edge-connected graph contains a $\log\log(n)^{O(1)}/k$-thin tree.To tackle the Thin Tree conjecture, we build upon the recent resolution of the Kadison-Singer problem by Marcus, Spielman, and Srivastava. We answer the following question by providing sufficient conditions: Given a set of rank 1 quadratic forms, can we select a subset of them from a given collection of subsets, whose total sum is bounded by a fraction of the sum of all rank 1 quadratic forms?Finally we address the problem of designing polynomial time approximation algorithms, algorithms that also output a solution, matching the guarantee of the estimation algorithm. We prove that this entirely relies on finding a polynomial time algorithm for our extension of the Kadison-Singer problem. Namely we prove that ATSP can be $\log(n)^\epsilon$-approximated in polynomial time for any $\epsilon>0$ and that it can be $\log\log(n)^{O(1)}$-approximated in quasi-polynomial time, assuming access to an oracle which solves our extension of Kadison-Singer.",ucb,,https://escholarship.org/uc/item/0gf14980,,,eng,REGULAR,0,0
165,1601,The Large-scale Structure of the Universe: Probes of Cosmology and Structure Formation,"Noh, Yookyung","White, Martin;Quataert, Eliot;",2013,"The usefulness of large-scale structure as a probe of cosmology and structure formation is increasing as large deep surveys in multi-wavelength bands are becoming possible. The observational analysis of large-scale structure guided by large volume numerical simulations are beginning to offer us complementary information and crosschecks of cosmological parameters estimated from the anisotropies in Cosmic Microwave Background (CMB) radiation. Understanding structure formation and evolution and even galaxy formation history is also being aided by observations of different redshift snapshots of the Universe, using various tracers of large-scale structure.This dissertation work covers aspects of large-scale structure from the baryon acoustic oscillation scale, to that of large scale filaments and galaxy clusters. First, I discuss a large- scale structure use for high precision cosmology. I investigate the reconstruction of Baryon Acoustic Oscillation (BAO) peak within the context of Lagrangian perturbation theory, testing its validity in a large suite of cosmological volume N-body simulations. Then I consider galaxy clusters and the large scale filaments surrounding them in a high resolution N-body simulation. I investigate the geometrical properties of galaxy cluster neighborhoods, focusing on the filaments connected to clusters. Using mock observations of galaxy clusters, I explore the the correlations of scatter in galaxy cluster mass estimates from multi-wavelength observations and different measurement techniques. I also examine the sources of the correlated scatter by considering the intrinsic and environmental properties of clusters.",ucb,,https://escholarship.org/uc/item/0gg775z6,,,eng,REGULAR,0,0
166,1602,Bees in urban landscapes: An investigation of habitat utilization,"Wojcik, Victoria A.","McBride, Joe R;",2009,"Bees are one of the key groups of anthophilies that make use of the floral resources present within urban landscapes. The ecological patterns of bees in cities are under further investigation in this dissertation work in an effort to build knowledge capacity that can be applied to management and conservation. 	Seasonal occurrence patterns are common among bees and their floral resources in wildland habitats. To investigate the nature of these phenological interactions in cities, bee visitation to a constructed floral resource base in Berkeley, California was monitored in the first year of garden development. The constructed habitat was used by nearly one-third of the locally known bee species. Bees visiting this urban resource displayed distinct patterns of seasonality paralleling those of wildland bees, with some species exhibiting extended seasons.	Differential bee visitation patterns are common between individual floral resources. The effective monitoring of bee populations requires an understanding of this variability. To investigate the patterns and trends in urban resource usage, the foraging of the community of bees visiting Tecoma stans resources in three tropical dry forest cities in Costa Rica was studied. Substantial variability was noted between individual T. stans resources in each of the three populations. The observed variability is driven by the quality of the food resource as measured by the number of individual flowers available. Additionally, the regional landscape plays a role in general species occurrence patterns at a resource. 	The urban landscape presents a heterogeneous mosaic patchwork of habitat resources. To investigate the influence of this local variability on resource usage, the foraging patterns of bees in tropical and temperate landscapes were examined. In the dry forest of Costa Rica, bee foraging on T. stans was studied in the cities of Bagaces, CaÃ±as, and Liberia. In the coastal grassland region of California, bee foraging on California poppy (Eschscholzia californica) was studied in the cities of Berkeley, Emeryville, and Oakland. In both regions, resource abundance and spatial distribution were the main drivers of bee visitation in all taxon groups. Land use and uniquely urban landscape variables influenced the occurrence of certain bee taxa.",ucb,,https://escholarship.org/uc/item/0gp9j2q7,,,eng,REGULAR,0,0
167,1603,"Diversity, Institutions and Economic Outcomes","Santacreu Vasut, Estefania","DeLong, J.Bradford;",2010,"Why may social diversity be bad for growth? In this thesis, I arguethat diversity affects the extent of information asymmetries thatdetermine the design of contracts and institutions. Becauseinformation asymmetries generate information rents, these contractsand institutions foster lower economic growth and persist over time.I proceed as follows: First, I model the impact of workforcediversity on the design of contracts and the shape of the firm. Ifind that diversity decreases the incentives given inprincipal-agent interactions and multiplies the number of layersbureaucracies need. Furthermore, the relation between diversity andproductivity is institution dependent. Second, I compare the spreadof industrialization in Japan and British India; and I provide newevidence of the organization, managerial beliefs, and workforcediversity of the three biggest textile centers in Bombay province. Ifind that workforce diversity was pervasive in British India, butnot in Japan, allowing the latter but not the former to introduceorganizational improvements and develop. In British India, centerswith higher workforce diversity had more supervisors per worker andtheir managers were the most likely to believe that their workerswere lazy.",ucb,,https://escholarship.org/uc/item/0gx5w2kw,,,eng,REGULAR,0,0
168,1604,Phonological Encoding in Aided Augmentative and Alternative Communication,"Dukhovny, Elena","Soto, Gloria;",2011,"Short-term memory for words is typically described in terms of phonological storage and rehearsal. However, research has shown that task demands, such as alternative means of output, may alter characteristics of short-term word storage. Alternative / Augmentative Communication (AAC) via high-technology Speech Generating Devices (SGDs), typically used by people with profound communication impairments, involves production of words via device-specific motor sequences. No study, however, has systematically considered potential effects of SGD-based production on short-term memory for words. In the current study, modality of short-term word storage was evaluated in a group of adult typical speakers trained to use SGDs, as well as a small group of authentic long-term users of SGDs. Results indicated that neurotypical subjects continued to store word lists phonologically when using SGDs, while authentic users of SGDs demonstrated phonological encoding more strongly during recall of high frequency `core' words than during recall of lower frequency `fringe' words. Thus, phonological encoding appears to remain a robust means of short-term word storage across output modalities.",ucb,,https://escholarship.org/uc/item/0h56354d,,,eng,REGULAR,0,0
169,1605,Cooperative Multiplexing in Wireless Relay Networks,"Nagpal, Vinayak","Nikolic, Borivoje;",2012,"Wireless networks are experiencing an explosive growth in the number of users and the demand for data capacity. One of the methods to improve capacity is to use tighter cooperation between terminals. In order to design a cooperative wireless link, several theoretical as well as practical challenges need to be addressed. In this dissertation we develop tools for the design of practical cooperative links that perform very close to fundamental limits.Using the tools of information theory, we begin by showing that cooperative relaying provides additional degrees-of-freedom for communication. For a simple network with a single-antenna source, single-antenna half-duplex relay and a two antenna destination, we show that cooperation allows the link throughput to increase approximately by a factor of 2.This gain is achievable using the recently introduced quantize-map-and-forward (QMF) cooperation scheme. However, QMF requires joint decoding of multiple information streams at the destination. The computational complexity of joint decoding is prohibitive for practical implementation. We address this problem by developing a low-complexity practical coding and system design framework for QMF relaying. The framework presents several pragmatic design choices to achieve cooperative degree-of-freedom gains in practice. The framework uses a combination of LDPC and LDGM codes decoded jointly over a low complexity factor graph. Signal processing requirements at all terminals are shown to have linear time complexity. Density evolution tools are developed for the design of specialized linear codes and mapping functions. Based on these tools, we demonstrate the design of cooperative links that perform within 0.5-1.0dB of information-theoretic limits.",ucb,,https://escholarship.org/uc/item/0h68q09r,,,eng,REGULAR,0,0
170,1606,Exposure to Ambient Air Pollution and Potential Biological Mechanisms/Biomarkers in Minority Children with Asthma Living in the United States,"LEE, EUNICE YUJUNG","Eisen, Ellen;",2017,"Rationale: Exposure to ambient air pollution is a major environmental risk factor for chronic diseases such as asthma. Children with asthma can be even more susceptible to the effects of air pollution since their respiratory system is not fully developed and some of the air pollutants can trigger asthma attacks. Over the past decades, scientists and researchers recognized the need to improve our understanding in the biological response mechanisms. The exact underlying mechanisms linking air pollution to disease outcomes, however, are not clear. Objectives: The overarching aims of this thesis are to investigate the association between exposure to ambient air pollutants and adverse health effects among minority children and identify potential biological pathways from exposure to health end points by considering genetic ancestry and, asthma endotype (atopy) as effect modifiers of the relation between air pollution and telomere length. In Chapter 1, we investigated the association between ambient air pollutants and asthma exacerbations in urban minority children, as well as effect modification by atopy status and African ancestry. In Chapter 2, we conducted a pilot study to gather preliminary information about how telomere length varies in relation to polycyclic aromatic hydrocarbons (PAH) exposure in children living in a highly polluted city. In Chapter 3, we examined the association between ambient air pollutants and telomere length in minority children to understand the potential damage caused by air pollution at the molecular level.    Methods: In Chapter 1, air pollutant exposures were estimated based on residence using U.S. EPA monitoring data and inverse distance weighting. The associations between average daily exposures and asthma exacerbations were estimated by the incident rate ratio (IRR) from a negative binomial regression model. In Chapter 2, we selected asthmatic and non-asthmatic subjects based on their annual average PAH level and described patterns of telomere length, measured by using uniplex polymerase chain reaction (PCR). In Chapter 3, the annual average daily exposure to each of four air pollutants was examined in relation to telomere length. Results: In chapter 1, exposure to ambient O3 and NO2 were associated with asthma exacerbations. Results for PM2.5 were null. Exposure-response relationships were linear for O3 and NO2 among non-atopic subjects and inconsistent among atopic subjects. Effect modification by African genetic ancestry was present only for O3; the impact of exposure appeared to be larger for those with higher African ancestry. In chapter 2, we found an inverse linear relationship between PAH and telomere length in a small pilot study. In chapter 3, the association between ambient SO2 and telomere length was significantly negative, whereas results for PM2.5, NO2 and O3 were null. Conclusions: Our results provide further evidence that exposure to ambient air pollution is a serious environmental risk factor that causes adverse health outcomes among minority children.",ucb,,https://escholarship.org/uc/item/0h8074z3,,,eng,REGULAR,0,0
171,1607,Vibration Harvesting using Electromagnetic Transduction,"Waterbury, Andrew","Wright, Paul K;",2011,"Embedded condition monitoring sensors that eliminate unanticipated failures of critical or high value equipment improve asset utilization while streamlining maintenance and support operations. General Electric and Rolls Royce use embedded sensors on their jet engines that have eliminated failures and allowed maintenance to be performed on an as-needed basis. As a result, airplane utilization increases while enabling the consolidation of maintenance operations. In more traditional industrial settings, condition monitoring of manufacturing and industrial equipment can quickly impact bottom lines through improved productivity and streamlined operations in ways comparable to what is already being realized in the aviation industry.Wireless sensor nodes provide embedded sensing with little overhead or infrastructure cost as long as appropriate power sources are available to sustain the node over its target lifetime. Energy is the limiting factor for sensor node lifetimes and data streams. Since most manufacturing and industrial equipment have some associated vibration spectrum when operating, transducing the mechanical energy of vibrations to a small amount of electrical energy electromagnetically was explored as a way of powering condition monitoring sensors.  Large pump motors and a machine tool were surveyed to characterize input vibration accelerations associated with manufacturing and industrial operations. Harvestable acceleration peaks occurred below 120 Hz and had magnitudes near or less than 0.1 g. Metal cutting vibrations were characterized and shown to have vibration frequencies proportional to the number of cutting teeth and the spindle RPM. It was also shown that, the 0.4-1.0 g acceleration impulses associated with the rapid axis motion of a machine tool are harvestable. Simple magnet and coil as well as coreless electromagnetic architectures were pursued using an overall device size constrain of a cube with 2.5 cm sides. That device size was roughly the same as a c-cell battery that is capable of powering a wireless sensor node for five to ten years. The target power for the harvester designs was the time-averaged powers of hundreds of microwatts to single milliwatts required by commercial wireless sensor nodes. Prototype vibration harvesters based on magnet-coil and voice-coil transducer designs were fabricated and evaluated. Both were able to produce about a milliwatt on a vibration platform for an input acceleration amplitude of 0.1 g at frequencies consistent with those characterized on the pump motors and machine tool. The power densities of the unoptomized proof of concept prototypes were comparable to commercial vibration harvester but at less than one seventh the size. The voice-coil prototype was installed on several 15-30 kW pump motors running support systems for a microfabrication lab, and unrectified powers of 0.2Â¬-1.5 mW were harvested. Similarly, 0.8-1.8 mW was harvested from metal cutting vibrations while facemilling cast iron and stainless steel, showing that powers comparable to commercial sense node requirements could be harvested from industrial settings.  Coreless motor architectures proved to be best suited for industrial settings because the unconstrained magnetic flux of simple magnet-coil designs interacted with the iron and steel mounting surfaces commonly found on large machines. Simulated coreless magnetic circuit designs showed that gap magnetic flux densities approaching one tesla could be possible but were not implemented.",ucb,,https://escholarship.org/uc/item/0hg645mt,,,eng,REGULAR,0,0
172,1608,Essays on Corruption and Political Favoritism,"Szucs, Ferenc","Finan, Frederico;",2018,"Corruption and political favoritism are considered major impediments to economic development. Although there is a growing consensus about the adverse efficiency consequences of corruption we still have a limited understanding of how corruption is shaped by political and economic institutions and how it affects our democracies. An increasing literature documents political favoritism and its welfare consequences relative to a no misallocation benchmark. In my dissertation, I complement this line of research by quantifying the effects of favoritism relative to relevant policy counterfactuals. My work highlights the importance of transparency and limiting regulatory discretion in improving the efficiency of public spending.In the first chapter, I investigate the determinants and consequences of increasing a buyer's discretion in public procurement. I study the role of discretion in the context of a Hungarian policy reform which removed the obligation of using an open auction for contracts under a certain anticipated value. Below this threshold, buyers can use an alternative  ""high-discretion"" procedure to purchase goods and services. At the threshold, I document large discontinuities in procurement outcomes, but I also find a discontinuity in the density of anticipated contract value, indicating that public agencies set contract values strategically to avoid auctions. To distinguish the causal effects of increased discretion from the self-selection of agencies into high-discretion procedures, I exploit the time variation of the policy reform. I find that discretion increases the price of contracts and decreases the productivity of contractors. To dig deeper into the motivations of public agencies, I use a structural model to identify discretion's impact on rents from corruption.  I also use the same structural approach to simulate the effect of alternative value thresholds. I find that the actual threshold redistributes about 2 percent of the total contract value from taxpayers to firms and decreases the average productivity of contractors by approximately 1.6 percent. My simulations suggest that the optimal threshold would be about a third of the actual. Moreover, case studies suggest that in addition to rent extraction corruption provides opportunities to buy political support in weakly institutionalized democracies (e.g. McMillan and Zoido (2004)). Consequently, detrimental effects of political favoritism may not be limited to misallocation of public resources but also constrain governmental accountability. In the second chapter, my coauthor Adam Szeidl and I confirm this conclusion by investigating political favoritism in the Hungarian media market. We scrutinize three different markets, printed media, billboards, and online newspapers. We establish three main results about favoritism in the Hungarian media. First, we document distortive two-way favors between politicians and the media, in the form of government advertising and media coverage. For both directions of favors, our empirical strategy is to compare the allocations of actors with changing versus unchanging connection status. More specifically we compare advertising behavior of state-owned and private companies and media content of outlets with more and less political connections. Since friendly news coverage systematically moves together with advertising favors we interpret our findings as media capture. Second, we document an organizational change in favoritism: a first phase when favored media was controlled by a single connected investor; a second phase when this relationship broke down and two-way favors were terminated; and a third phase when control of newly favored media was divided between multiple connected investors. Our preferred interpretation is that governments with more de-jure power shift the organization of favors towards a divide-and-rule style arrangement. Third, we develop and implement a portable structural approach to measure the economic cost of misallocative favoritism.",ucb,,https://escholarship.org/uc/item/0hg9234w,,,eng,REGULAR,0,0
173,1609,"Competition, Entrepreneurship, and Innovation","Wang, Xinxin","Malmendier, Ulrike;Morse, Adair;",2017,"What kind of market structure promotes innovation and growth? This dissertation delves into the relationship between market power, innovation, and returns. On one hand, competition exerts downward pressure on costs and provides incentives for efficient organization of production. On the other, the Schumpeterian hypothesis suggests that monopolist rents are necessary to encourage technological disruption. I study the effects of competition in two distinct settings: the acquisition of early-staged high-growth startups and horizontal mergers between public companies. In Chapter 1, Catering Innovation: Entrepreneurship and the Acquisition Market, I study the role of the financial market of acquisitions on an inventor's decision to begin a new venture and his or her subsequent innovation. After documenting its increasing importance as the dominant exit path for entrepreneurs, I test a novel catering theory of innovation: Does the market structure of potential acquirers have a measurable impact on inventors' start-up decisions? I construct a new dataset of early stage start-ups using the uniquely broad coverage of CrunchBase and VentureXpert data. I disambiguate and match the resulting data to employment data from LinkedIn and to the entire universe of patent data. Using the prior citation history of entrepreneurs for exogenous variation, I construct a formal proxy variable and employ the Heckman selection model to establish causality. I find that a one standard deviation increase in acquirer market concentration decreases the inventor's propensity to become an entrepreneur by 4%. This first result suggests that fragmented markets are appealing entry markets. My main finding is that a one standard deviation increase in acquirer concentration and market size increases the quality of patents, as measured by citations per patent, and the catering of entrepreneurs, as measured by technological overlap with potential acquirers. The magnitudes suggest that 5-16% of entrepreneurial innovation can be attributed to the influence of acquisition markets, particularly in the information technology and biotechnology industries. In Chapter 2, Market Power in Mergers and Acquisitions, I explore the value implications of market power changes in public mergers and acquisitions. Using a large sample of horizontal mergers from 1980 to 2012, I provide evidence that a significant portion of merger announcement returns are explained by changes in market power resulting from the merger. I find that a 0.1 expected increase in product market concentration leads to a 2.3% increase in cumulative abnormal returns over a three day period. In the long-run, mergers with high expected changes in market power revert to pre-announcement levels. My results suggest the ""announcement effect"" in M&A is due to misplaced market power expectations by investors.",ucb,,https://escholarship.org/uc/item/0hx4s77m,,,eng,REGULAR,0,0
174,1610,Studies in Optics and Optoelectronics,"Byrnes, Steven John Feinman","Wang, Feng;",2012,"This thesis will detail four projects aimed at understanding and applying the principles of optics and optoelectronics.In Chapter 1, we describe phase-sensitive sum-frequency vibrational spectroscopy (PS-SFVS), a nonlinear optical technique that can probe the molecular structure of the top few monolayers of a liquid-vapor interface. We use this technique to investigate the air-water interface, using a number of water samples with different dissolved salts. The information is used to draw inferences about the surface propensity of these salt ionsâ€”information that can shed light on both atmospheric chemistry and water solvation theory. We also give a detailed description of the experimental methodology for PS-SFVS, its rationale, and the issues that can arise.PS-SFVS measurements, such as those described in Chapter 1, can be fruitfully used by comparing them with the signal predicted by molecular simulation. However, the relationship between a molecular configuration and its nonlinear optical signal is not thoroughly understood in the theoretical chemistry community. In particular, the procedures used in the literature to predict an PS-SFVS signal within a molecular simulation have been ambiguous, depending on arbitrary parameters. In Chapter 2, we review PS-SFVS theory at a fundamental level, then map it to modern simulation methods, thereby explaining the ambiguities as consequences of improper truncation of a multipole expansion. A molecular-dynamics simulation of the water-air interface is used as an example, illustrating the consequences of different simulation methods and suggesting which ones should be most accurate.Chapter 3 explores a different aspect of nonlinear optics: The compression and characterization of ultrafast pulses of light. These pulses have been explored for a variety of scientific and technological applications. Ideally, an optical pulse can be reduced in duration up to the limit imposed by its spectral bandwidth via the uncertainty principle. However, the presence of ""nonlinear chirp"" (different frequencies arriving at different times in a nonlinear fashion), which is especially common in mode-locked fiber lasers,  can be a major factor preventing the shortening of a pulse. We describe a new technology, a type of patterned glass phase plate, that promises to reduce nonlinear chirp in a convenient, adjustable, inexpensive, and high-throughput manner. After showing simulations, we describe how we made the plate, and then how we used frequency-resolved optical gating (FROG) to watch the plate change the duration and shape of a pulse from a fiber laser.Finally, Chapter 4 discusses a new architecture for solar cells that uses the field effect, rather than the traditional p-n junction, to separate charge. This could be advantageous for semiconductor materials that are difficult to dope to both p- and n-type, such as oxides, sulfides, and nanoparticles. We discuss the underlying physics and rule-of-thumb design principles, along with both finite element simulations and experimental verifications.",ucb,,https://escholarship.org/uc/item/0k0918t7,,,eng,REGULAR,0,0
175,1611,Oracle-Guided Design and Analysis of Learning-Based Cyber-Physical Systems,"Ghosh, Shromona","Seshia, Sanjit A.;Sangiovanni-Vincentelli, Alberto L.;",2019,"We are in world where autonomous systems, such as self-driving cars, surgical robots, robotic manipulators are becoming a reality. Such systems are considered \textit{safety-critical} since they interact with humans on a regular basis. Hence, before such systems can be integrated into our day to day life, we need to guarantee their safety. Recent success in machine learning (ML) and artificial intelligence (AI) has led to an increase in their use in real world robotic systems. For example, complex perception modules in self-driving cars and deep reinforcement learning controllers in robotic manipulators. Although powerful, they introduce an additional level of complexity when it comes to the formal analysis of autonomous systems.  In this thesis, such systems are designated as Learning-Based Cyber-Physical Systems~(LB-CPS). In this thesis, we take inspiration from the Oracle-Guided Inductive Synthesis~(OGIS) paradigm to develop frameworks which can aid in achieving formal guarantees in different stages of an autonomous system design and analysis pipeline. Furthermore, we show that to guarantee the safety of LB-CPS, the design (synthesis) and analysis (verification) must consider feedback from the other.  We consider five important parts of the design and analysis process and show a strong coupling among them, namely (i) Robust Control Synthesis from High Level Safety Specifications; (ii) Diagnosis and Repair of Safety Requirements for Control Synthesis; (iii) Counter-example Guided Data Augmentation for training high-accuracy ML models; (iv) Simulation-Guided Falsification and Verification against Adversarial Environments; and (v) Bridging Model and Real-World Gap. Finally, we introduce a software toolkit \verifai{} for the design and analysis of AI based systems, which was developed to provide a common formal platform to implement design and analysis frameworks for LB-CPS.",ucb,,https://escholarship.org/uc/item/0tm3q8b5,,,eng,REGULAR,0,0
176,1612,Manipulation-resistant online learning,"Christiano, Paul Francis","Vazirani, Umesh;",2017,"Learning algorithms are now routinely applied to data aggregated from millions of untrusted users, including reviews and feedback that are used to define learning systemsâ€™ objectives. If some of these users behave manipulatively, traditional learning algorithms offer almost no performance guarantee to the â€œhonestâ€ users of the system. This dissertation begins to fill in this gap.Our starting point is the traditional online learning model. In this setting a learner makes a series of decisions, receives a loss after each decision, and aims to achieve a total loss which is nearly as low as if they had chosen the best fixed decision-making strategy in hindsight.We extend this model by introducing a set of users U. Each of the learnerâ€™s decisions is made on behalf of a particular user u âˆˆ U, and u reports the loss they incur from the decision. We assume that there is some (unknown ) set of â€œhonestâ€ users H âŠ‚ U, who report their losses honestly, while the other users may behave adversarially. Our goal is to ensure that the total loss incurred by users in H is nearly as small as if all users in H had used the single best fixed decision-making strategy in hindsight. We say that an algorithm is manipulation-resistant if it achieves a bound of this form.This dissertation proposes and analyzes manipulation-resistant algorithms for prediction with expert advice, contextual bandits, and collaborative filtering. These algorithms guarantee that the honest users perform nearly as well as if they had known each othersâ€™ identities in advance, pooled all of their data, and then used a traditional learning algorithm. This bounds the total amount of damage that can be done per manipulative user. More significantly, we give bounds that can be considerably smaller in the realistic setting where the users are vertices of a graph (such as a social graph) with disproportionately few edges between honest and manipulative users.As a key technical ingredient, we introduce the problem of online local learning, and propose a novel semidefinite programming algorithm for this problem. This algorithm allows us to effectively perform online learning over the exponentially large space of all possible sets H âŠ‚ U, and as a side-effect provides the first asymptotically optimal algorithm for online max cut.",ucb,,https://escholarship.org/uc/item/0w22c86t,,,eng,REGULAR,0,0
177,1613,SHAPE (Sound and Habitat Audio Prototyping Environment),"Cress, Jason","Campion, Edmund;",2020,"Sound and Habitat Audio Prototyping Environment (SHAPE) is a collection of nature-inspired electroacoustic devices created for sound art in public spaces. It is part of an audio feedback research project at the Center for New Music and Audio Technologies (CNMAT). By repurposing discarded electronics and manufactured objects, low-cost materials are used to make interactive sound sculptures and novel music instruments. Subtle gestures and actions by participants change the sound in real time. The project attempts to question the dichotomy between sound art and common environmental sounds through a zero-waste, collective action framework. With SHAPE, natural and artificial materials converge; construction and deconstruction hold equal weight; raw materials reclaim another existence; and sounds from unusual sources expand into fully resonating bodies. Audio transfer is based on two input types for each device: a piezoelectric contact mic and an electret air mic. These elements combine to sense both vibration in material and pressure waves in the air. Sound energy is then converted into an analog and a digital signal. Both analog and digital electronic environments are highly programmable, allowing for quick on-site prototyping. Six devices from this project will be highlighted and described in detail. Aside from the PCB fabrication, smartphone, and case construction, all of the e-components for these devices can be easily found in old discarded speaker systems and reused. Proprietary devices such as the iRig are currently being used, but these will be reverse engineered for future open-access integration. Open-source software such as Pure Data and MobMuPlat make any Android or iOS device compatible with this system, thus facilitating second-hand use of virtually all smartphone models. Considering the portability and cost effectiveness of this project, SHAPE is particularly adept at facilitating outdoor applications such as sound installations or musical performances.",ucb,,https://escholarship.org/uc/item/0wh27245,,,eng,REGULAR,0,0
178,1614,Studies on Effects of Arsenic on Human Beta-Defensin-1,"Dangleben, Nygerma Laurent","Smith, Martyn T;Skibola, Christine F;",2012,"Arsenic (As) is a well established cause of cancer in humans, and increasing evidence indicates that As has deleterious effects on the immune system that are not directly related to carcinogenesis. However, the mechanisms of As toxicity remain poorly understood. Our laboratory previously reported decreased urinary levels of human beta-defensin-1 (HBD1) peptides in As-exposed individuals from two cross-sectional studies based in Nevada and Chile, and confirmed in vitro that As exposure suppressed HBD1 mRNA expression which is encoded by the DEFB1 gene. DEFB1 is constitutively expressed in epithelial tissues, plays a role in both the innate and adaptive branches of the immune system, and is implicated in anti-tumor immunity. Therefore, the objectives of this dissertation are to review the immunotoxicological effects of As, characterize the effects of As on DEFB1 gene and protein expression in relevant in vitro model systems, investigate the molecular mechanisms mediating these effects, and explore the influence of other metals on HBD1 levels. A comprehensive review of the literature on the immune-related effects associated with As exposure in humans, animals and in vitro models reveals that chronic exposure to As can severely impair various aspects of immune function and consequently result in elevated risk of infections and chronic diseases. However, further investigation is needed to better understand the relationship between As exposure and the development of disease, and several recommendations are discussed to help bridge the gaps in knowledge.The current research investigated the effects of As exposure on DEFB1 in cells derived from target tissues of toxicity using immortalized non-tumorigenic human HOK-16B keratinocytes and HK-2 kidney epithelial cells. DEFB1 mRNA levels were more abundant in HK-2 cells than in HOK-16B cells, and were suppressed by exposure to arsenite (AsIII) or monomethylarsonous acid (MMAIII), the postulated more toxic metabolite. The suppressive effect of AsIII and MMAIII treatments on DEFB1 transcript levels continued for several passages after removal of As. HBD1 peptide levels were significantly reduced following exposure to AsIII, but were not affected by treatment with lead, cadmium or chromium, suggesting that decreased HBD1 may be a specific response to As. Finally, AsIII treatment was found to suppress DEFB1 promoter activity, indicating that the inhibition of DEFB1 mRNA by As is likely due to transcriptional down-regulation. Taken together, the research presented here provides evidence that our previous findings of decreased urinary HBD1 levels are likely due to a direct effect of As on the kidney, and suggest a novel mechanism by which As exposure may promote cancer development.This dissertation summarizes the known in vivo and in vitro effects of As on the immune system, characterizes the effects of As on DEFB1 using relevant cell culture models, and establishes DEFB1 as a potentially relevant biomarker of response to As. Future studies should address the role of DEFB1 inhibition in As immunotoxicity and carcinogenicity.",ucb,,https://escholarship.org/uc/item/0xm6v10q,,,eng,REGULAR,0,0
179,1615,Microseismic event location with multiple arrivals: application in the Newberry Enhanced Geothermal System and the Marcellus Shale,"Zhang, Zhishuai","Rector, James W.;",2017,"Multistage fracturing technique, together with horizontal drilling, make production from organic-rich shale possible. Microseismic monitoring of hydraulic fractures has been an important technology for far-field fracture diagnostics. It can provide us hydraulic fracture geometry and its growth behavior vs. time. Getting accurate microseismic event location is important to interpretation. Various methods originally developed for earthquake location have been used for microseismic event location.The main objective of this dissertation is to make use of multiple arrivals of microseismic data to improve microseismic event location accuracy. The improvement can be achieved from two aspect: (1) simultaneous inversion of multiple microseismic data for event locations and velocity model and (2) improving microseismic event location accuracy with head wave arrival time. We begin this dissertation by laying out the inverse problem theory as the basis of the simultaneous inversion. Then, we built a Bayesian framework to simultaneously invert for microseismic event locations and the velocity model. We developed a software package, BayesTomo, based on the simultaneous inversion framework. The first application is the simultaneous inversion of microseismic event locations and the velocity in a microseismic survey at Newberry Enhanced Geothermal System (EGS). We successfully applied the developed method on both synthetic examples and real data from the Newberry EGS. Comparisons with location results based on a traditional predetermined velocity model method demonstrated that we can construct a reliable effective velocity model using only microseismic data and determine microseismic event locations without prior knowledge of the velocity model.The second application is on the microseismic data acquired from a geophone array deployed in the horizontal section of a well drilled in the Marcellus Shale near Susquehanna County, Pennsylvania. We identified the existence of prominent head waves in some of the microseismic data. The head waves are refractions from the interface between the Marcellus and the underlying Onondaga Formation. The source locations of microseismic events can be significantly improved by using both the P-, S-wave direct arrival times and the head wave arrival times in place the traditional method of using direct arrival times and P-wave polarizations. The traditional method had substantially greater uncertainty in our data due to the large uncertainty in P-wave polarization direction estimation. Our method was applied to estimate the locations of perforation shots as well as microseismic events. Comparison with traditional location results shows improved location accuracy thanks to head wave arrival times.",ucb,,https://escholarship.org/uc/item/0xt5x2k6,,,eng,REGULAR,0,0
180,1616,"Crafting Words and Wood: Myth, Carving and HÃºsdrÃ¡pa","Schjeide, Erik","Lindow, John;",2015,"In the poem HÃºsdrÃ¡pa, ca. 985, Ãšlfr Uggason described woodcarvings of mythological scenes adorning an Icelandic hall owned by the chieftain Ã“lÃ¡fr pÃ¡i. The performance, of which some verses have been passed down in writing, was an act of referential intermedia, insofar as the art form of skaldic poetry presented with woven words the content of a wood-carved medium that has long since rotted away. Hence, the composition of the poem combined with the carvings created a link that opens up a union between extant literary sources and material culture which contributes to expanding cultural insight. This study draws from a range of sources in order to answer central research questions regarding the appearance and qualities of the missing woodcarvings. Intermedia becomes interdisciplinary in the quest, as archaeological finds of Viking Age and early medieval woodcarvings and iconography help fill the void of otherwise missing artifacts. Old Norse literature provides clues to the mythic cultural values imbued in the wooden iconography. Anthropological and other theories drawn from the liberal arts also apply as legend, myth and art combine to inform cultural meaning. The study reveals that the appearance and function of the woodcarvings merge as they were understood not only aesthetically but also to possess a certain agency. The dissertation is in two parts. The first portion provides background information regarding woodcarving and iconography found in Northern Europe as it refers to poetry, sagas and legends for contextualization. The second portion continues the investigation with an emphasis on Iceland and a close reading of the poem HÃºsdrÃ¡pa. In these sections a synthesis of saga scenes, skaldic poetry, myth and applicable iconography informs analysis and hypothetical prototypes of the carvings.In addition to figures such as sketches and photos included in the appendix, there are eight animations available for download labeled Animation 1 - 8. Each of these animations help illustrate the appearance and qualities of both extant and hypothetical wood-carved mythic scenes from the Viking Age.",ucb,,https://escholarship.org/uc/item/1017z7d7,,,eng,REGULAR,0,0
181,1617,Cryo-EM Studies of Inflammasomes,"Haloupek, Nicole","Nogales, Eva;",2017,"Following invasion by disease-causing biological entities, before a threat-specific response is mounted by the adaptive immune system, the innate immune system initiates a campaign to restrict the pathogen. In animals and plants, members of the Nucleotide-binding domain, Leucine-rich repeat-containing (NLR) superfamily of proteins are sentinels of the innate immune system that detect a wide array of pathogensâ€™ molecular signatures. NAIP5 (NLR family, apoptosis inhibitory protein 5), for example, is activated by binding the bacterial protein flagellinâ€”a component of the flagellum many bacteria use for locomotion. After binding, the NAIP5â€“flagellin complex associates with multiple NLRC4 (NLR family, CARD [Caspase Activation and Recruitment Domain]-containing 4) protomers, forming an inflammasome that activates the protease Caspase-1 by juxtaposing the proteaseâ€™s CARDs. The active Caspase-1 sparks a cascade that results in pyroptosis, a programmed form of cell death that summons an immune response and causes inflammation. Another such protein, NLRP1 (NLR family, pyrin domain containing 1), also forms inflammasomes that, when activated by cleavage by anthrax lethal factor, activate Caspase-1.I used cryo-electron microscopy (cryo-EM) to determine the structure of the NAIP5â€“NLRC4 inflammasome. Analysis of the structure of the NAIP5-NLRC4 inflammasome revealed how the bacterial ligand flagellin is detected and how the complex assembles and uncovered a possible mechanism by which NLRs restrict pathogen evasion of detection by the innate immune system. Structural investigation of the NLRP1 inflammasome is at an earlyâ€”but promisingâ€”stage. In this thesis, I also describe the new methods for preparing cryo-EM samples that made this work possible and may be useful for cryo-EM studies of other macromolecular complexes.",ucb,,https://escholarship.org/uc/item/1087x3p1,,,eng,REGULAR,0,0
182,1618,Examining Factors Associated with Learning and Performance in Primary Care Graduate Medical Education Organizations,"Kim, Jung Gook","Rodriguez, Hector P;",2019,"Despite calls to improve Graduate Medical Education (GME), little is known about the organizational factors influencing training design, resident learning, and assessment. This dissertation examines the organizational behavior factors in primary care GME associated with time spent training in ambulatory care, resident clinical competency learning rates, and quality of care. Linked databases from medical education accreditors and policymakers, population health sources, federal cost reports, and an integrated health system were analyzed to investigate the extent to which primary care GMEâ€™s competing internal and external organizational factors influence the professional training environment and performance of primary care residents. Key findings include: 1. Experience in ambulatory care for residents varies among their ACGME-accredited programs, with more time in ambulatory care settings most strongly associated with additional faculty, receipt of federal Teaching Health Center GME funding, and accreditation warnings; 2. Improved resident learning rates in the Accreditation Council for GME (ACGME) Milestones for family medicine and internal medicine programs were more associated with external factors than internal factors. Patient care, practice-based learning and improvement, and systems-based practice learning rates were dependent on the programâ€™s geographic setting, organizational structural characteristics, and the type of resident learning experiences; and 3. Healthcare Effectiveness Data and Information Set (HEDIS) measure reliability in ambulatory care for residents varies among ACGME-accredited primary care residency programs with potential opportunities to utilize publicly reported quality data for GME programs. Overall, these empirical studies help clarify the organizational and associated environmental factors influencing training design, resident learning, and performance in order to assist policymakers in understanding the fragmented GME learning environment and move GME toward improved accountability. As trainee experiences may have a downstream impact on patient care, the systematic study on primary care GME organizations helps improve the design of the resident learning environment and training the future primary care workforce, especially in ambulatory care, the most common delivery setting for primary care health services today.",ucb,,https://escholarship.org/uc/item/10h629k6,,,eng,REGULAR,0,0
183,1619,Phonetic Attention and Predictability: How Context Shapes Exemplars and Guides Sound Change,"Manker, Jonathan Taylor","Johnson, Keith;",2017,"In this dissertation, I investigate how word predictability in context modulates the listenerâ€™s attention to phonetic details, and how this in turn affects sound change.  Three sets of experiments are designed to investigate these questions:  In the first set of experiments, involving discriminability tasks, I demonstrate that (1) contextual predictability affects speech perception, and that listeners attend more to the phonetic details of unpredictable speech.  In the second set of experiments I use the phonetic accommodation paradigm to show that (2) the effect of contextual predictability on speech perception in turn affects speech production.   This by itself suggests relevance in sound change.  In the third set of experiments I apply the model to a specific example of sound change: the reduction of function words.  Using an error detection task I show that (3) listeners attend to the details of content words more than function words (with all other variables controlled for) which is linked to their differences in contextual predictability.  I then propose a two-step model of sound change involving the propagation of contextually-modulated variation with a perceptual (rather than production) bias followed by the acquisition of new variants.	The results build and expand on several strands of literature which have not been fully connected previously.   The findings for the effect of predictability on speech perception corroborate a number of past experiments showing that higher level linguistic information can have the effect of aiding speech recognition (Miller, Heise & Lichten 1951, Pollack & Pickett 1963), perceptually restoring missing information (Warren 1970, Marslen-Wilson, & Welsh 1978, Samuel 1981), or generally diverting attention from the raw auditory signal (Cole, Jakimik, & Cooper 1978, Ganong 1980).  Additionally, this research considers dual-processing models of speech perception (Norris & Cutler 1979, Lindblom et al. 1995, Hickok and Poeppel 2004, 2007) in a broader context, considering how word predictability and expectancy modulate the type of listening used.   The findings also add to the literature on exemplar theory (Johnson 1997, Pierrehumbert 2002, Goldinger 2007), particularly to hybrid models including both abstractions and exemplar clouds within the lexicon.  Finally, I propose a new model of perception-based sound change driven by contextual predictability that can account for cross-linguistically common patterns of function word and morpheme reduction (Bell et al. 2001, Jurafsky et al. 2001, Beckman 1998) that does not rely on teleological production-based accounts of reduction (Lindblom 1990, Alyett & Turk 2004).",ucb,,https://escholarship.org/uc/item/10r90282,,,eng,REGULAR,0,0
184,1620,Earthquake Resilient Tall Reinforced Concrete Buildings at Near-Fault Sites Using Base Isolation and Rocking Core Walls,"Calugaru, Vladimir","Panagiotou, Marios A;",2013,"This dissertation pursues three main objectives: (1) to investigate the seismic response of tall reinforced concrete core wall buildings, designed following current building codes, subjected to pulse type near-fault ground motion, with special focus on the relation between the characteristics of the ground motion and the higher-modes of response; (2) to determine the characteristics of a base isolation system that results in nominally elastic response of the superstructure of a tall reinforced concrete core wall building at the maximum considered earthquake level of shaking; and (3) to demonstrate that the seismic performance, cost, and constructability of a base-isolated tall reinforced concrete core wall building can be significantly improved by incorporating a rocking core-wall in the design.First, this dissertation investigates the seismic response of tall cantilever wall buildings subjected to pulse type ground motion, with special focus on the relation between the characteristics of ground motion and the higher-modes of response. Buildings 10, 20, and 40 stories high were designed such that inelastic deformation was concentrated at a single flexural plastic hinge at their base. Using nonlinear response history analysis, the buildings were subjected to near-fault seismic ground motions as well as simple close-form pulses, which represented distinct pulses within the ground motions. Euler-Bernoulli beam models with lumped mass and lumped plasticity were used to model the buildings. The response of the buildings to the close-form pulses fairly matched that of the near-fault records. Subsequently, a parametric study was conducted for the buildings subjected to three types of close-form pulses with a broad range of periods and amplitudes. The results of the parametric study demonstrate the importance of the ratio of the fundamental period of the structure to the period of the pulse to the excitation of higher modes. The study shows that if the modal response spectrum analysis approach is used--considering the first four modes with a uniform yield reduction factor for all modes and with the square root of sum of squares modal combination rule--it significantly underestimates bending moment and shear force responses. A response spectrum analysis method that uses different yield reduction factors for the first and the higher modes is presented. Next, this dissertation investigates numerically the seismic response of six seismically base-isolated (BI) 20-story reinforced concrete buildings and compares their response to that of a fixed-base (FB) building with a similar structural system above ground. Located in Berkeley, California, 2 km from the Hayward fault, the buildings are designed with a core wall that provides most of the lateral force resistance above ground. For the BI buildings, the following are investigated: two isolation systems (both implemented below a three-story basement), isolation periods equal to 4, 5, and 6 s, and two levels of flexural strength of the wall. The first isolation system combines tension-resistant friction pendulum bearings and nonlinear fluid viscous dampers (NFVDs); the second combines low-friction tension-resistant cross-linear bearings, lead-rubber bearings, and NFVDs. The designs of all buildings satisfy ASCE 7-10 requirements, except that one component of horizontal excitation is used in the two-dimensional nonlinear response history analysis. Analysis is performed for a set of ground motions scaled to the design earthquake (DE) and to the maximum considered earthquake (MCE). At both the DE and the MCE, the FB building develops large inelastic deformations and shear forces in the wall and large floor accelerations. At the MCE, four of the BI buildings experience nominally elastic response of the wall, with floor accelerations and shear forces being 0.25 to 0.55 times those experienced by the FB building. The response of the FB and four of the BI buildings to four unscaled historical pulse-like near-fault ground motions is also studied. Finally, this dissertation investigates the seismic response of four 20-story buildings hypothetically located in the San Francisco Bay Area, 0.5 km from the San Andreas fault. One of the four studied buildings is fixed-base (FB), two are base-isolated (BI), and one uses a combination of base isolation and a rocking core wall (BIRW). Above the ground level, a reinforced concrete core wall provides the majority of the lateral force resistance in all four buildings. The FB and BI buildings satisfy requirements of ASCE 7-10. The BI and BIRW buildings use the same isolation system, which combines tension-resistant friction pendulum bearings and nonlinear fluid viscous dampers. The rocking core-wall includes post-tensioning steel, buckling-restrained devices, and at its base is encased in a steel shell to maximize confinement of the concrete core. The total amount of longitudinal steel in the wall of the BIRW building is 0.71 to 0.87 times that used in the BI buildings. Response history two-dimensional analysis is performed, including the vertical components of excitation, for a set of ground motions scaled to the design earthquake and to the maximum considered earthquake (MCE). While the FB building at MCE level of shaking develops inelastic deformations and shear stresses in the wall that may correspond to irreparable  damage, the BI and the BIRW buildings experience nominally elastic response of the wall, with floor accelerations and shear forces which are 0.36 to 0.55 times those experienced by the FB building. The response of the four buildings to two historical and two simulated near-fault ground motions is also studied, demonstrating that the BIRW building has the largest deformation capacity at the onset of structural damage.",ucb,,https://escholarship.org/uc/item/1256t25p,,,eng,REGULAR,0,0
185,1621,Heaven is Empty: A Cross-Cultural Approach to Religion and Human Agency in Early Imperial China,"Marsili, Filippo","Nylan, Michael;",2011,"This dissertation is about the religious (extra-human) legitimation of political power during the Western Han dynasty (206 BCE- 9CE). It reexamines the correlation between religious, cultural, and political unity, closely analyzing Sima Qian's (ca. 145-86 BCE) Records of the Grand Historian (Shiji), the first universal narrative of Chinese civilization from its origins through the first century of the Western Han empire. This text became the model for all dynastic histories until 1911, when the imperial age came to an abrupt end. The contrast between Sima Qian's treatment of religious practices, official and unofficial, and accounts in the classical Greco-Roman historiography about imperial cults and propaganda provides an intriguing point of departure from which my thesis questions the applicability of paradigms imported and applied to the case of early China from the ancient Mediterranean world (e.g., â€œreligion,â€ â€œmetaphysics,â€ â€œdivinity,â€ â€œsacred vs. secular,â€ â€œscripture,â€ â€œmyth,â€ and â€œritualâ€). This dissertation contributes to our understanding of the relationship between â€œreligion,â€ â€œmorality,â€ and â€œcultural identityâ€ in China by calling into question those very categories.  By adopting a comparative approach, it shows how the discourse on the sacred by historians and philosophers has been often informed by intellectual prejudices and pre-formed conceptions that have hindered the mutual understanding between East and West.  To overcome these obstacles, this dissertation proposes a new trans-cultural attitude aimed both at the deconstruction of these ethnocentric biases and at the reconstruction of Sima Qian's own analytical criteria and concerns.",ucb,,https://escholarship.org/uc/item/12f937fh,,,eng,REGULAR,0,0
186,1622,"Iron, Oil, and Emeryville: Resource Industrialization and Metropolitan Expansion in the San Francisco Bay Area, 1850-1900","Lunine, Seth","Walker, Richard;",2013,"Scholars have largely overlooked the formative role of industry in both California's economic development and the San Francisco Bay Area's metropolitan expansion during the late nineteenth century.  Beginning in the early 1880s, leading firms in San Francisco's specialized industries, such as the iron and chemicals sectors, dispersed to the metropolitan periphery.  This process of industrial suburbanization created an integrative metropolitan economy, as well as individual suburbs. In this dissertation, I explore the creation of one of the Bay Area's earliest industrial suburbs, Emeryville.  I argue that an analysis of industrial dynamism on the regional scale is integral for understanding metropolitan development and industrial suburbanization.  Symbiotic relations between resource extraction and industrial dynamism structured California's distinct mode of capitalist development.  The expansion and diversification of resource extraction and processing industries fueled metropolitan growth.  Within the broader context of regional capitalism, I examine the process of industrial suburbanization and the formation of Emeryville.  I show how two processes greatly influenced industrial dispersal and factory relocation:  the creation of an industrial property market and the endogenous logic of industrial production.  A coalition of land developers, politicos, and transportation entrepreneurs created a new suburban industrial space.  High rates of innovation and accumulation, as well as fierce competition, enabled certain firms to eschew the industrial core and locate their factories in early Emeryville.  I draw on array of archival material and primary sources to weave together this distinctive story of California landscapes and industries, and cities and suburbs.  My examination of the formation of Emeryville also presents a case study of how industry engenders metropolitan transformation.  This dissertation provides insights into the necessarily conjoined processes of city development and industrial suburbanization.",ucb,,https://escholarship.org/uc/item/13q7r2gq,,,eng,REGULAR,0,0
187,1623,"States of Disunion: American Marriage and Divorce, 1867â€“1906","Roehrkasse, Alexander Fort","Fligstein, Neil;",2019,"This dissertation comprises three essays on the historical relationship between capitalist development, state formation and marriage and divorce patterns in the United States.The first examines the effects of liberalizing womenâ€™s property rights on divorce. In the late nineteenth century, most American states gave married women new rights to own and control assets and earnings. Using administrative data on most U.S. divorces between 1867 and 1906, I show that rights transfers gave women financial independence from husbands that enabled them to exit undesirable unions at greater rates. However, husbands also filed for more divorces following womenâ€™s economic gains, suggesting that the violation of traditional gender norms of household governance also destabilized unions.The second essay explores the legal behavior of men and women who faced significant restrictions on divorce. Before the 1970s, U.S. states allowed legal divorces only for specified causes. Examining data on the causes cited by divorce seekers, I document the routinization of divorce procedure over historical time. I also exploit legal changes to demonstrate that individuals adapted to divorce regulations by changing the causes they cited. Strategic legal behavior was widespread but differed by gender, with men being more prone toward routinization and women being more likely to adapt to new rules. The third essay reflects critically on the quality of available data on nineteenth-century marriages. I compare vital records of marriages to census microdata on marital duration, showing that the latter exhibit significant measurement error. Despite growing interest by elite state actors in measuring marriage and divorce at the population level, vital recording, which had administrative origins in the clarification of individual legal statuses, seems to have elicited more reliable participation in official knowledge projects. I analyze the extent and distribution of mismeasurement, which has consequences for both the study of American political development and the validity of quantitative historical research on the family.",ucb,,https://escholarship.org/uc/item/1412c4hj,,,eng,REGULAR,0,0
188,1624,The Effect of Unreliable Commuting Time on Commuter Preferences,"Koskenoja, Pia Maria K.",,1996,"Unreliable travel time is defined to mean a distribution of possible commute durations. This dissertation identifies occupational groups and shows how an individual's occupation can be expected to indicate how that person is going to behave in risky commuting stations. Individual occupations attract a certain personality type. Also, individual occupations require different amounts of team work and pose idiosyncratic supervisory requirements for the employer. These effects create systematic variations among employer imposed work rules concerning employee's time use and employee expectations and reactions to the rules. The outcome is both personality driven and situation specific response to risky commuting situations. A psychological construct -- locus of control -- draws a boundary between what an individual believes is influenced by her own actions and what is caused by factors external to her. A person with an internal locus of control is optimistic about her possibilities to influence the outcomes of risky situations, while a person with an external locus of control tends to see the cause of events as random or influenced by some powerful others. Commuters with an external locus of control take fewer planned risks, reserving more slack time between planned arrival and official work start time. If something unanticipated throws them off the habitual path, they are less likely to go out of their way to maintain the planned arrival time. The commuters with more internal locus of control are more willing to take planned risks and are more committed to see that the risk pays off. I use occupational classification developed by John Holland and resource exchange theory of Uriel Foa to establish a partial order from most external to most internal occupational groups. The dissertation also includes models where the commuter trades off different elements of unreliable travel time: expected mean travel time, expected schedule delay early, and expected schedule delay late. Occupations affect these tradeoffs even when income and family composition are controlled.",ucb,,https://escholarship.org/uc/item/0rj9z9cv,,,eng,REGULAR,0,0
189,1625,"Deployment, Design, and Commercialization of Carbon-Â­Negative Energy Systems","Sanchez, Daniel Lucio","Kammen, Daniel M;Callaway, Duncan S;",2015,"Climate change mitigation requires gigaton-scale carbon dioxide removal technologies, yet few examples exist beyond niche markets. This dissertation informs large-scale implementation of bioenergy with carbon capture and sequestration (BECCS), a carbon-negative energy technology. It builds on existing literature with a novel focus on deployment, design, commercialization, and communication of BECCS. BECCS, combined with aggressive renewable deployment and fossil emission reductions, can enable a carbon-negative power system in Western North America by 2050, with up to 145% emissions reduction from 1990 levels. BECCS complements other sources of renewable energy, and can be deployed in a manner consistent with regional policies and design considerations. The amount of biomass resource available limits the level of fossil CO2 emissions that can still satisfy carbon emissions caps. Offsets produced by BECCS are more valuable to the power system than the electricity it provides. Implied costs of carbon for BECCS are relatively low (~$75/ton CO2 at scale) for a capital-intensive technology.Optimal scales for BECCS are an order of magnitude larger than proposed scales found in existing literature. Deviations from optimal scaled size have little effect on overall systems costs â€“ suggesting that other factors, including regulatory, political, or logistical considerations, may ultimately have a greater influence on plant size than the techno-economic factors considered.The flexibility of thermochemical conversion enables a viable transition pathway for firms, utilities and governments to achieve net-negative CO2 emissions in production of electricity and fuels given increasingly stringent climate policy. Primary research, development (R&D), and deployment needs are in large-scale biomass logistics, gasification, gas cleaning, and geological CO2 storage. R&D programs, subsidies, and policy that recognize co-conversion processes can support this pathway to commercialization. Here, firms can embrace a gradual transition pathway to deep decarbonization, limiting economic dislocation and increasing transfer of knowledge between the fossil and renewable sectors.Global cumulative capital investment needs for BECCS through 2050 are over $1.9 trillion (2015$, 4% real interest rate) for scenarios likely to limit global warming to 2 Â°C. This scenario envisions deployment of as much as 24 GW/yr of BECCS by 2040 in the electricity sector. To achieve theses rates of deployment within 15-20 years, governments and firms must commit to research, development, and deployment on an unprecedented scale.Three primary issues complicate emissions accounting for BECCS: cross-sector CO2 accounting, regrowth, and timing. Switchgrass integration decreases lifecycle greenhouse gas impacts of co-conversion systems with CCS, across a wide range of land-use change scenarios. Risks at commercial scale include adverse effects on food security, land conservation, social equity, and biodiversity, as well as competition for water resources. This dissertation argues for an iterative risk management approach to BECCS sustainability, with standards being updated as more knowledge is gained through deployment. Sustainability impacts and public opposition to BECCS may be reduced with transparent measurement and communication.Commercial-scale deployment is dependent on the coordination of a wide range of actors, many with different incentives and worldviews. Despite this problem, this dissertation challenges governments, industry incumbents, and emerging players to research, support, and deploy BECCS.",ucb,,https://escholarship.org/uc/item/0rs8n38z,,,eng,REGULAR,0,0
190,1626,Studies of Auction Bidding with Budget-Constrained Participants,"Kotowski, Maciej Henryk","Kariv, Shachar;",2011,"Consider a first-price, sealed-bid auction where participants have affiliated valuations and private budget constraints; that is, bidders have private multidimensional types. We offer sufficient conditions for the existence of a monotone equilibrium in this environment along with an equilibrium characterization. Hard budget constraints introduce two competing effects on bidding. The direct effect depresses bids as participants hit their spending limit. The strategic effect encourages more aggressive bidding by participants with large budgets. Together these effects can yield discontinuous equilibrium strategies stratifying competition along the budget dimension. The strategic consequences of private budget constraints can be a serious confound in interpreting bidding behavior in auctions. Evidence from an experimental auction market lends support to the strategic importance of budget constraints.",ucb,,https://escholarship.org/uc/item/0s84s1bw,,,eng,REGULAR,0,0
191,1627,Exploring the Localization of Transportation Planning: Essays on research and policy implications from shifting goals in transportation planning,"King, David Andrew",,2009,"Transportation planning has long focused on large scale projects using a civil engineering approach of maximizing throughput and minimizing interactions with the surrounding environment. Such efforts greatly increased the overall mobility and accessibility of individuals within and across metropolitan regions, but it is clear that in the future such enormous initiatives are unrealistic due to political, financial, spatial and social concerns. The field of transportation planning is shifting away from this old model of planning towards one where transportation systems are considered part of the overall quality of life of communities.This dissertation explores how local transportation planning is adapting to these changing dynamics of transportation planning through three essays. The first considers how cities are already planning for transportation through their general plans without strong mandates from regional governments. The second essay estimates the spatial variation in commute mode choice in order to show the complexity of travel due to geographic factors of infrastructure provision and land uses. The final essay discusses what flexible localized transportation policies look like, using cruising for parking as an example. Ultimately this research highlights a way forward for transportation planning as a quality-of-life issue, traditionally the purview of local governments.",ucb,,https://escholarship.org/uc/item/0tx230gc,,,eng,REGULAR,0,0
192,1628,Theoretical and Computational Tools for Analyzing the Large-Scale Structure of the Universe,"Hand, Nicholas","Seljak, Uros;",2017,"The analysis of the large-scale structure (LSS) of the Universe can yield insights into some of the most important questions in contemporary cosmology, and in recent years, has become a data-driven endeavor. With ever-growing data sets, optimal analysis techniques have become essential, not only to extract statistics from data, but also to effectively use computing resources to produce accurate theoretical predictions for those statistics. Future LSS experiments will help answer fundamental questions about our Universe, including the physical nature of dark energy, the mass scale of neutrinos, and the physics of inflation. To do so, improvements must be made to theoretical models as well as the computational tools used to perform such analyses.This thesis examines multiple aspects of LSS data analysis, presenting novel modeling techniques as well as a software toolkit suitable for analyzing data from the next generation of LSS surveys. First, we present nbodykit, an open-source, massively parallel Python toolkit for analyzing LSS data. nbodykit is both an interactive and scalable piece of scientific software, providing parallel implementations of many commonly used algorithms in LSS. Its modular design allows researchers to integrate nbodykit with their own software to build complex applications to solve specific problems in LSS. Next, we derive an optimal means of using fast Fourier transforms to estimate the multipoles of the line-of-sight dependent power spectrum, eliminating redundancy present in previous estimators in the literature. We also discuss potential advantages of our estimator for future data sets. We then present a novel theoretical model for the redshift-space galaxy power spectrum and demonstrate its accuracy in describing the clustering of galaxies down to scales of k = 0.4 h/Mpc. Finally, we analyze the large-scale clustering of quasars from the extended Baryon Oscillation Spectroscopic Survey to constrain the deviation from Gaussian random field initial conditions in the early Universe, known as primordial non-Gaussianity.",ucb,,https://escholarship.org/uc/item/0vk2x0cs,,,eng,REGULAR,0,0
193,1629,Energy Demands and Efficiency Strategies in Data Center Buildings,"Shehabi, Arman","Nazaroff, William W;Horvath, Arpad;",2009,"Information technology (IT) is becoming increasingly pervasive throughout society as more data is digitally processed, stored, and transferred.  The infrastructure that supports IT activity is growing accordingly, and data center energy demands have increased by nearly a factor of four over the past decade.  Data centers house IT equipment and require significantly more energy to operate per unit floor area than conventional buildings.  The economic and environmental ramifications of continued data center growth motivate the need to explore energy-efficient methods to operate these buildings.  A substantial portion of data center energy use is dedicated to removing the heat that is generated by the IT equipment.  Using economizers to introduce large airflow rates of outside air during favorable weather could substantially reduce the energy consumption of data center cooling.  Cooling buildings with economizers is an established energy saving measure, but in data centers this strategy is not widely used, partly owing to concerns that the large airflow rates would lead to increased indoor levels of airborne particles, which could damage IT equipment.  The environmental conditions typical of data centers and the associated potential for equipment failure, however, are not well characterized.  This barrier to economizer implementation illustrates the general relationship between energy use and indoor air quality in building design and operation.  This dissertation investigates how building design and operation influence energy use and indoor air quality in data centers and provides strategies to improve both design goals simultaneously.As an initial step toward understanding data center air quality, measurements of particle concentrations were made at multiple operating northern California data centers.  Ratios of measured particle concentrations in conventional data centers to the corresponding outside concentrations were significantly lower than those reported in the literature for office or residential buildings.  Estimates using a material-balance model match well with empirical results, indicating that the dominant particle sources and losses - ventilation and filtration - have been characterized.  Measurements taken at a data center using economizers show nearly an order of magnitude increase in particle concentration during economizer activity.  However, even with the increase, the measured particle concentrations are still below concentration limits recommended in most industry standards. The research proceeds by exploring the feasibility of using economizers in data centers while simultaneously controlling particle concentrations with high-quality air filtration.  Physical and chemical properties of indoor and outdoor particles were analyzed at a data center using economizers and varying levels of air filtration efficiency.  Results show that when improved filtration is used in combination with an economizer, the indoor/outdoor concentration ratios for most measured particle types were similar to the measurements when using conventional filtration without economizers.  An energy analysis of the data center reveals that, even during the summer months, chiller savings from economizer use greatly outweigh the increase in fan power associated with improved filtration.  These findings indicate that economizer use combined with improved filtration could significantly reduce data center energy demand while providing a level of protection from particles of outdoor origin similar to that observed with conventional design.  The emphasis of the dissertation then shifts to evaluate the energy benefits of economizer use in data centers under different design strategies.  Economizer use with high ventilation rates is compared against an alternative, water-side economizer design that does not affect indoor particle concentrations.  Building energy models are employed to estimate energy savings of both economizer designs for data centers in several climate zones in California.  Results show that water-side economizers consistently provide less energy savings than air-side economizers, though the difference in savings varies by location.  Model results also show that conventional limits on humidity levels in data centers can restrict the energy benefits of economizers.  The modeling efforts are then extended to estimate national data center energy use.  Different size data centers are modeled to represent the national variation in efficiency and operation of associated mechanical equipment.  Results indicate increased energy efficiency opportunities with larger data centers and highlight the importance of temperature setpoints in maximizing economizer efficiency.  A bottom-up modeling approach is used to estimate current (2008) United States data center energy use at nearly 62-70 billion kWh annually.  The model indicates that more about 65-70% of this energy demand can be avoided through energy efficient IT and cooling infrastructure design, equivalent to an annual energy efficiency resource of approximately 40-50 billion kWh available at a national level.  Within the context of greenhouse gas emissions, benefits can be significantly increased by incorporating site location into energy-efficient design strategies.  The framework of this dissertation contributes to general building energy efficiency efforts by shifting the perspective of building design to address indoor and outdoor environmental impacts simultaneously, ensuring that one design goal does not eclipse the other.  More specifically, the results presented here outline opportunities to temper the growing data center energy demand, so that IT can evolve into an energy efficient utility with the potential to facilitate a more sustainable expansion of goods and services.",ucb,,https://escholarship.org/uc/item/0xd1t108,,,eng,REGULAR,0,0
194,1630,Elucidating Heterogeneities and Dynamic Processes at the Nanoscale with Cathodoluminescence and Cathodoluminescence-Activated Microscopies,"Bischak, Connor Gregory","Ginsberg, Naomi S;",2017,"Super-resolution imaging has revolutionized how the structure of biological systems are observed at the nanoscale. Yet, observing dynamic processes in biology with high temporal and spatial resolution remains a significant challenge. Additionally, elucidating the nanoscale structure and dynamics in functional materials, particularly in optoelectronics, would greatly aid in the development of more efficient devices, such as solar cells and light-emitters. Unfortunately, most super-resolution microscopy platforms are designed for imaging biological samples and are incompatible with complex, functional materials. To extend super-resolution imaging to capture both biological dynamics and nanoscale material properties, we have developed cathodoluminescence imaging with low electron exposure (CILEE) and cathodoluminescence-activated imaging by resonance energy transfer (CLAIRE). Both imaging methods use cathodoluminescence (CL) microscopy to achieve nanoscale spatial resolution. The main drawback of CL microscopy is damage caused by the relatively high energy electron beam. In CILEE, the electron beam dose is significantly reduced to image samples only moderately robust to the electron beam. In CLAIRE, more fragile samples can be imaged by placing a thin scintillator film between the sample and electron beam. When excited by a focused electron beam, the scintillator film acts as a nanoscale optical excitation source, providing contrast based on interactions between luminescent dopant atoms in the sctillator and the adjacent sample in the near field. In this dissertation, the development of CILEE and CLAIRE are outlined, as well as many examples of uncovering new nanoscale phenomena with both imaging platforms.Part I of this dissertation, which includes Chapters 2-5. focuses on using CILEE to elucidate the nanoscale structure and dynamic properties of lead halide hybrid perovskites, which are promising materials for optoelectronics. Using CILEE, we reveal a surprising degree of heterogeneity at the surface of hybrid perovskite thin films that differs greatly from the more homogeneous environment found in the bulk. Our CILEE study suggests that solar cells composed of a hybrid perovskite active layer can improve in efficiency by decreasing the heterogeneity through synthetic approaches. We also use CILEE to investigate the process by which mixed halide hybrid perovskites phase separate upon photoexcitation, a process that severely limits solar cell efficiency. Through a combination of CILEE and multiscale modeling, we find that phase separation is driven by polaronic strain in the lattice. Our results represent a new type of nanoscale phase transformation that is unique to hybrid materials. The emergence of CILEE as new approach to non-invasive super-resolution imaging has led to a greater understanding of the complex structure and dynamics in hybrid perovskite materials.Part II of this dissertation, which includes Chapters 6-11, introduces CLAIRE as a new super-resolution imaging platform designed to image soft materials, such as organic or biological samples. In this dissertation, we describe the production of thin, free-standing scintillator films for CLAIRE and the incorporation of these scintillator films into a functional imaging device. We demonstrate that CLAIRE is capable of imaging soft materials and dynamic processes. The capability of CLAIRE to image biological samples with endogenous chromophores, such as photosynthetic membranes, is also demonstrated. Together, CILEE and CLAIRE extend non-invasive super-resolution optical imaging to new classes of soft materials that are incompatible with current super-resolution optical imaging approaches and traditional electron microscopy. These new nanoscale imaging methods provide promising opportunities to visualize biological dynamics at high spatial and temporal resolution and to interrogate the nanoscale optical properties of functional optoelectronic materials to understand their fundamental properties, leading to higher efficiency devices.",ucb,,https://escholarship.org/uc/item/11j686v3,,,eng,REGULAR,0,0
195,1631,Rejuvenation of the Aged Stem Cell Niche: Signal Transduction and Reversing the Decline of Adult Hippocampal Neurogenesis and Myogenesis,"Yousef, Hanadie","Schaffer, David;Conboy, Irina;",2013,"Although functional organ stem cells persist in the elderly, tissue damage invariably overwhelms tissue repair, ultimately leading to the failure of major organ systems. It has been demonstrated that the microenvironment, or niche in which adult stem cells reside critically influences stem cell function. The delicate balance between positive and negative signaling regulators controls the decision of adult stem cells to remain quiescent, self-renew or differentiate, a crucial balance for the maintenance of tissue homeostasis. In this dissertation, we provide evidence that the same key morphogenic signaling pathways become deregulated with age and contribute to the decline of both hippocampal neurogenesis and skeletal muscle regeneration with aging, leading to the decline in regenerative performance of both brain and muscle tissue stem cells. Furthermore, we demonstrate that the aged tissue niches can be rejuvenated to enhance native stem cell function in muscle and brain by youthful calibration of the intensity of these morphogenic signaling pathways. In particular, local attenuation of BMP signaling in the aged hippocampus, systemic and local attenuation of TGF-beta signaling in both the aged hippocampus and aged skeletal muscle, and specific proteins secreted by human embryonic stem cells that act through MAPK and Notch signaling rejuvenate brain and muscle tissue precursor cell function by normalizing the signaling strength of the pathways that are chronically overexpressed or underexpressed with aging. Summarily, by better understanding the age-imposed decline in the regenerative capacity of stem cells, the debilitating lack of organ maintenance in the old, including decline in neurogenesis and skeletal muscle regeneration, can be ameliorated.",ucb,,https://escholarship.org/uc/item/11s7t6kd,,,eng,REGULAR,0,0
196,1632,Gesturing Through Time: Holds and Intermodal Timing in the Stream of Speech,"Park-Doob, Mischa Alan","Sweetser, Eve E.;Hanks, William F.;",2010,"Most previous work examining co-speech gestures (the spontaneous bodily movements and configurations we engage in during speaking) has emphasized the importance of their most salient or energetically expressive moments, known as gesture 'strokes' (Kendon 1980). In contrast, in this dissertation I explore the potential functions of intervals of gestural stasis, or gesture 'holds', in which the hands or body maintain particular configurations across variable spans of time, interwoven with the stream of speech. Through the embodiment of a constant form within continuously evolving face-to-face interactions, holds make possible a unique and understudied array of functions relating to the maintenance of ideas and contexts across time. Chapter 1 introduces the corpus of videotaped dyadic conversations from which all of the examples are drawn, discusses the history of the concepts of 'stroke' and 'hold', and illustrates the structural possibilities for the timing of holds with respect to co-expressive speech: they bear content that is not just simultaneous with, but also 'retrospective' and/or 'prospective' of, portions of the full composite utterances in which they occur. Chapter 2 illustrates that holds lasting across pauses and disfluencies support continued expressiveness and interpretability, alternately presaging new content that will also be part of a fluent resumption, or maintaining retrospective links to prior content that can contextualize the resumption. Chapter 3 discusses the frequent expressive complementarity of co-timed speech and gesture, as it relates to the debate on speech-gesture synchrony, and further demonstrates that preliminary commitments to utterances are often partially fulfilled from the earliest moments because of gestural cues that are interpretable at all points of their lifecycles, including preparatory phases. Chapter 4 discusses the implications for attention and memory of gesture holds acting as temporary cognitive artifacts, forming 'bridges' across interruptions and competing representations by interlocutors, thereby functioning retrospectively as 'recall cues' to previous moments of the interaction. Chapter 5 focuses on instances of gesture holds combined with listener-directed gaze that are maintained across turn transitions, then released, allowing speakers to 'hand off' control while enforcing a context for the next turn. Chapter 6 synthesizes the preceding chapters and suggests directions for future research.",ucb,,https://escholarship.org/uc/item/12f968ck,,,eng,REGULAR,0,0
197,1633,"Lived Experience in New Mexico, 1754-2019: A Historical Archaeology With and For a Genizaro Community","McCleary, Alexandra","Sunseri, Jun U;",2020,"Deep contestations of essentialized identity categories are a contemporary reality for communities for whom cultural patrimony of land and water resources play a crucial role. Yet, archaeology has not been able to adequately recognize the dynamics of the changing nature of identity practices which shaped interactions between groups of people, particularly in areas with a sustained colonial presence and resource-challenged ecologies. The high-elevation, semi-arid climate and historical complexity of Northern New Mexico provide such a context. My research objective is to understand how Genizaro Indian communities are sensitive to the historically particular dynamics of ethnopolitical empowerment and racialization in the 18th to early 20th centuries. This project uses an examination of the documentary record, faunal remains, and commensurate data from excavated materials from Genizaro communities in New Mexico to build upon existing models of cultural hybridity and ethnogenesis.",ucb,,https://escholarship.org/uc/item/1415b7n3,,,eng,REGULAR,0,0
198,1634,Fully Integrated Complementary Metal Oxide Semiconductor (CMOS) Bio-Assay Platform,"Florescu, Octavian","Boser, Bernhard E;",2010,"We present a post-processed 2.5mm x 2.5mm 0.18um Complementary Metal Oxide Semiconductor (CMOS) platform that leverages the advantages of super-paramagnetic bead labeling to integrate on-chip the label separation and detection functionalities required for high sensitivity bio-assays. The surfaces of the CMOS chip and of the magnetic beads are functionalized with bio-chemicals complementary to a target analyte. In a sandwich capture format, the presence of the target analyte will strongly bind 4.5um magnetic bead labels to the surface of the chip. The undesired background signal is minimized by the removal of the unbound magnetic beads from the detection array via magnetic forces generated on-chip. The remaining strongly bound magnetic beads are respectively magnetized and detected by an array of 128 stacked micro-coil/Hall sensor elements. This single chip solution does not require any external components like pumps, valves or electromagnets and is capable of detecting purified Human antibodies down to concentrations of 100pg/ml as well as anti-Dengue antibodies in human serum samples.A whole blood sample preparation system based on membrane filtration alleviates the need for centrifugation and can be readily combined with the assay platform into a high performance, Point-of-Need (PON) In-Vitro Diagnostic (IVD) device.The present prototype relies on an electronic reader for controlling the assay and reporting results. Implementing this function on the bio-sensor front-end enables a very simple design consisting of only the bio-sensor and an attached results display. Such low-cost, easy-to-use, high performance devices are needed for lowering health costs through more decentralized distribution of medical care.",ucb,,https://escholarship.org/uc/item/1519g7kt,,,eng,REGULAR,0,0
199,1635,Essays on Environmental Economics,"Tang, Qu","Rausser, Gordon C.;",2015,"AbstractEssays on Environmental EconomicsbyQu TangDoctor of Philosophy in Agricultural and Resource EconomicsUniversity of California, BerkeleyProfessor Gordon C. Rausser, ChairThis dissertation is comprised of three essays that apply microeconomics theory and econometric methods to study important issues in environmental economics. In the first essay, I investigate the impacts of imposing inter-state trade restrictions on the compliance costs of coal-fired electric generating units (EGUs) in the context of a U.S. SO2 emissions trading program (the Acid Rain Program). Over the past decade, tremendous efforts have been devoted to modifying emissions trading programs to address cross-state air pollution problems. The modification involves imposing more restrictions on emissions trading across geographical areas. The empirical question is how severe trade restrictions affect the regulated firmsâ€™ compliance costs. Using rich data from the Acid Rain Program, this essay developed a discrete-continuous model to estimate electric generating unitsâ€™ compliance strategies and marginal abatement costs associated with the nationwide uniform emissions trading as the program was implemented in practice. Based on the estimation results, this essay then simulated unitsâ€™ compliance behaviors and the corresponding compliance costs if interstate trading had been prohibited. The results show that the aggregate compliance costs would increase more than one and a half times for the same emissions reduction goal due to the narrower trading markets in the counterfactual policy design with trade restrictions, and the costs would vary dramatically across space. Combined with the analysis on the benefit side, the results of this essay could be used to predict welfare impacts associated with trade restrictions at both national level and state level. And it may shed light on the future modification and implementation of EPAâ€™s cross-state air pollution regulations.The second essay applies an equilibrium sorting model to a brand-new housing market in Beijing, China to estimate household preferences for neighborhood public goods provision, including public transportation services, public primary schools, and environmental amenities. The equilibrium sorting model is based on a discrete choice model of household residential location decisions. Relying on a unique, detailed data set on housing location, price, and other household characteristics, I estimate the model following the two-step BLP method, taking into account the heterogeneity of household preferences, incorporating neighborhood-specific unobservable characteristics, and addressing the endogeneity of housing prices using instrumental variables. The results suggest that in general, lower housing price, better environmental amenities, and being closer to job centers will increase the choice opportunity of a neighborhood, and public transportation systems play a more important role in the neighborhoods far away from urban centers. Moreover, different households show varying preferences for these public goods. A distinct fact is that in addition to income, peopleâ€™s preferences vary greatly with generation (head age of households) and job type (whether there are public employees), which reveal the significant differences between generations and illustrate the welfare for public employees within the context of the transitional economy in China. This preference heterogeneity implies that future policies should be more geographically asymmetric, locally targeted and tailored based on specific socio-economic characteristics.The third essay estimates the impact of climate change on the crop yields in China. I use a 11-year county-level panel data set covering more than 1,000 counties to estimate the effects of random year-to-year variation in weather on three major crops yields, including rice, wheat, and corn. Because it is not easy for small-scale farmers to adapt to climate change quickly in short time, these estimates could be used to plausibly predict the short to medium-run impacts of climate change on crop yields in China. The essay finds that over the period 2040-2060, projected climate change would reduce rice yield by 1.18% under a comparatively high emission scenario and by 0.08% under a medium-low scenario, reduce corn yield by 2.21% and 1.64% under the two emission scenarios, respectively, and increase wheat yield by 6.68% and 5.48% under the two emission scenarios, respectively. These findings may shed light on future policy designs to enhance the adaptive capacity of agriculture in China and thus ensure food security in the context of climate change.",ucb,,https://escholarship.org/uc/item/159713r1,,,eng,REGULAR,0,0
200,1636,"Our Fanatics: Figurations of Religious Fanaticism in Ian McEwan, Chimamanda Ngozi Adichie, and Marilynne Robinson","Sambrooke, Jerilyn","Lye, Collen;Naddaff, Ramona;",2017,"Our Fanatics: Figurations of Religious Fanaticism in Ian McEwan, Chimamanda Ngozi Adichie, and Marilynne Robinson examines how three contemporary novelists complicate oft-repeated accounts that oppose religious fanaticism to reasoned argumentation and secular politics. My dissertation features novels that focus intently on the interiority of protagonists who encounter figures of religious fanaticism, portraying religious fanaticism as something to be negotiated rather than defended against. By analyzing twenty-first century novels that variously figure religious fanaticism in oppositional, paradoxical, and genealogical terms, this project examines how religious fanaticism is constitutive ofâ€”rather than external toâ€”the worlds of these novels.The first chapter reaches back to Ian McEwanâ€™s Enduring Love (1998), comparing it to his 9/11 novel, Saturday (2005), and, more recently, The Children Act (2014). I argue McEwanâ€™s novels frame religious fanaticism as a form of irrational certainty that generates epistemological uncertainty for the novelsâ€™ protagonists. These texts frustrate a simple triumphant narrative whereby secular rationalism prevails over religious fanaticism. More recently, however, McEwanâ€™s fiction resolves such tensions with increasing authority, gradually eliminating the experimental dimensions of McEwanâ€™s early work. Chapter two features Chimamanda Ngozi Adichieâ€™s Purple Hibiscus (2003), which develops an apparently paradoxical religious fanaticâ€” politically admirable but privately violent. I investigate this paradox by analyzing the novelâ€™s cyclical plot, which echoes the Catholic liturgical calendar and which distinguishes it from Chinua Achebeâ€™s Things Fall Apart (1958), a comparison that has dominated Adichieâ€™s critical reception. The third chapter reads Marilynne Robinsonâ€™s Gilead trilogyâ€”Gilead (2004), Home (2008), and Lila (2014)â€”as an extended meditation on the lingering effects of religious fanaticism across the generations of a small mid-Western town. The trilogyâ€™s genealogical figuration of religious fanaticism ties abolitionism to civil rights activism, delivering a resounding critique of â€œmainlineâ€ Protestant disavowals of such fanaticism.The religious fanatics that appear across this dissertation cannot be described in any easy sense as â€œours.â€ My title draws attention to the smaller, subtler way that these novels approach religious fanaticism through intimate relationships and private spaces, positioning religious fanaticism as internal to communities, to families, and, particularly in Adichie and Robinson, to Christian traditions.",ucb,,https://escholarship.org/uc/item/15r2c4vf,,,eng,REGULAR,0,0
201,1637,"Race Across Borders: Transnationalism and Racial Identity in African-American Fiction, 1929-1945","Agbodike, Kanayo Jason","JanMohamed, Abdul R;",2012,"Race Across Borders: Transnationalism and Racial Identity in African-American Fiction, 1929-1945, examines four African-American literary texts that employ transnational themes and aesthetics as a means of resisting a logic of racial essentialism that governed the production and reception of black literature in the United States during the early 20th century. I examine the ways in which Dark Princess by W. E. B. Du Bois, Their Eyes Were Watching God by Zora Neale Hurston, Banjo: A Story Without a Plot by Claude McKay, and If He Hollers Let Him Go by Chester Himes employ various formal and stylistic techniques to critique and reconfigure the dominant codes of racial identity that shaped their context. I argue that each of these texts exemplifies a conflict between a nationalist mode of racial representation and a transnational orientation that destabilizes received notions of race. Whereas the cultural field in which interwar African-American novels were situated involved a manifest nationalist topography which reproduced a racially divided polity, these texts inscribe transnational forces that disrupt the racial underpinnings of the 20th-century American national narrative. Because of the hegemonic status of the nationalist framework, the critique of that framework tends to appear in the formal aspects of the novels rather than their explicit contents.	The first chapter considers how Dark Princess explores the intersections between African-American and anti-colonial politics by way of the story of a romantic relationship between an African American man involved in local politics and an Indian woman involved in an international Third World liberation movement. I consider how the juxtaposition of national and transnational forms of solidarity within the text is paralleled by a tension between naturalism and romance in  its formal economy. While the techniques of naturalism tend to characterize the parts of the novel that represent national and racial politics, the parts that imagine a transnational anti-colonial movement draw on the codes of literary romance. Through this utopian gesture, the novel gives shape to the conflict between national and transnational perspectives on minority politics without offering a clear resolution to that conflict. The second chapter challenges dominant critical interpretations of Their Eyes Were Watching God, which construe the novel as a written representation of African American oral tradition. While such readings are illuminating, they overlook significant aspects of the text's racial thematic by emphasizing how it presupposes racial forms of identity. Although the novel does reproduce such forms, I argue that it simultaneously resists them, particularly in some of its more marginal characters and moments, and that it is precisely through the representation of dialect speech that these hidden resistances become visible. 	The third chapter examines McKay's use of the aesthetic concept of the sublime in articulating the problematic gulf separating modern Blacks from metropolitan culture and society. In Banjo the sublime mediates between these terms rather than the rationally free subject and a causally determined Nature. Banjo differs from the mainstream European realist novel by denying the teleological narrative of reconciliation as unsuitable to the concerns of a radically excluded black collective. By taking as his protagonists an international band of black vagabonds based in the cosmopolitan French port city Marseilles, McKay imagines an alternative to the grand narrative of national identity. The final chapter focuses on notions of embodiment and psychological affect within Himes's narrative of thwarted integration. Simultaneously foreclosing on both a successful outcome for such a project and the death of the protagonist, the novel moves towards an ambivalent and open-ended reflection on the possibilities of social transformation. In light of this ambivalence, I view the brief but frequent points at which the protagonist identifies with marginalized Mexican-Americans and Japanese-American internment-camp prisoners as moments that both disrupt the received black/white binary as a schema for American social reality and contrasts a trans-national anti-colonial solidarity with racial nationalism as an alternative mode of political agency.",ucb,,https://escholarship.org/uc/item/16v2k9z2,,,eng,REGULAR,0,0
202,1638,Nonparametric Optimization with Objective Operational Statistics,"Yu, Lian","Lim, Andrew E.B.;",2010,"In the first part of this thesis, we study the non-parametricmethods for estimation and optimization. In particular, a newnon-parametric method, objective operational statistics, is proposedto inventory control problems, where the only information availableis the sample data and we do not assume any relationship betweendemands and order quantities. A kernel algorithm based on objectiveoperational statistics is constructed to approximate the objectivefunction directly from sample data. Moreover, we give conditionsunder which the operational statistics approximation functionconverges to the true objective. Numerical results of the algorithmwith applications to newsvendor problem show that the objectiveoperational statistics approach works well for small amount of dataand outperforms the previous parametric and non-parametric methods.In the second part of this thesis, we present a robust hedgingproblem under model uncertainty and the bounds of the optimalobjective value are derived by duality analysis.",ucb,,https://escholarship.org/uc/item/18z443nf,,,eng,REGULAR,0,0
203,1639,Biomechanics and evolution of flight in stick insects,"Zeng, Yu","Dudley, Robert;Wake, David;",2013,"Many unresolved questions in animal flight evolution relate to the transition between flightless and volant forms. Functional analysis of transitional modes using anatomical intermediates may help to assess the biomechanical underpinnings to such transitional processes. The group of stick insects exhibits tremendous diversity in wing sizes, which is potentially correlated with selection gradient for wing size. This dissertation work uses stick insects as a model system to address the ecological context of controlled aerial behaviors and the evolutionary consequences of flight loss. Chapter 1 explores the behavioral context of controlled aerial descent in the newly hatched larvae of an Australian stick insect  Extatosoma tiaratum, which exhibit ephemeral ant-mimicry and hyperactive dispersal activities. Through lab experiments, we documented the ontogenetic variation in various taxic behaviors and voluntary drop as an escaping response. Chapter 2 explores the visual ecology of directed aerial descent in larval  E. tiaratum. We investigated how visual contrasts are used as locomotor references during directed aerial descent. Our results suggest the use of vertically constant contrast edges is a major component of directional targeting. The utility of contrast as visual cues was further shown to be dependent on both the heterogeneity of the visual environment and the quality of perceived signals. Chapter 3 addressed the biomechanics of aerial righting behaviors in larval  E. tiaratum. Through high--speed filming and three--dimensional motion reconstruction, the results showed highly controlled leg movements are involved in righting maneuvers. Through posture control, the insects achieve effective righting rotation. More subtle leg movements were also shown in stroke--like patterns, which are instantaneously correlated with whole--insect rotation. This study provided useful information for understanding the ecology and evolution of controlled aerial behaviors in invertebrates. Chapter 4 addressed the functional consequences of progressive wing size reduction along an altitudinal gradient in three populations of a stick insect  Asceles tanarata  native to Malay Peninsula. We investigated how wing and body morphology change along the altitudinal gradient, and further studied the biomechanics of flight in each flight morph. The results indicate the reduction of fight apparatus leads to changes in wing design and wing kinematics. Reduced flapping fight performances resemble parachuting and gliding. Due to both altered kinematics and flight trajectories different from conventionally recognized flapping flight, the aerodynamics of wing flapping is characterized by large advance ratio and reduced half--stroke asymmetries. Although the reduction in flight performance is closely correlated with the reduction of wing size, wing aerodynamics shows more complex pattern along the performance gradient.",ucb,,https://escholarship.org/uc/item/1cr698rw,,,eng,REGULAR,0,0
204,1640,Constraints on slow slip from landsliding and faulting,"Delbridge, Brent Gregory","Burgmann, Roland;",2017,"The discovery of slow-slip has radically changed the way we understand the relative movement of Earthâ€™s tectonic plates and the accumulation of stress in fault zones that fail in large earthquakes. Prior to the discovery of slow-slip, faults were thought to relieve stress either through continuous aseismic sliding, as is the case for continental creeping faults, or in near instantaneous failure. Aseismic deformation reflects fault slip that is slow enough that both inertial forces and seismic radiation are negligible. The durations of observed aseismic slip events range from days to years, with displacements of up to tens of centimeters. These events are not unique to a specific depth range and occur on faults in a variety of tectonic settings. This aseismic slip can sometimes also trigger more rapid slip somewhere else on the fault, such as small embedded asperities. This is thought to be the mechanism generating observed Low Frequency Earthquakes (LFEs) and small repeating earthquakes.I have preformed a series of studies to better understanding the nature of tectonic faulting which are compiled here. The first is entitled â€œ3D surface deformation derived from airborne interferometric UAVSAR: Application to the Slumgullion Landslideâ€, and was originally published in the Journal of Geophysical Research in 2016. In order to understand how landslides respond to environmental forcing, we quantify how the hydro-mechanical forces controlling the Slumgullion Landslide express themselves kinematically in response to the infiltration of seasonal snowmelt. The well-studied Slumgullion Landslide, which is 3.9 km long and moves persistently at rates up to âˆ¼2 cm/day is an ideal natural laboratory due to its large spatial extent and rapid deformation rates. The lateral boundaries of the landslide consist of strike-slip fault features, which over time have built up large flank ridges.The second study compiled here is entitled â€œTemporal variation of intermediate-depth earthquakes around the time of the M9.0 Tohoku-oki earthquakeâ€ and was originally published in Geophysical Research Letters in 2017. The temporal evolution of intermediate depth seismicity before and after the 2011 M 9.0 Tohoku-oki earthquake reveals interactionsbetween plate interface slip and deformation in the subducting slab. I investigate seismicity rate changes in the upper and lower planes of the double seismic zone beneath northeast Japan. The average ratio of upper plane to lower plane activity and the mean deep aseismic slip rate both increased by factor of two. An increase of down-dip compression in the slab resulting from coseismic and postseismic deformation enhanced seismicity in the upper plane, which is dominated by events accommodating down-dip shortening from plate unbending.In the third and final study included here I use geodetic measurements to place a quantitative upper bound on the size of the slow slip accompanying large bursts of quasi-periodic tremors and LFEs on the Parkfield section of the SAF. We use a host of analysis methods to try to isolate the small signal due to the slow slip and characterize noise properties. We find that in addition to subduction zones, transform faults are also capable of producing ETSs. However, given the upper-bounds from our analysis, surface geodetic measurements of this slow slip is likely to remain highly challenging.",ucb,,https://escholarship.org/uc/item/1dv2x1sk,,,eng,REGULAR,0,0
205,1641,Fantasies of Friendship: Ernst JÃ¼nger and the German Right's Search for Community in Modernity,"Bures, Eliah Matthew","Jay, Martin E.;",2014,"This dissertation argues that ideas and experiences of friendship were central to the thinking of German radical conservatives in the twentieth century, from the pre-WWI years to the emergence, beginning in the 1970s, of the New Right.I approach this issue by examining the role of friendship in the circle around the writer Ernst JÃ¼nger (1895-1998). Like many in his generation, JÃ¼nger's youthful alienation from a ""cold"" bourgeois society was felt via a contrast to the intimacy of personal friendship. A WWI soldier, JÃ¼nger penned memoirs of the trenches that revealed similar desires for mutual understanding, glorifying wartime comradeship as a bond deeper than words and a return to the ""tacit accord"" that supposedly marked traditional communities. After 1933, JÃ¼nger turned from a right-wing opponent of democracy into a voice of ""spiritual resistance"" to the Nazi regime. For JÃ¼nger and other non-Nazi Germans, friendship was a crucial space of candid communication and nonconformity to the norms of the Third Reich. JÃ¼nger's writings from these years also issued coded signals to sympathetic readers to keep alive conservative values for a post-Nazi future. After WWII, JÃ¼nger became one of Germany's most controversial figures, a critic of modernity who was at the center of a friendship network that joined the veterans and heirs of Weimar's radical right into a counterculture opposed to what they believed was the decadence of German life. In JÃ¼nger's later works, he portrayed friendship as the last true site of community, an idea that shaped the elitist attitudes of new members of the German right.  I use published texts and letters alongside new archival material to make two broad contributions. First, by investigating friendship among twentieth-century German radical conservatives, I bring to light the important work that friendship has done for those facing quintessentially modern problems like alienation and social fragmentation. I argue that the work of friendship for figures like Ernst JÃ¼nger has primarily been the provision of needs for affirmation, communication, and mutual understanding. Recognizing these needs helps us see that anxieties about being understood, longings for fellowship, and concerns for the quality of interpersonal relationships have often underlain radical conservatism's explicit ideas about, say, the virtues of ""organic"" community or the perils of democratic leveling. I show how these needs and anxieties were closely bound up with the radical conservative critique of modernity, including its elitism, ultra-nationalism, and disdain for mass society and mass culture. It is through friendship, I argue, that German radical conservatives have understood the shortcomings of modern life and envisioned ways to overcome or cope with modernity.My second contribution is methodological. The study of friendship, I argue, can uncover emotional needs and intimate states of mind that are otherwise difficult for the historian to bring to light. Examining friendship among twentieth-century radical conservatives provides fundamental insights into motives, helping us understand why certain emotional demands were felt at certain moments in German history, and how these emotions in turn drove the decision for particular ideological positions. Asking these questions of the German radical right offers a fresh angle on a group usually dealt with through a reductive focus on cultural pathologies and formal ideology. Taking Ernst JÃ¼nger and his many friends and interlocutors as a case study, I provide a rich biographical historicization of German radical conservative thinking as it developed over multiple stages throughout the twentieth century. Stressing recurring needs for communication and mutual understanding, I locate new motives for radical conservative ideas.",ucb,,https://escholarship.org/uc/item/1f32711j,,,eng,REGULAR,0,0
206,1642,Role of Cdc48 unfoldase in the ubiquitin-proteasome proteolytic pathway,"Olszewski, Michal M","Martin, Andreas;",2018,"The ubiquitin proteasome system (UPS) controls essential cellular processes such as cell cycle progression, cellular signaling, stress responses and maintenance of protein folding homeostasis via regulated proteolysis of substrates by the 26S proteasome. Proteins destined for degradation need to meet two criteria: i) carry a polyubiquitin degron signal and ii) have an unstructured initiation region that allows for the engagement by the proteasome and subsequent unfolding, translocation and degradation. Yeast Cdc48, also known as p97 or VCP in higher eukaryotes, is an essential, abundant and highly conserved AAA+ ATPase that uses its unfoldase activity to extract ubiquitinated protein substrates from complexes and membranes. It has been hypothesized that Cdc48 also plays an important role in protein degradation pathways, working upstream of the 26S proteasome. For my dissertation, I established an in vitro reconstituted system that allowed me to study the interactions between Cdc48 and its cofactors, determine the kinetics of substrate unfolding by Cdc48 and demonstrate the functional role of Cdc48 in the proteasomal degradation.First, I established that recombinant Cdc48 can bind to the Ufd1/Npl4 substrate recognition heterodimer, as well as to the Otu1 deubiquitinase. Those interactions are mutually exclusive, meaning that Ufd1/Npl4 and Otu1 cannot both form a complex with Cdc48 in the absence of substrate. I also observed interaction between the Cdc48 unfoldase and the Ufd2 ubiquitin ligase. Additionally, I tested the activity of Ufd2 and developed conditions under which it can ubiquitinate model substrates containing linear ubiquitin fusion. That substrate can be used to study the activity of Otu1 and its modulation by Cdc48, as well as to measure unfolding by Cdc48. Furthermore, by utilizing the fluorescent protein mEOS3.2 N-terminally fused to tetra ubiquitin as a model substrate, and extending this ubiquitin using Ufd2, I was able to measure kinetics of Cdc48 unfolding under single and multiple turnover conditions. Finally, I demonstrated for the first time that compact substrates lacking an initiation segment can still be degraded by the 26S proteasome when first processed by Cdc48.  Overall, my data suggest that Cdc48â€™s primary role in the UPS is to create unstructured initiation regions in compact substrates refractory to proteasomal engagement.",ucb,,https://escholarship.org/uc/item/1h25d3p8,,,eng,REGULAR,0,0
207,1643,Association with Foci,"Toosarvandani, Maziar Doustdar","Mikkelsen, Line;",2010,"Association with focus has, since Jackendoff's (1972) dissertation, been the object of intense study. Most researchers, however, have concentrated on explaining the semantic variability of only and even, whose truth conditions vary with the position of focus. I take as my starting point another property of associating expressions. Both only and even restrict the distribution of focus, a property that, I argue, they share with a range of other lexical items. But, while only and even take a single argument and require there to be a focus somewhere inside that argument, expressions like adversative but and let alone take two arguments, thereby associating with two foci.Associating expressions, of both the one- and two-place varieties, have two things in common. First, they are crosscategorial in their syntax, taking arguments of a variety of different types. Second, they evoke multiple alternativesâ€”different possible answer to a question. Together, these two independent properties of associating expressions interact with the question under discussion (Roberts 1996, 2004) to give rise to the restriction on the distribution of focus.  My approach to association with focus departs from previous ones in important ways. Associating expressions neither make reference to focus in their lexical entry (Rooth 1985, 1992, 1996) nor to the question under discussion (Beaver and Clark 2008), providing a more satisfying answer to the question of why only some expressions associate with focus.",ucb,,https://escholarship.org/uc/item/0ns534g1,,,eng,REGULAR,0,0
208,1644,"Essays on Crime, Unemployment and Health","Chaidez, Lilia","Magruder, Jeremy;",2015,"This dissertation is composed of three chapters and studies issues related to crime, unemployment and health.  The first chapter looks at the effect of funding for public safety on drug related violence.  The second chapter, which is joint work with Santiago Guerrero, examines the effect of unemployment on crime during the latest great recession.  The third chapter examines the effect that the introduction of ultra-low sulfur diesel has had on infant mortality.  The first paper develops a simple framework to describe the effect of increases in fighting capacity on violence and uses a large program in Mexico to empirically estimate the effect of funding for public safety on violence, specifically drug related violence.  Starting in 2008, Mexico implemented a large program designed for the strengthening of the municipal police, the assignment of which was based on an index.  The main areas of allowed expenditures for these funds were: the purchase of fighting equipment, technology infrastructure and training of the police force.  Instrumenting funding with the arbitrary initial eligibility cutoff, I find that the funds led to large increases in drug related violence.  Evidence is consistent with the funds allowing the police to fight criminal organizations which weakened organizations and in turn led to turf wars.  The effect is not higher for PAN municipalities, the party whose main platform during the study period was to fight organized crime.  Also, there does not seem to be an increase in violence in politically stable municipalities as a result of the program, but there is a decrease in areas with low land productivity.  Consistent with theory, I also find suggestive evidence of an inverted U-shaped relationship between baseline funding for public safety and the effect of the program. The second paper estimates the effect of unemployment on crime in Mexico.  This study uses the variation in unemployment across metropolitan areas in Mexico induced by the latest great recession.  Areas that were highly dependent on the US economy experienced the largest increases in unemployment, thus we instrument unemployment with the initial manufacturing and tourism labor share interacted with US GDP and find that increases in unemployment have led to decreases in crime in Mexico.  The results are consistent with the decrease in potential targets due to the increases in unemployment outweighing the positive effect coming from the decrease in the opportunity cost of engaging in criminal activities as unemployment increases. The third paper estimates the effect of the introduction of ultra-low sulfur diesel on infant health in Mexico.  In 2006 the Mexican government began the rollout of ultra-low sulfur diesel in metropolitan areas, starting with border municipalities.  Using a difference in differences approach, I find that, despite its potential to improve health outcomes, there is no evidence that sulfur regulation had a substantial effect on infant mortality outcomes.",ucb,,https://escholarship.org/uc/item/0021q7dw,,,eng,REGULAR,0,0
209,1645,"Making Ivan-Uzbek: War, Friendship of the Peoples, and the Creation of Soviet Uzbekistan, 1941-1945","Shaw, Charles David","Slezkine, Yuri;",2015,"This dissertation addresses the impact of World War II on Uzbek society and contends that the war era should be seen as seen as equally transformative to the tumultuous 1920s and 1930s for Soviet Central Asia. It argues that via the processes of military service, labor mobilization, and the evacuation of Soviet elites and common citizens that Uzbeks joined the broader â€œSoviet peopleâ€ or sovetskii narod and overcame the prejudices of being â€œformerly backwardâ€ in Marxist ideology. The dissertation argues that the army was a flexible institution that both catered to national cultural (including Islamic ritual) and linguistic difference but also offered avenues for assimilation to become Ivan-Uzbeks, part of a Russian-speaking, pan-Soviet community of victors. Yet as the war wound down the reemergence of tradition and violence against women made clear the limits of this integration. The dissertation contends that the war shaped the contours of Central Asian society that endured through 1991 and created the basis for thinking of the â€œSoviet peopleâ€ as a nation in the 1950s and 1960s. The first chapter addresses the experience of soldiers in the Red Army, paying special attention to the armyâ€™s policies to support Central Asian men with agitation. The second chapter focuses on the laborers who faced high mortality in the mines and industrial sites of the Urals and Siberia. Deprived of cultural support, agitators, and segregated from Slavic workers, they offer a case study in how the Soviet war-time state could operate both as a nation and an empire at the same time. The next two chapters address the Uzbek homefront, the contributions of Uzbek women who stayed in the region, and changing gender roles. Via an â€œemancipation of necessityâ€ Uzbek women continued the professional gains they made during collectivization and replaced men in mechanized agriculture and in leadership positions. I examine the wartime contributions of three noteworthy women to show how the state both respected cultural mores that prevented them from serving at the front, but also pressed them into new, public roles. The next chapter focuses the interaction between evacuated Russian and Uzbek writers. I argue that their cooperation facilitated the narrative of Friendship of the Peoples while also allowing the evacuees to assert their tutorial rights as elder brother and masters of socialist realism. The final chapter addresses the durability of the Ivan-Uzbek identity in the face of social breakdown and resurgent religious tradition after the war.",ucb,,https://escholarship.org/uc/item/00p3q4kq,,,eng,REGULAR,0,0
210,1646,Essays on Financial Risk Taking and Embedded Heuristics,"Stimmler, Mary","Staw, Barry;",2013,"This dissertation investigates the relationship between the use of financial risk taking and the complex mathematical models used to quantify risks. In the first essay, laboratory experiments demonstrate that students who have taken courses in finance and economics take more risk when they are exposed to complex mathematical models, even when these models do not provide more information. Furthermore, students who have a strong belief in the power of risk quantification to produce accurate assessments of future events are more likely to take more risk when it is accompanies by complex quantitative models. The second essay uses data on U.S. banks from 1994-2007 and investigates popular claims that innovative risk measures, such as value at risk (VAR) resulted in greater risk seeking by banks prior to the financial crisis. It theorizes that as formal risk assessment models, such as the risk models used by banks, become institutionalized within organizations, a model's abstract representation of reality becomes reified and treated as though it is real and complete. This results in organizationally-embedded decision-making heuristics that shape how choices are made within the firm. For risk models, this meant that when banks encouraged the use of risk models, they generated an implicit belief that these models represented accurate assessments of future outcomes. This reduced uncertainty about future outcomes and led to greater risk taking.",ucb,,https://escholarship.org/uc/item/01z2b0hj,,,eng,REGULAR,0,0
211,1647,Flipping the Hemoglobin Switch and Discovering Regulators Involved in Fetal Hemoglobin Reactivation,"Boontanrart, Mandy","Corn, Jacob E;",2020,"The fetal to adult hemoglobin switch is a developmental process by which fetal hemoglobin becomes silenced after birth and replaced by adult hemoglobin.  Diseases caused by defective or missing adult hemoglobin, such as Sickle Cell Disease or Î²-Thalassemia, can be ameliorated by reactivating fetal hemoglobin. We discovered that knockdown or knockout of Î²-globin, a subunit of adult hemoglobin, led to robust upregulation of Î³-globin, a subunit of fetal hemoglobin. This phenomenon suggested that red blood cells have an inherent ability to upregulate fetal hemoglobin in the event that adult hemoglobin is lacking.We developed multiple gene-editing tools in an immortalized erythroid cell model to investigate the molecular mechanisms behind the increase in fetal hemoglobin. Time-course transcriptomics identified ATF4, a transcription factor, as a causal regulator of this response. Further analysis also converged upon downregulation of MYB and BCL11A, known repressors of Î³-globin, described in detail in chapter 2. Further work in chapter 3 explores other possible fetal hemoglobin regulators as discovered by CRISPRi arrayed mediated knockdown experiments. This work furthers our understanding of fundamental mechanisms of gene regulation and how cellular and molecular events influence red blood cell differentiation.",ucb,,https://escholarship.org/uc/item/02m6p6v6,,,eng,REGULAR,0,0
212,1648,"Synthesis, Characterization, and Fabrication of Boron Nitride and Carbon Nanomaterials, their Applications, and the Extended Pressure Inductively Coupled Plasma Synthesis of Boron Nitride Nanotubes","Fathalizadeh, Aidin","Zettl, Alex K.;",2016,"Nanoscale materials made of carbon, boron, and nitrogen, namely BCN nanostructures, exhibit many remarkable properties making them uniquely suitable for a host of applications. Boron nitride (BN) and carbon (C) nanomaterials are structurally similar.  The forms studied here originate from a two-dimensional hexagonally arranged structure of sp2 bonded atoms.  These nanomaterials exhibit extraordinary mechanical and thermal properties.  However, the unique chemical compositions of carbon and boron nitride result in differing electrical, chemical, biological, and optical properties.  In this work, we explore the single layer sheets of sp2 bonded carbon (graphene), and their cylindrical forms (nanotubes) of carbon and boron nitride.In the first part of this work, we look at carbon based nanomaterials.  In Chapter 2, the electron field emission properties of carbon nanotubes (CNTs) and their implementation as nanoelectromechanical oscillators in an integrated device will be discussed.  We show a technique hereby a single CNT is attached to a probe tip and its electron field emission characterized.  We then delve into the fabrication of a field emitting CNT oscillator based integrated device using a silicon nitride membrane support.  We then present the electron field emission capabilities of these devices and discuss their potential use for detection of nuclear magnetic resonance (NMR) signals.Graphene is the subject of study in Chapter 3.  We begin by extensively examining the synthesis of graphene using a chemical vapor deposition (CVD) process, ultimately establishing techniques to control graphene domain size, shape, and number of layers.  We then discuss the application of the single-atom thick, but ultra-mechanically strong graphene as a capping layer to trap solutions in a custom fabricated silicon nitride membrane to enable transmission electron microscopy (TEM) of liquid environments.  In this manner, the volume and position of liquid cells for electron microscopy can be precisely controlled and enable atomic resolution of encapsulated particles.In the second portion of this work, we investigate boron nitride nanostructures and in particular nanotubes.  In Chapter 4, we present the successful development and operation of a high-throughput, scalable BN nanostructures synthesis process whereby precursor materials are directly and continuously injected into a novel high-temperature, Extended-Pressure Inductively-Coupled plasma system (EPIC).  The system can be operated in a near-continuous fashion and has a record output of over 35 g/hour for pure, highly crystalline boron nitride nanotubes (BNNTs).  We also report the results of numerous runs exploring the wide range of operating parameters capable with the EPIC system.In Chapter 5, we examine the impurities present in as-synthesized BNNT materials.  Several methods of sample purification are then investigated.  These include chemical oxidation, using both gas and liquid phase based methods, as well as physical separation techniques.The large scale synthesis of BNNTs has opened the door for further studies and applications.  In Chapter 6, we report a novel wet-chemistry based route to fill in the inner cores of BNNTs with metals.  For the first time, various metals are loaded inside of BNNTs, forming a plethora of structures (such as rods, short nanocrystals, and nanowires), using a solution-based method.  We are also able to initiate and observe dynamics of the metallic nanoparticles, including their movement, splitting, and fusing, within a BNNT.",ucb,,https://escholarship.org/uc/item/03m2x0zt,,,eng,REGULAR,0,0
213,1649,Improving Exposure-Response Estimation in Air Pollution Health Effects Assessments,"Beckerman, Bernard Sam","Jerrett, Michael L.B.;",2014,"Of the 3.7 million deaths attributed to outdoor air pollution, ischemic heart disease (IHD) represents 40% of the total deaths, or approximately 1.48 million deaths, which occur mainly in older adults. IHD is the largest single causes of death attributable to ambient air pollution. Research on the progression and incidence of IHD are pointing to ambient fine particulate matter (PM) as a major contributor to morbidity and mortality outcomes. In this context, improvements in air pollution exposure assessment methods and health effects assessments are developed and investigated in this thesis.  With the exposure assessment, methods and tools were created that had utility for improving air pollution exposure assessment. Two exposure assessment chapters are presented. The first of these is focused on the creation of a national-level spatio-temporal air pollution exposure model. In the second exposure chapter, emphasis is placed on the development and evaluation of methods used to estimate annual average daily traffic - a local source of ambient particulates and other air pollutants thought to have heightened toxicity.A model was created to predict ambient fine particulate matter less than 2.5 microns in aerodynamic diameter (PM2.5) across the contiguous United States to be applied to health effects modeling (Chapter 2). We developed a novel hybrid approach that combine a land use regression model (LUR) and Bayesian Maximum Entropy (BME) interpolation of the LUR space-time residuals,. The PM2.5 dataset included observations at 1,464 monitoring locations with approximately 10% of locations reserved for cross-validation across the contiguous United States. In the LUR, variables based on remote sensing estimates of PM2.5, land use and traffic indicators were made available to the Deletion/Substitution/Addition machine learning algorithm used to select predictive models describing local variability in PM2.5.  Two modeling configurations were tested.  The first included all of the available covariates; and the second did not include the remote sensing.  The remote sensing variable was not based on any ground information. Specific results showed that normalized cross-validated R2 values for LUR were 0.63 and 0.11 with and without remote sensing, respectively; suggesting remote sensing is a strong predictor of ground-level concentrations. In the models including the BME interpolation of the residuals, cross-validated R2 were 0.79 for both configurations; the model without remotely sensed data described more fine-scale variation than the model including remote sensing.   Our results suggest that our modeling framework effectively predicts ground-level concentrations of PM2.5 at multiple scales over the contiguous U.S.The network interpolation tool used to estimate traffic is described in Chapter 3.  The program was created using free open-source software, namely Python 2.7 and its related libraries. It was applied to two county study areas in California, USA (Alameda and Los Angeles), where inverse distance weighted (IDW) and kriging annual average daily traffic (AADT) models were estimated.  These estimates were compared to: each other; to an entirely independent dataset; and against a traffic model using similar methods to those used in the traffic estimates employed in the exposure model in Chapter 2.Results show different levels of predictive agreement.  Using cross-validation methods, the R2 for these models were 0.36 and 0.32 in Alameda and 0.46 and 0.47 in Los Angeles, for IDW and Kriging, respectively.  Differences in model performance seen between and within the study area suggest that data issues may have materially contributed; these include: temporal discordance in the measurements and mischaracterization of road types. A comparison of network interpolation methods to those used to estimate traffic in Chapter 2 found the network methods to be superior.For the health effects analysis that that estimated an exposure response curve describing the effect of PM2.5 on ischemic heart disease mortality, monthly ambient PM2.5 estimates (from the model outlined in Chapter 2) were averaged to represent long-term exposure at the home.  Super Learner evaluated 14 models that fell within the classes of parametric, semi-parametric, and non-parametric models.   A generalized additive model with splined terms was identified as being most predictive of life expectancy.  Over the range of exposure 3-27 Âµg/m3 the estimated years of life lost over this interval was 0.6 years.  This relationship, however, was not linear.  It followed the pattern reported in previous studies with increased risk rates at lower exposures and a flattening out of the curve at higher exposures.  An inflection point appeared to occur near 10 Âµg/m3.  These estimates failed to reach significance at the 95% confidence criteria but were close enough to be suggestive of a relationship. Results from a complementary simulation showed that left truncation characteristics of the cohort likely biased to results towards the null.  In addition, the use of inverse probability of censoring weights to control for bias induced by right censoring added variability to the estimator that likely reduced the power to detect and effect.  This research has shown the utility of machine-learning algorithms for improving health effects assessments in the field of air pollution epidemiology. In exposure science, they have proven their utility in creating estimates of exposure that can be used to characterize multiple scales of variability. In health effects assessments, in combination with causal inference methods, this work has shown the utility of these methods to detect non-linear effects in novel parameter estimates in individual cohort studies. In addition to the methodological contribution, the health effects results contribute to the discussion about the burden of disease attributable to particulate matter.",ucb,,https://escholarship.org/uc/item/0416s9hf,,,eng,REGULAR,0,0
214,1650,Charge accommodation dynamics of cluster and molecular anions produced by photo-initiated intracluster charge transfer,"Yandell, Margaret Ashley","Neumark, Daniel M;",2014,"Time-resolved photoelectron imaging spectroscopy is used to examine the dynamics of charge accommodation by solvent species and biomolecules upon photo-initiated intracluster charge transfer.  Excitation of a charge transfer state of an iodide-complexed molecule or cluster with a UV pulse and subsequent interrogation by photodetachment with a lower energy probe enables detection of changes in photoelectron signals over hundreds of femtoseconds.  Velocity map imaging detection permits simultaneous collection of electron kinetic energy (eKE) and photoelectron angular distributions that provide insight into the strength and structure of the association between the cluster or molecule and the excess electron. 	Application of this methodology to iodide-containing clusters of small polar molecules such as water, methanol, and ethanol elucidates the stability and extent of intramolecular forces within a given cluster.  In complexes of iodide with small solvent clusters (â‰¤ 10 molecules), iodide is situated somewhat outside of the solvent network.  Interaction of iodide-water clusters with a UV pulse to produce iodine and a free electron results in the partial solvation of the excess charge through hydrogen bonding interactions over hundreds of picoseconds before electron autodetachment.  In contrast, methanol and ethanol cluster networks can only support the excess charge for tens of ps.  Notably, stable bare water cluster anions have previously been measured with as few as two molecules, while upwards of seventy methanol molecules are necessary to stabilize an excess electron.  Drawing an analogy between electron autodetachment and statistical unimolecular decay, an excited iodide-water cluster with a given number of water molecules might be expected to decay most rapidly given its significantly smaller density of states.  The observation of the opposite pattern, as well as the similarity between iodide-methanol and -ethanol cluster anion lifetimes, suggests that energetics, rather than molecular structure, play a larger role in stabilizing an excess charge to autodetachment.  Applying a thermionic emission model confirms this result.          The dynamics of charge accommodation are also examined for small biomolecules.  Radiative damage to DNA caused by low energy electrons is thought to originate in the attachment of an electron to a nucleobase unit of a nucleotide in the DNA double helix.  Previous experiments have examined binding motifs and fragmentation patterns of transient negative ions (TNIs) of nucleobases using Rydberg electron transfer from excited noble gas atoms or collision of the nucleobase with a beam of electrons of defined energy.  Here, nascent TNIs of the nucleobase uracil are created by intracluster charge transfer from a complexed iodide ion and their decay examined with time-resolved photoelectron imaging.  Anions created with several hundred meV of excess energy appear as valence anions and are observed to decay biexponentially with time constants of hundreds of fs and tens of ps by iodine atom loss and autodetachment.  Repetition of these experiments with uracil molecules methylated at the N1, N3, or C5 positions results in a dramatic reduction of the longer time constant.  The addition of the methyl group may hasten the intramolecular vibrational energy redistribution process preceding autodetachment.          Photoelectron spectroscopy of isolated nucleobase anions has measured only the dipole-bound state (DBS) of the anion consisting of an electron weakly associated with the molecular dipole moment and very delocalized over the molecular structure.  Though the valence anion has not been directly measured, the DBS has been posited to serve as a `doorway' to the valence-bound state (VBS).  Such a mechanism has also been proposed for nitromethane.  In contrast, acetonitrile should only support a DB anion state.  Examination of nascent acetonitrile and nitromethane anions excited near the vertical detachment energies of their corresponding iodide-molecule complexes indeed produces the DB acetonitrile anion, which then decays biexponentially with time constants of few and hundreds of ps by iodine atom loss and autodetachment.  The nitromethane DB anion decays rapidly over hundreds of fs to form the valence anion, which decays biexponentially with time constants similar to those measured for the acetonitrile DB anion.  This study marks the first direct observation of a transition from a dipole-bound anion to a valence anion and will inform future studies of iodide-nucleobase complexes.",ucb,,https://escholarship.org/uc/item/05h0f36c,,,eng,REGULAR,0,0
215,1651,Decolonizing the White Colonizer?,"Lucas, Cecilia Cissell","Baquedano-Lopez, Patricia;",2013,"This interdisciplinary study examines the question of decolonizing the white colonizer in the United States. After establishing the U.S. as a nation-state built on and still manifesting a colonial tradition of white supremacy which necessitates multifaceted decolonization, the dissertation asks and addresses two questions: 1) what particular issues need to be taken into account when attempting to decolonize the white colonizer and 2) how might the white colonizer participate in decolonization processes? Many scholars in the fields this dissertation draws on -- Critical Race Theory, Critical Ethnic Studies, Coloniality and Decolonial Theory, Language Socialization, and Performance Studies -- have offered incisive analyses of colonial white supremacy, and assume a transformation of white subjectivities as part of the envisioned transformation of social, political and economic relationships. However, in regards to processes of decolonization, most of that work is focused on the decolonization of political and economic structures and on decolonizing the colonized. The questions pursued in this dissertation do not assume a simplistic colonizer/colonized binary but recognize the saliency of geo- and bio-political positionalities. As a result of these different positionalities, white U.S. citizens committed to participating in our own decolonization and in the decolonization of our (social, political, educational, and economic) structures and relationships with others must learn from but cannot simply imitate or appropriate decolonial methodologies developed by indigenous people and people of color.The title of this dissertation posits decolonization as an active ongoing process (through the use of the verb-form, i.e. ""decolonizing"") without guarantees (through the use of the question mark). Each chapter addresses a different yet interrelated aspect of this process:Chapter One intervenes in the reconstructionism versus abolitionism debate in Whiteness Studies, and offers p/reparations as a framework for redistributory practices and (inter)personal transformation and as a methodology through which the white colonizer might contribute to racial justice and decolonization projects. P/reparations processes are open-ended and include apologies, material and cultural redress, and structural change to ensure non-recurrence. By highlighting historical and contemporary processes of accumulation by dispossession, p/reparations processes emphasize interconnectedness and challenge the illusion of autonomous individuals, groups and nation-states. Thus, a p/reparations framework intervenes into discourses of meritocracy and equal opportunity; denaturalizes notions of citizenship, immigration, and the borders of nation-states; and provides counter-narratives to discourses of aid and charity which assume the assets being redistributed were legitimately acquired and that acts of redistribution should thus be met with gratitude. Chapter Two examines the ways in which the geographical control of bodies has been a key technology of white supremacist colonialism. Given the entanglement of geographical (im)mobility with social (im)mobility and an unequal racialized distribution of premature death, decolonization and the dismantling of white supremacy necessitates not only the redistribution of political and economic resources but divesting from U.S.-ness itself. As such, decolonization requires not only white abolitionism but also U.S.-abolitionism. This chapter interrogates the use of the trope of ""the criminal"" by both the nation-state and the prison industrial complex, and the ways in which these discourses are mobilized as threats to the white colonizer's ""home."" As such, this chapter argues that, for the white colonizer, one aspect of decolonization may require developing a relationship to home as a foreign concept as well as (in many cases) pursuing downward rather than upward mobility.Chapter Three suggests power-conscious hybridity as a technology the white colonizer can employ in the face of this challenge of needing to claim whiteness and U.S.-ness even as we seek to participate in their abolition. Hybridity emphasizes that no one is reducible to any particular ""identity."" In order not to disappear into colorblind ""humanness,"" engage in cultural appropriation, and/or revalorize whiteness, however, the white colonizer's employment of hybridity must simultaneously involve (de)facing whiteness. (De)facing implies a double movement: facing whiteness, in all of its horror, without resorting to white flight; and defacing whiteness, both in the sense of destroying it and in the sense of de-facing it, i.e. undoing the notion that whiteness is human.Chapter Four examines issues of pedagogy and curricula inside and outside the classroom as they pertain to processes of recreating and transforming colonial white supremacy. This chapter critiques discourses of ""equality of opportunity"" as a primary ideological mechanism supporting colonial white supremacy in the current age of colorblind racism. Through participant-observation of two different attempts at ""social justice"" schooling (one at the high school level, one at the college level), it examines the creation of what Michel Foucault calls ""docile bodies,"" and draws on pedagogies from theater as possibilities for cultivating counter-disciplines of the body. This chapter ends with a list of specific skills the white colonizer needs to learn for the purpose of decolonization. ""Chapter"" Five attempts to ""practice what I preach"" (in particular in relation to the colonial white supremacy institutionalized as epistemological hierarchies in the academy) by revisiting the topics of this dissertation in a live performance. This theoretical and methodological intervention enacts a response to critiques of the mind/body split in colonial epistemologies, and positions performance as analysis which must be engaged on its own terms -- rather than only as a methodology or phenomenon that is then analyzed in writing. This is also a pedagogical intervention which insists on the importance and legitimacy of multiple modalities of communication beyond writing within academia, and seeks to make academia feel accessible to a wider range of people with a range of learning and teaching styles.The Inconclusion addresses the question of why the white colonizer would want to decolonize. It argues that the prerequisite for wanting to decolonize is recognizing oneself as colonizer and all beings as interconnected. Then decolonization becomes not so much a choice as a spiritual--which is also to say political--imperative. As such, this dissertation argues not only against the mind/body split, but also against the mind/body/soul split by emphasizing the importance of politicizing and embodying spirituality and infusing political movements with spiritual convictions.",ucb,,https://escholarship.org/uc/item/06g372z8,,,eng,REGULAR,0,0
216,1652,A Utility-Theory-Consistem System-of-Demand-Equations Approach to Household Travel Choice,"Kockelman, Kara Maria",,1998,"Modeling personal travel behavior is complex, particularly when one tries to adhere closely to actual casual mechanisms while predicting human response to changes in the transport environement. There has long been a need for explicitly modeling the underlying determinant of travel- the demand for participation in out-of-home activities; and progress is being made in this area, primarily through discrete-choice models coupled with continous-duration choices. However, these models tend to be restircted in size and conditional on a wide variety of other choices that could be modeled more endogenously.Â ",ucb,,https://escholarship.org/uc/item/06x0k5r4,,,eng,REGULAR,0,0
217,1653,"""Vous Ãªtes hombre de bienâ€: A study of bilingual family letters to and from colonial Louisiana, 1748-1867","Thomas, Jenelle","McLaughlin, Mairi;",2017,"In this dissertation, I use a bilingual epistolary corpus to examine the interaction of language contact, language- and genre-specific conventions, and speakersâ€™ individual communicative strategies. Although multilingualism was a common historical condition, many traditional language histories and studies of historical speech only consider monolinguals, or bilingualism as it affects a particular language, rather than seeing multilingualism as part of an individual and community repertoire. In contrast, this study uses both quantitative and qualitative methods to examine the linguistic practices of a bilingual network of speakers in both of their languages. The corpus analyzed here is a collection of private letters which I selected and transcribed from the family correspondence of Francisco Bouligny, a soldier and military governor in colonial New Orleans. These letters were written in both French and Spanish as the family and acquaintances corresponded between New Orleans, France, and Spain for a period spanning from the mid-18th to the mid-19th century.I address the following research questions through case studies at three levels: phonological/orthographic, morphosyntactic, and pragmatic.(1)	In what way(s) does bilingualism affect language usage? How do bilingual and monolingual usage differ?(2)	How does bilingualism interact with other factors such as convention, genre, audience, and stance, and how do bilingual writers use the resources of their two languages in their production?In Chapter 3, I find widespread orthographic variation conditioned by factors such as age and geographic location. However, I argue for the inclusion of language-specific education and literacy as an additional factor in variation, as I find that the written standard can obscure variation and contact effects while also serving as a resource for bilinguals when writing in a second language. Chapter 4 addresses the contrast between the complex and simple past tenses in both Spanish and French. As the monolingual patterns of use in each language diverge, I find that bilinguals as a group do not follow monolinguals in showing an increased use of the complex past in French but not Spanish. Although individual patterns vary greatly, bilinguals use the French complex past overall less frequently than monolinguals, arguably because of the restraining influence of Spanish contact. This contact influence can be seen in discourse-pragmatic uses of the two tenses, used to shape the narrative or create temporal contrast. In Chapter 5, I consider the construction of identity and interpersonal relationships in bilingual correspondence through choice of language, forms of address, and expressions of sincerity. I find that speakers choose and continue to use only one language with their addressees, even when both speakers are bilingual, and that this choice is motivated largely by the characteristics of the addressee. The choice of second person pronouns (T or V) patterns fairly rigidly according to language and familial relationship, but I argue that speakers vary opening formulae to express more subtle distinctions in the intimacy of the relationship. Some speakers similarly appeal to existing formulae to convince the addressee of their sincerity, but this can be shown to be particularly true of less literate or second-language speakers, while other speakers eschew explicit mentions of sincerity for other strategies. Overall, although speakers show awarenes of epistolary norms in each language and in many ways adhere to language-specific practices, they also manipulate these conventions in meaningful ways. This study is the first to delve into issues of historical bilingualism through a balanced bilingual epistolary corpus and one of few to explore language use in Spanish Louisiana. I find that individual speakers show evidence of using resources from their bilingual repertoire to aid in composition and construction of meaning in various ways. This is particularly true of loci of variation in the individual language systems or structures that share typological similarities across the systems. However, at the community level the variety of individual patterns and the force of the monolingual norms and written formulae appear to inhibit the spread of change. I underscore the importance in future studies of considering a speakerâ€™s (and communityâ€™s) language use in its entire context, including other languages, literacies, and considerations of genre, as we explore how speakers use the available resources for interpersonal communication and how that translates to the community level.",ucb,,https://escholarship.org/uc/item/07b228j2,,,eng,REGULAR,0,0
218,1654,The Hecke Stability Method and Ethereal Forms,"Schaeffer, George Johann","Venkatesh, Akshay;",2012,"The purpose of this thesis is to outline the Hecke stability method (HSM), a novel method for the computation of modular forms.The HSM relies on the following idea: A finite-dimensional space of ratios of modular forms that is stable under the action of a Hecke operator should consist of modular forms (i.e., without poles). This principle is correct over the complex numbers, but more care is required over finite fields due to complications arising near the supersingular points on modular curves. Formalizing this main idea as a theorem comprises most of our theoretical work.Though it can be utilized in a variety of settings, the main application of the Hecke stability method is the computation of weight 1 modular forms. These spaces cannot be computed using the algorithms (e.g., modular symbols algorithms) that are typically employed to compute modular forms of higher weight.Furthermore, to provide a complete picture of the weight 1 modular forms of level N, we must account for certain sporadic discrepancies between the space of classical forms and the space of mod p modular forms. Ultimately, our approach is motivated by the effect this ""ethereality"" phenomenon may have on the statistics of number fields via the theory of modular Galois representations.",ucb,,https://escholarship.org/uc/item/07n8235q,,,eng,REGULAR,0,0
219,1655,Ca Isotopes in Igneous and High-Temperature Metamorphic Systems and the Hydrothermal Chemistry of Paleoseawater,"Antonelli, Michael A","DePaolo, Donald J;",2018,"In the last two decades it has been increasingly recognized that Ca isotope fractionation at high-temperatures can greatly exceed fractionation at surface conditions. These effects have been used to trace recycling of crustal materials into the mantle, to estimate equilibration temperatures in mantle rocks, and to understand kinetic isotope fractionation during diffusion in experimental melts. However, little is known about the competition between kinetic and equilibrium Ca isotope effects in natural samples, which suggests that the use of stable Ca isotope variations as a proxy for carbonate recycling (or for mantle temperature) is significantly underdetermined.The Ca isotope evolution of continental crust, which is currently understudied, is intimately linked to the geochemical evolution of Earthâ€™s mantle, oceans, and atmosphere. As such, this dissertation focuses dominantly on understanding radiogenic, kinetic, and equilibrium Ca isotope variations in igneous and metamorphic rocks from the continental crust, but also includes radiogenic Sr isotope models of hydrothermal circulation at mid-ocean ridges, which are critical to understanding continental weathering rates and the geochemical evolution of seawater over time.Chapter 2 explores the potential feed-back between the chemical and isotopic composition of paleoseawater and high-temperature hydrothermal fluids, which has been implied by Sr-isotope measurements in hydrothermal minerals over time (Antonelli et al., 2017). Chapter 3 presents results from radiogenic Ca isotope measurements in lower-crustal rocks and minerals, confirming that K is effectively lost from the lower crust during high-T metamorphism (Antonelli et al., 2018). Chapter 4 focuses on stable Ca isotope measurements in lower-crustal rocks and minerals, demonstrating that kinetic Ca effects are abundant in nature, both at the whole-rock and inter-mineral scales, and that they can be used to understand paragenesis and Ca diffusion in lower-crustal rocks (Antonelli et al., 2019b). Chapter 5 presents stable Ca isotope results from volcanic and sub-volcanic rocks and minerals, which imply that Ca isotopes can be used to estimate crystal growth-rates in igneous crystals (e.g. phenocrysts, comb-layers, and orbicules) and that associated volcanic eruption/recharge events are generally short-lived (Antonelli et al., 2019a).",ucb,,https://escholarship.org/uc/item/0825f42v,,,eng,REGULAR,0,0
220,1656,Engineering Magnetoelectric-Multiferroic Composites Using FeRh,"Clarkson, James David","Ramesh, Ramamoorthy;",2016,"This thesis presents the study of magnetoelectricity in artificial multiferroic heterostructures, focussing on the FeRh metamagnetic phase transition. Spin based electronic systems are ideal for many applications, primarily as a result of long term stability. Toward this goal, there is a need for electrically modifiable and differentiable magnetic states. First, new methodologies to determine and control the antiferromagnetic axis in the low temperature  antiferromagnetic phase of FeRh are considered. Second, In an effort to increase the ability to control magnetism with an applied electric field through the converse magnetoelectric effect, FeRh is combined with piezoelectric materials. In association with the large volumetric magnetostriction coefficient of FeRh, a composite multiferroic with the largest magnetoelectric coupling is observed. Finally, the out-of-plane magnetic anisotropy is investigated as a function of the ferromagnetic phase fraction and epitaxial strain. This is applied toward investigation of zero magnetic field magnetoelectric control of the magnetic phase and the apparent memory of the magnetization direction across repeated phase reversal.",ucb,,https://escholarship.org/uc/item/09979501,,,eng,REGULAR,0,0
221,1657,Influencing cell fate decisions using physical and chemical cues,"Sia, Junren","Li, Song;",2016,"Directed genetic reprogramming of cells from one identity to another offers tremendous potential in regenerative medicine, disease modelling and drug testing.  However, its application is limited by the low efficiency at which it occurs, and existing methods to improve efficiency mostly utilize additional molecular biology and biochemical manipulations. This thesis explored an alternative paradigm for improving reprogramming efficiency: presentation of physical cues. To this end, I first showed that simply agitating an adherent culture with an orbital shaker enhanced its efficiency of reprogramming to induced pluripotent stem cells (iPSCs). I further demonstrated that convective mixing of the culture medium by orbital agitation blunted the upregulation of CDK inhibitor p57/Kip2 that was caused by the culture becoming overconfluent, which in turn enhanced the efficiency of reprogramming to iPSCs. Next, I showed that culturing reprogramming cells on solid supports scored with microgrooves enhanced their reprogramming into cardiomyocytes. I demonstrated that the microgrooves caused upregulation of the activity of the transcription factor megakaryoblastic leukemia-1 (Mkl1) / myocardin-related transcription factor A (Mrtf-a) and also enhanced organization of sarcomeric structure, with both effects contributing to better reprogramming efficiency. In addition to physical cues, I also explored whether treatment with only small molecules could reprogram fibroblasts into skeletal muscle cells. Indeed, I found that an optimized basal medium (10% FBS in DMEM with 50 Î¼g/ml of ascorbic acid and 50 ng/ml of basic fibroblast growth factor (bFGF)) containing just 2 small moleculesâ€” 616452 [an inhibitor of the protein kinase activity of the transforming growth factor-beta (TGF-Î²) type I receptor (R1)] and forskolin (a plant diterpene that stimulates adenylyl cyclase and elevates the intracellular level of 3',5'-cyclic-AMP)â€”was sufficient to achieve reprogramming at high efficiency. In summary, this thesis described how both physical and chemical cues can contribute to enhancing the reprogramming of cell identity.",ucb,,https://escholarship.org/uc/item/0bq39484,,,eng,REGULAR,0,0
222,1658,Synthesis of Doped Graphene Nanoribbons from Molecular and Polymeric Precursors,"Cloke, Ryan","Fischer, Felix R;",2015,"Abstract Synthesis of Doped Graphene Nanoribbons from Molecular and Polymeric Precursorsby Ryan Randal ClokeDoctor of Philosophy in ChemistryUniversity of California, Berkeley Professor Felix Fischer, Chair As electronic devices continue to shrink and energy problems continue to grow, nanoscale materials are becoming increasingly important.  Graphene is a material with exceptional promise to complement silicon in next-generation electronics because of its extraordinary charge carrier mobility, while also finding a role in cutting-edge energy solutions due to its high surface area and conductivity.  Improving on this material even further by reducing the width of graphene to nanoscale dimensions with atomically-precise dopant patterns is the subject of this thesis.  Nanometer-wide strips of graphene, known as graphene nanoribbons (GNRs), offer the advantages of semiconducting behavior, combined with more accessible surface area compared to bulk graphene (Chapter 1).  Additionally, it is demonstrated that GNRs can be doped with atomic precision, allowing for intricate modulation of the electronic properties of this material, which was studied by STM, STS, and nc-AFM (Chapter 2).  Controlled growth of GNRs on surfaces is still an outstanding challenge within the field, and to this end, a variety of porphyrin-GNR template materials were synthesized (Chapter 3).  The GNRs obtained in this work were also synthesized in solution, and it was shown that these materials possess excellent properties for applications in hydrogen storage, carbon dioxide reduction, and Li-ion batteries (Chapter 4).  A prerequisite for solution-synthesized GNRs, conjugated aromatic polymers are an important class of materials in their own right.  Therefore, Ring-Opening Alkyne Metathesis Polymerization was developed using conjugated, strained diynes (Chapter 5).  The resulting conjugated polymers were explored both for their own materials properties due to a remarkable self-assembly process that was discovered, and also as precursors to GNRs (Chapter 6).  This work advances the fundamental understanding of carbon-based nanostructures, as well as the large-scale production of GNRs for next-generation energy and electronics applications.",ucb,,https://escholarship.org/uc/item/0cm8d86r,,,eng,REGULAR,0,0
223,1659,"Approximate counting, phase transitions and geometry of polynomials","Liu, Jingcheng","Sinclair, Alistair;",2019,"In classical statistical physics, a phase transition is understood by studying the geometry (the zero-set) of an associated polynomial (the partition function).  In this thesis, we will show that one can exploit this notion of phase transitions algorithmically, and conversely exploit the analysis of algorithms to understand phase transitions.  As applications, we give efficient deterministic approximation algorithms (FPTAS) for counting $q$-colorings, and for computing the partition function of the Ising model.",ucb,,https://escholarship.org/uc/item/0f2195k5,,,eng,REGULAR,0,0
224,1660,Decoupled Vector-Fetch Architecture with a Scalarizing Compiler,"Lee, Yunsup","Asanovic, Krste;",2016,"As we approach the end of conventional technology scaling, computer architects are forced to incorporate specialized and heterogeneous accelerators into general-purpose processors for greater energy efficiency.  Among the prominent accelerators that have recently become more popular are data-parallel processing units, such as classic vector units, SIMD units, and graphics processing units (GPUs).  Surveying a wide range of data-parallel architectures and their parallel programming models and compilers reveals an opportunity to construct a new data-parallel machine that is highly performant and efficient, yet a favorable compiler target that maintains the same level of programmability as the others.In this thesis, I present the Hwacha decoupled vector-fetch architecture as the basis of a new data-parallel machine.  I reason through the design decisions while describing its programming model, microarchitecture, and LLVM-based scalarizing compiler that efficiently maps OpenCL kernels to the architecture.  The Hwacha vector unit is implemented in Chisel as an accelerator attached to a RISC-V Rocket control processor within the open-source Rocket Chip SoC generator. Using complete VLSI implementations of Hwacha, including a cache-coherent memory hierarchy in a commercial 28 nm process and simulated LPDDR3 DRAM modules, I quantify the area, performance, and energy consumption of the Hwacha accelerator.  These numbers are then validated against an ARM Mali-T628 MP6 GPU, also built in a 28 nm process, using a set of OpenCL microbenchmarks compiled from the same source code with our custom compiler and ARM's stock OpenCL compiler.",ucb,,https://escholarship.org/uc/item/0fm0z48h,,,eng,REGULAR,0,0
225,1661,Creating Community Among Leaders: Leveraging Shared Practices for School Improvement,"Williams, Sarah June","Gifford, Bernard;",2017,"Principals work within a complex context where multiple stakeholders make many competing demands of them. Chief among these demands includes district initiatives, which serve to create leadership expectations but often do not contain clear methods or practices for implementation. Additionally, demands of the local community and interests of teachers and students create layers of complexity which can confound and isolate leaders. While principals may feel cut off from their peers dealing with these intricacies, the reality is, regardless of initiative or priority, principals have many common problems of practice. Establishing highly effective teacher collaborative groups, is an example of an implementation most principals come to face.The Early Release Wednesday Toolkit was developed to support leadership practices for implementing highly effective collaborative teacher groups. To create the toolkit, a sample of principals engaged in a co-development process to capture effective practices already in place, and share them with the larger principal group. In so doing, the principal Community of Practice was strengthened. The findings of this study suggest that principals gain from relying on each other for problem solving where their leadership is concerned, and may serve to inform other leaders about effective ways to learn from one another. This design study is centered on action research and includes two primary research elements, evaluation of the design outcome and assessment of the design process.",ucb,,https://escholarship.org/uc/item/0gq200m9,,,eng,REGULAR,0,0
226,1662,Promoting Learning of Instructional Design via Overlay Design Tools,"Carle, Andrew Jacob","Canny, John;",2012,"Design is a notoriously difficult profession to practice, and it is even more difficult to learn.  Traditionally, learning of design skills has been situated in the context of apprenticeships or formal design studios.  Unfortunately, these methods are inaccessible to practicing professionals due to constraints on time and location.  And, indeed, professional designers must continuously update their knowledge as paradigm shifts in design practice threaten to make their skills obsolete.  An ideal resolution to this problem is to situate the learning of design skills within the professional practice of design.  This dissertation studies an approach to this mode of situated learning, focusing on integrating learning mechanisms into practical design tools.  These tools provide scaffolding for novices as they construct an understanding of best practices in design while engaging in real design work.I begin by introducing Virtual Design Apprenticeship (VDA), a learning model â€” built on a solid foundation of education principles and theories â€” that promotes learning of design skills via overlay design tools.  In VDA, when an individual needs to learn a new design skill or paradigm she is provided accessible, concrete examples that have been annotated with design rationale.  These annotations make expert thinking visible and allow the novice to immediately use, and gradually understand, new best practices.  By combining abstract rationale with concrete design instances, annotated artifacts become more useful than either could be alone.  I describe the essential components of the VDA framework: annotated design artifacts, a repository of carefully chosen annotated examples, and a community of experts and learners.  I walk the reader through an example of how VDA scaffolds learners as they move from a novice's understanding of a design space towards that of an expert.  Within the context of this example, I present a set of design principles that guide the creation of VDA design tools â€” user interfaces built to mediate an individual's interactions with the three core VDA components.While VDA is applicable to most design fields, I narrow the scope of consideration to one particular domain of design by focusing in-depth on the instructional design difficulties that university-level faculty members face and how the VDA approach can address them.  These instructors face precisely the type of design paradigm shift that VDA was developed to ease as they attempt to move away from traditional, lecture-based pedagogical methods and towards more modern, learner-centered techniques.I engaged with these instructors and a curriculum design research group in a six-year period of contextual inquiry.  Findings from this study influenced my formulation of the VDA framework and the design of PACT, a design tool that leverages the learning principle of making thinking visible to assist novices as they transition from concrete to abstract reasoning about curriculum design.  The central focus of PACT is the incorporation of annotated references to pedagogical design patterns â€” abstract representations of best practices in instructional design.  I discuss the iterative design and implementation of PACT in detail, highlighting the ways in which it embodies the VDA design principles for promoting learning of instructional design via overlay design tools.  Next, I study the challenges of converting abstract best practices and design patterns into concrete annotations that can be applied directly to content.  My solution, the PACT Annotation Schema, is a formal mechanism for generating tags and pattern annotations from freeform pattern text.  Formal representations of patterns are far more useful than generic references, both as scaffolds for learning and for structuring user interactions with design artifacts.  Using this schema, I have generated the PACT Annotation Library, a collection of 56 tags and 74 pattern annotations based on the work of the Pedagogical Patterns Project.  Visual representations of these formal annotations are the centerpiece of PACT's user interface.The PACT tool was evaluated in two distinct stages.  First, I present a formative study conducted with early, prototype versions of the PACT tool.  This study examines the utility of PACT for expert curriculum designers and curriculum research groups, using a sample annotation process â€” and reflection on the outcomes of that process â€” to demonstrate that my approach is feasible and useful for those groups. I then present a summative user study of the utility of PACT for novice learner-centered curriculum designers.  I demonstrate PACT's significant impact on how novice designers learn from expert-generated examples, how they perceive the credibility of those examples, and the quality of curriculum designs those novices can produce.  These findings show that the VDA approach to learning works and that the PACT overlay curriculum design tool is a successful realization of VDA's design principles.Last, I discuss future directions for this work.  PACT is a fully developed design tool that can and should be used by curriculum designers as they create new courses and build their own understanding of the principles of learner-centered design.  The PACT Annotation Schema is a useful mechanism that can be further improved to allow the generation of more accurate and complete annotations based on design patterns.  The PACT Annotation Library should be continuously expanded as new patterns and principles are developed.  Finally, the Virtual Design Apprenticeship model for learning is a robust and highly-principled approach to integrating design learning and design practice.  It is applicable across a wide range of design domains and can help promote learning of design skills in them all.",ucb,,https://escholarship.org/uc/item/0hh0r9rp,,,eng,REGULAR,0,0
227,1663,Mass Transport of Condensed Species in Aerodynamic Fallout Glass from a Near-Surface Nuclear Test,"Weisz, David Gabriel","van Bibber, Karl;Knight, Kim;",2016,"In a near-surface nuclear explosion, vaporized device materials are incorporated into molten soil and other carrier materials, forming glassy fallout upon quenching. Mechanisms by which device materials mix with carrier materials have been proposed, however, the specific mechanisms and physical conditions by which soil and other carrier materials interact in the fireball, as well as the subsequent incorporation of device materials with carrier materials, are not well constrained. A surface deposition layer was observed preserved at interfaces where two aerodynamic fallout glasses agglomerated and fused. Eleven such boundaries were studied using spatially resolved analyses to better understand the vaporization and condensation behavior of species in the fireball. Using nano-scale secondary ion mass spectrometry (NanoSIMS), we identified higher concentrations of uranium from the device in 7 of the interface layers, as well as isotopic enrichment (>75% 235U) in 9 of the interface layers. Major element analysis of the interfaces revealed the deposition layer to be chemically enriched in Fe-, Ca- and Na-bearing species and depleted in Ti- and Al-bearing species. The concentration profiles of the enriched species at the interface are characteristic of diffusion. Three of the uranium concentration profiles were fit with a modified Gaussian function, representative of 1-D diffusion from a planar source, to determine time and temperature parameters of mass transport. By using a historical model of fireball temperature to simulate the cooling rate at the interface, the temperature of deposition was estimated to be âˆ¼2200 K, with 1Ïƒ uncertainties in excess of 140 K. The presence of Na-species in the layers at this estimated temperature of deposition is indicative of an oxygen rich fireball. The notable depletion of Al-species, a refractory oxide that is highly abundant in the soil, together with the enrichment of Ca-, Fe-, and 235U-species, suggests an anthropogenic source of the enriched species, together with a continuous chemical fractionation process as these species condensed.",ucb,,https://escholarship.org/uc/item/0hp34897,,,eng,REGULAR,0,0
228,1664,Pattern Matching for Advanced Lithographic Technologies,"Rubinstein, Juliet Alison","Neureuther, Andrew R;",2010,"This dissertation extends fast-CAD kernel convolution methods for the identification of unintended effects in optical lithography, including OPC-induced sensitivities, high-NA and polarization vector effects.  A more accurate through-focus physical model is incorporated, and the application of layout decomposition guidance for double patterning is demonstrated.  All layout regions react differently to lithographic processes such as aberrations, and the vulnerabilities are non-intuitive and hard to capture with design rules.  The pattern matcher is a fast tool, developed by Gennari and Neureuther, for quickly scanning layouts to find vulnerabilities to unintended effects of lithographic processes.  Kernel convolutions are performed between Maximum Lateral Test Patterns (MLTPs) and mask layouts, and are over a factor of 104 times faster than rigorous simulation.  Challenges faced in pattern matching extensions include MLTP derivation, edge movement prediction, defocus accuracy improvement, and integration of image effect estimation into real-time guidance for layout decomposition.As motivation for why variability and yield are important, a study is presented in which a probabilistic distribution of transistor Critical Dimensions (CD) is generated given a focus-exposure joint distribution.  An interpolation model is used to generate CD response surfaces, producing a fast method for the analysis of average CD variation for each transistor, the spread of individual variations, the OPC performance, the Across Chip Linewidth Variation (ACLV), and yield distribution.  This study motivates the importance of understanding the variability in a layout; the remainder of the dissertation demonstrates how pattern matching can provide a fast approximation to variability due to lithographic effects.MLTPs, derived as the inverse Fourier Transform of the Zernike polynomials, are the theoretically most sensitive patterns to lens aberrations. As well as being used as input to the pattern matcher, MLTPs can also be etched onto a mask to function as aberration monitors.  However, MLTPs are inherently very costly and unfriendly for mask manufacturing, due to round edges and touching phases.  Both a mask-friendly handmade pattern and an automated method of monitor modification are presented. The handmade pattern retains 68% of its sensitivity to defocus and orthogonality to other aberrations, and the automatically generated pattern passes all DRC checks with only minimal modifications.Use of the pattern matcher on pre-OPC layouts admits the identification of problematic hot-spots earlier in the design flow.  Several studies are presented on the effects of different OPC algorithms on match factors.  In most cases, match factors do not vary significantly between the pre-OPC layout and the post-OPC layout, and the pre-OPC match factor is a good indicator for the sensitivity of the post-OPC layout area.  However, in some circumstances, especially when SRAFs are present, the pre- and post-OPC match factors can vary by a larger amount.  It is shown that defocus and proximity sensitivities occur in different locations on a layout, and if OPC targets the best-case simulation, then it is possible for OPC to worsen sensitivities to aberrations.  As a consequence, pattern matching should be used on post-OPC layouts to check for any created sensitivities.Extensions of the pattern matcher are presented for high-NA and polarization vulnerabilities.  This involves the generation of three to five match patterns for either on-axis or off-axis illumination, with a vulnerability score being calculated as a weighted sum of the match factors.  The patterns are tested against simulation, and found to be good predictors of vulnerability to high-NA and polarization vector effects.  High-NA and polarization vector effects are significant, causing intensity changes of 40% or 10% respectively for the on-axis case, and 8% for the off-axis case.The accuracy of the pattern matcher is evaluated, and improved.  A method for predicting edge movement through coma, rather than just change in intensity, takes the image slope into account and improves the R2 from 0.73 to 0.95.  A major contribution of this dissertation is the improvement of the pattern matching model for defocus.  A quadratic model for defocus is presented, using both the Optical Path Difference (OPD) and OPD2, rather than just the linear term.  OPD2 expands to yield two new patterns, Z0 and Z8, to be used in addition with Z3, which is derived from the OPD term.  Using the three match patterns, prediction of the change in intensity through focus improves from completely non-predictive to an R2 value of 0.92.  Results show that the Z3, pattern and a combined Z0 and Z8,  pattern predict change in intensity through defocus at line ends with an R2 of 0.96, indicating that two match factors with algebraic weighting factors are likely possible.  These results are of great importance, as defocus is not a small aberration, reaching typical values of nearly one Rayleigh unit, and the ability to find defocus-induced hot-spots is of practical interest.Double patterning is identified as an emerging technique that benefits from the application of pattern matching. In double patterning, a layout is split into two masks, each mask being exposed separately, effectively doubling the pitch.  A process flow is presented showing that pattern matching can add value both within the double patterning decomposition algorithm, and also on the post-decomposition layout.  Pattern matching is tested on post-decomposition layouts, showing that in one particular case using complementary dipole illumination, the match factors for coma are increased significantly on the post-decomposition layout.  In another case for annular illumination, introducing an extra split is shown to reduce the variability through coma, and reduces the match factor by 55%.  Furthermore, when splitting an H-structure, a number of different splits are scanned by the pattern matcher, and the split with the lowest intensity change through defocus (which was two thirds smaller than the largest change) is correctly identified.  These examples show that the pattern matcher is an appropriate tool for double patterning, that can quickly provide a measure of intensity change through defocus during the layout decomposition process.",ucb,,https://escholarship.org/uc/item/0hx624s1,,,eng,REGULAR,0,0
229,1665,"The New Interculturalism: Race, Gender and Immigration in Post-Celtic Tiger Ireland","McIvor, Charlotte Ann","Glazer, Peter R;Steen, Shannon;",2011,"""There are wonders that I want to perform"" says the name of Ireland's first African-Irish theatre company, Arambe Productions, which derives from the Nigerian saying ara m be ti mo fe da. The company performs stories of the African-Irish community, yet their dramatizations ponder a larger reality of an Ireland that has gone from a country of emigrants to a nation re-shaped by inward-migration. The sudden shifts brought on by the mid-1990s Celtic Tiger economic boom and unprecedented immigration have plunged the Irish population at large into a state of wondering. What does it mean that the non-Irish born population residing in the Republic grew from less than 5% to more than 12% in a little over a decade?  How will Ireland model a vision of interculturalism that avoids the failures of multiculturalism in Western Europe and the U.S.?  How have race and gender created a hierarchy amongst migrant communities and subjects?  Through performance, Arambe Productions transforms such wondering into a process of ""working together,"" signaling a second meaning of the company's name: harambee in Swahili means ""work together.""  The company's collective labors aim to create a post-Celtic Tiger intercultural vision of Irish identity and belonging.  But can this vision be performed into existence?My dissertation project, ""Performing the `New Irish': Race, Gender, and Interculturalism in the Post-Celtic Tiger Nation,"" argues that performance is at the center of conceptualizing interculturalism as social policy, philosophy and aspiration in contemporary Ireland.  While some might see interculturalism as referring to two cultures meeting in the moment of performance, I argue, rather, that in Ireland today, the term refers to the process of inventing a new pluralistic Irish identity, one that accommodates Irish-born as well as migrant communities.  Irish interculturalism connotes practical policy measures regarding integration, access to social benefits and services, and public eduction about racism, but it also translates into cultural initiatives that stress the arts as a zone of contact between diverse populations.  My research examines theatres, public festivals and arts/social organizations that make use of performance to theorize interculturalism as embodied practice.  Theatre companies like Arambe, Camino de Orula Productions, Calypso Productions, and NGOs like Spirasi, Migrant Rights Centre Ireland, and the Forum on Migration and Communication bid for cultural recognition of minority groups through performance, arts, and media activism. These efforts are endorsed by diverse governmental and non-governmental bodies, which range from the Office of the Minister of Integration, the now-defunct National Consultative Committee on Racism and Interculturalism, to the Irish government Task Force on Active Citizenship.  The diverse sponsors and forums for these projects, however, generate tension between state-managed visions for interculturalism and the goals of community-based or non-governmental groups advocating for an interculturalism from below which remains critical of the Irish state's treatment of minority groups and management of inward-migration more generally.     My investigation of the interplay between social and aesthetic theories of interculturalism exposes the embodied challenges of analyzing relationships between the Irish state, minority communities and the nation at large.  Using ethnographic methods, I position performance as the crucible in which Irish theories of interculturalism are tested and reimagined through the work of bodies who must bear the labor of social change.  I trace the struggles to craft an analytical language around race and ethnicity in Ireland frames these projects, and how the intersection of gender with these former categories complicates this task. My sites range from the Abbey Theatre stage to the Migrant Rights Center's photography exhibit by domestic workers and the Dublin St. Patrick's Festival Parade in order to capture the diversity of venues in which performing bodies are called upon to embody post-Celtic Tiger social change.  My case studies interrogate whether these projects have the power to push against material limits of social access, paths to citizenship and racism/discrimination and reveal that these performances frequently reinscribe relationships of power between minority and Irish-born communities by falling back on top-down models of interculturalism.  Perhaps it is through the reiterative power of performance that the wonders of an egalitarian Irish interculturalism can come into being, but these moving bodies must first be situated in broader matrixes of power which index the role of race and gender in shaping the future of post-Celtic Tiger Irish identities.",ucb,,https://escholarship.org/uc/item/0k75g3rv,,,eng,REGULAR,0,0
230,1666,"Competing Visions of the Modern: Urban Transformation and Social Change of Changchun, 1932-1957","Liu, Yishi","AlSayyad, Nezar;Yeh, Wen-hsin;",2011,"Examining the urban development and social change of Changchun during the period 1932-1957, this project covers three political regimes in Changchun (the Japanese, the Nationalist, and the Communist), and explores how political agendas operated and evolved as a local phenomenon in this city.  I aim to reveal connections between the colonial past and socialist ""present"". I also aim to reveal both the idiosyncrasies of Japanese colonialism vis-Ã -vis Western colonialism from the perspective of the built environment, and the similarities and connections of urban construction between the colonial and socialist regime, despite antithetically propagandist banners, to unfold the shared value of anti-capitalist pursuit of exploring new visions of and different paths to the modern.  The first three chapters relate to colonial period (1932-1945), each exploring one facet of the idiosyncrasies of Japanese colonialism in relation to Changchun's urbanism. Chapter One deals with the idiosyncrasies of Japanese colonialism as manifested in planning Changchun are the subject of the next chapter. Chapter Two charts out the plurality of architectural styles in the city, and analyzes the diversities, ambivalences, and ambiguities in the practice of statecraft and urban construction. Chapter Three gives a picture of how the downtown of Changchun was reconstructed to meet new political agenda when Socialist Realism took sway of aesthetic program. I also examine in this chapter the nature of Japanese colonialism in Manchukuo from the perspective of rituals and pubic pageantries, by using Yamamuro's analogy of the client state to a hybrid beast of chimera.The last two chapters examine Changchun's development since 1945. Chapter Four pictures how the downtown of Changchun was reconstructed to meet new political agenda. Chapter Five explores Changchun's urban expansion under Maoism: the construction of the First Automobile Works, a key project of Maoist industrialization. The purposes of the dissertation have been anchored by an overall objective to fill up this vacancy from the perspective of urban construction and urban life.This dissertation has unfolded a proliferation of competing formulations of the modern in Changchun's urban history, some inspired by Western creations but more competing with Western concerns. In the competition for the dominance of the world, Japanese colonialism in Manchukuo and Chinese socialism both represented massive anti-capitalist and anti-imperialist qualities.",ucb,,https://escholarship.org/uc/item/0149581v,,,eng,REGULAR,0,0
231,1667,Chip-Scale Lidar,"Behroozpour Baghmisheh, Behnam","Boser, Bernhard E;",2016,"The superiority of lidar compared to radio-frequency and ultrasonic solutions in terms of depth and lateral resolution has been known for decades. In recent years, both application pull such as 3D vision for robotics, rapid prototyping, self-driving cars, and medical diagnostics, as well as technology developments such as integrated optics and tunable lasers have resulted in new activities.Pulsed, amplitude-modulated continuous-wave (AMCW), and frequency-modulated continuous-wave (FMCW) lidars can all be used for ranging. The latter option enables excellent depth resolution at the micron level. Achieving this level of performance is contingent on a precision light source with accurate frequency modulation. This thesis presents a fully integrated solution realizing an electro-optical phase-locked loop (EO-PLL) fabricated on separate complementary metal-oxide-semiconductor (CMOS) and silicon-photonic wafers interconnected with through-silicon vias (TSVs).The system performs 180,000 range measurements per second, with a root-mean square (RMS) depth precision of 8 Î¼m at distances of Â±5cm from the range baseline increasing to 4.2 mm RMS error at a range of 1.4 m, limited by the coherence length of the laser used in these experiments. Optical elements including input light couplers, waveguides, and photodiodes are realized on a 3 mm by 3 mm silicon-photonic chip. The 0.18 Î¼m CMOS application-specific integrated circuit (ASIC) of the same area comprises the front-end trans-impedance amplifier, analog electro-optical PLL, and digital control circuitry consuming 1.7 mA from a 1.8-V supply and 14.1 mA from a 5-V supply. The latter includes 12.5 mA bias current for the distributed Bragg reflector (DBR) section of the tunable laser. Also presented in the thesis is a novel dual mode lidar that combines FMCW and chirped-AMCW operation to simultaneously achieve precision depth resolution and a longer operating range not limited by Laser coherence length.",ucb,,https://escholarship.org/uc/item/01b3362w,,,eng,REGULAR,0,0
232,1668,"Theatre, Calvinism, and Civil Society in Eighteenth-Century Edinburgh and Geneva","Leyba, Ashley","Laqueur, Thomas;",2014,"Over the course of eighteen months in 1756 and 1757, theatre crises, large-scale debates about the morality of the stage, erupted in both Edinburgh and Geneva.  Traditionally, these debates have been explained away as examples of Calvinist anti-theatricality.  This dissertation argues, however, that this understanding is inaccurate.  Beyond the fact that there was no consistent tradition of Calvinist anti-theatricality in the early modern period, taking such a narrow view of the theatre crises undermines their importance.  The theatre debates of 1756 and 1757 must be understood in the context of the Enlightenment and changing notions about the relationship between the Calvinist church and civil society.  The theatre symbolized the birth of civil society and the end of a particular brand of Calvinism.  When the eighteenth-century debates about the stage are understood only as examples of ""Calvinist anti-theatricality,"" though, this importance is lost.  This project remedies the current gap in scholarship by demonstrating that these debates were not simply about the theatre; they were about the fate of Calvinism in an increasingly polite, enlightened society.",ucb,,https://escholarship.org/uc/item/01h927nz,,,eng,REGULAR,0,0
233,1669,"Institutional Determinants of Cyber Security Promotion Policies: Lessons from Japan, the U.S., and South Korea","Bartlett, Benjamin Gosnell","Aggarwal, Vinod;",2018,"Ensuring the cyber security of the private sector requires both theproduction of and consumption of cyber security technology.States vary in the degree to which they promote production and consumption.Taking an institutionalist approach, I argue that the differencebetween states can be explained as the result of two policylegacies. States with a policy legacy of maintaining strongtraditional national security capabilities have the instrumentsnecessary to promote production of cyber security technology, as wellas actors---the military and intelligence agencies---who are motivatedto do so. States with a policy legacy of economic guidance have theinstruments to promote the consumption of cyber security technology,and economically-oriented bureaucratic actors who see it as theirresponsibility to do so.To provide evidence for my hypotheses, I do a comparative case studyof Japan, the U.S., and South Korea. Japan, with a policy legacy ofrestrained traditional security capabilities and a legacy of economicguidance, does little to promote production but is active in promotingconsumption. The U.S., with a legacy of maintaining strong traditionalsecurity capabilities but without a legacy of economic guidance, isactive in promoting production but does little to promoteconsumption. South Korea, which has a policy legacy of maintainingstrong traditional security capabilities and a legacy of economicguidance, promotes both.The key implication of this research is that a state's ability topromote cyber security in the private sector is heavily determined notonly by past policies, but past policies that were unrelated to cybersecurity. States without the proper policy legacies will have to findways to build substituting institutions in order to promote bothproduction and consumption of cyber security.",ucb,,https://escholarship.org/uc/item/02f4879m,,,eng,REGULAR,0,0
234,1670,Discrete-Time H2 Guaranteed Cost Control,"Conway, Richard Anthony","Horowitz, Roberto;",2011,"In this dissertation, we first use the techniques of guaranteed cost control to derive an upper bound on the worst-case H2 performance of a discrete-time LTI system with causal unstructured norm-bounded dynamic uncertainty. This upper bound, which we call the H2 guaranteed cost of the system, can be computed either by solving a semi-definite program (SDP) or by using an iteration of discrete algebraic Riccati equation (DARE) solutions. We give empirical evidence that suggests that the DARE approach is superior to the SDP approach in terms of the speed and accuracy with which the H2 guaranteed cost of a system can be determined.We then examine the optimal full information H2 guaranteed cost control problem, which is a generalization of the state feedback control problem in which the H2 guaranteed cost is optimized. First, we show that this problem can either be solved using an SDP or, under three regularity conditions, by using an iteration of DARE solutions. We then give empirical evidence that suggests that the DARE approach is superior to the SDP approach in terms of the speed and accuracy with which we can solve the optimal full information H2 guaranteed cost control problem.The final control problem we consider in this dissertation is the output feedback H2 guaranteed cost control problem. This control problem corresponds to a nonconvex optimization problem and is thus ""difficult"" to solve. We give two heuristics for solving this optimization problem. The first heuristic is based entirely on the solution of SDPs whereas the second heuristic exploits DARE structure to reduce the number of complexity of the SDPs that must be solved. The remaining SDPs that must be solved for the second heuristic correspond to the design of filter gains for a estimator.To show the effectiveness of the output feedback control design heuristics, we apply them to the track-following control of hard disk drives. For this example, we show that the heuristic that exploits DARE structure achieves slightly better accuracy and is more than 90 times faster than the heuristic that is based entirely on SDP solutions.Finally, we mention how the results of this dissertation extend to a number of system types, including linear periodically time-varying systems, systems with structured uncertainty, and finite horizon linear systems.",ucb,,https://escholarship.org/uc/item/02w9h3kz,,,eng,REGULAR,0,0
235,1671,Cognitive Structures Underlying Gendered Language Usage in Germany: Narration and Linguistic Fieldwork,"Kolar, Meredith","Rauch, Irmengard;",2011,"This study intends to expand the historical language and gender debate (Chapter 1) by examining the cognitive structures that underlie human beliefs about gender.  Although the work does not profess to be a feminist work, it does seek to offer an opinion about how and why linguistic and social change can occur within a population. It examines the current state of gendered language usage and the potential for change in gendered language usage within a Western population.  The foundational methods for this study include cognitive linguistic and metaphor theories (Chapter 2) combined with narrative theory (Chapter 3), and the study incorporates Christian theological (Chapter 4) and feminist history (Chapters 1 & 4) as a basis for understanding the cultural conventions about gender in the West.  Narratives are considered to be ""Instruments of Mind"" (3.6).  They consist of systematic structures necessary for all human cognition, principally consisting of metaphorical mappings between source and target domains (2.6).  Narrative structures therefore enable us to reason throughout daily life.  As a crucial part of our reasoning strategies, narratives point to the details in our moral systems (Chapter 4).  A moral system is the coherent foundation of a person's beliefs and choices.  Moral systems are culturally shared, but there may be several versions of moral systems in any given culture (4.1).  Due to the prolific capacity of metaphorical reasoning, spreading activation in neural structures that enables such reasoning (2.4), and the radial characteristics of real human categorization strategies (2.2, 2.3), no human being reasons with complete consistency.  Exceptions abound and point to the blending of moral systems in individuals' reasoning strategies (Chapter 10).  Crucially, exceptions indicate both the potential for change and an innate human creativity (2.11, Chapter 10). We can draw inferences (3.1) about human reasoning structures and individuals' moral systems from the language individuals choose to discuss culturally shared stories.  Constellations of words, collocations, phrases, and metaphors point to the values, or moral systems, of each individual.  Constellations and collocations (3.4) often demonstrate beliefs in cultural folk models (2.3, 4.1.5).  Folk models primarily consist of prototypes and basic-level effects (2.2), and speakers employ these to make speedy and efficient judgments about people, things, and actions in everyday life.  Prototype categories, however, are radial categories (2.2, 2.3), which means that membership in a category is based on relationship to the central member, but that categories have indistinct boundaries and allow for unique or novel inclusion radiating from the central members.  The capacity for novel usage (2.11) is one of the most salient qualities of human cognition, and it is the quality that allows for both linguistic and social change through cognitive transformation.The primary folk models in the West point to two moral systems used by speakers to reason about daily, mundane and complex functions and actions.   Both prototypical moral systems stem from the Christian heritage: the Strict Father system of morality (SFM) and the Nurturant Parent system of morality (NPM) (Chapter 4).  SFM involves hierarchies, strict boundaries, moral strength, and purity, while NPM is based on empathy and dissolves notions of hierarchies.  This study demonstrates through interviews with 26 native speakers of modern German regarding stories of Christian saints (Chapters 5-9) that the leading moral system both historically and currently in this Western population segment is SFM (Chapter 10).  While many speakers demonstrate occasional features of NPM reasoning, female consultants tend to demonstrate more of these features than male consultants (Chapters 7-10).  It appears that women's historical status as a subordinate group under a SFM system may predispose them to the use of empathy (10.1) and therefore to the use of NPM reasoning.  Women tend to be the primary instigators of change in gendered language usage.  Finally, the analysis of the study suggests that language and social change occur over time as a result of the creative potential inherent in empathetic cognition, found more often in subordinate groups, due to their perception of a need for alternatives from the norm (Chapter 10).  Change rarely occurs ""from above"", through those who make up the status quo, but originates out of a need by subordinate groups to break down strict boundaries and rigid divisions.  Change is always possible, as human cognition is based on fuzzy boundaries and radial categories.  Nonetheless, change is a slow process because it requires long-term and often radical alterations in the tenacious narrative and cognitive structures of a shared culture.",ucb,,https://escholarship.org/uc/item/0329d50c,,,eng,REGULAR,0,0
236,1672,Empathic Communication During Mother-Adolescent Conflict Management,"Main, Alexandra","Zhou, Qing;Campos, Joseph J;",2013,"Interpersonal conflict management is a context in which empathy and emotion regulation can be both challenging and of vital necessity. The present study examined the effects of empathic communication on conflict management between mother-adolescent dyads (N = 50). Mother-adolescent dyads engaged in a 10-minute discussion of a topic of frequent conflict in their relationship. Following the discussion, mothers and adolescents independently completed a post-conflict discussion questionnaire to assess their satisfaction with the discussion. Emotional behaviors during the discussion were coded using the Specific Affect Coding System (SPAFF). Empathic communication was coded as (1) validation and (2) interest in the other's perspective and feelings. The present study explored several questions related to (1) adolescent age differences in mother and adolescent empathic communication and conflict management, and (2) relations between empathic communication and conflict management. Notably, older adolescents and their mothers displayed more validation than younger adolescents and their mothers. Furthermore, mother's validation was marginally positively correlated with adolescents' satisfaction with the discussion, and this relation was mediated by the degree to which adolescents perceived that their mother understood their point of view and feelings during the discussion. Findings indicate that empathic communication in response to adolescent negative emotion plays a unique role in effective conflict management between mothers and adolescents. Implications for research on empathy and interventions targeted at facilitating effective conflict management between parents and adolescents are discussed.",ucb,,https://escholarship.org/uc/item/04r9g4cz,,,eng,REGULAR,0,0
237,1673,Magnetic Exchange Coupling and Single-Molecule Magnetism in Uranium Complexes,"Rinehart, Jeffrey Dennis","Long, Jeffrey R;",2010,"This dissertation describes the research that led to the discovery of single-molecule magnetism in the actinides. Chapter One is an introduction to the concepts that lead to single-molecule magnet behavior with an emphasis on the specific qualities of the f-elements that make them interesting for such studies. A simple model for predicting ligand field environments that should be amenable to single-molecule magnet behavior is presented along with several examples of its application to lanthanide and actinide systems. The study of magnetic exchange coupling in uranium-containing multinuclear complexes is discussed and the literature on the subject is reviewed.	Chapter Two describes how the homoleptic dimer complex [U(Me2Pz)4]2 (Me2Pz- = 3,5-dimethylpyrazolate) can be cleaved via insertion of terminal chloride ligands, such that reactions with (cyclam)MCl2 (M = Ni, Cu, Zn; cyclam = 1,4,8,11-tetraazacyclotetradecane) in dichloromethane generate the linear, chloride-bridged clusters (cyclam)M[(Î¼-Cl)U(Me2Pz)4]2. Variable-temperature magnetic susceptometry is used to reveal the presence of weak ferromagnetic coupling between the Ni(II) (S = 1) and U(IV) centers and no coupling between the Cu(II) (S = 1/2) and U(IV) centers. Consistent with a simple superexchange mechanism for the coupling, density functional theory calculations performed on a [(Me2Pz)4UCl]âˆ’ fragment of the cluster show the spin resides in 5fxyz and 5fz(x2-y2) orbitals, exhibiting delta symmetry with respect to the U-Cl bond.	Chapter Three extends the analysis of exchange coupling in Chapter Two to include the (cyclam)Co[(Î¼-Cl)U(Me2Pz)4]2 cluster. As in the Cu(II) case, Co(II) has a single unpaired electron (S = 1/2), however this unpaired electron resides in a dz2 orbital and is therefore oriented directly along the superexchange pathway. This provides a significantly better magnetic exchange pathway leading to the strongest magnetic coupling of the series.	Chapter Four deviates briefly from the pursuit of molecular magnets to study a series of multinuclear clusters formed from the activation of the 3,5-dimethylpyrazolate anion by uranium(III) via two-electron reductive cleavage of the N-N bond to form 4-ketimidopent-2-ene-2-imido (kipi3-) units, as isolated in three related tetranuclear uranium cluster compounds, two of which are mixed valent. The kipi3- ligand represents an exotic latecomer to the acetylacetonato (acac-) ligand family. Unlike the related and widely-utilized Î²-diketimido (nacnac-) ligands, kipi3- can be represented as containing both imido and ketimido functionalities. Thus, it provides a true nitrogen-based, isoelectronic analogue of acac-, a ligand that has played a long and vital role in coordination chemistry.	Chapter Five turns from the synthesis of exchange coupled clusters to mononuclear species. Drawing on the model of f-element anisotropy presented in Chapter One, the trigonal prismatic complex U(Ph2BPz2)3 was chosen for study. Ac magnetic susceptibility measurements performed on it demonstrate the presence of slow magnetic relaxation under zero applied dc field. Analysis of both the temperature and frequency dependence of the ac susceptibility indicate a temperature regime (T > ~3 K) where Arrhenius behavior dominates the relaxation processes, leading to a spin relaxation barrier of Ueff = 20 cmâˆ’1. The dc field dependence of the relaxation time is studied to reveal evidence of quantum tunneling processes occurring at lower temperatures. The results represent the first example of an actinide complex displaying single-molecule magnet behavior and confirm the general strategy for identifying further uranium(III)-based single-molecule magnets by concentrating ligand field contributions above and below the equatorial plane of an axially-symmetric coordination complex.	Chapter Six builds on the results presented in Chapter Five to characterize the related complex the trigonal prismatic complex U(H2BPz2)3. This tricapped trigonal prismatic complex is characterized by single crystal x-ray diffraction and ac magnetic susceptibility measurements. The ac susceptibility data demonstrate the presence of multiple processes responsible for slow magnetic relaxation. Out-of-phase signals observed at ac switching frequencies between 1 and 1500 Hz in dc fields of 500-5000 Oe indicate a thermal relaxation barrier of ca. 8 cm-1 for the molecule, with a temperature-independent process taking over at the lowest temperatures probed. Significantly, an unprecedented, slower relaxation process becomes apparent for ac switching frequencies between 0.06 and 1 Hz, for which a monotonic increase of the relaxation time with applied dc field suggests a direct relaxation pathway.",ucb,,https://escholarship.org/uc/item/06g3k0kp,,,eng,REGULAR,0,0
238,1674,"Ion Nanocalorimetry: Measuring Absolute Reduction Potentials, and Investigating Effects of Water on Electron Solvation and Ion Fluorescence","Donald, William Alexander","Williams, Evan R;",2010,"This dissertation reports on the development of a new gas-phase ion nanocalorimetry technique, in which electrochemistry is performed using large ""aqueous"" nanodrops in vacuo to obtain absolute half-cell potentials in bulk solution. Absolute recombination energies (REs) of nanometer-sized water droplets containing a divalent or trivalent metal ion are obtained from the number of water molecules lost upon electron capture (EC). REs are obtained from the experimentally measured average number of water molecules lost from the cluster, and from both the sum of the threshold water molecule binding energies and the sum of energy that is partitioned into the translational, rotational and vibrational modes of the products for each water molecule lost.  The energy removed by the lost water molecules is obtained from established theoretical models.  The width of the product ion distribution in these experiments is predominantly attributable to the distribution of energy that partitions into the translational and rotational modes of the water molecules that are lost.  These results are consistent with a singular value for the recombination energy.  Ion nanocalorimetry has been used to obtain a value for the absolute standard hydrogen electrode potential from three different nanocalorimetry based methods that all agree within 5% of each other (+4.05, +4.11, and +4.21 V).  Our extrapolation method, in which REs of size-selected and thermalized Eu3+(H2O)n, n = 55 to 140, are extrapolated to infinite size to obtain the absolute reduction potential of Eu3+(aq) and a value for the absolute SHE potential (+4.11 V), should be the most accurate because a solvation model is not used and therefore, errors associated with solvation models are eliminated.          Water clusters containing ions for which one-electron reduction potentials in aqueous solution are not readily measurable, such as alkaline earth divalent metal ions and most of the trivalent lanthanide ions, form solvent separated metal ion and electron ion pairs upon EC, as long as there are a sufficient number of water molecules to stabilize the ion pair.  The dependence of the RE values for Ca(H2O))n2+ on cluster size suggest that the electron is delocalized on the surface of the cluster for n = 32-47, but a transition to a more highly solvated electron is indicated for n = 47-62 by the constant RE values for these ions. For La3+(H2O)n (n = 42 to 160), the trend in recombination energies as a function of hydration extent is consistent with a structural transition from a surface-located excess electron at smaller sizes (n â‰¤ ~56) to a more fully solvated electron at larger sizes (n â‰¥ ~60). The recombination enthalpies for n > 60 are extrapolated as a function of the geometrical dependence on cluster size to infinite size to obtain the bulk hydration enthalpy of the electron (-1.3 eV), which is within the wide range of values obtained from previous methods (-1.0 to -1.8 eV).  The ion nanocalorimetry method has the advantage that it does not require estimates for the absolute solvation energy of the proton or the H atom.         Whereas EC by hydrated metal ions resulted in only the full internal conversion of the RE into the reduced precursor, some ions can fluoresce upon electronic excitation.  We report a new highly sensitive method for detecting the fluorescence of isolated, partially hydrated ions for the first time.  Fluorescence is indirectly detected based on the distribution of water molecules lost upon absorption of a UV photon. Photodissociation of hydrated protonated proflavine (n = 13-50) undergoes three photophysical processes upon absorption of a 248 nm photon and excitation to a high energy singlet excited state: full internal conversion and fluorescence to the ground electronic singlet state, and formation of a long-lived triplet state, which slowly undergoes non-radiative intersystem crossing to the ground singlet state.  The high sensitivity of this method should make it possible to perform FÃ¶rster resonance energy transfer experiments with gas-phase biomolecules in a microsolvated environment to investigate how a controlled number of water molecules effects biomolecular structure and dynamics.         Although the precision in the nanocalorimetry method is excellent, the absolute uncertainty obtained is more difficult to assess because the energy removed by the lost water molecules has not been experimentally measured for large hydrated metal ions. Laser induced photodissociation experiments, in which M2+(H2O)n are dissociated by absorption of UV laser light at 193 (6.4 eV) and 248 nm (5.0 eV), are used to directly relate the average number of water molecules lost to the energy that is deposited into the cluster, which can be used to directly convert the average water molecules lost in EC experiments to experimentally measured RE values.  These results demonstrate that absolute solution phase reduction potentials can be obtained entirely from experimental data, with no modeling, and should provide the most direct route to establishing an absolute electrochemical scale with high accuracy.",ucb,,https://escholarship.org/uc/item/06x143rt,,,eng,REGULAR,0,0
239,1675,Molecular Imaging Approaches to Understanding the Roles of Hydrogen Peroxide Biology in Stress and Development,"Dickinson, Bryan Craig","Chang, Christopher J.;",2010,"The production of hydrogen peroxide (H2O2) in biological systems is associated with a variety of pathologies including neurodegenerative diseases, cancer, and the general process of aging. However, a growing body of evidence suggests that the reactivity of this particular reactive oxygen species (ROS) is also harnessed for physiological processes. Molecular imaging using fluorescence microscopy offers a valuable approach for deciphering the multifaceted roles of H2O2 in biological processes. The use of aryl boronates for the selective detection of H2O2 in biological systems is a validated approach to the development of H2O2-responsive fluorophores. This dissertation describes the design, synthesis, and characterization of an assortment of new boronate-based fluorescent probes for H2O2, as well as their application toward uncovering new roles for H2O2 in stress and development. Peroxyfluor-2, Peroxyfluor-3, Peroxy Yellow 1, and Peroxy Orange 1 are turn-on fluorescent probes that can detect physiological levels of H2O2 produced for cell signaling, as well as monitor multiple ROS simultaneously in single cells. Mitochondria Peroxy Yellow 1 is a bifunctional probe featuring a triphenylphosphonium group for mitochondrial targeting and a single boronate for H2O2 detection that allows for the detection of mitochondrial H2O2 associated with a Parkinson's diseases model. Nuclear Peroxy Emerald 1 is a nuclear-targeted probe that reveals Sirtuin-dependent changes in nuclear H2O2 metabolism in C. Elegans. Peroxyfluor-6 (PF6) is a bifunctional probe featuring acetoxymethylester-protected phenol and carboxylic acid functionalities for enhanced cellular uptake and retention. PF6 reveals endogenous H2O2 production within neural stem cells and molecular biological experiments expose a new role for H2O2 in growth signaling within this critical brain cell population in vitro and in vivo. Finally, Peroxy Yellow 1 Methyl-Ester, a probe designed for analysis by flow cytometry, reveals that Aquaporins 3 and 8, but not Aquaporin 1, can mediate H2O2 uptake across the plasma membrane of mammalian cells, and that Aquaporin 3 can facilitate the uptake of endogenous H2O2 relevant to cell signaling.",ucb,,https://escholarship.org/uc/item/07n8n80h,,,eng,REGULAR,0,0
240,1676,Electrocatalytic Oxidation of Formate with Rh(III) and Co(III) Electrocatalysts,"Kellenberger, Daniel Louis","Arnold, John;",2015,"AbstractElectrocatalytic Oxidation of Formate with Rh(III) and Co(III) ElectrocatalystsByDaniel Louis KellenbergerDoctor of Philosophy in ChemistryUniversity of California, BerkeleyProfessor John Arnold, ChairChapter 1. The hydrogen fuel cell is an environmentally friendly alternative to fossil fuel combustion for the powering of vehicles and other mobile applications. The storage of hydrogen in appreciable densities and the difficulty of its distribution are prohibitive factors for the large scale, societal adoption of current hydrogen fuel cell technologies. A hydrogen fuel cell based on the reversible storage of hydrogen equivalents in liquid, organic substrates would alleviate both of these issues. A hydrogen fuel cell based on the hydrogenated/dehydrogenated pair of formic acid and carbon dioxide is a promising example that would allow for the regeneration of the fuel at an external plant away from the point of release. There exist numerous examples of the dehydrogenation of formic acid to generate dihydrogen. However, the direct electrocatalytic oxidation of formic acid to its constituent protons, electrons, and carbon dioxide byproduct is presented as a more attractive alternative that would allow for the incorporation of a homogeneous electrocatalyst into the fuel cell itself. A generalized mechanism is envisaged that would allow for the direct electrocatalytic oxidation of formic acid by a homogenous, organometallic electrocatalyst.Chapter 2. The Rh(III)-centered complex [Cp*Rh(bpy)(MeCN)][PF6]2 (Cp* = pentamethylcyclopentadienyl, bpy = 2,2â€™-bipyridyl) was selected as a possible electrocatalyst for the electrocatalytic oxidation of formate. Analogues for each of the intermediates in the electrocatalytic cycle as presented in Chapter 1 were either isolated, modelled, or directly observed. The Rh(I) complexes Cp*Rh(bpy) and Cp*Rh(phen) (phen = 6,10-phenanthroline) were synthesized and the latter was structurally characterized. Reaction of the Rh(III) complex with formate in acetonitrile resulted in decomposition but coordination of formate was modelled with the isolation of the acetate analogue, [Cp*Rh(bpy)(OAc)][PF6]. The complex [Cp*Rh(6,6â€™-Me2-2,2â€™-bipyridyl)(MeCN)][PF6]2 featuring a bulkier chelating ligand was synthesized and monitoring the reaction with formate in acetonitrile by 1H NMR revealed the in situ generation of a Rh(III) hydride, [Cp*Rh(6,6â€™-Me2-2,2â€™-bipyridyl)(H)]+. Electrocatalytic oxidation of formate in benzonitrile was achieved as determined by constant potential coulometry experiments conducted at the impressive potential of -900 mV vs. Ag/Ag+.Chapter 3. A series of Rh(III) electrocatalysts were synthesized of the type [Cp*Rh(chelate)(MeCN)]2+ (1) featuring the chelates (a) 2,2â€™-bipyridyl, (b) 6,10-phenanthroline, (c) 4,4â€™-Me2-2,2â€™-bipyridyl, and  (d) 6,6â€™-Me2-2,2â€™-bipyridyl to determine the influence of the chelate on the complexâ€™s observed reactivity. Computational chemistry was in agreement with the proposal that the observation of two electrochemical reductions for complexes 1a-c resulted from an equilibrium in solution of 1 with a 16-electron complex, [Cp*Rh(chelate)]2+. The solid state structures of 1a-d determined by single crystal, x-ray diffraction experiments and the gas phase structures calculated with computational chemistry suggested the observed irreversible electrochemical reduction of 1d was due to increased steric effects from its bulkier chelating ligand which would favor dissociation upon reduction. Additionally, a system of calculations were developed to address the competing abilities of the Rh(III) hydrides to act as proton sources or hydride donors with the former being desired for the electrocatalytic oxidation of formate observed in Chapter 2. Chapter 4. [Cp*Co(bpy)(MeCN)]2+ was synthesized and characterized as a first-row analogue the Rh(III) electrocatalysts studied in Chapter 2 and Chapter 3. A new, simplified synthesis of [Cp*Co(MeCN)3][PF6]2 was achieved by reaction of Cp*Co(CO)2 with two equivalents of AgPF6 in acetonitrile. Reaction of the tris-acetonitrile complex with either 2,2â€™-bipyridyl or 6,6â€™-Me2-2,2â€™-bipyridyl resulted in the generation of the complex [Cp*Co(chelate)(MeCN)][PF6]2. Unlike with the Rh(III) analogues, reaction of the [Cp*Co(bpy)(MeCN)][PF6]2  species with formate generated an isolable Co-formate adduct, [Cp*Co(chelate)(OC(O)H)][PF6]2. The addition of formate to [Cp*Co(bpy)(MeCN)][PF6]2 to generate the Co-formate adduct in situ resulted in the appreciable anodic shift of the oxidation potential of formate by ca. 750 mV.",ucb,,https://escholarship.org/uc/item/082970n6,,,eng,REGULAR,0,0
241,1677,"Estimation, Identification and Data-Driven Control Design for Hard Disk Drives","Bagherieh, Omid","Horowitz, Roberto;",2017,"The demand for online storage has been increasing significantly during the last few years. Hard disk drives are the primary storage devices used in data centers for storing these online contents. The servo assembly of the dual-stage Hard Disk Drive (HDD) is composed of the Voice Coil Motor (VCM) and the Mili-Actuator (MA), where the VCM is responsible for coarse positioning at low frequency regions and the MA is responsible for fine positioning at high frequency regions. Controlling these two actuators is very critical in precision positioning of the read/write head, which is mounted at the edge of the servo assembly. In this dissertation, the precision positioning of the head during the self-servo writing process as well as feed-forward and feedback controls in the track following mode are considered. This dissertation discusses three control design methodologies for hard disk drives servo systems, in order to improve their performance as well as their reliability. The first is a state estimator for non-uniform sampled systems with irregularities in the measurement sampling time, which estimates the states at a uniform sampling time. The second is an online uncertainty identification algorithm, which parameterizes and identifies the uncertain part of transfer functions in a dual-stage HDD. The third is a frequency based data-driven control design methodology, which considers mixed H_2/H_infinity control objectives and is able to synthesize track following servo systems for dual stage actuators utilizing only the frequency response measurement data, without the need of identifying the models of the actuators.The state estimator design for non-uniform sampled systems with irregularity in the measurement sampling time is considered, where it is proposed to design an observer to estimate the states at a uniform sampling time. This observer is designed using a time-varying Kalman filter as well as a gain-scheduling observer. The Kalman filter has the optimal performance, while the gain-scheduling observer requires relatively lower computational power. Simulations are conducted involving the self-servo writing process in hard disk drives, where performance as well as computational complexity of these two observers are compared under different noise scenarios.Uncertainties in system dynamics can change the closed loop transfer functions and affect the performance or even stability of the control algorithm. These uncertainties are parameterized as stable terms using coprime factorizations, and are identified in an online fashion. The uncertainty identification, in comparison to the complete transfer function identification, requires less computational power as well as a smaller order for the identified transfer function.The proposed online uncertainty identification algorithm is utilized to factorize and identify the uncertain part of transfer functions in a dual-stage Hard Disk Drive (HDD). The dual-stage actuators' gains and resonance modes are affected by temperature variations, which in turn affect all closed loop transfer functions. Therefore, these transfer functions must be periodically updated in order to guarantee the convergence and stability criteria for the adaptive Repeatable Run-Out (RRO) following algorithm proposed in [61, 62]. Experimental results conducted on a hard disk drive equipped with dual-stage actuation, confirm the effectiveness of the proposed identification algorithm.A frequency based data-driven control design considering mixed H_2/H_infinity control objectives is developed for multiple input-single output systems. The main advantage of the data-driven control over the model-based control is its ability to use the frequency response measurements of the controlled plant directly without the need to identify a model for the plant. In the proposed methodology, multiple sets of measurements can be considered in the design process to accommodate variations in the system dynamics. The controller is obtained by translating the mixed H_2/H_infinity control objectives into a convex optimization problem. The H_infinity norm is used to shape closed loop transfer functions and guarantee closed loop stability, while the H_2 norm is used to constrain and/or minimize the variance of signals in the time domain.The proposed data-driven design methodology is used to design a track following controller for a dual-stage HDD. The  sensitivity decoupling structure[34] is considered as the controller structure. Thecompensators inside this controller structure are designed and compared by decoupling the system into two single input-single-output systems as well as solving for a single input-double output controller.",ucb,,https://escholarship.org/uc/item/0998n097,,,eng,REGULAR,0,0
242,1678,Manipulating 1D Conduction Channels; from Molecular Geometry to 2D Topology,"Pedramrazi, Zahra","Crommie, Michael F.;",2019,"This dissertation is divided into two segments, both of which focus on creating and manipulating one-dimensional (1D) conduction channels in novel 1D and two-dimensional (2D) systems, characterized by scanning tunneling microscopy (STM).The first half describes how the electronic properties of quasi-1D graphene nanoribbons (GNRs) are manipulated by controlling their width and edge geometry at the atomic scale. A bottom-up approach is used for fabricating three different armchair GNR (AGNR) systems, allowing the geometry and hence the electronic properties of resultant AGNRs to be controlled. Successful molecular bandgap engineering in 1D AGNR heterojunctions is described, as well as the electronic and topographic characterization of the concentration dependence of boron-doped AGNRs. The discovery of two new in-gap dopant states with different symmetry is described. The successful fabrication and characterization of S-AGNRs having sulfur atoms substitutionally doped at the AGNR edges is also described. Our results indicate that S-doping induces a rigid shift of the energies for both the valence and conduction bands.The second half of this thesis describes how the 1Tâ€™ phase of monolayer transition metal dichalcogenides (TMDs) can be used as a platform to create 2D topological insulators (TIs). These novel TI systems are characterized in great detail. The successful growth and characterization of single-layer 1Tâ€™â€“WTe2 is described. This material is shown to host a bulk bandgap and helical edge states at the 1Tâ€™â€“vacuum interface. The growth and characterization of mixed phase-WSe2 is described. New techniques for creation and manipulation of edge conduction channels at interfaces between materials of different topologies are described.",ucb,,https://escholarship.org/uc/item/09z5w3jz,,,eng,REGULAR,0,0
243,1679,"Essays in Innovation, Past and Present","Gross, Daniel Pincus","Handel, Benjamin R.;",2015,"This dissertation studies the economics of historical and modern innovation. The first chapter makes inroads into understanding how competition and incentives shape the creative process which lies at the heart of all technological progress. The creative act is a classic example of a black box in academic research: we can see the inputs and outputs, but we know little about what happens in between. This paper uses new tools for measuring the content of digital media to see how commercial graphic designersâ€™ work evolves in winner-take-all competition. In this chapter, I show that competition both creates and destroys incentives for innovation: some competition is necessary to motivate high-performers to experiment with novel, untested ideas over tweaking tried-and-true approaches, but heavy competition will drive them out of the market.In the second chapter, I study the effects of performance feedback on innovation in competitive settings. Feedback typically serves two functions: it informs agents of their relative performance, and it also helps them improve the quality of their product. The presence of these effects suggests a tradeoff between participation and improvement, as the revelation of asymmetries discourages effort. Using data from the same setting as chapter one, I first show that this tradeoff is real. I then develop a structural model of the setting -- the first of its kind in the literature -- and use the results to evaluate counterfactual feedback policies. The results suggest that feedback is on net a desirable mechanism for a principal seeking high-quality innovation.In the third chapter, I use the farm tractor as a case study to demonstrate that technologies diffuse along two distinct margins: scale and scope. Although tractors are now used in nearly every field operation and with nearly all crops, early models were far more limited in their capabilities, and only in the late 1920s did the technology begin to generalize for broader use with row crops such as corn. Diffusion prior to 1930 was accordingly heavily concentrated in the Wheat Belt, while growth in diffusion from 1930-1940 was concentrated in the Corn Belt. Other historically important innovations in agriculture and manufacturing share similar histories of expanding scope. The key to understanding the pace and path of technology diffusion is thus not only in explaining the number of different users, but also in explaining the number of different uses.A common theme across all three chapters is the focus on developing tools or strategies to study innovation that are less dependent on patent data than the extant literature, since the majority of innovation is not patented (and often not patentable), and doing so while advancing the empirical literature on innovation in new directions.",ucb,,https://escholarship.org/uc/item/0c65m44m,,,eng,REGULAR,0,0
244,1680,The Life and Death of a Tectonic Plate: Imaging the Juan de Fuca Plate with Amphibious Seismic Data,"Hawley, William Bythewood","Allen, Richard M;",2019,"Understanding of Earthâ€™s evolution has been hindered, in part, by the technical challenges associated with placing seismic instruments on the seafloor. As technology improves, more arrays of ocean bottom seismometers are being deployed around the ocean basins. Perhaps the most ambitious such array, the Cascadia Initiative, covered the entirety of a small oceanic plate, the Juan de Fuca plate, with ocean bottom seismometers in a four-year experiment. That array is the primary motivation for this study.The two models presented in this work, CASC16-P and CASC19-S, are teleseismic tomographic studies. They are each the first of their kind to use offshore and onshore data to simultaneously image the P- and S-wave seismic velocity structure of the mantle beneath the Juan de Fuca and North American plates. These models have provided insight into the tectonics of oceanic plates: how they are created, how they might interact with the mantle over the course of their life, how they might be influenced by an overriding continental plate, how they behave in the mantle after subduction, and how subduction could cease.This work focuses on the on- and offshore regions of the Pacific Northwest of the United Statesâ€”in particular, on the interaction between the Juan de Fuca plate, the North American plate, and the upper mantle. But it also demonstrates the value in oceanic arrays. The offshore instruments used in this study have allowed the identification of new features offshore, and better resolution of features near the coastlines. Ocean bottom seismology represents a rapidly growing new frontier that has the potential to investigate earth evolution in detail that was not previously possible. The work we present here has both helped illuminate the tectonics of the Pacific Northwest, and helped generate detailed hypotheses about the nature of global tectonics that will be tested elsewhere.",ucb,,https://escholarship.org/uc/item/0cg454nd,,,eng,REGULAR,0,0
245,1681,The construction of the Colombian territory: Images of the Colombian Armed Conflict 2002- 2010,"Salamanca, Claudia Liliana","Esmeir, Samera;Bates, David;",2015,"My dissertation, The construction of the Colombian territory: Images of the Colombian Armed Conflict 2002-2010, critically analyzes the Colombian internal conflict and its relationship with the narrative of the nation in a global theater of operations of war in its different forms of visualization.  Through a detail textual analysis of documentaries, films, recorded military operations, media military operations, and proof of life videos, I examine the effect moving images have had as part of configuring the battlefield. My research proceeds from the premise  that there is a visual culture of warfare - grammars, imaginaries, and technologies â€“ that has organized the global space of security and has brought new forms of territory that paradoxically exceed the idea of the sovereign nation and at the same time confirm it. In other words, I examine the creation and exercise of national security in a globalized world and its effects on the concept of the nation and national territory through the concept of politics of the visual. Colombia is considered a successful example for nation building, counterinsurgency tactics and U.S. intervention. As my case of study, I analyze the epistemological assumptions that emerge in the global war on terror, specifically how they reflect in the construction of the idea of Colombia as a unified image, one sovereign nation, and one territory through the production of different visual narratives.",ucb,,https://escholarship.org/uc/item/0cm960g2,,,eng,REGULAR,0,0
246,1682,Combining Structure and Usage Patterns in Morpheme Production: Probabilistic Effects of Sentence Context and Inflectional Paradigms,"Cohen, Clara",,2014,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0cs612p7,,,eng,REGULAR,0,0
247,1683,Adaptive Optimization Methods in System-Level Bridge Management,"Liu, Haotian","Madanat, Samer M;",2013,"In 2012, over 25% of the bridges in the United States were rated as structurally deficient or functionally obsolete. Moreover, 35% of bridges are serving beyond their theoretical design lifespan and the number has been projected to increase over the next decade. The imperative needs of improving the overall condition of the bridge system has been impeded by the shortage of funding available for bridge repairs and maintenance. In 2006 the gap between Federal Highway Administration's (FHWA) estimates to eliminate the bridge maintenance backlog and the actual appropriations to bridges for repairs and maintenance from the Highway Bridge Program was $43.4 Billion. In 2009, the gap increased to $65.7 Billion. Such conflict has made effective bridge management more critical than ever.	In bridge management, agencies collect bridge condition data and develop deterioration models that predict the bridges' future conditions and associated costs, based on which maintenance, rehabilitation and reconstruction (MR&R) decisions are made. It is therefore critical to have accurate deterioration models. However, limited availability of data and incomplete understanding of the deterioration process result in inaccurate models, which lead to sub-optimal MR&R decisions and significant cost increases. 	To address the inaccuracy stemming from limited bridge condition data, researchers have proposed Adaptive Control (AC) methods that update the deterioration models successively as new data become available. The underlying belief is that agencies can obtain more accurate deterioration models through updating and subsequently improve their MR&R decisions and achieve cost savings. State-of-the-art bridge management systems, such as Pontis, use a class of AC procedures known as Certainty Equivalent Control (CEC). The procedure used in Pontis updates the transition probabilities (i.e., the parameters of the component deterioration models) after each condition survey, and uses the updated probabilities in subsequent planning of MR&R decisions. Unfortunately, CEC does not necessarily lead to more accurate models, or guarantee savings in system costs; in other words, updating of the type in Pontis is not necessarily beneficial. 	In the present dissertation, an AC method, Open-Loop Feedback Control (OLFC), is proposed for system-level bridge management. The performance of OLFC and the Pontis CEC is tested in a numerical study and empirical results show that OLFC has superior performance with respect to two criteria. In terms of 	improvement in model accuracy, the Pontis CEC yields systematic bias in model parameter estimates and therefore does not improve model accuracy. In all testing scenarios, the resulting deterioration models lead to faster deterioration than the true models. OLFC, on the other hand, results in consistent convergence to the true models in all testing scenarios and improves model accuracy. When evaluated by system costs, the Pontis CEC consistently results in higher system costs than the no-updating scenario. The increases are on the order of $180 Million at the level of the State of California. To the contrary, updating with OLFC consistently achieves system costs savings compared to the no-updating scenario, and results in system costs that do not differ significantly from the system costs when true models are used for MR&R decision-making.	In addition, a computationally tractable optimization routine is developed for MR&R decision-making. The routine ensures strict conformity to system budget constraints and achieves satisfactory computational efficiency even given high levels of heterogeneity in bridge systems.",ucb,,https://escholarship.org/uc/item/0dm7g2d2,,,eng,REGULAR,0,0
248,1684,Machine Learning: Why Do Simple Algorithms Work So Well?,"Jin, Chi","Jordan, Michael I;",2019,"While state-of-the-art machine learning models are deep, large-scale, sequential and highly nonconvex, the backbone of modern learning algorithms are simple algorithms such as stochastic gradient descent, gradient descent with momentum or Q-learning (in the case of reinforcement learning tasks). A basic question endures---why do simple algorithms work so well even in these challenging settings?To answer above question, this thesis focuses on four concrete and fundamental questions:- In nonconvex optimization, can (stochastic) gradient descent or its variants escape saddle points efficiently?- Is gradient descent with momentum provably faster than gradient descent in the general nonconvex setting?- In nonconvex-nonconcave minmax optimization, what is a proper definition of local optima and is gradient descent ascent game-theoretically meaningful?- In reinforcement learning, is Q-learning sample efficient?This thesis provides the first line of provably positive answers to all above questions. In particular, this thesis will show that although the standard versions of these classical algorithms do not enjoy good theoretical properties in the worst case, simple modifications are sufficient to grant them desirable behaviors, which explain the underlying mechanisms behind their favorable performance in practice.",ucb,,https://escholarship.org/uc/item/1h25x268,,,eng,REGULAR,0,0
249,1685,Regulation Of Vascular Stem Cells By Transcription Factor And Microenvironment,"Chang, Julia Haewon","Li, Song;Ngai, John;",2015,"Cardiovascular diseases (CVDs) cause about 31% of all global deaths. As people age, CVD progress due to the accumulation of fatty materials, immune cells, and smooth muscle cells form stabilized fatty plaques in blood vessels through a process known as atherosclerosis. Atherosclerosis plaques affect vessel wall integrity, elasticity, and occlude blood flow, which can result in death. Top risk factors of CVDs include tobacco use, alcohol, blood pressure, and blood cholesterol levels. In order to develop better drug treatment strategies, it is important to elucidate the cellular mechanisms that drive CVD progression within the vessel wall. In healthy blood vessels, smooth muscle cells have a low proliferating rate, low synthesis activity, and high expression of contractility proteins and ion channels important in responding to vessel contraction, tone, and blood pressure.In chapter one, I discuss current theories on what cells directly contribute to vessel wall repair and/or disease progression. The de-differentiation hypothesis proposes that vascular smooth muscle cells directly contribute to CVDs. This phenotypic switch, or de-differentiation, results in smooth muscle cells that down-regulate expression of contractility proteins, rapidly proliferate and migrate into the damaged area. The heterogeneous population of cells observed in diseased vessels is the result of smooth muscle cell de-differentiation. The isolation and characterization of side populations of vascular stem cells (VSCs) in the vessel wall presents an additional cell source that may also contribute to healthy and diseased vessel wall repair. Previous work from our group characterized a population of multipotent VSCs. In healthy vessel repair, VSCs are activated and differentiate to smooth muscle cell end fates. However, in diseased vessels, aberrant differentiation of VSCs could contribute to the heterogeneous population of fat, bone, and chondrogenic cells.In chapter two, we characterized how matrix elasticity regulates VSC proliferation, expression of SMC markers, and cellular localization of mechanical signaling factors. We began by confirming trends of previous DNA microarray results when rat VSCs were seeded on polyacrylamide hydrogels (pAAm). While average nuclear area also decreases as matrix elasticity decreases, the rate of cell proliferation was approximately equal compared to glass control. Based on previous work, we tested whether global decreases in nuclear area also led to global chromatin remodeling, primarily detected through histone tail modifications. We did not find global changes in response to matrix elasticity. Instead, we were able to show that smooth muscle marker protein expression decreases as a function of matrix elasticity. As matrix elasticity decreases, F-actin stress fibers decrease, which increases relative pools of G-actin. G-actin can bind to Myocardin-related transcription factor A, which activates gene transcription of contractile and cytoskeletal proteins, including smooth muscle a-actin.In chapter three, we describe multiple attempts to characterize the role of Sox family of transcription factor by either targeted knockdown or overexpression. We hypothesized that high expression of two Sox family members, Sox10 and Sox17, in early passage rat VSCs affects vascular stem cell maintenance and/or prevents differentiation. We attempted to use short- and long-term targeted knockdown of Sox protein in rat VSCs. Specifically, we characterized the effects on VSCs proliferation, smooth muscle cell gene expression, cell migration, and directed differentiation. Due to the recovery of Sox gene and protein expression, our results were inconclusive. When we tried to overexpress a GFP-tagged Sox10, we saw dramatic decreases in GFP+ cells after 7 days in culture.In this work, we were able to determine: (1) Rat VSCs respond to matrix elasticity, a mechanical signal, and decrease expression of smooth muscle cell markers. (2) Matrix elasticity decreases MRTF-A nuclear localization in rat VSCs. (3) As a result, direct downstream targets of MRTF-A, like SMA, also decrease.Our data for targeted knockdown of Sox10 and Sox17 in rat VSCs were inconclusive. Rat VSCs showed recovery of Sox protein expression after siRNA transfection. Instead, we tried to generate stable shRNA rat VSC lines, but did not see reduced Sox10 staining compared to scrambled control.",ucb,,https://escholarship.org/uc/item/0mq634qn,,,eng,REGULAR,0,0
250,1686,A System-level Approach to Fault and Variation Resilience in Multi-core Die,"Markovskiy, Yury","Wawrzynek, John;",2009,"With shrinking transistors and growth in parametric variability, statically managing die yield is no longer possible. Design for Manufacturing (DFM) techniques use increasingly bigger guard-bandsthat waste area, power, and performance, impeding Moore's Law of semiconductor device scaling. Process Voltage Temperature (PVT) variations can turn a nominally homogeneous many-core die into a set of cores with heterogeneous performance.Network-on-Chip provides an effective and scalable way to integrate hundreds of heterogeneous cores without forcing each to give up its own PVT-induced operating point for the chip-wide common worst case. As with asynchronous logic, a NoC of regular, redundant, many-CLK/VDD cores can deliver the average rather than the worst case system performance with greater power efficiency and fault tolerance than its globally synchronous monolithic counterparts.  This work shows that the Voltage-Frequency Island (VFI) architectures are also the key to tolerating and compensating for PVT variations.The VFI advantages cannot be realized without run-time task-to-core mapping and adaptive network routing that optimally match application resource requirements with heterogeneous cores and communication fabric. These systematic techniques are more effective at mitigating a varietyof faults and variations than layout and circuit DFM. Most importantly, the gains from these techniques can be translated into die yield improvements and smaller DFM guard-bands.This work investigates core sparing and network routing. The developed models demonstrate that core sparing reduces the die cost asymptotically from O(A3) to O(AÂ½), and it is more cost efficient than larger design guard-bands of layout and circuit redundancy. The analysis outcome favors a greater number of smaller unreliable cores as opposed to a fewer larger reliable cores given a fixed die area. This points to the limitations and ultimately the futility of DFM techniques in the future semiconductor process generations.Adaptive network routing enables core sparing. More critically, it simultaneously combats the two sources of network load imbalance: on-die performance heterogeneity from PVT variations and application communication topology. With stochastic PVT variations, the developed Minimal Adaptive Total Congestion (MATC) router increases the expected network saturation bandwidth by 7--23% and reduces its variance by 2--10x as compared to the Dimension Order router. With systematic PVT variations, the improvements are 5--35%. These gains of the adaptive router cancompensate for degradation due to performance variations and can thus be used to reduce design guard-bands.By treating cores as units of fault and variation tolerance, these systematic techniques provide a simple and consistent way to deal with static and dynamic performance variations and faults.These techniques are more effective than isolated DFM solutions. Rather than fighting and minimizing the on-die parametric variations, our approach takes advantage of the platform heterogeneity and manages its net system performance impact.",ucb,,https://escholarship.org/uc/item/0nd0b98v,,,eng,REGULAR,0,0
251,1687,Logical and Substantive Scales in Phonology,"Mortensen, David",,2006,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0sp1b9w8,,,eng,REGULAR,0,0
252,1688,"Circuit Analysis in Metal-Optics, Theory and Applications","Staffaroni, Matteo","Yablonovitch, Eli;",2011,"In the first part of the dissertation we provide electrical circuit descriptions for bulk plasmons, single-surface plasmons, and parallel-plate plasmons. Simple circuits can reproduce the exact frequency versus wave-vector dispersion relations for all these cases, with reasonable accuracy. The circuit paradigm directly provides a characteristic wave impedance that is rarely discussed in the context of plasmonics. Owing to the presence of kinetic inductance, a plasmonic transmission line can support very large characteristic impedances on the order of kilo-Ohms. The ability to adjust the plasmonic wave impedance allows voltage transformer action at optical frequencies, through tapered metallic structures. This transformer action can be used to engineer efficientdelivery of optical power to the nanoscale, or as an impedance matching tool toward molecular light emitters.In the second part of the dissertation we discuss at length the application of plasmonic impedance matching to the problem of heat assisted magnetic recording (HAMR) where an optical antenna is used to concentrate optical power to nanoscale dimensions on the surface of amagnetic hard-disk drive.",ucb,,https://escholarship.org/uc/item/0sx1d5sm,,,eng,REGULAR,0,0
253,1689,Symmetrical Windowing for Quantum States in Quasi-Classical Trajectory Simulations,"Cotton, Stephen Joshua","Miller, William H;",2014,"An approach has been developed for extracting approximate quantum state-to-state information from classical trajectory simulations which ""quantizes"" symmetrically both the initial and final classical actions associated with the degrees of freedom of interest using quantum number bins (or ""window functions"") which are significantly narrower than unit-width.  This approach thus imposes a more stringent quantization condition on classical trajectory simulations than has been traditionally employed, while doing so in a  manner that is time-symmetric and microscopically reversible.  To demonstrate this ""symmetric quasi-classical"" (SQC) approach for a simple real system, collinear H + H2 reactive scattering calculations were performed [S.J. Cotton and W.H. Miller, J. Phys. Chem. A 117, 7190 (2013)] with SQC-quantization applied to the H2 vibrational degree of freedom (DOF).  It was seen that the use of window functions of approximately 1/2-unit width led to calculated reaction probabilities in very good agreement with quantum mechanical results over the threshold energy region, representing a significant improvement over what is obtained using the traditional quasi-classical procedure.  The SQC approach was then applied [S.J. Cotton and W.H. Miller, J. Chem. Phys. 139, 234112 (2013)] to the much more interesting and challenging problem of incorporating non-adiabatic effects into what would otherwise be standard classical trajectory simulations.  To do this, the classical Meyer-Miller (MM) Hamiltonian was used to model the electronic DOFs, with SQC-quantization applied to the classical ""electronic"" actions of the MM model--representing the occupations of the electronic states--in order to extract the electronic state population dynamics.  It was demonstrated that if one ties the zero-point energy (ZPE) of the electronic DOFs to the SQC windowing function's width parameter this very simple SQC/MM approach is capable of quantitatively reproducing quantum mechanical results for a range of standard benchmark models of electronically non-adiabatic processes, including applications where ""quantum"" coherence effects are significant.  Notably, among these benchmarks was the well-studied ""spin-boson"" model of condensed phase non-adiabatic dynamics, in both its symmetric and asymmetric forms--the latter of which many classical approaches fail to treat successfully. The SQC/MM approach to the treatment of non-adiabatic dynamics was next applied [S.J. Cotton, K. Igumenshchev, and W.H. Miller, J. Chem. Phys., 141, 084104 (2014)] to several recently proposed models of condensed phase electron transfer (ET) processes.  For these problems, a flux-side correlation function framework modified for consistency with the SQC approach was developed for the calculation of thermal ET rate constants, and excellent accuracy was seen over wide ranges of non-adiabatic coupling strength and energetic bias/exothermicity.  Significantly, the ""inverted regime"" in thermal rate constants (with increasing bias) known from Marcus Theory was reproduced quantitatively for these models--representing the successful treatment of another regime that classical approaches generally have difficulty in correctly describing.  Relatedly, a model of photoinduced proton coupled electron transfer (PCET) was also addressed, and it was shown that the SQC/MM approach could reasonably model the explicit population dynamics of the photoexcited electron donor and acceptor states over the four parameter regimes considered.    The potential utility of the SQC/MM technique lies in its stunning simplicity and the ease by which it may readily be incorporated into ""ordinary"" molecular dynamics (MD) simulations.  In short, a typical MD simulation may be augmented to take non-adiabatic effects into account simply by introducing an auxiliary pair of classical ""electronic"" action-angle variables for each energetically viable Born-Oppenheimer surface, and time-evolving these auxiliary variables via Hamilton's equations (using the MM electronic Hamiltonian) in the same manner that the other classical variables--i.e., the coordinates of all the nuclei--are evolved forward in time.  In a complex molecular system involving many hundreds or thousands of nuclear DOFs, the propagation of these extra ""electronic"" variables represents a modest increase in computational effort, and yet, the examples presented herein suggest that in many instances the SQC/MM approach will describe the true non-adiabatic quantum dynamics to a reasonable and useful degree of quantitative accuracy.",ucb,,https://escholarship.org/uc/item/0tb3z1jq,,,eng,REGULAR,0,0
254,1690,Public health response systems in action: Retrospective analyses of acute and emergency incidents to inform future preparedness,"Hunter, Jennifer Coleman","Aragon, Tomas J;Reingold, Arthur;",2013,"While public health threats have always existed, federal investment in preparedness has surged in the past decade. Interested in evaluating return on their investments in public health preparedness, congressional and public stakeholders have pushed for the development of assessment, accountability, and improvement measures.  However, the evidence base to support this priority has lagged behind.  There is still little agreement on how to measure, let alone improve, public health response performance.  A number of challenges have been cited as barriers to research advancements in this field, including the infrequent nature of large-scale public health emergencies, heterogeneity of emergency events and of public health delivery structures, and difficulties gaining access to incident leadership during real-world emergencies.  Limitations in our knowledge pose significant barriers to public heath authorities who seek to evaluate their own response capacity and direct resources towards evidence-based improvements.  They also hinder stakeholders' ability to develop valid, reliable, and realistic measures to evaluate how well health public health systems are performing.  In the case of acute events, the quality of public health performance can make a difference in the number of lives saved, illnesses or injuries averted, and the economic and social costs to a community.  Consequently, it is of utmost importance to ensure that the evidence base used to guide the development of standards and measures is the best possible.This dissertation includes examples of three research initiatives that seek to: (1) improve the public health emergency response evidence base by characterizing key structural and functional dimensions of the response systems during real-world urgent and emergency events, (2) identify factors which influence variation in this system, and (3) demonstrate that advancements in conceptual models of public health response operations are both possible and relevant.",ucb,,https://escholarship.org/uc/item/0tm658fd,,,eng,REGULAR,0,0
255,1691,"Students Developing Voices in New Learning Ecologies: Voice, Identity, Position and Function as a Framework to Support Multimodal Investigations of Learning Mathematics over Multiple Timescales","El Chidiac, Fady","Schoenfeld, Alan;",2018,"This dissertation lives at the intersection of two essential and under-researched domains. The first concerns the impact of new pedagogies at the university level. Although lecture is still the dominant mode of teaching in university mathematics, some mathematics faculty have been exploring the use of small-group work as a primary instructional mode. Little has been documented, at a fine-grained level, regarding the impact of small-group learning at university. The second concerns the growth and change of university studentsâ€™ mathematical identities, especially at key points in their academic careers. Far more students begin their university careers intending to be mathematics majors than actually graduate as such, with much of the blame for attrition being placed on the teaching methods used. Moreover, many students who major in mathematics avoid pursuing advanced mathematical studies at the graduate level because of their struggle with proof-intensive courses at undergraduate level. A key question is how students grapple with the demands of these kinds of courses. This dissertation is situated in such a course, a one-semester course in number theory. It provides a detailed examination of the experiences of two focal students as they negotiated the challenges of group work in a mathematically demanding context. A new theoretical framework and methodological tools were created to unpack what took place, at a fine grained level of detail, over three timescales: in classroom groupwork, over the course, and in rapport to the major program. Calls for evidence-based innovative teaching at the college level have been growing since the late 1980s, specifically in STEM (Science, Technology, Engineering, and Mathematics) fields, in large part to mitigate attrition. Inspired by the educational reform in K-12, some faculty have turned to the so-called active learning or student-centered pedagogies. As indicated by this dissertation and other research, such pedagogies have the potential to rekindle some new interests in collegiate students and to foster positive relationships with the disciplines. Nevertheless, there is a history of some student resistance to active learning pedagogies, either because students are not prepared for such pedagogies or have had negative past experiences with the same. To understand the failure and success of innovative pedagogies, research must closely attend to the voices that students develop as they interact with the new learning ecologies.  Building on psychoanalytical, socio-linguistic, and socio-cultural theories, this dissertation (Chapter 1) proposes and leverages a specific conceptualization of voice that addresses studentsâ€™ past, present, and future. Such a conceptualization is needed to account for the realities of undergraduate students, who draw upon their resources, individualized in past experiences, as they engage in learning activities to prepare themselves for the future. The construct of voice is broken into three constituents: identity, position, and function; hence, the name of the VIP+function framework. The framework looks at the individualized identities that students actuate by animating positions in their ongoing activities to accomplish functions that forge their near and more distal futures. Additionally, this project (Chapter 2) enhances the investigation of identity formation and development by re-purposing an existing data collection technique, renamed as stimulated construction of narratives about interactions (SCNI). The SCNI technique attempts to generate data that can be jointly studied by two robust theoretical approaches of identity, narrative and situated approaches, which have been largely independent to this point. The impact of pedagogies should be assessed not only by examining studentsâ€™ engagement or performance in a program or course, but also by their power of transforming studentsâ€™ identities. The case studies reported in this dissertation unpack the processes by which two students, Ted and Bettie, boosted their mathematical identities as they adapted to a proof-intensive course and small-group learning. Ted developed the confidence to pursue advanced mathematical studies (Chapter 3). Bettie faced events that challenged and diminished her mathematical identity, which she restored and strengthened over the course of the semester (Chapter 4). Overcoming several impediments, Bettie developed a new form of active engagement with the content, in contrast to her previous reliance on memorization and practice. (Chapter 5). The case studies of this work document some of the power of the pedagogies of small-group learning, despite their limitations. They reveal processes by which the two focal students were able to support each otherâ€™s learning development through groupwork, in ways that teaching based on lecture would not afford. They also highlight some advantages of having students work in the same group over multiple sessions: In cases similar to the ones discussed here, students needed time to build accurate understandings of each otherâ€™s behaviors and together establish a group culture that optimally supported each memberâ€™s learning processes. In sum, this dissertation explores the impact of new pedagogies on studentsâ€™ identity development by providing theoretical/methodological tools and analytic examples, which are applied to University level with the potential for application at K-12 education as well.",ucb,,https://escholarship.org/uc/item/0v49f7g1,,,eng,REGULAR,0,0
256,1692,Pastoral at the Boundaries: The Hybridization of Genre in the Fourteenth-Century Italian Eclogue Revival,"Combs-Schilling, Jonathan David","Ascoli, Albert R.;",2012,"AbstractPastoral at the Boundaries:The Hybridization of Genre in the Fourteenth-Century Italian Eclogue RevivalbyJonathan David Combs-SchillingDoctor of Philosophy in Italian StudiesUniversity of California, BerkeleyProfessor Albert Russell Ascoli, ChairThis dissertation demonstrates that one of the principal aspects of pastoral is its identity as a metagenre by focusing attention on the Latin pastoral production of the fourteenth-century eclogue revival in Italy, and by integrating that production into larger estimations of pastoral's history.  Long taken to be a closed circuit within, or a derivative offshoot of, the genre's long history, the eclogues of Dante, Petrarch and Boccaccio at once accentuated the metageneric elements of classical pastoral and influenced the future of pastoral representations through their consistent exploration of pastoral's boundary-crossing double move--an expansionary movement outward into the representational terrain of other genres and a recuperative (and incorporative) return to the fold.My first chapter, ""Dante's Two Reeds: Pastoral Hierarchies and Hybrids,"" examines Dante in terms of this double move, positions his eclogues at the origins of a new era of pastoral representation, and addresses why the first continuous tradition of pastoral production in the history of the genre begins with their 165 verses.  I individuate one of the principal causes of their influence to be the epistolary context out of which they were forged.  By sending his eclogues as letters to the protohumanist Giovanni del Virgilio, Dante harnesses the boundary-traversing movement of epistolarity and invests the metageneric movement of pastoral with a new critical thrust.  With this new orientation, Dante utilizes the genre's self-figured humility to interrogate and overturn Giovanni's rigid distinctions between high and low literature and, in the systematically hybrid second eclogue, produces a pastoral fiction that is at once low and high.There is both a vertical and a horizontal aspect to Dante's use of pastoral as metagenre, and though the two are conjoined, my next two chapters argue that Petrarch explores its vertical aspect through a methodical comparison with epic, while Boccaccio explores its horizontal aspect through a narratologically innovative breach of pastoral's diegetic horizon.  In chapter 2, ""Translatio bucolicorum: Pastoral and the Place of Epic in Petrarch's Bucolicum carmen,"" through an analysis of his first eclogue and his letters, in particular Familiares 21.15, I reveal that Petrarch conceived of his collection as a response to the letter that Giovanni had sent Dante, whereby he not only attempts to replace Dante as the originator of the pastoral revival but also figures his collection as an ""answer"" to Giovanni's request for a new Latin epic.  In this light, I proceed to examine Petrarch's pervasive appropriation of epic tropes and strategies, in particular translatio imperii, as he extends the implications of Dante's high-low hybrid to pastoralize the book of epic, and in the process generates the first modern collection of eclogues.In my third chapter, ""Tending to the Boundary: Between Inner and Outer Pastoral in Boccaccio's Buccolicum carmen,"" I redress critical estimations of Boccaccio's usage of allegory in his eclogue collection to show that it is not a departure from ""authentic"" pastoral but rather works in concert with the other innovations he brings to the genre, most noticeably the sharp increase in dramatic movement within the eclogues and the narratological complexity deployed in the songs of their protagonists.  By narrativizing the arrival of allegory to the landscape of pastoral, as well as its departure, Boccaccio drastically increases the genre's representational purview while also maintaining the autonomy of its fictions.  This provokes a doubling of the landscape, which becomes mapped into two distinct but overlapping spaces: an inner pastoral, a more conventional Arcadian scene, and an outer pastoral, a liminal space where the genre encounters and pastoralizes the system of literature beyond its borders.  The cumulative effect of this inquiry is a two-fold recognition: first, while still respecting the distinctions between the pastoral poetics of the tre corone, we can meaningfully speak of the fourteenth-century eclogue revival as a movement; and, second, it was a movement that helped shape pastoral's future as a metagenre.",ucb,,https://escholarship.org/uc/item/0w23t9r3,,,eng,REGULAR,0,0
257,1693,Dynamics of excess electrons in atomic and molecular clusters,"Young, Ryan Michael","Neumark, Daniel M;",2011,"Femtosecond time-resolved photoelectron imaging (TRPEI) is applied to the study of excess electrons in clusters as well as to microsolvated anion species.  This technique can be used to perform explicit time-resolved as well as one-color (single- or multiphoton) studies on gas phase species.  The first part of this dissertation details time-resolved studies done on atomic clusters with an excess electron, the excited-state dynamics of solvated molecular anions, and charge-transfer dynamics to solvent clusters.  The second part summarizes various one-color photoelectron imaging studies on tetrahydrofuran clusters with an excess electron or doped with an iodide ion in order to probe the solvent structure of these clusters.  Finally, a mixed study is presented exploring the effect of warmer cluster conditions on both the binding energies and relaxation times of excess electrons in water clusters.	Time-resolved studies on mercury cluster anions (Hg)nÂ¯ (7 â‰¤ n â‰¤ 20) demonstrate the different timescales of electron-phonon and electron-electron scattering in small systems.  Low-energy (1.0-1.5 eV) excitation of the excess electron to a higher-lying electronic state decays via a cascade through the conduction band on a 10-40 ps timescale.  Conversely, high-energy (4.7 eV) excitation of an electron from the valence band into the conduction band opens a second relaxation pathway: emission of the excess electron via Auger decay.  The larger number of charge carriers and the geometrical changes to the cluster following the creation of the valence band hole state increase the relaxation rate, causing relaxation to occur on a 100s of fs timescale.  The size dependence of both relaxation timescales becomes much less significant around n = 13 near the van der Waals-to-covalent bonding transition seen in other studies of mercury clusters.	The solvated acetonitrile dimer anion, (CH3CN)nÂ¯ (20 â‰¤ n â‰¤ 50) is also studied using TRPEI.  The dimer anion is selectively excited with 790 nm (1.57 eV) pulses and probed with 395 nm (3.14 eV) pulses, detaching both the ground and excited states.  The excited clusters are observed to autodetach on a timescale of ~200-300 fs with no size dependence.  The excited-state autodetachment shows a direct link for the first time between the two different binding motifs observed in the gas phase with the two isomers observed in solution from their absorption profiles.	Electron solvation dynamics following charge-transfer-to-solvent excitation from iodide to small methanol clusters, IÂ¯(CH3OH)n (4 â‰¤ n â‰¤ 11) are also examined with TRPEI.  After electron transfer, the excited state spectrum undergoes significant evolution in both its position and shape.  Considerations of the geometries of the initial iodide-doped methanol cluster as well as the intermediate bare methanol anion cluster and final neutral clusters suggest the electron is solvated, as at least one methanol molecule rotates to bring its hydroxyl group inward toward the cluster center, maximizing the hydrogen bond network.  The observed relaxation timescales for both the vertical detachment energies and the spectral width (5-30 ps) are consistent with this type of motion.  An autodetachment feature is also observed at all pump-probe delays, indicating that this is the primary decay pathway for these clusters, which is consistent with the lack of observed stable methanol cluster anions in this size range.	One-color, one photon photoelectron imaging is applied to study tetrahydrofuran cluster anions (THF)nÂ¯ (1 â‰¤ n â‰¤ 100) to probe the nature of the solvated electron in that solvent.  An anion at the same mass-to-charge ratio as the THF anion is observed, though THFÂ¯ is not expected due to its closed shell electronic structure, high HOMO-LUMO gap and dipole moment.  Two peaks are observed in the photoelectron spectrum for this species, one of which is attributed to a long-chain C4H8OÂ¯ anion formed after ring-opening from the secondary electron attachment.  The other peak is likely due to a metastable THF transient negative ion arising from fragmentation of the larger clusters.  These features persist until n = 5.  By n = 6, the photoelectron spectra change shape, becoming much larger, and maintain that shape through n = 100.   This transition is accompanied by an abrupt change in the photoelectron angular distribution.  These changes are attributed to onset of the solvated electron state in THF clusters.  The binding energy for the smallest cluster of this species is 1.96 eV, much higher than that for other solvated electron clusters at onset.  Extrapolation to infinite cluster sizes yields a bulk value of 3.10 Â± 0.03 eV.  The energetics are analyzed in the frameworks of dielectric continuum theory and the proposed cavity structure for bulk THF.	Iodide-doped THF clusters, IÂ¯(THF)n (1 â‰¤ n â‰¤ 30), are also studied using ultraviolet photoelectron imaging in order to understand the nature of their solvation in THF and in attempt to define their structures.  A substantial decrease in the stabilization energy is seen by n = 9, indicating the coordination number is maximized.  However, the iodide ion continues to be significantly stabilized with addition of THF molecules, suggesting that the solvation shell is not completely closed.  Larger sizes are stabilized in a manner similar to the bare cluster anions.  Ab initio calculations suggest the iodide is at least partially embedded in the solvent cluster near the surface, surrounded by a sub-structure of 7-9 solvent molecules.	 The effect of warmer clustering conditions on electron binding energies and relaxation times in water clusters is investigated by using neon instead of argon as the carrier gas in the adiabatic expansion.  Only isomer I water cluster anions are observed, with their binding energies only slightly perturbed by the change in cluster internal energy.  The relaxation dynamics following p â† s excitation is monitored using time-resolved photoelectron imaging.  Internal conversion lifetimes are seen to be shorter for anions formed in neon compared to those formed in argon, though they appear to converge to the same bulk limit.",ucb,,https://escholarship.org/uc/item/0wj1n3xv,,,eng,REGULAR,0,0
258,1694,Trajectories of Children's Development in Pre-Kindergarten: Six Months of Repeated Measures,"Campbell, Kelly Marie","Cunningham, Anne E;",2016,"Early writing during pre-kindergarten is increasingly the subject of basic research and applied classroom practice. Findings from multiple disciplines highlight the role of young childrenâ€™s print-related skills in predicting and enhancing later literacy abilities. This growing body of research, however, lacks a refined model of writing development for this age group. Moreover, the early literacy field needs streamlined, valid assessments of writing progression from scribble lines to letters, first words, and sentences. In this study I investigate the writing development of 62 children (ages 3 and 4) across six months in a state-funded pre-K program. The Play Plan, a daily activity from the Tools of the Mind curriculum that includes childrenâ€™s speech, drawing, and writing, served as the source of repeated measures.  I created a synthetic, 10-point coding system (Early Writing-10; EW10) to score the continuum of early writing across an average of 16 weekly samples per child.  The inter-coder agreement was excellent (r = .94, p < .001). Findings from Hierarchical Linear Modeling demonstrate that growth in early writing was substantial, highly variable, and often rapid across the 24 weeks sampled.  The overall shape of the trajectory yielded significant linear, quadratic, cubic, and quartic trends.  Among predictor variables, only name-writing ability, assessed at school entry, strongly predicted writing scores at the end of the investigation, but univariate analyses showed higher writing abilities for 4-year-olds than 3-year-olds. Three trajectory profiles of early writing development were categorized: slower, incremental, and rapid. Future steps include examining the trajectory scores as predictors of subsequent literacy development, evaluating the EW10 scoring system in larger and more diverse samples, and extending it to more advanced writing development in kindergarten. I discuss implications for a developmental model of early writing and for the use of early writing assessments by researchers and educators.",ucb,,https://escholarship.org/uc/item/0xn0k1zk,,,eng,REGULAR,0,0
259,1695,"Privatizing Public Health: Social Marketing for HIV Prevention in Tanzania, East Africa","Mahaffey, Erin Elizabeth","Hayden, Cori P;",2012,"This dissertation explores U.S. commercial marketing's influence on HIV prevention programming in Tanzania, particularly the practice of social marketing. Social marketing NGOs in Tanzania uphold the goal of creating commercial markets in condoms and promoting HIV prevention behaviors among the public through commercial advertising. Their aim is to address health inequalities among urban low income communities through application of new theories regarding ""the social"" nature of markets to make privatized access to health goods equitable and sustainable. This dissertation analyzes and historicizes social marketing naturalizations of interventions in human decision-making and economies based on the idea that humans are driven to pursue pleasures which undermine their ability to make rational choices in the interest of health. By drawing on the accounts of individuals - including members of Islamic-based health NGOs, Tanzanian entrepreneurs, and individuals in impoverished neighborhoods targeted by health programs - this work describes the politics and stakes of social marketing interventions, including unanticipated economic and health marginalization. Each of these groups drew from moral understandings of the imbrication of economic, political, and social life to critique the privatization of public health. This dissertation maps these controversies not only as debates about public health and political economy grounded in terms of a critique of private property, but as entailing epistemological and ethical claims about health, markets, and human nature.",ucb,,https://escholarship.org/uc/item/0xt7x708,,,eng,REGULAR,0,0
260,1696,Investigation of Alternative Fuels and Advanced Engine Technology: Improving Engine Efficiency and Reducing Emissions,"Rapp, Vi Hai","Dibble, Robert W.;",2011,"The internal combustion engine has vastly improved over the past 100 years. With global warming and pollution being a rising concern, engineers are working towards improving efficiency and emissions of engines. The spark-ignited engine (or gasoline engine) offers improvement in emissions with a sacrifice in thermal efficiency. The compression ignition engine (Diesel engine) increases the thermal efficiency, due to operation at higher compression ratios, but emits high amounts of particulate matter and oxides of nitrogen (NOx). Although improvements in fuel refinement have decreased the amount of engine pollutants, the output of pollutants for both spark-ignited and Diesel engines is still too great.This dissertation explores two advanced engine concepts with alternative fuels for improving thermal efficiency and reducing emissions in automobiles. The first concept investigated is a spark-ignited internal combustion engine operating using hydrogen, oxygen, and argon. Basic engine theory predicts such an engine will see a considerable improvement in engine efficiency (theoretically ~75%, and in practice ~50% including heat transfer and friction losses) over standard engines. These gains in thermal efficiency are due to argon's monatomic structure, which yields a high specific heat ratio (Î³ = 1.67 compared to Î³ < 1.4 for air). The water produced by the combustion of hydrogen can be extracted in the exhaust by a condenser, allowing the recycling of nearly pure argon in a closed loop system. Therefore, argon re-fueling is not required.Achieving efficiencies above 50% with a hydrogen-oxygen-argon engine, however, is difficult due to engine knock limiting spark advance. In an effort to obtain the highest efficiency of this engine concept, experiments were conducted using single and dual spark-ignition for high argon concentrations. Results showed dual spark-ignition slightly increased indicated thermal efficiency, but was still limited by engine-knock. A three-zone model showed that argon as a working fluid increases in-cylinder temperatures, unburned gas temperatures, and laminar flame speed. The model suggests that specific heat ratio affects end gas temperatures more than increasing flame speed.The second engine concept investigates variables and fuel trends for predicting ignition in homogenous charge compression ignition (HCCI) engines. Octane number, a metric for fuel performance in gasoline engines, and cetane number, a metric for fuel performance in Diesel engines, do not accurately predict fuel performance in HCCI engines. To develop a metric for predicting fuel performance in HCCI engines, correlations between ignition of fuels in an HCCI engine and varying engine parameters are investigated. A relationship between fuel chemistry and ignition in HCCI engines is also explored. Results show that previous methods for predicting ignition do not correlate well with experimental data and auto-ignition is highly sensitive to fuel chemistry.A single-zone well-mixed-reactor model is used to investigate three different mechanisms for predicting auto-ignition in the HCCI engine. All three mechanisms accurately predicted the auto-ignition order of fuels containing isooctane and n-heptane, but did not predict auto-ignition of blends containing toluene and ethanol. Blends of toluene and n-heptane were further investigated using the model to identify potential problems with the toluene mechanisms. The model results showed increasing the amount of toluene linearly by volume did not result in a linear advance in auto-ignition.",ucb,,https://escholarship.org/uc/item/10j3k3k4,,,eng,REGULAR,0,0
261,1697,"Politics and Religion in Late Antique Honorific Monuments: Portrait Heads, Statues, and Inscriptions of the Administrative Elite","Wueste, Elizabeth Wueste","Hallett, Christopher H;",2016,"This dissertation examines the material evidence of honorific statue monuments of the administrative elite throughout the Roman world from the third through sixth centuries CE.  This includes the extant portrait heads, statue bodies, and inscribed bases, and their significance as indicators of and participants in the larger socio-cultural conversation about the relationship between religion and politics during Late Antiquity.  What was the effect of Christianity on local and imperial politics during this transitional period? Were members of the administrative elite pressured to convert to Christianity and advertise their conversion because of imperial pressure? What social benefits and/or liabilities were involved in publically proclaiming religious affiliation?  How is material evidence involved in the projection of religious self identity, especially in public arenas and visual form?  How were these visual messages communicated, understood, and received by the viewing audience? I argue that the honorific monuments of the late antique elite reveal a surprising tension between politics and Christianity, and while neither the honorands nor the honorers fully proclaim their religious affiliations, they are not entirely silent either. I argue we should adopt a more nuanced conception of Christianityâ€™s role in the political landscape during this transitional period precisely because ambiguity, religious fluidity, and a broad, if vague, public appeal was politically and socially useful.  Previous scholarship has tended to isolate either the sculptures or the inscribed bases of honorific monuments and examine them separately, that is, art historically or epigraphically, respectively.  Removed from the archaeological, spatial, and historical contexts, these approaches are fundamentally flawed in that they ignore at least half of the monuments as a whole, and therefore do not consider the most immediate display context.  When components are found and studied in isolation, as is overwhelmingly the case, it may indeed appear that portrait heads are divinely inspired by a Christian god, statue bodies are wearing priestly costumes and holding attributes loaded with religious meaning, and honorific inscriptions are overrun with Christian crosses and direct appeals to God.  However, the components of honorific monuments were deliberately combined by a single agent and were intended to be received as a single statement, and thus should be similarly studied together for full comprehension. This project draws its dataset of portrait heads, statue bodies, and inscribed bases from the excellent database of the Last Statues of Antiquity project, directed by R.R.R. Smith and Bryan Ward-Perkins at Oxford University. My dataset includes evidence from across the Roman world, dating from 284 to 550 CE.  This chronological range encompasses the primary core of the late antique statue habit.  In order to more closely study the interactions between politics and religion, the identity of the honorands has been restricted to isolate the main players in late antique politics.  Therefore, monuments honoring women, athletes, deities, personifications, and heroes, emperors, and the imperial family have been excluded. My examination of the religious and political imperatives communicated by late antique honorific monuments is divided into three sections.  In the first section, I examine the physical evidence independently by component:  portrait heads (Chapter II), statue bodies (III), and inscribed bases (IV). This is appropriate as they are most often found alone and detached from the other parts of the honorific monument, and very few of them can be positively and reliably reattached to their constituent components.  While similar studies have already been conducted on portraits and bodies, the compilation of the honorific inscriptions from this period is novel, and has yielded surprising insights into late antique administration, linguistics, and social structure.  In the second section, I examine the only six monuments Empire-wide that can be reliably reconstructed with head, body, and base as test cases against the much larger corpus of disassembled pieces (V). The combination of complementary and contrasting identities, the mix of new and reused materials, and the varied historical and social contexts they represent produce complex and surprising composites.  In the last section, I extend the results of the six test cases to the larger body of disarticulated elements in order that they might be understood and examined as a cohesive body of evidence, within their literary, archaeological, and especially historical and religious contexts (VI). This dissertation reaches three main conclusions:  1) while the disparate components of honorific monuments are almost always found separately, sheer numbers indicate that they were all part of the same honorific dialogue and should therefore be studied as a cohesive whole; 2) when the components are recombined, the late antique honorific monument was overwhelmingly political in nature, and was not concerned with openly telegraphing religious affiliation, but rather tends to avoid the question; and 3) elite players may not have immediately chosen sides because they were not actively devoted to one religion over another, or more interestingly, because religious ambiguity was socially and politically useful.  Contrary to the traditional scholarship that still posits religious extremism and the cultural crush of Christianity across all realms of life, I argue that the landscape was less polarized and more mediated, and that religious identity was more fluid than we once might have thought.",ucb,,https://escholarship.org/uc/item/10s2r87p,,,eng,REGULAR,0,0
262,1698,"Religion, Politics and Sex: Contesting Catholic Teaching and Transnational Reproductive Health Norms in the Contemporary Philippines","Chow, Jonathan Tseung-Hao","Hassner, Ron E;",2011,"How does religion shape transnational norms and the ways in which they are contested or adopted? Although constructivist international relations theory has made significant strides in understanding the role of norms in shaping political outcomes, there has been little research into how religion affects norm dynamics. This dissertation seeks to address this gap by developing a theory of ""religious norms"", which I define as standards of proper behavior that arise from actors' religious beliefs. I argue that while religious norms bear many similarities to secular norms, they differ in that believers understand them to emanate from the highest authority of all, that of the sacred. This can lead religious adherents to treat religious norms as having overriding importance, especially when they perceive them to be under attack from competing norms. When this happens, religious adherents can frame the religious norm as highly salient, constitutive of the faith and under threat, a process that I call ""defensive sacralization"". Defensive sacralization seeks to mobilize believers in opposition to competing norms and to preserve the integrity of religious norms. At the same time, it can stifle theological debate, harden the boundaries of the faith, and raise the costs of accommodating competing norms, leading to increased polarization through a ""ratcheting"" effect that I call the ""sacralization trap"". I study the nature of religious norms, defensive sacralization and the sacralization trap by attempting to explain why the Philippines, which has signed international legal documents affirming reproductive health (including access to contraception) as a human right, has repeatedly failed to pass legislation that would implement these international obligations. I argue that this failure can be attributed to two main factors: first, the domestic political power of the Roman Catholic Church in the Philippines, which enables it to wield an informal veto in issue areas relating to sexual morality; and second, the Church's defensive sacralization of its teachings against contraception, which it perceives to be under threat from transnational reproductive health norms. Through field interviews in the Philippines with activists, theologians, clergy, government officials and scholars, I show how defensive sacralization has sidelined Catholic theologians who believe that the Church may legitimately accommodate the Philippine state's adoption of a national reproductive health policy. By drawing on the history of the Catholic Church's moral theology on contraception and its response to reproductive health norms at major international conferences, I demonstrate how the Church's defensive sacralization in the Philippines is rooted in a broader transnational normative struggle even as it is conditioned by the Philippines' unique local sociopolitical environment. More broadly, religious norms, defensive sacralization and the sacralization trap provide a new conceptual vocabulary to describe some of the distinctive ways in which religion shapes political processes and outcomes. By apply constructivist international relations theory to the study of religion in politics, this dissertation seeks to begin building a conceptual bridge between the two disciplines.",ucb,,https://escholarship.org/uc/item/125971gn,,,eng,REGULAR,0,0
263,1699,Plant Water Use and Growth in Response to Soil Salinity in Irrigated Agriculture,"Runkle, Benjamin Reade","Liang, Xu;Dracup, John A;",2009,"Soil salinity levels are an important determinant of plant evapotranspiration and carbon uptake.  In this dissertation I develop, evaluate, and test a model of plant evapotranspiration and carbon uptake in the context of a saline soil environment, and drive the model using leaf physiological parameters determined from field measurements.  This modeling work is performed in the context of three research questions: (1) How are leaf gas exchange parameters characterizing photosynthesis in perennial pepperweed best determined for seasonal scale landscape flux analysis? (2) How can the effects of soil salinity on root water uptake be represented in order to account for changes in the diurnal cycle and in the uptake of carbon dioxide by plants? (3) How sensitive are modeled results to changes in model input parameters, and how may these sensitivities limit the predictive abilities of the model? These questions are assessed using data from a relatively wet pasture-peatlands in the San Francisco Bay - Sacrament River Delta region of California, with the dominant land-cover species perennial pepperweed (Lepidium latifolium), a mildly salt-tolerant and invasive herbaceous weed.  	Presented in this research is a characterization of pepperweed as a highly capable invasive species, able to take advantage of local resources such as light, carbon, water, and nitrogen.  Modeling results from each section also demonstrate its ability to photosynthesize under higher temperatures and vapor pressure deficits than standard plant models suggest.  Incorporating soil salinity into a whole-plant model increases the ability to describe how different soil and atmospheric parameters influence evapotranspiration and photosynthesis in such an environment.  The model's sensitivity analysis reveals two pairs of parameters that may constrain each other, and demonstrates how improved measurements of plant conductance and leaf water potential can constrain other portions of the parameter space.",ucb,,https://escholarship.org/uc/item/14h2q0cj,,,eng,REGULAR,0,0
264,1700,Causes and Consequences of Convergence,"Heath, Jevon S.",,2017,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/161690xx,,,eng,REGULAR,0,0
265,1701,Microlocal sheaves and mirror symmetry,"Gammage, Benjamin","Nadler, David;",2019,"In this thesis, based on the work completed by the author during his time in graduate school, we explain various ways in which microlocal sheaf methods in symplectic geometry can be used to prove homological mirror symmetry. We explain how tropical methods originally developed by Mikhalkin can be used to prove homological mirror symmetry for any hypersurface in (C*)^n, and we also present a proof of homological mirror symmetry in the case of multiplicative hypertoric varieties, emphasizing the features which we expect will prove common to all K-theoretic Coulomb branches.",ucb,,https://escholarship.org/uc/item/16v6m174,,,eng,REGULAR,0,0
266,1702,Choreographing 'Productive' Citizenship: On the Cultural Work of Music in NGOs in Uganda.,"Mugishagwe, Allan","Guilbault, Jocelyne;",2013,"This project seeks to examine the cultural work of music in two Non-Governmental Organizations (NGOs) in Kampala, Uganda: Watoto Child Care Ministries and Uganda Heritage Roots (UHR). By `cultural work' I mean the ways in which music is connected to the historical, social, political and economic forces that constitute lived experience and relations between people. Over a period of twenty years (1966 - 1986), Uganda experienced several turbulent political regimes and violent confrontations causing numerous arrests and deaths, as well as poor economic conditions throughout the country. More recently, the AIDS epidemic, the recently ended war in the northern part of Uganda, as well as the issue of poverty throughout the country have escalated the humanitarian problem. Numerous NGOs are currently in Uganda providing assistance to individuals who have been traumatized by these catastrophes. These NGOs aim to engage in development activities that address the quality of life of the aid recipients. The two organizations that are central to my project feature musical practices in their intervention. They teach musical practices to some of their aid recipients as part of the process of creating possibilities for the improvement of their living conditions by raising them into becoming self-sufficient and `productive' citizens. The project is guided by a twofold inquiry: 1) in what ways are musical practices conceived by the NGOs to be crucial in their efforts to create possibilities to improve the living conditions of their aid recipients and to raise them into `productive' citizens, and 2) and in what ways does the learning, practicing, and performance of featured musical practices actually impact the aid recipients' living conditions?",ucb,,https://escholarship.org/uc/item/17d250n9,,,eng,REGULAR,0,0
267,1703,Snakeâ€™s Tail: Modernism and the Paradox of Self-Reference,"Blevins, Jeffrey Scott","Altieri, Charles;",2016,"This dissertation explores how modernists envisioned thinking, judging, and acting in conditions of paradox. I hold modernism up against historical developments in logic, mathematics, and analytic philosophy to argue that T.S. Eliot, Robert Frost, Gertrude Stein, and I.A. Richards generated distinctive aesthetic, phenomenological, and affective responses to paradoxical situations. I anchor the work of these modernists in twentieth-century intellectual contexts with which they were all familiar, including the transition out of classical logic into a supposedly unparadoxical new symbolism; the waning of idealism and subsequent waxing of analytic philosophies; and the drive to â€œcompleteâ€ mathematics. I demonstrate how modernists drew from these contexts the overarching problem of the liar paradox, whose paradoxical self-reference resisted all of logicâ€™s attempts to resolve it. Articulating an aesthetics of paradox that is shaped by, yet often resistant to, these nascent new philosophies that were themselves defined by the liar paradox, modernists attend to the lived consequences, stylistic repercussions, and emotional tonalities of judging and acting in paradoxical situations. I argue that they bear witness to logicâ€™s struggles against paradox with profound consequences for narrative, poetics, form, and style. And I claim that they deepen approaches to logical thinking with a focus on what self-reference looks and feels like as an aesthetic experience: on paradoxes that link stylistic fragmentation with bodily harm (Eliot); self-referential structures that model human suffering (Frost); circular predicates that mimic processes of thought (Stein); and the metalinguistic consequences of self-reference in the context of close reading (Richards). Affective and stylistic dimensions of paradox mediate between the scales of concept, art, and intellectual history: Eliotâ€™s poetic illusions and hallucinations emerge from grammatical self-reference and a graduate-level study of logic; Frostâ€™s depictions of marital strife root in â€œunvicious circlesâ€ that mirror ones Frost studied at Harvard; Steinâ€™s drive to capture consciousness in a totalizing self-referential style carries on a mathematical dream of completeness learned from A.N. Whitehead; and Richardsâ€™s metalinguistic project borrowed from logic to develop many of the formalist tools that literary scholars use to this day. Throughout I draw connections between these aesthetic presentations of paradox and our current literary practices, offering updated accounts of inference, evidence, figuration, and especially formâ€”as logical concept, linguistic quodlibet, literary-critical object, and stylistic protocol.",ucb,,https://escholarship.org/uc/item/180434mb,,,eng,REGULAR,0,0
268,1704,Hemispheric Asymmetry and Multisensory Learning Impact Visual Perceptual Selection,"Piazza, Elise Ann","Silver, Michael A;Banks, Martin S;",2015,"When visual input is consistent with multiple perceptual interpretations (e.g., the Necker cube), these interpretations compete for conscious awareness. The process of determining which interpretation will be dominant at a given time is known as perceptual selection. We study this process using binocular rivalry, a bistable phenomenon in which incompatible images presented separately to the two eyes result in perceptual alternation between the two images over time. In one study, we showed that a well-established asymmetry in spatial frequency processing between the brainâ€™s two hemispheres applies to perceptual selection. Specifically, a lower spatial frequency grating was more likely to be selected when it was presented in the left visual field (right hemisphere) than in the right visual field (left hemisphere), while a higher spatial frequency grating showed the opposite pattern of results. Surprisingly, this asymmetry persisted for the entire duration of a continuously presented pair of static rivaling stimuli (30 seconds), which is the first demonstration that hemispheric differences in spatial frequency processing continue long after stimulus onset.In another study, we found that very recently formed audio-visual associations influence perceptual selection. Here, we used a brief (8-minute) crossmodal statistical learning paradigm to expose subjects to arbitrary, consistent pairings of images and sounds. In a subsequent binocular rivalry test, we found that a given image was more likely to be perceived when it was presented with a sound that had been consistently paired with it during exposure than when presented with previously unpaired sounds. Our results indicate that the audio-visual associations formed during the brief exposure period influenced subsequent visual competition and that this effect of learning was largely implicit, or unconscious.Together, these results demonstrate an unconscious influence of complex perceptual mechanisms on perceptual selection, the process by which we form conscious representations of ambiguous sensory input.",ucb,,https://escholarship.org/uc/item/18782599,,,eng,REGULAR,0,0
269,1705,Active DNA Demethylation in the Arabidopsis Endosperm,"Ibarra, Christian Anthony","Fischer, Robert L;",2012,"DNA methylation is one of the most well studied repressive epigenetic marks in eukaryotes. In plants, DNA methylation silences genes and transposons, and establishes genomic imprinting.  Genomic imprinting is the mono-allelic expression of a gene occurring in a parent-of-origin specific manner. Imprinted genes tend to be expressed in nutritive tissues and structures that function to support the growing embryo.  In mammals, this is the placenta, and in plants this is the endosperm. In Arabidopsis, the female gametophyte contains the egg and central cell, while the male gametophyte contains the vegetative cell and two sperm cells.  Both the central cell and vegetative cells are companion cells to the egg and sperm cells respectively.  When pollen lands on specialized cells at the tip of the gynoecium, the vegetative cell forms a pollen tube that transports the two sperm cells to the ovule. Within the ovule, one sperm fertilizes the egg cell to form the embryo, whereas a second sperm fertilizes the central cell to form the endosperm, which supports embryo development by providing nutrients. In Arabidopsis, the DME DNA glycosylase carries out active DNA demethylation in the central cell of the female gametophyte, which has been shown to establish genomic imprinting in Arabidopsis.  Recently, DME-mediated DNA demethylation has also been shown to occur in the vegetative cell of the male gametophyte, as well. The aim of the work carried out in my thesis has been to investigate the role of DNA methylation and DME-mediated DNA demethylation in the central cell and vegetative cell.  In particular, I was interested in identifying and understanding the regions in the genome where DME-mediated DNA demethylation occurs.  My overall approach was to perform high-throughput bisulfite sequencing using the illuminaTM platform on the Arabidopsis endosperm, embryo, and dme-mutant endosperm, and to analyze their DNA methylation profiles (methylomes).  In my initial study (Chapter II), I measured non-allele-specific methylomes in which endosperm and embryo dissected from selfed-plants were used.  In my subsequent study (Chapter III), I determined allele-specific (maternal versus paternal) methylomes by analyzing endosperm and embryo derived from seeds generated by crossing different Arabidopsis ecotypes.  Because there are no current methods for determining the Arabidopsis central cell methylome, I sought to use the maternal endosperm genome as a proxy for the maternal central cell genome.  Through collaboration, I also had the opportunity to investigate and explore the methylomes of wild-type and dme-mutant vegetative cell and sperm cells, which enabled me to compare the maternal and paternal reproductive methylomes of the plant. To the best of my knowledge, this is the first time a high-throughput DNA methylation study was done on the endosperm of a flowering plant.  With my collaborators, we discovered that the Arabidopsis endosperm exhibits a genome-wide reduction in DNA methylation compared to the embryo.  We also found that the DME DNA glycosylase regulates non-CG methylation in the endosperm, particularly siRNA-mediated CHH methylation.  Moreover, our analysis of DME-dependent DNA demethylation in companion cells suggested a mechanism for reinforcing silencing of transposons in the plant male and female gamete cells.  We found that DME demethylated many of the same loci in both the maternal endosperm and vegetative cell genomes, suggesting that DME has a much more robust and primitive role in Arabidopsis reproduction than previously thought.  Interestingly, many of these DME-dependent sites were heavily enriched in small transposons and repetitive elements.  Moreover, these small transposons tended to reside adjacent to genes, enabling us to propose a new model for the observed phenomenon of gene-adjacent DNA demethylation in Arabidopsis. That is, regulation of gene imprinting may not be the basal function of DME demethylation. These studies suggest that the primary function of demethylation of transposons in companion cells may be to reinforce transposon silencing in plant gametes.",ucb,,https://escholarship.org/uc/item/1d62f8g0,,,eng,REGULAR,0,0
270,1706,"Dynamic actin polymerization on endosomes regulates integrin trafficking, cell adhesion and cell migration","Duleh, Steve Niessen","Welch, Matthew W;",2012,"Activators of the Arp2/3 complex, termed nucleation-promoting factors (NPFs), are required for the proper spatial and temporal control of actin assembly in cells.  Mammalian cells express several NPFs, each of which serve distinct functions in specific cellular processes, including N-WASP in phagocytosis and endocytosis, WAVE and JMY in cell migration, and WHAMM in ER-to-Golgi transport.  Although another NPF termed WASH was recently identified, the cellular function and activity of this protein are not well defined.  We demonstrate that human WASH potently activates the Arp2/3 complex in vitro and in cells.  Furthermore, we show that WASH localizes to early/sorting endosomes and recycling endosomes.  These WASH-positive compartments are often associated with actin networks.  Silencing of WASH or Arp2/3 complex expression by RNAi, or disruption of actin function by drug treatments, leads to enlargement and elongation of endocytic compartments.  Moreover, disruption of actin dynamics or WASH depletion delays EGF transport to LAMP1-positive late endosomes.  These observations indicate that actin polymerization downstream of WASH influences the shape and maturation of endosomes, and shed light on a previously unrecognized role for WASH and the Arp2/3 complex in endocytic trafficking.WASH and actin dynamics have now been implicated in multiple endocytic trafficking pathways including receptor recycling, cargo degradation, and retromer-mediated receptor retrieval.  All of these trafficking pathways converge at the early/sorting endosome, raising the question whether WASH plays a general role in sorting and trafficking cargo through this compartment or if there is a requirement for WASH in trafficking specific cargos.  To answer this question, we must characterize the influence of WASH on trafficking cellular cargos that utilize distinct routes through endocytic pathway.  Here, we investigate the influence of WASH and Arp2/3 complex activity during integrin recycling, cell adhesion and migration.  We observed that subdomains of early/sorting endosomes associate with dynamic WASH and filamentous actin (F-actin), and that alpha5-integrins traffic through this population of endosomes.  Depletion of WASH causes accumulation of alpha5-integrins in intracellular compartments, reduction of alpha5-integrin localization at adhesive structures, and reduction in focal adhesion number.  Recycling of alpha5-integrins from internal endocytic structures to focal adhesions is disrupted upon WASH depletion or Arp2/3 complex inhibition.  Furthermore, WASH-depleted cells display greatly reduced affinity for specific ECM proteins including fibronectin.  Interestingly, the reduced adhesion capacity of WASH-depleted cells results in more rapid migration of these cells in wound healing assays.  These results implicate WASH, the Arp2/3 complex, and actin in the specialized trafficking of integrins.  Our findings highlight a role for actin dynamics influencing cell adhesion and migration via endocytic trafficking of integrins, in addition to the well-established role of actin in plasma membrane dynamics and contractility.",ucb,,https://escholarship.org/uc/item/1df73810,,,eng,REGULAR,0,0
271,1707,Random Matrices and the Sum-of-Squares Hierarchy,"Schramm, Tselil","Raghavendra, Prasad;Rao, Satish;",2017,"We study the Sum-of-Squares semidefinite programming hierarchy via the lens of average-case problems.The Sum-of-Squares Hierarchy is a formulaic family of convex relaxations to polynomial optimization problems, which allows one to trade runtime for accuracy in a smooth manner. The Hierarchy has been studied since the early 2000â€™s, both from the perspective of optimization and control and as a proof system. In the past five years, the Hierarchy has become a focus of intensive study in the theory of computation community. This is because recent results give us reason to hope that Sum-of-Squares algorithms may refute importantconjectures on hardness of approximation. However, our understanding of the guarantees of the Hierarchy remains relatively incomplete. In this dissertation, we present three results which make modest progress towards understanding the power and limitations of the Sum-of-Squares Hierarchy; all three works use average-case problems as a lens for the Sum-of-Squares algorithms, by enabling us to userandom matrix theory as a tool in the analysis.First, we analyze the performance of the Hierarchy for strongly refuting random constraint satisfaction problems (CSPs). We obtain a full characterization of the Sum-of-Squares Hierarchy for strong refutation of random CSPs, and give new subexponential-time strongrefutation algorithms for CSPs with super-linear density.Next, we give impossibility results for solving the planted clique problem via a Sum-of-Squares algorithm, demonstrating that the degree-4 Sum-of-Squares algorithm cannot distinguish graphs which contain a planted clique from uniformly random graphs.Finally, even in the asymptotically polynomial-time regime, the Sum-of-Squares algorithm is often prohibitively slow. We show that for average-case problems, polynomial-time Sum-of-Squares algorithms can often be replaced with fast spectral algorithms, which run in linear or near-linear time in the input size.",ucb,,https://escholarship.org/uc/item/1fx0s9n0,,,eng,REGULAR,0,0
272,1708,Indigenous Literacies in the Techialoyan Manuscripts of New Spain,"Stair, Jessica J.","Olson, Todd;Trever, Lisa;",2018,"Though alphabetic script had become a prevailing communicative form for keeping records and recounting histories in New Spain by the turn of the seventeenth century, pre-Columbian and early colonial artistic and scribal traditions, including pictorial, oral, and performative discourses still held great currency for indigenous communities during the later colonial period. The pages of a corpus of indigenous documents created during the late-seventeenth and early-eighteenth centuries known as the Techialoyan manuscripts abound with vibrantly painted watercolor depictions, alphabetic inscriptions, and vivid invocations of community eldersâ€™ speeches and embodied experiences. Designed in response to challenging viceregal policies that threatened land and autonomy, the Techialoyans sought to protect and preserve indigenous ways of life by fashioning community members as the noble descendants of illustrious rulers from the pre-Columbian past. The documents register significant events in the histories of communities, often creating a sense of continuity between the colonial present and that of antiquity. What is more, they provide the limits of the territory within a depicted landscape using a reflexive, ambulatory model. Representations of place evoke ritual practices of walking the boundaries from the perspective of the ground, enabling readers to acquire different forms of knowledge as they move through the pages of the book and the envisioned landscape to which it points. The different communicative forms evident in the Techialoyans, including pictorial, alphabetic, oral, and performative modes contribute to understandings of indigenous literacies of the later colonial period by demonstrating the diverse resources and methods upon which indigenous leaders drew to preserve community histories and territories.The Techialoyans present an innovative artistic and scribal tradition that drew upon pre-Columbian, early colonial, and European conventions, as well as the contemporary late-colonial pictorial climate. The artists consciously juxtaposed traditional indigenous materials and conventions with those of the contemporary colonial moment to simultaneously create a sense of both old and new. Not only did the documents recount indigenous communitiesâ€™ histories and affirm their noble heritages, they also proclaimed possession of an artistic and scribal tradition that was on par with that of their revered ancestors, thereby strengthening corporate identity and demonstrating their legitimacy and autonomy within the colonial regime.",ucb,,https://escholarship.org/uc/item/0p87f5zh,,,eng,REGULAR,0,0
273,1709,Understanding the Effects of Access to the U.S. Social Safety Net,"Shiferaw, Leah","Hoynes, Hilary;",2020,"This dissertation studies various ways in which federally funded social safety net programs impact low-income households. Over 38 million people in the US live in poverty, and the federal government dedicates nearly one-sixth of its annual budget towards programs designed to reduce the negative consequences of poverty. In this dissertation, I use various empirical techniques and policy settings to explore the causal impact of increasing access to federal assistance benefits on outcomes for those in need.In Chapter 1, I study how reducing the costs of participation in the Supplemental Nutrition Assistance Program (SNAP, previously called Food Stamps) influences program participation and downstream measures of infant health. Incomplete take-up is common in many transfer programs: more than 7 million individuals who were eligible for SNAP benefits in 2017 did not participate in the program. An understanding of the causes and downstream consequences of incomplete take-up is necessary in order to understand whether low take-up of public programs is limiting their effectiveness in reducing economic disparities. In this chapter, I use a unique setting to shed light on the role that participation costs play in determining food assistance take-up, and quantify the effects of increasing take-up on infant health at birth. I do this by studying a reform that made it easier to receive and use food assistance benefits in the US: the adoption of the Electronic Benefit Transfer (EBT) debit card for SNAP benefit disbursement. Prior to the adoption of EBT, SNAP benefits were distributed via food stamp coupons. Thus, the switch to EBT reduced both the time that it took to receive benefits each month and the visibility (or stigma) associated with using benefits at the grocery store. I use the staggered county level rollout of the EBT card in California between 2002 and 2004 to estimate event study regressions of its effect. I find that EBT adoption led to a large and persistent increase in caseloads and applications for the program, as well as higher retailer participation in high poverty neighborhoods. I document that this rise in food stamp benefit take-up led to a meaningful increase in average birth weight for births most likely impacted by the policy, with effects concentrated in the bottom half of the birth weight distribution. These estimates provide new evidence that reducing the barriers to participation in food assistance programs can lead to potentially large gains in health for disadvantaged children. The adverse consequences of childhood poverty are extensive and long-lasting, suggesting an important role for government policies that have direct impacts on early-life outcomes. There is a large literature on the incentive effects of cash tax and transfer programs in the US, yet less is known about how these programs impact the children of recipients. In Chapter 2, I estimate the causal effect of unconditional means-tested cash assistance on outcomes for low-income children by studying the first federally funded cash welfare program in the US: Aid to Dependent Children (ADC), a means-tested program implemented as part of the Social Security Act of 1935.Using newly digitized data on the full population of the US in 1940, I leverage discontinuities in ADC benefit generosity across state borders to estimate the effects of cash assistance within contiguous county pairs. I find that a one standard deviation increase in the maximum ADC benefit was associated with a 2.5 percentage point increase in the probability of school enrollment for disadvantaged children, reductions in overcrowded living arrangements, and evidence of substitutions away from work towards schooling for older children. Consistent with the existing literature on the labor supply disincentives of unconditional transfers, I also find that higher benefit generosity is associated with reductions in maternal labor supply.In Chapter 3, I study the effect of geographic access to authorized retailers in the Supplemental Nutrition Assistance Program (SNAP). When determining the optimal design of means-tested transfer programs, policymakers must consider the features of policy design that may unintentionally serve as barriers to limit the effectiveness of the program. Some barriers, such as those discussed in Chapter 1, may prevent eligible people from participating in the program, while others, like those I explore in Chapter 3, may impact how households use their benefits. Given the substantial geographic variation in access to grocery stores authorized to accept SNAP benefits, it is important to understand whether policies to maximize the number of retailers authorized to accept SNAP benefits are needed. Using a difference-in-differences design, I estimate the effect of a food retailer gaining SNAP authorization in a zip code on household grocery spending behavior. I find that when a small grocer or convenience store gains authorization to accept SNAP benefits, eligible households in the area shift some of their food expenditures away from supermarkets and supercenters and towards these smaller SNAP retailers. I find mixed evidence of the impact of SNAP retailer access on the healthfulness of food purchases. Lastly, I examine whether increased access to a SNAP authorized retailer has any effect on intra-month cyclicality in food spending. I find that access to a new SNAP supermarket or super center does reduce the severity of this ""SNAP cycle"" in food expenditures, with less evidence of any impact for smaller stores. These results suggests that there could be gains to incentivizing increased retailer participation in SNAP.",ucb,,https://escholarship.org/uc/item/0pp0p5v0,,,eng,REGULAR,0,0
274,1710,The Adaptable City: The Use of Transit Investment and Congestion Pricing to Influence Travel and Location Decisions in London,"Broaddus, Andrea Lynn","Deakin, Elizabeth;",2015,"This dissertation investigates two key transportation policies influencing travel behavior and location decisions in London towards sustainability: bus priority and congestion charging. Traffic congestion is a problem faced by cities worldwide, imposing time delays on travelers and decreasing economic efficiency. Congestion is increasing as cities are successful at attracting population and employment growth. This increasing urban density increases competition for urban road space, resulting in traffic congestion and forcing prioritization among road users. At the same time, sustainability goals increasingly set reduction targets for vehicle miles travelled (VMT). Transportation policy must both reduce congestion and VMT, while maintaining a fair distribution of costs and benefits among residents. London has navigated these challenges through pairing bus priority as a â€˜carrotâ€™ and congestion charging as a â€˜stickâ€™ policy. Beginning in the late 1990s with the introduction of a bus priority network, and continuing with the 2003 introduction of congestion charging in the central business district, London has achieved a decade-long trend of declining VMT and mode shift to transit and non-motorized modes. This research investigates the processes of how they were implemented, including consideration of the necessary politics, financing, and institutional authority. The synergistic impacts of these two policies over a decade is investigated in three dimensions: travel behavior, costs and benefits, and firm location decisions. First, the role of bus priority in the successful implementation of congestion charging is explored. Built from 1994 to 2003, Londonâ€™s regional bus priority network was in place when congestion charging implemented in 2003. The development and scope of this bus priority network is discussed using previously unpublished data. It was found that approximately 15% of Londonâ€™s arterial road network was reallocated to create a 1,100 mile (1,800 km) regional network of dedicated bus lanes. An analysis of Census travel to work data before and after congestion charging showed that declines in driving and increases in bus ridership were highest along bus network corridors. It is argued that bus priority explains why car use and VMT began to decline the London region in 1999, and then played a key role in preparing transport system capacity to absorb drivers switching to transit.Secondly, the costs and benefits of Londonâ€™s congestion charging policy are evaluated over the decade since it was implemented in 2003. Due to the rare implementation of congestion charging, this study is the first longer-term evaluation of a congestion charging policy using empirical data. It is the first to consider impacts on Londonerâ€™s culture and attitudes, and reasons why the central charge zone was non-controversial while the western extension zone was removed. The research revealed congestion charging has had wider impacts on traffic levels beyond the charged zone, leading to travel time savings for vehicular traffic throughout Inner London. It had mixed results in meeting its stated goals of reducing traffic volumes, increasing vehicle travel speeds, improving public transit service, reducing vehicle emissions, and improving safety for bicycles and pedestrians. Traffic volumes entering the charged area fell immediately and remained stable, while transit ridership increased and has continued an upward trend. However, travel speeds increased in the short term but fell in the longer term, such that congestion levels today in the charged zone are approximately the same as they were before congestion charging. The study found that this is not considered a negative outcome by city officials. By clearing cars off the roads, congestion charging allowed for a â€˜capacity grabâ€™ where road space and travel time savings were reallocated to buses and pedestrians. Bus speeds and reliability have vastly improved, pedestrian fatality and serious injury rates have plummeted, bicycle use has more than doubled, and air quality has shown some small benefits. City officials assert these outcomes would not have been achievable without the network reconfigurations made possible by released capacity.Thirdly, the longer-term impacts of the accessibility improvements realized in the central charged zone (CCZ) due to bus priority and congestion charging were investigated. It was hypothesized that improved accessibility in the congestion charging zone is being capitalized into higher land and rent costs. Firms valuing accessibility were expected to be the most likely to remain or to move in, in spite of rising rents. This hypothesis was tested using two panels of firms for the period 1997 to 2012 created with microdata from the UK Business Structure Database (BSD). One panel had only micro enterprises with ten or fewer employees, the other had larger firms with more than ten employees. Rent and accessibility data was added to each panel to explore the role of these factors. The study found the concentration of larger firms has been increasing inside the CCZ since 2004. Industry sectors that depend upon agglomeration economies have been concentrating there at the highest rate, especially â€˜knowledgeâ€™ industries like Computers/Telecomm/Research & Development and Business Management Consulting. Evidence was found of increased churn, or rates of firm relocation into and out of the CCZ. For larger firms, the ratio of moves in to moves out increased from 1.02 in 1998-2002 to 1.14 in 2008-2012, indicating a preference for locating in the CCZ. Most firms that moved into the CCZ improved their transit accessibility by 20% to 40%. The net flow of jobs moving in to out also increased from .80 in 1998-2002 to 1.16 in 2008-2012. Retention of tourism sector firms increased in the CCZ, including Theatre & Cinema and Sports & Culture. Sectors vulnerable to rising rents and factor costs, including Retail and Restaurants, had increased odds of moving out of the CCZ. Rising rents were a statistically significant factor for firms moving out, and these firms were likely to have reduced accessibility after the move. The pull of accessibility on firms moving into the CCZ was stronger than the push of rising rents on those moving out.  The dissertation concludes with a discussion of the findings, policy implications, and next steps for research.",ucb,,https://escholarship.org/uc/item/0px3f6gk,,,eng,REGULAR,0,0
275,1711,Essays in Labour Economics,"Subrahmanya Krishnan, Kaushik","Card, David E;",2017,"This dissertation contains three chapters. The first studies police killings in USA and its determinants. Among other things, it finds that federal data collected on police killings undercount the number of kills by at least two-thirds. Additionally, we find evidence that some policy relevant levers such as the racial diversity of the force affect the number of police killings in an area.Chapters two and three both relate to the importance of new firms to a local labour market. Studying the region of Veneto, Italy, I find that new firms benefit the local regions they set up in through the creation of new jobs, many of which go to local inhabitants including those not previously employed. I find evidence that a large new firm opening can affect the hazard rate of reentry into the labour force by 1\%. However, I find limited evidence of spillovers in wages. New firms mechanically raise the wages of their workers but do not affect the overall region's wages. However, they enable the flow information that allows those workers not initially hired by new firms to find better paying jobs, demonstrating the importance of social networks in wage growth and job creation.",ucb,,https://escholarship.org/uc/item/0qz7392x,,,eng,REGULAR,0,0
276,1712,A Polyglutamine Domain Enables Transcriptional Reprogramming in Response to pH Change,"GutiÃ©rrez, Juan Ignacio","Koshland, Douglas;Holt, Liam;",2017,"pH is tightly controlled in the cell as it influences almost all processes. Intracellular pH in Saccharomyces cerevisiae changes according to nutritional state, acidifying as the cell runs out of nutrients. We found that cytosolic acidification plays a role in transcriptional reprograming during carbon starvation. A polyglutamine domain in the SWI/SNF complex senses this pH change, enabling the induction of about 180 glucose-repressed genes. We propose that polyglutamines undergo phase transitions in a pH dependent manner, which allows glutamine transactivation domain proteins to engage and disengage in transcription. This could be a general mechanism for transcriptional reprogramming events within cells.",ucb,,https://escholarship.org/uc/item/0gh3p4mz,,,eng,REGULAR,0,0
277,1713,The assembly of plant-pollinator communities,"Ponisio, Lauren C.","Kremen, Clarie;",2016,"With continued degradation of ecosystems, we need to know how to restore biodiversity --- both for conservation and to ensure the provision of essential services provided by nature. To manage and restore diversity in human-modified systems, however, we need to understand the mechanisms that originally maintained biodiversity. A fundamental and widely supported theory of biodiversity is the idea that diversity begets biodiversity (i.e., environmental heterogeneity, disturbance, biodiversity itself).  These processes contribute to turnover of species through space and time and subsequent heterogeneity of community composition ($\beta$-diversity) --- primary determinants of the total species richness supported by a landscape. Communities are being homogenized as human actions such as habitat conversion, land management practices and invasive species disrupt the processes maintaining diversity.In this dissertation, I examine the assembly of plant-pollinator communities in a variety of landscapes through time and space to better understand how environmental, disturbance and interaction diversity sustains biodiversity. I focus on mutualistic communities because they are influential biological interactions for the generation and maintenance of biodiversity. Plant-pollinator mutualisms are also particularly important for service provision.  Pollination systems, however, are under increasing anthropogenic threats. Understanding how to maintain plant-pollinator community biodiversity is this both timely and imperative.I first investigate the capacity of environmental, disturbance and interaction diversity to sustain biodiversity in a system in Yosemite National Park where nature still drove these processes. In frequent fire forests in Yosemite National Park, California, I found that fire diversity is important for the maintenance of flowering plant and pollinator diversity, and shifts towards lower diversity fire regimes will negatively influence the long-term species richness of these communities. Changing climate and fire suppression are eroding fire diversity and thus homogenizing communities, and thus we must explore management practices that can maintain fire diversity. In these systems, fire diversity is promoted directly through prescribed fires with varied burn conditions and allowing wildfires to burn. These management strategies are already recommended, and my results affirm that their usage should continue and expand.In Yosemite, I was able to examine the mechanisms sustaining diversity in a natural system and make recommendations for maintaining those processes. When a landscape is already degraded, however, we must determine what restoration efforts are able to reassemble functional communities of interacting organisms. This is often the case in agricultural landscapes where widespread conversion of natural ecosystems to agriculture, combined with intensification of farming practices, has led to the homogenization of biological communities.  In Chapter 2, I use a long-term pollinator survey data from the intensively managed agricultural landscape of the Central Valley of California to show that on-farm habitat restoration in the form of native plant ``hedgerows,'' when replicated across a landscape, can re-establish community spatial turnover. I also determined that the mechanism promoting community spatial heterogeneity was the successional dynamics of hedgerow communities promoted the assembly of phenotypically diverse communities, leading to the accumulation of differences in community composition between sites over time. This work elucidates the drivers of spatial and temporal diversity while also validating the role of small-scale restorations such as floral-enhancements for conserving biodiversity and promoting ecosystem services in agricultural areas.To fully understand the mechanisms maintaining communities we must also combine our understanding of the ecological processes enabling their persistence with the evolutionary processes that assembled those communities. Coevolution is a key process producing and maintaining complex networks of interacting species.  In Chapter 3, I use a theoretical approach to determine whether the structure of interactions varied depending on the community's evolutionary history. I found that coevolution leaves a weak signal on interaction patterns. Our results suggest that determining whether assembly processes structure interactions within a community requires a synthetic approach, combining data about the biogeographic history of the interacting lineages and their evolution.",ucb,,https://escholarship.org/uc/item/0gq2646f,,,eng,REGULAR,0,0
278,1714,Combinatorial analysis of continuous problems,"Tsukerman, Emmanuel","Sturmfels, Bernd;Williams, Lauren;",2017,"Many objects in mathematics, at first sight, seem to belong to the domain of continuous mathematics. These objects are continuous, smooth and infinite, far different from the discrete and finite objects that are the classical domain of combinatorics. Objects of the former type are, for instance, determinants of matrices (which can take on every complex value), Grassmannians (which are smooth manifolds), and the eigenvalues of matrices (which take on any tuple of complex values). In the latter class lie objects such as paths in graphs, finite groups and generating functions. Applications of the study of such finite objects to the continuous ones would seem unlikely, or at least, trivial. For example, one may count the number of minors of a matrix, but that's about it. As we will demonstrate, however, this is not the case. The field of combinatorics has developed into a mature field of study, and it is the author's view that combinatorics can be used as a toolbox to obtain interesting and deep information on all areas of mathematics, continuous especially. In this work, we will demonstrate this by studying three different continuous problems using the techniques of combinatorics. The first problem concerns the study of symmetric matrices and their principal- and almost-principal minors. Here the main result is a proof of a conjectural combinatorial formula of Kenyon and Pemantle (2014) for the entries of a square matrix in terms of its connected principal and almost-principal minors. The second problem is the study of Bruhat interval polytopes. These polytopes arise as the moment-map images of Richardson varieties of flag varieties. Their study is motivated in part by the integrable system called the Toda lattice. Information obtained about these polytopes can be readily translated to information about the Richardson varieties. For instance, the dimension of the polytope will be used to determine when the Richardson variety is toric.  The third problem will pertain to the study of the spectral theory of tensors via tropical methods. We show that an elegant theory in which there is a unique tropical eigenvalue arises. We describe briefly how the corresponding eigenvalue informs us of the asymptotic behavior of the corresponding classical eigenvalues.",ucb,,https://escholarship.org/uc/item/0jt7p5sp,,,eng,REGULAR,0,0
279,1715,Mask Edge Effects in Optical Lithography and Chip Level Modeling Methods,"Miller, Marshal","Neureuther, Andrew R.;",2010,"This dissertation presents a full framework for modeling transmission effects due to three-dimensional mask topography in optical lithography from solving Maxwell's equations using rigorous simulation through fast-CAD for full chip level aerial image quality characterization in optical projection printing. As the semiconductor industry advances to the 22nm technology node where features are sub-wavelength, lithography imaging must be accurate to the nanometer. Non-ideal transmission caused by scattering off of mask edges has become an increasingly important source of inaccuracies in lithography modeling. Here mask edge effects are treated in two modules: modeling the near field scattering phenomena and then moving that information into fast-CAD first cut accurate simulation. Phase errors induced by mask edges lead to an asymmetric behavior through focus, which when combined with polarization dependent effects lead to significant loss in the process window. Phase shifting masks, leveraging image benefits of 0 degree and 180 degree transmission, further complicate the interplay of partial signal delay and the resulting complex phase errors. It is shown that for even conservative imaging scenarios up to 40% of the focus latitude is lost. Two methods for characterizing this scattering induced by mask edges are introduced.The first is an experimental approach, which uses gratings to characterize the polarization dependent magnitude of these errors as might be utilized in an inexpensive mask monitoring apparatus. The second method examines the direct near field behavior with simulation, leading to more accurate phase information as well as guidelines for edge-to-edge cross-talk. A MoSi attenuating 180 degree phase shift mask was characterized in detail, with boundary layer values of about 20 nm (1/10 wavelengths) in mask dimensions even for high off-axis illumination. Non-attenuating chromeless masks and complicated mask stacks such as TaSiO2 showed significant electromagnetic errors as high as 1/4 wavelengths, suggesting that they are not viable for advanced lithography applications. Further, a study of a hypothetical thin phase shifting mask showed that the phase error effects is inherent to the use of neighboring phase wells, and cannot be remedied by material improvements.The most significant contribution of this dissertation is the development of Source-Pupil Kernel Convolution with Pattern Matching (SP-KCPM) that connects the information gained from boundary layer modeling to fast-CAD pattern matching tools, achieving a 10^4 speedup compared to conventional imaging. SP-KCPM is built on a computational engine developed by Frank Gennari that optimizes the process of pixel based multiplication of a target pattern across large layouts. The degree of similarity is then used in SP-KCPM to estimate aerial image values. Full complex interactions are included, and along with a pupil-based framework enables more general imaging by including additional phenomena such as defocus, zernike aberrations, measured aberrations, and potentially resist and polarization effects without needing separate kernels or algebraic perturbations. Since the pupil calculation is generated automatically and can combine many effects, the need for deriving and confounding multiple physical phenomena has been eliminated. Proximity effects between features are also accounted for, removing the need for a prior image calculation or restrictions to a specific image contour. A new coherent source model combined with source splitting is used to generalize the aerial image quality assessment to distributed off-axis sources utilized in advanced resolution enhancement techniques.This distributed source-pupil based convolution method has guaranteed impressive accuracy well beyond that historically reported for kernel convolution pattern matching methods at full chip speeds, thus enabling many new applications. Careful implementation considerations such as pattern size, gridding, normalization, and source clustering guided the development of a very accurate system. For various sources, dipole, annular, quad, and pixelated optimized sources, R^2 correlation is shown to be above 0.99. Additionally, effects of defocus, zernike aberrations, background aberrations, and asymmetric sources have all been shown to be accurate. As an example of new applications, SP-KCPM was tested on highly pixelated sources used in source-mask-optimization, and accuracy of R^2 = 0.99 was achieved on general layouts by splitting the source into 12 regions. This capability is used to demonstrate the ability to make decisions between source distributions and mask blanks. Realtime tracking of mask changes facilitates further applicability in optical proximity correction is sufficiently fact for interoperability as part of an optimization scheme. Hotspot detection is used to quickly make decisions between sources or mask types by assessing the impact an optimized source solution over a larger non-optimized layout region. Real time tracking of mask changes opens the door for SP-KCPM to be used for optimization techniques and optical proximity correction (OPC). SP-KCPM is shown to be a general tool, useful wherever fast imaging is at a premium with applicability in many forms of optical imaging such as inspection and character recognition, in addition to standard projection printing.",ucb,,https://escholarship.org/uc/item/0k12z798,,,eng,REGULAR,0,0
280,1716,Recognition Using Regions,"Gu, Chunhui","Malik, Jitendra;",2012,"Multi-scale window scanning has been popular in object detection but it generalizes poorly to complex features (e.g. nonlinear SVM kernel), deformable objects (e.g. animals), and finer-grained tasks (e.g. segmentation). In contrast to that, regions are appealing as image primitives for recognition because: (1) they encode object shape and scale naturally; (2) they are only mildly affected by background clutter; and (3) they significantly reduce the set of possible object locations in images.In this dissertation, we propose three novel region-based frameworks to detect and segment target objects jointly, using the region detector of Arbelaez et. al TPAMI2010 as input. This detector produces a hierarchical region tree for each image, where each region is represented by a rich set of image cues (shape, color and texture). Our first framework introduces a generalized Hough voting scheme to generate hypotheses of object locations and scales directly from region matching. Each hypothesis is followed by a verification classifier and a constrained segmenter. This simple yet effective framework performs highly competitively in both detection and segmentation tasks in the ETHZ shape and Caltech 101 databases.Our second framework encodes image context through the region tree configuration. We describe each leaf of the tree by features of its ancestral set, the set of regions on the path linking the leaf to the root. This ancestral set consists of all regions containing the leaf and thus provides context as inclusion relation. This property distinguishes our work from all others that encode context either by a global descriptor (e.g. GIST) or by pairwise neighboring relation (e.g. Conditional Random Field).Intra-class variation has been one of the hardest barriers in the category-level recognition, and we approach this problem in two steps. The first step studies one prominent type of intra-class variation, viewpoint variation, explicitly. We propose to use a mixture of holistic templates and discriminative learning for joint viewpoint classification and category detection. A number of components are learned in the mixture and they are associated with canonical viewpoints of the object through different levels of supervision. In addition, this approach has a natural extension to the continuous 3D viewpoint prediction by discriminatively learning a linear appearance model locally at each discrete view. Our systems significantly outperform the state of the arts on two 3D databases in the discrete case, and an everyday-object database that we collected on our own in the continuous case.The success of modeling object viewpoints motivates us to tackle the generic variation problem through component models, where each component characterizes not only a particular viewpoint of objects, but also a particular subcategory or pose. Interestingly, this approach combines naturally with our region-based object proposals. In our third framework, we form visual clusters from training data that are tight in appearance and configuration spaces. We train individual classifiers for each component and then learn to aggregate them at the category level. Our multi-component approach obtains highly competitive results on the challenging VOC PASCAL 2010 database. Furthermore, our approach allows the transfer of finer-grained semantic information from the components, such as keypoint locations and segmentation masks.",ucb,,https://escholarship.org/uc/item/0kt4j383,,,eng,REGULAR,0,0
281,1717,"Optimization, Robustness and Risk-Sensitivity in Machine Learning: A Dynamical Systems Perspective","Nar, Kamil","Sastry, Shankar;",2020,"Training models that are multi-layer or recursive, such as neural networks or dynamical system models, entails solving a nonconvex optimization problem in machine learning. These nonconvex problems are usually solved with iterative optimization algorithms, such as the gradient descent algorithm or any of its variants. Once an iterative algorithm is involved, the dynamics of this algorithm will become critical in determining the specific solution obtained for the optimization problem. In this dissertation, we use tools from nonlinear and adaptive control theory to analyze and understand how the dynamics of the training procedures affects the solutions obtained, and we synthesize new methods to facilitate optimization, to provide robustness for the trained models, and to help explain observed outcomes in a more accurate way.By studying Lyapunov stability of the fixed points of the gradient descent algorithm, we show that this algorithm can only yield a solution from a bounded class of functions when training multi-layer models. We establish a relationship between the learning rate of the algorithm and the Lipschitz constant of the function estimated by the multi-layer model. We also show that keeping every layer of the model close to the identity operation boosts the stability of the optimization algorithm and allows the use of larger learning rates.We use a classical concept in system identification and adaptive control, namely, the persistence of excitation, to study the robustness of multi-layer models. We show that when trained with the gradient descent algorithm, robust estimation of the unknown parameters in a multi-layer model requires not only the richness of the training data, but also the richness of the hidden-layer activations throughout the training procedure. We derive necessary and sufficient richness conditions for the signals in each layer of the model, and we show that these conditions are usually not satisfied by models that have been naively trained with the gradient descent algorithm, since the signals in their hidden layers become low-dimensional during training. By revisiting the common regularization methods for single-layer models, reinterpreting them in terms of enhancing the richness of the training data, and drawing an analogy for multi-layer models, we design a training mechanism that provides the required richness for the signals in the hidden-layers of multi-layer models. This training procedure leads to similar margin distributions for the training and test data for a neural network trained for a classification task, indicating its effectiveness as a regularization method.We study the dynamics of the gradient descent algorithm on dynamical systems as well. We show that when learning the unknown parameters of an unstable dynamical system, the observations taken from the system at different times influence the dynamics of the gradient descent algorithm in substantially different degrees. In particular, the observations taken from the system near the end of the time horizon imposes an exponentially strict constraint on the learning rate that could be used for the gradient descent algorithm, whereas such small learning rates cannot recover the stable modes of the system. We show that warping the observations of the system in a particular way and creating risk-sensitivity in the observations remedies this imbalance and allows learning both the stable and the unstable modes of a linear dynamical system.The results in this dissertation lay out the strong connection between the machine learning problems involving nonconvex optimization and the classical tools in nonlinear and adaptive control theory. While analyses with Lyapunov stability and persistence of excitation are able to help understand and enhance the machine learning models trained with iterative optimization algorithms, the major effect of altering the training dynamics on multi-layer machine learning models indicates the potential for improving system identification for dynamical systems by designing alternative loss functions.",ucb,,https://escholarship.org/uc/item/0j0039k0,,,eng,REGULAR,0,0
282,1718,Rule-encoding neurons in prefrontal and auditory cortex of rats performing a task similar to the cocktail party problem,"Rodgers, Christopher","DeWeese, Michael;",2013,"The human auditory system easily solves the ""cocktail party problem"" - that is, even when multiple people are speaking at once, we can easily select and pay attention to a single voice while ignoring the others. Though this seems easy to do, the problem is known to be quite computationally complex. It requires identifying the important sound, selecting it for special processing, and using information from it to make behavioral decisions; meanwhile, the other voices must not be allowed to distract us.How does the brain do this? In chapter 1, I review previous approaches to this question and motivate the choices I made in designing my experiments. In chapter 2, I present the data and conclusions I obtained in collaboration with my advisor, Dr Michael DeWeese. (We are submitting this chapter for publication separately.) In chapter 3, I present a detailed protocol for repeating our behavioral results.The final chapter, Chapter 4, is broader in scope. I discuss how our models and results relate to existing models of prefrontal control over other brain regions. Finally, I consider what my results have taught me about the scientific process of investigating neural function and ruminate on where this field may be headed next.",ucb,,https://escholarship.org/uc/item/0jf2s289,,,eng,REGULAR,0,0
283,1719,Improving Cloud Security using Secure Enclaves,"Beekman, Jethro Gideon","Wagner, Davig;",2016,"Internet services can provide a wealth of functionality, yet their usage raises privacy, security and integrity concerns for users.This is caused by a lack of guarantees about what is happening on the server side.As a worst case scenario, the service might be subjected to an insider attack.This dissertation describes the \emph{unalterable secure service} concept for trustworthy cloud computing.Secure services are a powerful abstraction that enables viewing the cloud as a true extension of local computing resources.Secure services combine the security benefits one gets locally with the manageability and availability of the distributed cloud.Secure services are implemented using secure enclaves.Remote attestation of the server is used to obtain guarantees about the programming of the service.This dissertation addresses concerns related to using secure enclaves such as providing data freshness and distributing identity information.Certificate Transparency is augmented to distribute information about which services exist and what they do.All combined, this creates a platform that allows legacy clients to obtain security guarantees about Internet services.",ucb,,https://escholarship.org/uc/item/0jn0n1xs,,,eng,REGULAR,0,0
284,1720,DUALISM AND NON-DUALISM: ELEMENTARY FORMS OF PHYSICS AT CERN,"Roy, Arpita","Rabinow, Paul;",2011,"The dissertation critically examines the process of discovery, thought and language at the frontier of modern science. It is based on two and a half years of ethnographic research at the particle accelerator complex, the Large Hadron Collider (LHC) at CERN, Switzerland. In March 2010, the LHC began the world's highest energy experiments as a probe into the structure of matter and forces of nature. In the light of the LHC experiments, the dissertation investigates the relation of general beliefs and technical procedures of science with the principles of classification of knowledge, to show how they conjointly constitute a specific cultural or symbolic mode of apprehending the world, and to inquire how this mode is expressed, affirmed and maintained in everyday behavior. Dwelling amongst the particle physics community at CERN, I observed that conceptions of matter and energy were derived from submerged assumptions about how the universe works. These assumptions took the form of proscriptions and dualisms: values do not affect physical reality, the mind does not participate in the universe, or conventions do not impinge on laws of physics. In spite of this, and perhaps more interesting, I found a few puzzling concepts in specific data-sets of theory, experiment and instrumentation, that confront and challenge, quite effectively to my mind, the separations of subject and object, or sign and thing, in a discipline that ostensibly proceeds from their strict separation.  The dissertation examines the classification of handedness (right and left) in particle interactions with the underlying question: Does physics admit of orientation? To characterize right or left presupposes an observer, and conventions. But if physics proceeds from the separation of subject and object, then how can it posit - as it does - a physical universe with a preferred orientation? The focus here shifts to the experimental concept of ""signatures."" Decays from particle collisions, such as a Higgs boson decaying into two photons, are termed signatures and constitute the unit of discovery in particle physics. Focusing on the physics signature, I inquire into the potential relevance of formal theories of semiotics in considering natural signs. Finally, my work explores the rich material culture of the laboratory through the lens of a concept of pure circulation - energy - as it flows in the magnetic fields and currents of the accelerator. By analyzing a concept that attempts to bring together Maxwell's equations of the field with the exigencies of machine parameters, the research arrives at a key moment in the life of a laboratory when the division of theory and practice stands critically exposed.",ucb,,https://escholarship.org/uc/item/0k15w1xt,,,eng,REGULAR,0,0
285,1721,The development and validation of a new coding system for dementia patient-caregiver dyads: the Dyadic Dementia Coding System,"Verstaen, Alice","Levenson, Robert W;",2017,"Dementia is a debilitating disease for the individuals who are afflicted, as well as the family members who care for them. Much research has focused on factors that characterize the symptoms and behaviors of the patient, the traits of the caregiver that enable them to be more or less resilient throughout caregiving, and, to a lesser degree, the qualities of the relationship between patients and caregivers. For the latter, research has focused on marital satisfaction, language use when communicating, and mutual gaze within dyads. However, the behaviors that emerge during interactions between dementia patients and caregivers have rarely been explored. Many dyadic behavioral coding systems are available and widely used, but all of these systems were developed with healthy couples and may therefore lack the ability to capture the nuances of behaviors that emerge during interactions between dementia patients and caregivers. The present study sought to develop a new dyadic behavioral coding system that would accurately capture the behaviors and symptoms of dementia patients and the behaviors and responses of caregivers, as demonstrated by acceptable reliability (inter-rater and split-half) and validity (construct, criterion, and incremental). The sample was composed of 35 patients with Alzheimerâ€™s Disease, 35 patients with behavioral variant frontotemporal dementia, 19 control patients, and their spousal caregivers. The final coding system consisted of nine patient and eleven caregiver codes, which captured patient behavioral symptoms (e.g., inappropriate laughter for patients) and caregiver responses (e.g., guiding appraisal). Results indicated that the dyadic behavioral coding system demonstrated acceptable reliability and validity. In addition, the new coding system was able to distinguish between different kinds of dementia as well as identifying caregivers who are at greater risk of developing depression, above and beyond an existing, commonly used dyadic coding system. Implications of these findings for the development of interventions to improve caregiversâ€™ mental health outcomes are discussed.",ucb,,https://escholarship.org/uc/item/0k76p166,,,eng,REGULAR,0,0
286,1722,Applications of Ionic Clusters in High Resolution Mass Spectrometry,"Leib, Ryan David","Williams, Evan R;",2010,"This dissertation reports a series of experiments undertaken to determine new uses for gas-phase cluster ions by investigation of their formation and thermochemistry.  Clusters of ions and neutrals can be formed from electrospray ionization of peptides, amino acids, metals, and metal complexes in solution, resulting in varied chemical distributions including both homogeneous analyte clusters and heterogeneous clusters, such as hydrated metal ions.  Large hydrated ions are ideal chemical thermometers, which can be used to measure the adiabatic internal energy deposition from a gas-phase reaction simply by determining the change in mass resulting from the evaporation of water from the cluster ion.  Ideally, each of these waters removes a discrete amount of energy - approximately the energy of two hydrogen bonds, or 10 kcal/mol - resulting in a ""ladder"" or ""scale"" of measured energies for a given reaction, as well as a width for the energy distribution imparted to the ion during the reaction.  These robust and adjustable ""nanocalorimeters"" are introduced here and used to determine the thermochemistry of one-electron recombination reactions with metal ions and ion complexes.  These results have key implications for the magnitude and character of the energy deposition in electron capture dissociation, a fundamental technique in top-down proteomics.  Of further interest is the fact that these nanocalorimetry measurements should become more comparable to bulk measurements made in solution as cluster ion size increases.  Initial experiments using these gas-phase measurements to obtain bulk values, such as the absolute value of the standard hydrogen electrode, that are not measureable in solution-phase electrochemistry are demonstrated.  Additionally, clusters of ions formed from a mixture of analyte components should be, in principle, indicative of the stoichiometry of the mixture if they are formed statistically.  Here, statistical analyses of cluster ion distributions are used to obtain reasonably accurate and rapid measurements of peptide and amino acid molar fractions in solution mixtures over a three order of magnitude range in lieu of traditional standards.  These measurements are not possible using individual ions, due to differences in ionization and detection efficiency of the discrete analytes which cause preferential enhancement or suppression of mixture components.  However, within a cluster composed primarily of like components, i.e., a clustering agent, these differences become small, and mixtures of peptides and amino acids containing up to ten components are quantified.  Taken together, these experiments reveal a robust series of applications for cluster ions previously regarded as a detriment to the efficient formation of ions.",ucb,,https://escholarship.org/uc/item/0m27t2wq,,,eng,REGULAR,0,0
287,1723,Cortical and Striatal Circuits for Learning Adaptive Behaviors and Wireless Ultrasonic Implants for Interfacing with the Nervous System,"Neely, Ryan M.","Carmena, Jose M;",2017,"AbstractCortical and Striatal Circuits for Learning Adaptive Behaviors and Wireless Ultrasonic Implants for Interfacing with the Nervous SystembyRyan M NeelyDoctor of Philosophy in NeuroscienceUniversity of California, BerkeleyProfessor Jose M. Carmena, ChairBrain and nerve interface systems have shown early promise for alleviating a wide range of debilitating conditions. In the field of brain-machine interfaces (BMI), movement kinematics have been decoded from cortical neurons and used as a control signal for prosthetic devices. In the periphery, recent insights into the connection between nerves and organ systems has sparked new interest in the therapeutic potential of accessing and altering activity in peripheral nerves. Investigating fundamental mechanisms through which networks of neurons coordinate to produce adaptive responses can inform the design of next-generation nervous system interfaces. However, technological challenges must also be addressed before these systems are ready for widespread clinical adoption.Using a brain-machine interface paradigm, we trained rodents to volitionally modulate the activity of primary visual cortex (V1) neurons. This approach allowed us to observe the instrumental learning process in cortical networks directly, and define the relationship between neural activity and behavioral outcomes. Learning occurred in the absence of visual input, suggesting that modulations were internally driven. Similar to demonstrations of instrumental learning in other cortical areas, learning in V1 engaged and required activity in the striatum, suggesting that cortico-striatal circuits are an essential component for behaviorally-relevant adaptation of cortical outputs. Next, we investigated how factors affecting behavioral choice are represented by striatal neurons as rodents performed a two-alternative probabilistic switching task. We found a rich encoding of task parameters in the dorsomedial striatum both at the level of single neurons and neural populations. We observed activity related to animalsâ€™ confidence in the current state of the task, and found that confidence levels modulated the strength and timing of signals predicting behavioral choice. Finally, we sought to address the limitations of current methods for interfacing with the nervous system. We designed, built, and tested mm-scale wireless implants for recording electrical activity in peripheral nerves and muscles. This system, called neural dust, utilized ultrasonic backscatter as a scalable means for powering and communicating with miniaturized devices implanted deep in tissue. We showed that this system is capable of recording electroneurogram and electromyogram activity with high fidelity in living animals.",ucb,,https://escholarship.org/uc/item/0m85k9qp,,,eng,REGULAR,0,0
288,1724,Renovating Democracy: The Political Consequences of Election Reforms in Post-War Brazil,"Hidalgo, Fernando Daniel","Collier, David;Sekhon, Jasjeet;",2012,"Elections in the wake of  transitions to democracy are often structured by formal and informal institutions that benefit anti-democratic elites and that reduce the potential of expanded suffrage to affect policy.  While most of the writing on counter-majoritarian institutions focuses on formal rules,  the political consequences of  informal institutions that can distort elections' capacity to accurately represent the electorate is less well understood.  Drawing upon historical and recent evidence from Brazil,  this study analyzes the specific aspects of the mechanics of Brazilian elections that interacted with informal practices to over-represent rural conservative interests, increased the ability of conservative political machines to win elections,  and de-facto dis-enfranchised large swaths of the Brazilian electorate. These informal practices had substantial consequences for the quality of representation both during Brazil's first experiment with mass democracy between 1945 and 1964, as well as its most recent experience with widespread suffrage.  Furthermore, this analysis considers the conditions under which interventions--such as the provision of information--can improve the extent to which elections can induce accountability and representation. he first chapter examines a long standing anti-democratic practice in Brazil: the de-facto disenfranchisement of millions of Brazilian voters and widespread voting fraud caused by the interaction of a difficult paper ballot and permissive electoral rules.  To provide such evidence, this analysis exploits the phased adoption of electronic voting in Brazil, a reform that increased the effective franchise in legislative elections by about 33 percent and eliminated fraud in the vote counting process. The research design relies on the fact that the reform was initially implemented in municipalities with an electorate over an arbitrary  threshold and consequently allows for  a regression discontinuity design. The two distinct effects of electronic voting--the enfranchisement of illiterates and other low information voters and the dramatic reduction of fraud--had consequences for the composition of the national legislature.  Against the predictions of recent economic models of democratization, the data show that the enfranchisement of illiterates and other low information voters caused a small increase in the vote shares of right-wing candidates. More importantly, newly enfranchised voters were dramatically more likely to cast a ""party list"" or partisan ballot as opposed to a personal or candidate ballot, which benefitted Brazil's more programmatic and ideologically coherent parties. In states with hegemonic conservative  parties, the introduction of electronic voting induced a roughly 20 percentage point swing against ""political machine"" candidates, which is attributable to a substantial reduction in fraud. Overall, the most important consequences of the reform was the strengthening of Brazil's major parties and a weakening of dominant subnational conservative political machines.The third chapter explores how interventions that increase the amount of information available to the electorate can affect political accountability.  An underlying assumption of much of the literature on political corruption is that if voters are provided with information about the performance of politicians by actors in civil society such as the media and non-governmental organizations, then the election of corrupt politicians is less likely.  Yet, heterogeneous views about the importance of corruption can determine whether increased information changes electoral outcomes. If partisan cleavages correlate with the importance voters place on corruption, then the consequences of information may vary by candidate, even when voters identify multiple candidates as corrupt. This chapter provides evidence of this mechanism from a field experiment in a mayoral election in SÃ£o Paulo where a reputable interest group declared both candidates corrupt. Informing voters about the challenger's record reduced turnout by 1.9 percentage points and increased the opponent's vote by 2.6 percentage points. Informing voters about the incumbent's record had no effect on behavior. This divergent finding is attributable to differences in how each candidate's supporters view corruption. Survey data and a survey experiment show that the challengers' supporters are more willing to punish their candidate for corruption, while the incumbent's supporters lack this inclination.",ucb,,https://escholarship.org/uc/item/15h624fw,,,eng,REGULAR,0,0
289,1725,Faces of Death: Towards the Sociogeny of Adolescent Incarceration,"Falzone, Gabrielle","Nasir, Na'ilah;Carter, Prudence;",2020,"Scholars, policymakers, and practitioners often frame the problem of youth incarceration as a problem of youth criminality; a criminality that stems from young peopleâ€™s inability to control their actions or the inability of families and neighborhoods to control their youth. This focus on criminality obscures other factors that push adolescents into the justice system. This dissertation takes an interdisciplinary approach to reframe the problem; rather than centering criminogenic factors that lead young people into the juvenile justice system, it seeks to understand the sociogeny of adolescent incarceration, that is, the structural and sociocultural factors that lead to incarceration and related intrapersonal suffering. The primary significance of this dissertation is in its novel ecological approach to conceptualize the ways schools and carceral systems contribute to adolescent incarceration and can detrimentally affect adolescent development. Using grounded theory methodology and ethnographic and interview methods with incarcerated youth and formerly incarcerated adults, this dissertation presents an ecological model to conceptualize participant experiences. This model is centered on three Faces of Death, where death is symbolic and represents three forms of suffering that schools and the carceral system inflicted or attempted to inflict on participants. The Three Faces of Death are: social death, which refers to the social suffering associated with criminalization, dehumanization and or disregard of youth; psychological death, which refers to the psychological suffering associated with social death; and biological death which refers to physical health consequences associated with social and psychological death. Together these Faces of Death advance our understanding of how schools and carceral systems are equally implicated in the sociogeny of adolescent incarceration. While this dissertation implicates schools in the suffering of adolescents, it also presents possibilities for education grounded in critical youth studies to help youth heal from, disrupt, and eradicate suffering from the Faces of Death.",ucb,,https://escholarship.org/uc/item/18f347w6,,,eng,REGULAR,0,0
290,1726,"Memory, Story, History: The Formation and Change of Collective Memory and Narrative of the Past in Early China","Kim, Tae Hyun Hyun","Csikszentmihalyi, Mark A;",2019,"Humans perceive and conceptualize who we are by making a consistent and coherent story of the past. Without making this story, existence is fragmented and dissolved into a series of physical, chemical, or biological states that we can only passively accept. Instead, we recall past moments, selecting and linking them to other ones in a logical manner, composing a reasonable story that explains our existence consistently and coherently. Only by choosing, connecting, and sequencing our experiences and signifying them with concepts, and thereby producing an understandable story, can we identify who we are and what we do. 	Constructing a story of the past is similar to composing a narrative fiction whereby we make sense of our identity with pre-existing signifiers, drawing upon values in the culture in order to establish meaning. The moments of existence that are not remembered or not selected in the story-making remain external to the being as if they had never existed. In this regard, we are creatures of our own story. The story provides us with an explanation of our identity through time and legitimizes how we will exist in the future.   Likewise, to identify and explain who the people of a society are and how they should behave, society needs its own story. That is, a society must compose its own story about what it has experienced through time. This group remembrance is referred to as collective memory or social memoryâ€”the constructed ideas of particular past event(s) that individuals have communally experienced. The social memory goes through editing processes such as selecting, excluding, elaborating, emphasizing, deleting, and re-sequencing procedures in the pre-existing linguistic, conceptual, ethical, aesthetic orders of the culture. In this sense, societyâ€™s story is essentially â€œfictionalâ€ in nature. Unlike individual/personal memory, however, those who experienced the same past event are plural in the society. Due to this plurality, there is tension resulting from different story-making of the same event in the past. The attempt to compose a different story about the past is not entirely resolved, but remains as a possibility for alternative story. Diversity in collective memory necessarily causes, in the society, a competition among the plural memories for broader, deeper, and stronger acceptance and recognition of a particular memory by fellow society members. In the contest that is conditioned and affected in political and cultural power-relations, one specific memory and story wins out and becomes prevalent and dominant. It is then imposed and embodied in social regulations such as law and justice, and in cultural practices such as education and mass media. The social story is thus a doing, a performance to be done over and over. In this regard, what the modern mind has termed as â€œhistoryâ€ is a societyâ€™s own self-constructed story that is narrated, written and re-written by its members out of numerous coexisting and competing memories of the past in a repetitive, reconstructive manner. Concerned more with signifying the identity of the society than with concrete facts, history is a dominant story of the memory that the community has come to approve as the narrativistic legitimation of its own identity through time.   Within this theoretical framework, this thesis studies how â€œhistoryâ€ emerged in so-called Early China, the period roughly from Warring States (ca. fifth to third century BCE) to Western Han (206 BCE-9 CE). It explores the cultural practice of sharing and transmitting various earlier collective memories of the past by representing them in the form of short narrative to establish an â€œauthenticâ€ and â€œofficialâ€ memory, i.e., a â€œhistory,â€ by manipulating, editing, revising, or developing the earlier social memories and adopting a developed version of the memory and discourse into the works that had been canonized as the â€œtrueâ€ representation of the past in the cultural tradition. For this, the current study first pays attention to a genre of writing, which I term â€œEpisode Text.â€ Often termed as â€œanecdotesâ€ that assumes to have trivial and inferior nature in cultural significance, the Episode Text represents an earlier social memory of a past event and its narrative representation in the culture. Consisting of a short story in various lengths, about a past event of political or cultural figures and their speech, it is free-standing and self-contained as one independent textual unit in nature. What makes the Episode Texts significant is that many stories in the Texts are comparable to those of transmitted classics of the past. Assuming that the Episode Text reveals earlier collective memory of the past and its literary representation, we can trace how the social memory of the certain past event has changed and developed. By comparing the parallels between the Episode Texts and received classics of â€œhistory,â€ we can see how earlier memories and stories have evolved or were modified when they were recognized and adopted as a part of the canonical texts in the later culture.      The Episode Text remained relatively unknown and paid less attention to until it was re-discovered and re-signified in modern archaeological excavation projects in the late twentieth and early twenty-first century. However, the Episode Text seems already popular in the socio-cultural reality around fourth century BCE, in which a robust cultural need arose for individual political entities to identify their connection to the past, particularly to their great earlier ancestors. The stories offered to explain and legitimize their current status by creating their own stories of the past after the breakdown of the former hegemonic Central State, Western Zhou, which had provided the conceptual, ethical, aesthetic orders to its subordinates with political and cultural power and imposed the Zhouâ€™s story to the subordinate individual entities. In this sense, Episode Texts were made and shared as a social effort for individuated small states to be released from Zhouâ€™s cultural hegemony after its breakdown, to cope with their new socio-political circumstances, to explain their origin, and to justify their existence. This was possible within the changing cultural environment where the one absolute cultural and political power no longer existed, and each entity pursued its own story of the past.  This study focuses on the stories in two canonical classics of â€œhistoryâ€ in Chinese tradition, Zuozhuan and Shangshu, and compares them to the newly found narratives in the Episode Texts that reflect earlier memories of the same events. This study shows that the creation and establishment of these two seminal texts was a long-term process in which earlier social memories were edited and re-written in various ways, including detailing, refocusing, merging, splitting, re-messaging, re-didacticization, deleting, and excluding. Notably, the case of textual comparison between the received â€œWuyuâ€ in the Guoyu and a bamboo slip manuscript found at Cili, Hubei convincingly suggests that long passages that comprise thousands of written characters in the received â€œhistoricalâ€ texts such as Guoyu, Zuozhuan, Shangshu may have been formed by merging several separate Episode Texts into a single text coherently. Generally, how later people cognized, conceived of, and understood what had occurred in the early past has been shaped and framed with these key references. Nonetheless, despite the strong and steady efforts to establish specific memories as a socio-cultural norm in the imperial setting of the Han, there remained intellectual attempts to diverge from the growingly dominant memories and reconstruct â€œhistoryâ€ from different threads of social memory from earlier days in the culture. These disparate threads of memory were also represented in the form of short narrative and widely shared in the society. They were often explicitly critical about the figures or concepts in the increasingly dominant stories. They pursued alternative values, thoughts, and ideas by employing different personalities and a more fictive and imaginative tone and style. The disparate threads of memory explain the plurality of collective memory and the tension for appropriating the past in the society. The received Zhuangzi text exemplifies the intellectual conflict and struggle for domination in remembering the past in Early China.   The cultural process of constructing, establishing, challenging, and reconstructing the normative discourse of the past through canonizing such works is understood as a part of the never-ending, repetitive process of a societyâ€™s own locating, identifying, and legitimating of itself through time. Thus, this thesis concludes that the process was the journey of the early communities to construct and reconstruct themselves as the ideal, the Center State of the cosmos, the state that now is rendered as China. In this course of consolidating discreet memories and producing the dominant ways of remembering and representing the past through canonical texts, the early societies were dreaming of themselves becoming that Center Stateâ€”namely, China.",ucb,,https://escholarship.org/uc/item/19d9m7pr,,,eng,REGULAR,0,0
291,1727,The Fluid Dynamics of Odor Capture by Crabs,"Waldrop, Lindsay D.","Koehl, Mimi AR;",2012,"Olfaction is an important process by which animals sense dissolved chemical cues (odors), in the surrounding fluid. Many organisms, including malacostracan crustaceans, use information provided by odors throughout their lifetimes to find suitable habitats for larval settlement, locate food, interact with conspecifics, avoid predation, locate mates, and mediate reproductive behaviors.	The first step in smelling chemical signals in the environment is capture of odorant molecules from the fluid around an organism. I used the crab, Callinectes sapidus, to study the physical process of odor capture. The lateral flagellum of a crab antennule, which bears a dense tuft of chemosensory hairs (aesthetascs), flicks through the water with a rapid downstroke and a slower return stroke. We measured fluid flow around and through the aesthetasc arrays on dynamically scaled models of lateral flagella to determine how such brush-like antennules sample the water around them. During the rapid downstroke of the flick, the closely spaced chemosensory hairs on the upstream side of the lateral flagellum splay apart passively because they are flexible. Water flows into the spaces between the aesthetascs in the array during the downstroke and then is retained within the array during the slower return stroke, when the hairs are pushed closer together into a clump on the downstream side of the flagellum. There is enough time during the return stroke and inter-flick pause for most of the odorant molecules in that trapped water sample to diffuse to the surfaces of the aesthetascs before the next flick, when a new water sample is taken. Thus, an antennule of C. sapidus takes a discrete sample of ambient water and the odorants it carries each time it flicks, so each flick is a sniff. I found that the flexibility of aesthetascs, which causes the gaps between aesthetascs to widen during the downstroke and narrow during the return stroke, has a big effect on water flow through the array of aesthetascs, but only if the antennule moves rapidly. Likewise, positioning the aesthetasc array on the upstream rather than the downstream side of the lateral flagellum only causes an increase in water speed within an array if the antennule moves rapidly. The change in antennule speed between the rapid downstroke and the slower return stroke of a flick is more effective at altering water speed within an aesthetasc array if the gaps between the aesthetascs are wide. Ambient water current enhances the sniffing by flicking antennules of C. sapidus.	Capturing odors from the surrounding water is important not only for adult crabs, but also for juvenile crabs, which can be up to two orders of magnitude smaller in length than adults. Sniffing by the flicking antennules of aquatic malacostracans occurs when water can flow into the aesthetasc array during the rapid downstroke and can be trapped in the array during the slower return stroke, which only occurs in a narrow range aesthetasc diameter, spacing, and flicking speed.  Juvenile crabs have much smaller antennules than adults, but they do display flicking behavior. Are crabs able to sniff throughout their ontogeny? I used the Oregon shore crab, Hemigrapsus oregonensis, as a model organism to study the ontogeny of sniffing by antennules.  Antennule flicking behaviors of crabs ranging in carapace width from 4 to 28 mm were recorded using high speed videography, and a morphometric analysis of one antennule from each of these crabs was conducting using scanning electron microscopy. Many features of antennule morphology (including aesthetasc diameter and length, antennule width, and number of aesthetascs) increased at a rate of only 28-60% the rate of increase in carapace width. Velocities of the downstroke and return stroke keep juvenile crabs within the range in which sniffing for malacostracan is possible. Furthermore, the aesthetascs on the antennules of juvenile crabs are bent and splayed apart more during the rapid flick downstroke than are those of adults.  I modeled the exoskeletons of aesthetasc s of different sizes as a hollow, cylindrical cantilevers and calculated their deflection by the hydrodynamic force they experience during the downstroke. I found that the greater deflection of juvenile aesthetascs was not due to differences in antennule shape, but rather to ontogenetic changes in the material properties of the aesthetasc cuticle.	To investigate water flow within aesthetasc arrays due to flicking during ontogeny, we constructed dynamically scaled physical models of antennules representing animals at seven different stages of growth, from 5-mm to 25-mm carapace width. I used particle image velocimetry (PIV) to measure fluid flow within the array of each model. Average water speed within the aesthetasc array increased with carapace width for the downstroke, where water speed was consistently low in the array during each return stroke. For each carapace width modeled, the fraction of the dendrite-containing area of the array that was refreshed with new water was high during the downstroke and very low during the return stroke, supporting the hypothesis that crabs' ability to sniff starts as early as post-settlement and continues throughout a crab's lifetime.	The fluid dynamics of discrete odor sampling are well studied in several marine species (spiny lobster, blue crab, Oregon shore crab), but odor sampling is unstudied in their air-dwelling relatives. The physics of odor capture in air is different than in water. Air has a higher kinematic viscosity than water, affecting the flow around the antennule. Molecular diffusivities in air are typically three to four orders of magnitude higher than water, affecting the delivery of odor molecules to the surface of aesthetascs. The terrestrial hermit crabs (Anomura: Coenobitidae) live their entire adult lives on land. The antennules of terrestrial hermit crabs have arrays of short, leaf-like aesthetascs in an shingle-like arrangement, and crabs wave their antennules back and forth in a motion similar to flicking by marine crabs. To investigate odor capture by antennule flicking of a terrestrial hermit crab, I examined: 1) the kinematics of antennule flicking by filming flicking behavior of ruggie hermit crabs (Coenobita rugosus);  and 2) the antennule morphometrics by collecting the antennules of each crab for scanning electron microscopy. I constructed a dynamically scaled physical model of the antennule, and particle image velocimetry (PIV) was used to measure fluid flow around the model. PIV results show that no air flows between the aesthetascs on the antennule's array during either the downstroke or return stroke. Although terrestrial hermit crabs share similar antennule movements and morphological features with marine crabs, they do not discretely sample odors. An odor-transport model based on the convective currents measured with PIV and diffusion of a common odorant was created to investigate the odor-capture performance of the antennule of C. rugosus in air and in water. This model revealed that odor-capture performance is three times higher in air than in water. In air, the majority of odor capture occurred during the downstroke (99.9%). In water, the trend was reversed and the majority of odor capture occurred during the return stroke (94%). Flicking movements serve to increase the probability of encountering odors in environmental air flow, and the high molecular diffusivity in air removes the need for a fluid-trapping return stroke, as seen in aquatic malacostracans.",ucb,,https://escholarship.org/uc/item/1b07356q,,,eng,REGULAR,0,0
292,1728,Spatial Modeling of Decentralized Wastewater Infrastructure: The Case for Water Reuse and Nitrogen Recovery,"Kavvada, Olga","Horvath, Arpad;Nelson, Kara L;",2017,"Climate change and increasing patterns of drought throughout the world are challenging the effectiveness of conventional water systems. A growing population in conjunction with more extreme weather events, threatens water supply infrastructure and increases uncertainty about how utilities will meet demand without sacrificing water quality. This issue has recently manifested in California, prompting utilities to invest in alternative water sources as a means of ensuring that water infrastructure is resilient to climate change scenarios.Decentralized wastewater treatment is a promising option for increasing the sustainability of water infrastructure as it spatially merges supply and demand, minimizing large conveyance requirements. Decentralization can also promote nutrient management and recovery as it enables the source separation of the different wastewater sources. Specifically closing the nitrogen loop, by capturing it and reusing it to generate valuable high-end products can potentially improve the efficiency of the system and create revenue streams. However, smaller decentralized water treatment units are potentially more energy intensive and costly than their centralized alternatives per unit of water treated. Due to these efficiency tradeoffs, planning tools and frameworks for holistically assessing decentralized water treatment systems need to be developed to optimally manage the new urban water supply paradigm. Better data management and data-driven decision support tools can provide valuable insight on the benefits and impacts of implementing future water systems.This research assesses the technical performance of emerging decentralized technologies and implementation scenarios for residential uses, by assessing the feasibility of integrating decentralized facilities in cities with existing wastewater infrastructure. This work aims to create algorithmic models that integrate the spatial design of a wastewater treatment and distribution network with a life-cycle assessment to determine the associated environmental impacts. This dissertation utilizes spatial modeling to contextually evaluate the implementation and distribution potential and uses a life-cycle assessment approach to provide an extensive analysis of all the life-cycle impacts. By incorporating environmental indicators and metrics, a planning support framework can be created to help guide the water industry towards smart investments for a less energy-intensive future. Specifically, this work will 1) investigate how spatial terrain variability, demographics and distribution affect the performance of decentralized water treatment systems, 2) analyze the major parameters that affect the energy intensity, cost and greenhouse gas emissions of these systems, 3) quantify the unit processes that mostly impact the prementioned metrics and 4) identify the optimal system scale for decentralized infrastructure implementation under various spatial and demographic conditions.The insights from this dissertation can help wastewater researchers and practitioners understand the complex relationships between the system scale and system performance. By evaluating the potential benefits and tradeoffs, this work can lead to management tools that will help transition away from traditional water management and create a water supply that (1) is resilient to changes in climate, (2) uses local water sources, and (3) leaves more water in natural ecosystems. This dissertation further adds to the growing body of literature on decentralized wastewater treatment assessing optimal scales, reuse potential, resource recovery and sewerage connections to investigate key factors affecting future implementation.",ucb,,https://escholarship.org/uc/item/1d0898f6,,,eng,REGULAR,0,0
293,1729,â€œComique et Laidâ€: Bitter Laughter and Dystopia in Francophone Caribbean and Urban Literatures,"Stofle, Corine Odile Laure","Britto, Karl A.;",2020,"In spite of the flashy, highly consumable traits that have earned it considerable success in the last two decades, the work of literary dystopias is complex, engaging nothing less than the past, the present and the future. It is easy to forget that beyond the stunning special effects of its cinematographic productions, or the suspenseful, page-turning poetics of its literary expressionsâ€”in short, beyond its undeniable entertainment valueâ€”the dystopian genre, in its most recent iteration, aims above all to shake the world out of the dangerous complacency of late-stage capitalism. There exists a tension between the genreâ€™s extravagant aesthetics and the latent threats it wishes to signal. Onscreen or on the page, protagonists vie for their physical lives in ostentatiously dangerous situations. These are metaphors for other, often invisible yet very real threats, including social death and cultural death are such threats. My dissertation explores specifically how authors of francophone expression seek to represent the experience of minoritized subjects by utilizing dystopian tropes that render these invisible threats visible. I take as a point of departure that the RÃ©publique universelle franÃ§aise continues to be animated by a utopian ethos inherited from a perennial humanist tradition. Still in search of an essential, universal understanding of both Frenchness and humanity, it struggles to relate to those among its citizens whose ontological particularities resist its homogenizing impulses. In its new postcolonial iteration, France seems to envision itself as having achieved utopia: it purports to be a good place. But for those who do not comfortably fit within these ideological walls, this utopia is perceived as a dystopia: a bad place. To be sure, it is a soft dystopia, one that operates on lower frequencies, which allows it to continue to pass as a utopia. My dissertation contends that the presence of dystopian tropes in the literary and cinematographic works I explore exposes the RÃ©publique universelle franÃ§aise for what it is. Fictional dystopia emerges out of and unmasks its real-word, underhanded alter ego.The frequent presence of laughter alongside these dystopian tropes is striking, and calls for close consideration. How can one laugh in the face of cultural death? What is the function of this laughter? Does it serve to soften the moral indictment of the Republic? Does it serve to render the critique more vitriolic, tipping it toward the satirical? Does it constitute a self-soothing gesture for the writer or the artiste? Can this laughter also be complicit? Can it be the sign of the surrender of the self to the powerful appeal of utopia? The search for answers to these questions has brought the present work to the intersection of postcolonial critique, utopian/dystopian studies and laughter theory.",ucb,,https://escholarship.org/uc/item/1fc7d3rc,,,eng,REGULAR,0,0
294,1730,The Demand-Withdraw Communication Pattern in Middle-Aged and Older Couples: A Longitudinal Study,"Holley, Sarah Rachel","Levenson, Robert W;",2010,"The demand-withdraw interaction pattern is a common, deleterious pattern in which one spouse blames or pressures while the other spouse avoids or withdraws (Christensen, 1988). Studies consistently show that: 1) there tends to be gender differentiation in the interaction roles, with women demanding and men withdrawing, and 2) demand-withdraw behaviors are associated with marital dissatisfaction.  The existing observational research on marital interactions, however, has been overwhelmingly conducted with relatively young couples and does not take into account other known predictors of marital dissatisfaction.  The present study examined demand-withdraw behaviors longitudinally in a sample of middle-aged and older couples.  Later life stages may be associated with changes in emotion-related behaviors (e.g., Carstensen, 1991) and in gender roles (e.g., Gutmann, 1987).  Studying demand-withdraw behaviors over time in middle-aged and older couples enables a determination of whether the manifestations of this set of behaviors, and its negative association with marital satisfaction, change during the later stages of development.  This study further evaluated the effect of demand-withdraw on marital satisfaction in relation to other factors known to be negatively associated with this important outcome (e.g., physiological arousal, self-reported negative affect, and negative emotion behaviors during conflict). A sample of 126 married couples (63 middle-aged, 63 older) were observed at three time points across a 13-year span as they engaged in a 15-minute conversation about an area of relationship conflict.  Conversations were videotaped and trained raters used an observational coding system to quantify each partner's demand and withdraw behaviors.  During or shortly following the conflict conversations, measures of physiological arousal, self-reported negative affect, and emotion behavior were also collected.  The couples also completed self-report measures of marital satisfaction at each of the three time points of observational data collection, as well as at two subsequent time points.Results showed that demand-withdraw behaviors occur during conflict in both middle-age and older couples at overall comparable rates.  Combining this finding with previous work indicates that this pattern is found throughout the life course.  Importantly, the specific pattern of behaviors changes with age.  There appears to be a marked increase in one type of withdraw behavior, avoidance, over time.  Moreover, contrary to theories proposed by Gutmann and others that gender differences diminish in later life, gender differentiation appears to become greater over time with wives in the demand role and husbands in the withdrawing role.  Results further demonstrated that the relationship between demand-withdraw behaviors and marital dissatisfaction remains the same across the lifespan.  That is, for both middle-aged and older couples, demand-withdraw behaviors are negatively associated with concurrent levels of marital satisfaction.  Furthermore, these behaviors show significant interactions with other factors known to be associated with lower levels of marital satisfaction (e.g., physiological arousal, self-reported negative affect, and negative emotion behaviors). The general pattern was that demand-withdraw behaviors, while deleterious on their own accord, are particularly pernicious when manifest in the context of other factors associated with marital dissatisfaction.  When examining the longitudinal effect of demand-withdraw behaviors, however, results indicated that after controlling for initial levels of marital satisfaction, these behaviors were not strong predictors of the trajectory of change in marital satisfaction over time. The findings are discussed in terms of socioemotional changes couples undergo as they move from middle-age into late life.  This study offers evidence not only about changes in demand-withdraw behaviors themselves but also as to how marital processes and gender roles may change over the life course.  Future work will be valuable in further elucidating changes in the nature of demand-withdraw behaviors and in the consequence of this interaction pattern for marital satisfaction at different life stages.",ucb,,https://escholarship.org/uc/item/1g37d6w6,,,eng,REGULAR,0,0
295,1731,"The Systematics, Evolution, and Ecology of Hawaiian Cydia (Lepidoptera: Tortricidae)","Oboyski, Peter T.","Roderick, George K;",2011,"Cydia HÃ¼bner 1825 is a genus of moths in the family Tortricidae with 231 named species and subspecies and is distributed on all continents except Antarctica. As larvae, many species feed within reproductive structures, such as fruits, seeds, and flowers, under bark, or within fleshy stems of at least 65 host-plant species including angiosperms and conifers. Many species, including codling moth, pea moth, spruce seed moth, pine seedworms, filbertworm, and hickory shuckworm are considered pests of agriculture and forestry. As a result, the biology, natural enemies, and pheromones of several species have been well-studied. The nomenclature and classification of Cydia has also been well-studied but is less resolved. Nineteen different genus names have been proposed for species in this genus, with Laspeyresia HÃ¼bner, and Carpocapsa Treitschke being in common usage until relatively recently. Following the rules governing the International Code of Zoological Nomenclature, Cydia is the valid genus name for all species congeneric with the codling moth, Cydia pomonella (Linnaeus), the type species of the genus. The relationship of Cydia to other genera in the tribe Grapholitini is a topic of continued debate. Some authors have suggested that the tribe is an evolutionary grade while others have presented evidence that Grapholitini is a monophyletic clade. Although some secondary sexual characters have been proposed, the genus Cydia can claim no synapomorphies that can be found in all Cydia species. To better understand the systematics and evolution of this group more detailed morphological, molecular, and ecological data are needed for non-pest species.At least 21 endemic species of Cydia are known from the Hawaiian Islands. Males of most species have a ventral pouch below the cubital vein of the hindwings similar to C. latiferreana (Walsingham), C. maackiana (Danilevsky), and several other Cydia species to a lesser extent, although this feature appears to have arisen independently in the Hawaiian group. Larvae, where known, feed on endemic plants in the family Fabaceae. Identification of species is made difficult by extreme polymorphism of wing patterns for some widespread species and a general reduction of morphological features in the genitalia of male moths, while some features of female genitalia, particularly the antrum and lamella postvaginalis, have diagnostic value. Eight new species of Hawaiian Cydia are described (C. mauiensis n.sp., C. velocilimitata n.sp., C. haleakalaensis n.sp., C. makai n.sp., C. koaiae n.sp., C. hawaiiensis n.sp., C. acaciavora n.sp., and C. anomalosa n.sp.) based on wing patterns and features of male and female genitalia. The thirteen previously known species are redescribed because original descriptions were inconsistent among authors and based solely on wing patterns. Distributions, host-plant affinities, and natural enemies for each species are discussed.A molecular phylogeny of 66 specimens representing 14 Hawaiian Cydia species plus 20 outgroup species was constructed using nuclear and mitochondrial DNA to assess the relative importance of host-plant affinities and geographic isolation in their diversification. Hawaiian Cydia is monophyletic and nested well-within the genus. They appear to have arrived in the Hawaiian Islands after the rise of Maui based on the basal position of several Maui and Hawaii Island species throughout the phylogeny. The earliest diverging species feed on Canavalia and dispersed across the high islands. Subsequent shifts to feeding on Sophora chrysophylla then Acacia koa were followed by speciation and the filling of these niches across the islands. The origin of Hawaiian Cydia remains obscure, but appears to be a separate colonization of remote Oceania from Cydia pseusomalesana Clarke in French Polynesia. It is likely that several more species of Hawaiian Cydia await discovery while several others probably have gone extinct in the 100 years since they were first collected. A broader survey of outgroup taxa from Asia and the Americas, and more informative genes in a molecular phylogeny may help resolve the origins of Hawaiian Cydia.",ucb,,https://escholarship.org/uc/item/0ns8m4m3,,,eng,REGULAR,0,0
296,1732,Decoding the mechanisms of cancer and stem cell immortality,"Chiba, Kunitoshi","Hockemeyer, Dirk;",2018,"Telomeres are the repetitive sequences at the ends of linear chromosomes. Thekey functions of telomeres are to protect the cells from losing genomic information and toprevent chromosome ends from being repaired by the double strand break repairmachinery. To counteract loss of telomeric DNA, cells can express a reversetranscriptase, telomerase, that synthesizes telomeric repeats de novo. In humans,telomerase activity is mostly restricted to germ and stem cells, so the telomeres of mostsomatic cells progressively shorten with each cell division. Once telomeres becomecritically short, they are recognized as sites of DNA damage and cells cease to proliferate.By this mechanism, telomere shortening functions as a tumor suppression mechanism.TERT, the protein component of telomerase, becomes silenced once stem cellsdifferentiate. However, in 90% of cancer cells, TERT is transcriptionally re-activated.Thus, telomerase regulation is crucial for our understanding of telomere length regulationin stem cell maintenance and tumorigenesis. To understand how telomerase acts ontelomeres, I attempted to endogenously tag telomerase. To do this I inserted epitope tagsat the endogenous TERT locus in hESCs using genome editing. However, I found that allthe tested tags cause defects in telomere maintenance, which was previously notappreciated in experiments using exogenous overexpression.Recently, point mutations in the TERT promoter were identified as the mostfrequent non-coding mutations in cancer. To elucidate the role of TERT promotermutations (TPMs) in tumorigenesis, I genetically engineered these TPMs into humanembryonic stem cells (hESCs) using genome editing. Using the resulting isogenic hESClines, I demonstrated that TPMs lead to a failure of TERT silencing upon differentiationfrom stem into somatic cells. To understand role of TPMs in tumorigenesis, I monitoredlong-term telomere maintenance and proliferation in human fibroblasts engineered tocarry TPMs. I found that TPMs immortalize cells but do not prevent telomere shorteningand telomere fusions. In vitro, around the time when telomere fusions occurred, TERTexpression was gradually increased. Thus, TPMs are required, but not sufficient, forcancer cell immortality and contribute to tumorigenesis in two steps. First, TPMs expandproliferation capacity of a cell by elongating only the shortest telomeres but do notprevent overall telomere shortening. In the second step, TPMs fuel tumorigenesis by notfully suppressing genomic instability. In order for cells to immortalize they need toupregulate TERT during this second step.",ucb,,https://escholarship.org/uc/item/0rd63701,,,eng,REGULAR,0,0
297,1733,NEM Relay Scaling for Ultra-low Power Digital Logic,"Yaung, Jack","King Liu, Tsu-Jae;",2014,"CMOS has been the building block for modern digital logic for decades and the performance and energy efficiency of CMOS continues to improve as technology develops, mainly through scaling. However due to the 60 mV/dec limit on MOS transistors, continue to reduce the power supply voltage would result in an increase in off-stage leakage current that would eventually dominate and increase the energy per operation of a transistor. In order to overcome this barrier, mechanical switches are proposed. Mechanical relays has the benefit of no leakage current through the air gap in the off state, which potentially enables further scaling of power supply voltage that can surpass MOS transistors. Several groups have been able to demonstrate mechanical switches with no leakage and abrupt on-off switching characteristics, however both the sizes of switches (~104 Î¼m2) and the operation voltages (> 10 V) are huge, causing the switching energy to be significantly larger than MOS transistors. Scaling efforts are needed to minimize the switching energy of a mechanical switch. In this thesis, prototype relay devices demonstrated by previous studies are discussed and the key factors that need to be addressed in order to minimize the switching energy are pointed out. The minimum switching energy is found to be limited by the contact adhesive force between contacts; studies on prototype devices with different contact areas shows that van der Waals force between contacts are the main source of adhesion in the prototype mechanical relays. Experiments show that by adding surface coating materials with low Hamaker constant can lower the contact adhesive force. In order to lower the structural stiffness, process development of poly-SiGe is optimized through multiple post-deposition processing techniques for reduced stress gradient of 100 nm thin films, a 10X reduction from the prototype devices. The combined learning from adhesive force work and thin film processing leads to the experiments of fabricating scaled devices of 500X smaller footprint. The results show that scaled relays can potentially operate at low voltages (~2V), but more process optimization needs to be done to demonstrate a fully operational device.",ucb,,https://escholarship.org/uc/item/0s2216x1,,,eng,REGULAR,0,0
298,1734,Essays on the Mobile App Platform Choice and Firm Innovation Disclosure,"Liu, Yongdong","Wright, Brian;",2015,"This dissertation combines two papers on industrial organizations and innovations. The first paper focuses on the market evolution in the mobile app platform and the second paper is an empirical study on firm's innovation disclosure and its impact on firm's intellectual property management. Chapter 1 studies mobile app's platform choice decisions. Ever since Apple and Google launched their mobile application (app) stores in 2008, the market for mobile apps has experienced rapid growth and represents an enormous business opportunity. The success of an app platform largely relies on a great variety of apps, especially innovative and high-quality apps. Given the existence of multiple app platforms, fundamental questions in the app industry are how app developers choose which app platform to enter and which market designs benefit the platform expansion. This chapter studies the platform choice decisions of app developers and the implications for the app market evolution through using a unique and big daily-level panel data set that contains information on every app in the two leading app stores, Apple's App Store and Google Play, over a 2-year period. Combining machine learning techniques for big data problems and computationally efficient econometric approaches, I construct and estimate a structural model for heterogeneous app developers' platform choice decisions within an incomplete-information game framework. I find that in general low-quality apps make the platform less favorable for high-quality entrants. In Google app store, the presence of low-quality apps tends to induce more low-quality apps to enter, while Apple app store exhibits strong competitive effects among high-quality apps. Increasing smartphone user base and improving user engagement are very useful measures to accelerate the platform expansion, but these policies simultaneously encourage many low-quality apps to enter. Regulations on low-quality apps and attenuating competition are more effective on attracting high-quality apps. Platforms can bundle these policies to achieve the optimal market design.Chapter 2 focuses on an interesting phenomenon in firm's intellectual property management. Owners of knowledge sometimes choose to disclose their private innovation to the public domain, instead of filing patents or keeping them in secret. Such behaviors are called knowledge disclosure. Once disclosed, the private innovation becomes public knowledge free to use and is no longer patentable. Since it is long believed that private companies take various measures to securely protect their proprietary innovations, the question of how firms benefit from disclosing innovations is worth exploring. Employing a very unique data set of IBM innovation disclosures, I empirically investigate firm's strategic disclosures of private innovation. I further study how such disclosures affect other firm's patented innovation and the focal firm's selective exploitation of follow-up innovation. First, empirical results show that disclosures are not very defensive. They do not undermine citing patents, but lead to stronger citing patents. I also find that IBM discloses relatively low quality innovation on the periphery of its expertise without patenting these. Meanwhile, IBM often discriminatorily cites other firm's patents that are built on its disclosures and are distant from IBM patents assents. This selective utilization results in broader IBM patents and therefore extends its innovation domain.",ucb,,https://escholarship.org/uc/item/0v49q4cw,,,eng,REGULAR,0,0
299,1735,"Rainfed Agriculture and Climatic Variability in Oaxaca, Mexico","Roge, Paul Sebastien B.","Altieri, Miguel A;",2013,"The study of agriculture practiced by the Mixtec people, or â€œthe people of the rainâ€ (Ã‘uu savi), in the highlands of southern Mexico reveals successful adaptation strategies for growing rainfed crops in an unpredictable and ever changing climate. Both culture and environment have been shaped by a long and challenging history, which continues to bear relevance in this era of globalization and climate change.My research from 2009 â€“ 2011 focused on farmer strategies for dealing with climatic variability in the Mixteca Alta Region of Oaxaca, Mexico. It was the product of a close collaboration with the farmer-to-farmer training network, the Center for Integrated Small Farmer Development of the Mixteca Alta (Centro de Desarrollo Integral Campesino de la Mixteca Alta, CEDICAM). I was impressed by the depth of farmer knowledge about sustainable agriculture. In fact, what appeared lacking was the grassroots mobilization and political action needed to foster dignified rural livelihoods and environmental stewardship. This topic deserves much greater attention in future research by agroecologists.My dissertation is organized in four chapters. Chapter 1 contextualizes my research by examining farming traditions of the Mixteca Alta that originated in different historical moments. I also provide a summary of the political economy of labor and sustainable farming.Chapter 2 examines the social, environmental, and cultural conditions of farming in the Mixteca Alta. Based on in-depth interviews with farmers from two communities, I trace how changes in farming systems has reflected both traditions of farming and an increasingly globalized economy.I found that a combination of agroecological strategies were important for families to approach self-sufficiency in grain production under highly variable rainfall conditions. Important changes in cropping systems were occurring, particularly shifts towards more precocious crop varieties. It appears that changes in cropping systems have been the result of social disintegration, soil degradation, and climatic changes.The farmers that I interviewed understood the ecology of their systems. However, were not necessarily farming sustainably. Shift in farming systems were concentrating agricultural labor into the rainy season, thus allowing farmers to work outside their communities for several months during the dry season. While such changes were ostensibly be intended to save labor, they may in fact accomplish the exact opposite. New farming could potentially increase costs in terms of time and money for families, thus diverting attention away from sustainable agricultural practices. Stemming the labor squeeze from farming regions such as the Mixteca Alta likely will require concerted political action, including grassroots mobilization.Chapter 3 describes participatory research with farmers in the CEDICAM network, as well as climatic studies, that aimed to place climate change mitigation and adaptation into the hands of small farmers. I facilitated workshops in which groups of small farmers described how they had adapted to and prepared for past climate challenges. Farmers reported that their cropping systems were changing for multiple reasons: more drought, later rainfall onset, decreased rural labor, and labor-saving technologies. Examination of climate data found that farmersâ€™ climate narratives were largely consistent with formal climatology data products. There have been increases in temperature and rainfall intensity, and an increase in rainfall seasonality that is likely perceived by farmers as later rainfall onset.Farmers identified 14 indicators that they subsequently used to evaluate the condition of their agroecosystems. Farmers ranked landscape-scale indicators as more marginal than farmer management or soil quality indicators. From this analysis, farmers proposed strategies to improve the ability of their agroecosystems to cope with climatic variability. Their recommendations, as well as the methodology used in the workshops, holds relevance for farmers and their allies. Notably, they recognized that social organizing and education are required for landscape-level indicators to be improved. This outcome suggests thatclimate change adaptation by small farmers involves much more than just a set of farming practices, but also community action to tackle collective problems.I conclude with Chapter 4, which highlights CEDICAMâ€™s contribution to mobilizing farmers in the Mixteca Alta region. They are taking action to reforest territories and advance agroecological farming. This chapter also reflects on how collaborative research outcomes contributed to CEDICAMâ€™s future outlook on farmer led research. CEDICAM was particularly interested in improving soil cover management and crop selection. CEDICAM as a network continues to promote the social conditions for agroecology to flourish.",ucb,,https://escholarship.org/uc/item/0zb469hm,,,eng,REGULAR,0,0
300,1736,Three Essays in Counterfactual Econometrics,"Pereda Fernandez, Santiago","Graham, Bryan S;",2014,"In the first chapter of this dissertation I present a new method to identify and estimate the strength of social spillovers in the classroom and the distribution of teacher and student effects. The identification depends on the assumptions of double randomization of teacher and students to classrooms and the linear in means equation of test scores. The linear independent factor representation of test scores allows the estimation of the parameters of interest by combining all the joint moments of different orders. I also present a theoretical model of social interactions in the classroom that yields the linear in means equation for test scores. In this model, the teacher and students play a game in which they choose how much effort to exert. The method I provide allows the estimation of moments of Rth order, recovering more features of the distribution of teacher and student effects than the mean and variance. Class size heteroskedastic teacher and student effects can be easily accomodated. For the estimation, I use a minimum distance procedure that combines the information coming from different moments. Using the Tennessee Project STAR dataset, I find sizeable spillovers in the classroom. Moreover, the distributions of teacher and student abilities seem to depart from the usual normality assumption, and the student distribution exhibits a high degree of heteroskedasticity in class size. Based on these estimates, I perform several counterfactual social planning experiments, comparing who are the losers and winners under different assignment rules. Assignment of good teachers to large classrooms increases the average test scores, with students in the left tail of the distribution benefiting more than the rest. Assignment of good students to small classrooms increases the test scores of students in the right tail of the distribution, while decreasing test scores of students in the left tail of the distribution, with an overall increase in mean test scores. Mixing good and bad students together results in a small effect on mean test scores, but reduces inequality.In the second chapter I propose an estimator of the conditional distribution of an outcome variable in the presence of heterogeneous effects and a continuous endogenous treatment. The model is triangular, with both the first and the second stage equations being a linear-in-covariates quantile process. The endogeneity of the model is captured by the quantile copula of both equations, and it is identified by inverting the quantile processes conditional on a vector of covariates. Using quantile regression techniques, I estimate both conditional quantile processes, and the copula distribution can then be estimated either nonparametrically or parametrically. Integration of the copula for a given vector of the instruments, estimates the conditional distribution of the outcome variable. This allows to then estimate the distribution of the covariates on the unconditional distribution of the outcome variable, or any other function such as the unconditional quantile function or the Gini Index. Similarly, to estimate the effect of a policy on the unconditional distribution of the outcome variable, one simply needs to integrate the conditional distribution over the marginal of the covariates under the counterfactual policy. Uniform asymptotic distribution for these estimators is provided, allowing to make inference on them and constructing the usual confidence sets. I use data on twins to estimate the the unconditional quantile treatment effect of increasing education by one year to all individuals in the dataset. The results show an increase in the distribution of wages that ranges between 8% and 20%, with those at the upper quantiles of the distribution benefiting the most.In the third chapter I propose an estimator of the unconditional distribution of an outcome variable, when this variable depends on a binary treatment that is endogenous to the unobservables, and the effect of the treatment and other exogenous variables on the outcome variable is heterogeneous. The estimator is based on a triangular model consisting on the probability of being treated and a quantile process that determines the outcome variable. Using a parametric assumption about the copula distribution and the exclusion restriction I identify the copula distribution. The estimation is a multi-step procedure that involves the estimation of the quantile process of the second stage equation, the probability of being treated by maximum likelihood, and the copula distribution. These estimators are then used to estimate the distribution of the outcome variable conditional on a set of instruments. Finally, I show the finite sample performance of the estimator with a Monte Carlo experiment.",ucb,,https://escholarship.org/uc/item/10s728p7,,,eng,REGULAR,0,0
301,1737,Tropical Geometry of Curves,"Brandt, Madeline Virginia","Sturmfels, Bernd;",2020,"Algebraic geometry is a classical subject which studies shapes arising as zero sets of polynomial equations. Such objects, called varieties, may be quite complicated but many aspects of their geometry are governed by discrete data. In turn, combinatorial structures arising from particularly meaningful varieties, such as moduli spaces, are interesting in their own right. In recent years tropical geometry has emerged as a robust tool for studying varieties. As a result, many rich connections between algebraic geometry and combinatorics have developed. Tropical geometry associates polyhedral complexes, like tropical varieties and skeletons, to algebraic varieties. These encode information about the variety or the equations they came from, providing insight in to the underlying combinatorial structure.In this thesis, I develop tropical geometry of curves from the perspectives of divisors, moduli spaces, computation of skeletons, and enumeration. Already, the realm of curves is rich to explore using the tools of tropical geometry. This thesis is divided into five chapters, each focusing on different aspects of the tropical geometry of curves.I begin by introducing algebraic curves, tropical curves, and non-Archimedean curves. The ways in which these objects interact will be a common theme throughout the thesis. The picture on the previous page expresses the idea that tropical curves are projections of non-Archimedean curves. Berkovich analytic spaces are heavenly abstract objects which can be viewed by earthly beings through their tropical shadows.In the second chapter, I develop divisors on tropical curves and tropicalize algebraic divisors. Many constructions for classical curves related to divisors carry over to the tropical world. This will include a tropical Jacobian, a tropical version of the Riemann-Roch theorem, and a tropical Abel-Jacobi map. I first define and compute these objects. Then, I focus on the symmetric power of a curve, because this functions as a moduli space for effective divisors on the curve. I prove that the non-Archimedean skeleton of the symmetric power of a curve is equal to the symmetric power of the non-Archimedean skeleton of the curve. Using this, I prove a realizable version of the tropical Riemann-Roch Theorem.In the third chapter I focus on moduli spaces. A recurring phenomenon in tropical geometry is that the non-Archimedean skeleton of an algebraic moduli space gives a tropical one. I will develop detailed examples of this. Then, I define a divisorial motivic zeta function for marked stable curves, and prove that it is rational.In the fourth chapter I compute abstract tropicalizations or non-Archimedean skeletons of a curve. In genus one and two there are known methods for computing these tropicalizations. I develop an algorithm for computing the abstract tropicalizations of hyperelliptic and superelliptic curves. In higher genus, these are the only known results for computing abstract tropicalizations of curves.In the final chapter I study enumerative problems. Tropical geometry has proven to be a very useful tool in counting curves in the plane. I turn my attention to surfaces in space, and develop tropical counting techniques in this domain. This leads to a preliminary count of binodal cubic surfaces.",ucb,,https://escholarship.org/uc/item/12614656,,,eng,REGULAR,0,0
302,1738,Contextual Visual Recognition from Images and Videos,"Gkioxari, Georgia","Malik, Jitendra;",2016,"Object recognition from images and videos has been a topic of great interest in the computer vision community. Its success directly impacts a wide variety of real-world applications; from surveillance and health care to self-driving cars and online shopping.Objects exhibit organizational structure in their real-world setting (Biederman et al., 1982). Contextual reasoning is part of human's visual understanding and has been modeled by various efforts in computer vision in the past (Torralba, 2001). Recently, object recognition has reached a new peak with the help of deep learning. State-of-the-art object recognition systems use convolutional neural networks (CNNs) to classify regions of interest in an image. The visual cues extracted for each region are limited to the content of the region and ignore the contextual information from the scene. So the question remains, how can we enhance convolutional neural networks with contextual reasoning to improve recognition? Work presented in this manuscript shows how contextual cues conditioned on the scene and the object can improve CNNs' ability to recognize difficult, highly contextual objects from images. Turning to the most interesting object of all, people, contextual reasoning is a key for the fine-grained tasks of action and attribute recognition. Here, we demonstrate the importance of extracting cues in an instance-specific and category-specific manner tied to the task in question. Finally, we study motion which captures the change in shape and appearance in time and is a way to extract dynamic contextual cues. We show that coupling motion with the complementary signal of static visual appearance leads to a very effective representation for action recognition from videos.",ucb,,https://escholarship.org/uc/item/1370p1b6,,,eng,REGULAR,0,0
303,1739,"Don't Show A Hyena How Well You Can Bite: Performance, Race and the Animal Subaltern in Eastern Africa","Williams, Joshua","Cole, Catherine;",2017,"This dissertation explores the mutual imbrication of race and animality in Kenyan and Tanzanian politics and performance from the 1910s through to the 1990s. It is a cultural history of the non-human under conditions of colonial governmentality and its afterlives. I argue that animal bodies, both actual and figural, were central to the cultural and political project of British colonialism in Africa â€“ and in particular eastern Africa, which continues to be imagined in many circles as both â€œsafari countryâ€ and the â€œcradle of humankind.â€ I build on extensive archival research to suggest that artistic and scientific activity in colonial Kenya, from the amateur and professional theatre to the natural-historical research conducted at the Coryndon Memorial Museum, helped to define a category of sub-political being that I call â€œthe animal subaltern.â€ The animal subaltern is a concatenation of all forms of animal life lived below the horizon of â€œthe human.â€ During the colonial period, this included the wildlife of eastern Africa, the pre-human hominids whose fossilized remains paleoanthropologists like Louis Leakey unearthed, and â€œnatives,â€ whose political subjectivity the colonial state was determined to suppress. I argue that the forced contiguity of these variously inflected forms of life had a pervasive, if uneven, racializing effect: all of these beings became black. In the post- Second World War struggle for political, cultural and economic independence in eastern Africa, members of the animal subaltern contested their exclusion from the category of the human. I read the work of the Kenyan writer and intellectual NgÅ©gÄ© wa Thiongâ€™o and the Tanzanian playwright Ebrahim Hussein as important interventions into this unfolding struggle and its implications for the postcolonial future of their communities. Finally, I consider the environmental activism of Kenyaâ€™s â€œRhino Man,â€ Michael Werikhe, whose performative blurring of the distinction between human and animal in the 1980s and 1990s helped to inaugurate a new model of interspecies solidarity that continues to play itself out to this day.",ucb,,https://escholarship.org/uc/item/0jf3488f,,,eng,REGULAR,0,0
304,1740,Essays on Stochastic Bargaining and Label Informativeness,"Ning, Z. Eddie","Villas-Boas, J. Miguel;",2019,"Many firms rely on salespersons to communicate with prospective customers. Such person-to-person interaction allows for two-way discovery of product fit and flexibility on price, which are particularly important for business-to-business transactions. In the first chapter, I model the sales process as a game in which a buyer and a seller discover their match sequentially while bargaining for price. The match between the product's attributes and the buyer's needs is revealed gradually over time. The seller can make price offers without commitment, and the buyer decides whether to accept or wait. Players incur flow costs and can leave at any moment. The discovery process creates a hold-up problem for the buyer that causes him to leave too early and results in inefficient no-trades. This can be alleviated by the use of a list price that puts an upper bound on the seller's offers. A lower list price encourages the buyer to stay while reducing the seller's bargaining power. But in equilibrium the players always reach agreement at a discounted price. The model thus provides a novel rationale for the pattern of ``list price - discount'' observed in sales. I examine whether the seller should commit to a fixed price or allow bargaining. When the seller's flow cost is high relative to the buyer's, both players are willing to participate in discovery if and only if bargaining is allowed. In such a case, bargaining leads to a Pareto improvement, which explains the prevalent use of bargaining in sales. If the buyer has private information on his outside option, the model predicts that, counter-intuitively, the buyer with a higher net value for the product pays a lower price. The chapter expands the bargaining literature by adding a discovery process that introduces a hold-up problem as well as making the product value stochastic.The second chapter examines how counter-offers affects the hold-up problem in stochastic bargaining. Firms increasingly rely on collaboration for the development and marketing of products. The expected surplus from such collaboration can change stochastically over time due to evolving market conditions or the arrival of new information. For collaboration to happen, both firms have to agree to collaborate as well as agree on how the profit is to be split. In such cases, at what point do firms form the alliance and how do they agree on the profit split? To answer these questions, I study a model of bilateral bargaining with a surplus that follows a Brownian motion. One firm can make repeated-offers to the other, and they switch roles after some time to allow for counteroffers. The frequency of counteroffers determines relative bargaining power, and the model captures different bargaining procedures by varying this frequency. The chapter shows that, when there is no outside option, firms collaborate after efficient delay. If there is a relevant outside option, the outcome is inefficient due to the existence of a hold-up problem faced by the weaker party. Firms form the alliance too early, taking the outside options too early, and the ex-ante probability of alliance becomes sub-optimal. Increasing the frequency of counteroffers improves social efficiency by balancing bargaining power and reducing the severity of hold-up. Furthermore, bargaining with more frequent counteroffers can lead to Pareto improvements; the proposer benefits, too, because the increased efficiency outweighs losses in bargaining power. The essay makes a step in understanding the effect of bargaining procedures on collaborative outcome, and shows how collaborators should (not) bargain.The third chapter studies the effect of product labelling on consumer behavior empirically. Cigarettes are sold in different strengths, commonly categorized as regular, light, or ultralight. In 2009, Congress passed Tobacco Control Act (TCA) which banned tobacco companies from communicating product strengths to consumers on any marketing or packaging materials. Cigarette companies continue to sell products of different strengths by using less informative color codes, i.e., relabeling Marlboro Light to Marlboro Gold or Camel Light to Camel Blue. Brands do not use the exact same color codes, creating room for confusion. This chapter investigates the effect of such change in label informativeness on consumer choice. Using a panel of smokers from 2007 to 2012, I find a sharp decline in price sensitivity after Tobacco Control Act was passed. The finding is robust in choice models that account for preference heterogeneity, state dependence, price endogeneity, and consideration sets. This result suggests that consumers perceive products as more differentiated when strength labels change to color codes. This essay provides new evidence on the linkage between product labeling and choice behavior.",ucb,,https://escholarship.org/uc/item/0xt9g1xq,,,eng,REGULAR,0,0
305,1741,Cloud-Edge Hybrid Robotic Systems for Physical Human Robot Interactions,"Tian, Nan","Sojoudi, Somayeh;",2020,"Cloud Robotics is a new paradigm where distributed robots are connected to cloud services via networks to access â€œunlimitedâ€ computation power. Combined with advanced network technology, such as 5G and Wi-Fi 6, it can support service robots operating under unstructured, human rich environments on a global scale. Cloud Robotics has scalable servers that host artificial intelligence, robotic vision, crowd-sourcing, and web-based human computer interface (HCI). These modular Cloud Robotic infrastructures enable control and monitoring of distributed service robots that require sophisticated physical human robot interactions (pHRIs) and human guided tele-operations. Cloud Robotics is also capable of scale up and down robotic service deployments based on rapid changes in user demands. A similar feature in Cloud-based video conferencing services has shown great value in scaling up and down based on user demands during the on-going Covid-19 pandemic. The ability to match user demands will be an important advantage of using Cloud Robotics to keep the operational cost down for service robots applications, where mixed Cloud Robotic modules can be selected for different environments on demand. Besides above advantages, Cloud Robotic systems pay the additional price of network communication.  There are three major network communication costs that hinder effective deployment of cloud robotics: (1) network bandwidth, (2) privacy and security, (3) network latency and variability. With the emerging high speed 5G and Wi-Fi 6 technology, the cost of network speed and bandwidth are dropping significantly, hence the value of Cloud Robotic services will eventually triumph the cost of network communication.  However, if we want to use Cloud Robotic services to control dynamic, compliant, service robots with feedbacks, unpredictable variable delays caused by network routine protocols over long physical distances presents a major obstacle.In this thesis, we propose a Cloud-Edge hybrid robotic system to enable dynamic, compliant, feedback controls for physical human robot interactions (pHRIs).  Specifically, we built a framework to (1) move centralized high-level controllers and computational intensive perception services to the Cloud; (2) deploy low latency, agile, Edge Robotic controller to handle dynamic and compliant motions; (3) implement a hybrid, two-level feedback controller leveraging both the Cloud and the Edge; (4) use robotic-learning algorithms to perform motion segmentation and synthesis to mitigate network latencies within the Cloud-Edge perception feedback loop.  We demonstrate the robustness of the above framework using different robots, including a dual arm robot (Yumi) from ABB, a dynamic self-balancing robot (Igor) and a compliant 5 degree-of-freedom (DoF) robot arm both from Hebi Robotics, and a humanoid robot (Pepper) from Softbank Robotics.  A copy of the dissertation talk including video demonstrations can be found here: https://drive.google.com/drive/folders/1rh8gCydsXCpGJCI6n31mwgTdsJdjJfn-?usp=sharing",ucb,,https://escholarship.org/uc/item/0z34k8p3,,,eng,REGULAR,0,0
306,1742,Investigating noncoding RNA biogenesis and function during mitosis,"Grenfell, Andrew Wang","Heald, Rebecca;",2017,"Mitosis is one of the most dramatic and complex processes in the life of a cell. Although first observed over 100 years ago, there are still many gaps in our understanding of this crucial process. Years of research into the mechanisms of cell division have uncovered many of the protein players required for the faithful execution of this essential process. However, very little is known about the roles of RNAs during mitosis. In my thesis work, I established several biological functions and regulatory modes employed by RNAs and the RNA biogenesis machinery that are essential for cell division. Using a variety of techniques, I showed that RNA biogenesis during mitosis is an essential step in the assembly of the mitotic spindle and mitotic chromatin. I also provided evidence that RNAs are required for microtubule generation, microtubule organization, and chromatin assembly. In addition, I developed automated image analysis tools to systematically assess mitotic spindle morphology for Xenopus egg extract spindle assembly reactions, a very challenging image analysis application. My work suggests that RNAs should be included as a broad class of regulatory molecules that are essential for the execution of eukaryotic cell division.",ucb,,https://escholarship.org/uc/item/0zb4p9zb,,,eng,REGULAR,0,0
307,1743,'Power in the Tongue:' Staging American Voice,"Marshall, Caitlin Simms","DeKosnik, Abigail;",2016,"Abstractâ€˜Power in the Tongueâ€™: Staging American VoiceByCaitlin Simms MarshallDoctor of Philosophy in Performance StudiesDesignated Emphasis in New MediaUniversity of California BerkeleyProfessor Abigail DeKosnik, ChairVoice is the chief metaphor for power and enfranchisement in American democracy. Citizens exercising rights are figured as â€˜making their voices heard,â€™ social movements are imagined as â€˜giving voice to the voiceless,â€™ and elected leaders represent â€˜the voice of the people.â€™ This recurring trope forces the question: does citizenship have a sound, and if so, what voices count? Scholars of American studies and theater history have long been interested in nineteenth-century national formation, and have turned to speech, oratory, and performance to understand the role of class, gender, and race in shaping the early republic (Fliegelman 1993, Looby 1996, Gustafson 2000, Lott 1993, Deloria 1998, Nathans 2009, Jones 2014). However, these studies are dominated by textual and visual modes of critique. The recent academic turn to sound studies has produced scholarship on the sonic formation of minoritarian American identity and an American cultural landscape. Yet this body of research all but overlooks voice performance as site of inquiry. As a result, research has disregarded a central sensory pathway through which democracy operates. Without academic inquiry on the vocal contours of citizenship, we are left with an incomplete understanding of how America selects its constituents, and on what terms. My project, â€˜Power in the tongueâ€™: Staging American Voice addresses this lacuna by analyzing the racialized and queer disabled dynamics of American voice from 1828 to 1861. Leading up to the Civil War, socio-political shifts in settler colonialism and slavery necessitated a new mode of American governmentality. These exigencies catalyzed the reconceptualization of voice from embodied performance practice to a sonic symbol that could record, reproduce, or contest a soundtrack of American citizenship. Taking up dramatic and dramatized literature, and using original archival research on minstrelsy and melodrama, dime museum exhibition, concert song, and dramatic reading, I show how popular performances â€œsplitâ€ black, Native, and queer disabled voices from their originary bodies nearly half a century before the phonograph. Staged as the signs of corporeal difference, these voices were deployed in contradictory ways and in service of competing social interests. In this dissertation, I go behind the scenes of performances by Edwin Forrest, Chief Push-ma-ta-ha, P.T Barnum, Harriet Beecher Stowe, Mary E. Webb, Elizabeth Taylor Greenfield, and Japanese Tommy to understand how each deployed subaltern voice to underscore their claims for national rights and recognitions. â€˜Power in the Tongueâ€™ began in the archives. In examining playbills, broadsides, newspaper reviews, songsters, and scripts at research centers like the Harvard Theatre Collection, the Harriet Beecher Stowe Center, the Library Company of Philadelphia, and many more, I began to hear a pattern that disrupted dominant narratives of American history. While scholars concur that by the nineteenth-century the ascendancy of print culture eclipsed public speech as the primary medium of national formation, the primary archival materials I viewed told a different story. They attested to the persistent importance of oral culture as a site of struggle over belonging in antebellum America, particularly for persons excluded from the elite, literary idea of nation: women, especially women of color, Native Americans, African Americans, and, prior to Andrew Jacksonâ€™s election, the white, common man. This dissertation hones in on voice performance as the site of struggle of, and between these social actors. Further, the dissertation plots how race and queer disability influenced an evolving counterpoint between embodied voice performance and textuality. I argue that whites like Edwin Forrest, P.T Barnum, and Harriet Beecher Stowe deployed subaltern voice performance alongside textual innovations to ensure their own entrÃ©e to American cultural hegemony and bring black, Native, and queer disabled bodies under control, while vocalists of color like Push-ma-ta-ha, Mary Webb, Elizabeth Taylor Greenfield, and Japanese Tommy played-back their sonic difference to contest both the aesthetic and ideological foundations of American citizenship, and white attempts to (re)produce such sonic and written scripts through subaltern bodies. In tracking the sonic signs of race and queer disability as they reel between archive and repertoire, I offer an historically located genealogy of performativity that accounts for the socio-political force of speech act. â€˜Power in the Tongueâ€™ also develops new methods for hearing history and listening to the past â€“ methods that ultimately offer new strategies for registering vocal difference today.",ucb,,https://escholarship.org/uc/item/13f6j78p,,,eng,REGULAR,0,0
308,1744,Nanomaterials and High-Contrast Metastructures for Integrated Optoelectronics,"Li, Kun","Chang-Hasnain, Constance J.;",2016,"Integrated optoelectronics has demonstrated its great potential in numerous fields. In the past decade, its applications have rapidly expanded from the conventional long-haul optical communication to emerging areas such as data center, consumer electronics, energy harnessing, environmental sensing, and biological imaging. This revolutionary progress benefits from the advancement in light generation, manipulation, detection and its interaction with other systems. Device innovation is the key in this advancement. My dissertation discusses this innovation from two aspects: the integration of nanomaterials and the incorporation of metastructures in optoelectronic devices.        The first topic focuses on material integration to facilitate on-chip optoelectronic devices. As microprocessors become progressively faster, chip-scale data transport has turned progressively more challenging. Optical interconnects for inter- and intra-chip communications are required to reduce power consumption and increase bandwidth. Integrating III-V compound semiconductors with superior optical proficiencies onto the silicon-based microelectronic backbone can pave the way towards a highly compact optoelectronic platform, combing the strengths of both materials. The direct growth of III-V micropillars on silicon substrate in the unique growth mode and under CMOS-compatible condition can yield single-crystal structures in sizes exceeding lattice-mismatched critical dimensions. These micropillars overcome the drawbacks of conventional nanowires, thus are endued with superior optical characteristics for on-chip lasers, photodetectors, and cost-effective solar cells. I will discuss the optical characterization of InP micropillars directly grown on silicon, and the optimization of their optical properties. The excellent material quality and device performance of these InP micropillars show promise for a variety of integrated optoelectronic devices.        The second topic centers at the function integration enabled by high-contrast subwavelength grating (HCG), on the platform of vertical-cavity surface-emitting lasers (VCSELs). VCSELs are key light sources in integrated optoelectronics, with the advantages of low power consumption, low packaging cost, and ease of fabrication into arrays for wafer-scale testing. Mode-hop-free, fast and widely tunable VCSELs are also an ideal candidate for the emerging swept-source optical coherence tomography (SS-OCT) as well as light detection and ranging (LiDAR) applications. I will investigate how HCG, an ultrathin monolithic layer of sub-wavelength metastructure, can function as a highly reflective broadband mirror to facilitate lasing of VCSEL. Widely tunable HCG-VCSELs emitting at 1060-nm are demonstrated, a prevalent light source in SS-OCT for 3D eye imaging. Besides functioning as a tunable mirror, the HCG can also be designed as an integrated beam-shaping element at the same time. The rich properties and the large design space of HCG enable direct tailoring of the output beam features of VCSELs, such as transverse-mode control and far-field emission patterning with angular and spatial modulations by HCG. This opens new avenues for direct laser beam shaping with a monolithic optical element for integrated optoelectronics.",ucb,,https://escholarship.org/uc/item/13r70263,,,eng,REGULAR,0,0
309,1745,"Beer, Blood, and the Bible: Economics, Politics, and Geolinguistic Praxis in Kongo-Ngola (Congo-Angola)","Davis IV, Edward C","Mchombo, Sam;Nader, Laura;",2018,"This dissertation argues that educational praxis rooted in local epistemologies can combat the erosion of ethno-histories and provide quotidian securities free of war and exploitative practices of extraction and overuse of the land for non-subsistence purposes, which deny basic human life. Colonial ethnocide, linguicide, and epistemicide serve as the central focus of this study, which uses mixed anthropological methods to investigate economic production, political history, and cultural transmission, with the goal of advancing language revitalization efforts concerning native epistemologies within the multidisciplinary fields of Africana, African, Black, African American, and African diaspora studies. I employ a toolbox of techniques unique to the four fields of anthropology (physical/biological, archaeological, but especially socio-cultural and linguistic anthropology) with a concentration on the four elements of culture [kinship/gender, economics, politics, and religion]. Three metaphors (Beer, Blood, and Bible) examine scientific agriculturalist economies, local jural systems of governance organized by uterine kinship tied to geospatial terrains among the Lunda, and sociolinguistic worlds of pre-colonial indigenous Kongo-Ngola, which occur contemporaneously alongside post-colonial capitalist, neoliberal geopolitical, and cosmological paradigms in present-day Congo-Angola. As such, geolinguistics, ethno-history, and terroir epistemologies become vital to survival and to the continuity of humanity and peace. By decolonizing science, deconstructing imperialist systems of power-knowledge, and reconfiguring ontologies of production and reproduction, this dissertation revitalizes locally grounded epistemologies which face extinction and extermination due to colonial wars of geological extraction, while recognizing significant depths of indigenous governance within opposing post-colonial structures, with respect to technologies of literacy, cosmological consciousness, and numeracy relevant to generational preservation and perpetuation of heritage into the future.    This work becomes significant to African American studies given the historical significance of missionaries educated at Historically Black Colleges and Universities who lived in Belgian Congo and Portuguese Angola from the 1880s into the early 1900s, both preserving and changing local culture, following the Conference of Berlin and leading to the independence movements. Their global goals of progressive work in the era of Old Jim Crow in US come to light in Chapter Four (Bible), which uses the legacy of these late nineteenth and early twentieth century Black American diasporic transnational returnees in order to transpose a practical five-language Swadesh list, where lexicography precedes cultural and linguistic revitalization techniques anthropologists on the ground would use to resurrect lost folkloric knowledge linked to local languages. Kongo-Ngola since migrations of Proto-Bantu speaking peoples parallels with Congo-Angola since 1880 as one of many contested sites, from whence to develop multiple comparative analyses of geolinguistic divisions of indigenous ethnic communities. This triangular metaphor of Beer, Blood, and the Bible concludes with an analysis of education in multiple spaces such that museums and schools teaching Kongo-Ngola native epistemologies in Congo-Angola, the United States, and Europe in deracinated colonial spaces, as well as in reclaimed territories of indigeneity. Perhaps the solution to colonial erasure and epistemicide rests within local universities in Angola, such as Universidade Lueji a Nkonde (ULAN)â€”named for the ancestress and founder of the Lunda Empire. This ethno-history of scientific, economic, linguistic, political, religious, musical performance, and educational epistemologies in the Democratic Republic of Congo and the Republic of Angola employs a rarely known interdisciplinary method known as geolinguistics, while following a metaphor of beer (production), blood (reproduction and power), and the bible (knowledge).",ucb,,https://escholarship.org/uc/item/15b4x1sj,,,eng,REGULAR,0,0
310,1746,Pattern formation in photosynthetic membranes: a physical and statistical approach,"Schneider, Anna Rachel","Geissler, Phillip L;",2013,"Photosystem II (PSII) and its associated light-harvesting complex II (LHCII) are highly concentrated in the stacked grana regions of photosynthetic thylakoid membranes from plants and green algae. PSII-LHCII supercomplexes can be arranged in disordered packings, ordered arrays, or mixtures thereof. The physical driving forces underlying array formation are unknown, and statistically robust methods of identifying arrays in micrographs are lacking, complicating attempts to determine a possible functional role for arrays in regulating light harvesting or energy conversion efficiency. This dissertation introduces new computational tools for studying the nano- and mesoscale organization of the membrane proteins that comprise the photosynthetic apparatus, and applies them to illuminate the origins of the diversity of local structural motifs in this adaptive biomaterial. First, we introduce a coarse-grained model of protein interactions in coupled photosynthetic membranes, focusing on a small number of particle types that feature simple shapes and potential energies motivated by structural studies. Reporting on computer simulations of the model's equilibrium fluctuations, we demonstrate its success in reproducing diverse structural features observed in experiments, including extended PSII-LHCII arrays. Free energy calculations reveal that the appearance of arrays marks a phase transition from the disordered fluid state to a system-spanning crystal. The predicted region of fluid-crystal coexistence is broad, encompassing much of the physiologically relevant parameter regime. Our results suggest that grana membranes lie at or near phase coexistence, conferring significant structural and functional flexibility to this densely packed membrane protein system. Upon extending this model to include simple representations of the membrane morphology and protein components found in thylakoid margins and stroma lamellae, we find that LHCII stacking and crowding in the grana are also primary determinants of the segregation of protein components between stacked and unstacked regions of the thylakoid membrane. In addition, we develop a statistical pipeline for identifying PSII crystals in nanometer-resolution micrographs of thylakoid membranes that would assist in future experimental tests of the predictions put forth in this dissertation; we validate our method on atomic force microscopy measurements of grana membranes isolated from Arabidopsis thaliana wild-type and soq1 mutant plants. As a whole, this work creates a foundation for future rigorous biophysical investigations of the structure, function, and dynamics of the photosynthetic apparatus.",ucb,,https://escholarship.org/uc/item/1681k0xg,,,eng,REGULAR,0,0
311,1747,A DENDROECOLOGY-BASED FIRE HISTORY OF COAST REDWOODS (SEQUOIA SEMPERVIRENS) IN CENTRAL COASTAL CALIFORNIA,"Striplen, Charles Joseph","Huntsinger, Lynn;",2014,"This dissertation focuses on fire history reconstructed from select coast redwood stands in the Santa Cruz Mountains, California based on the fire scar record in specimens collected between 2008 and 2013. The research is one component of a larger multidisciplinary examination of indigenous burning practices in the Central California Coast region (Lightfoot et al. 2013). Research presented herein exhibits concordance with results from related studies, including analyses of soil phytolith content (Evett and Cuthrell 2013), faunal assemblages (Gifford-Gonzalez et al. 2013), and microscopic pollen and charcoal evidence (Cowart and Byrne 2013). Fire history research was conducted in three coastal watersheds in Santa Cruz and San Mateo counties, California using standard dendroecological techniques, as well as a novel statistical approach developed to address undated, floating chronologies.A total of 103 coast redwood samples were collected from 95 sample trees in 19 plots within the study area. The fire return intervals recorded from the dated redwood samples in this study were relatively frequent. Fire information was estimated for three focal management eras: the native and ranching eras (1600-1850), intensive commercial logging (1850-1950), and the modern fire suppression/sustainable harvest era (1950-2013). Results from dated fire scars indicate that fires were less frequent in the native and ranching period (mean FRI 7.6 years; range 1-29) than the intensive logging period (mean FRI 3.1 years; range 1-11), as well as the modern period (mean FRI 4.6 years; range 1-12).However, use of a generalized linear mixed model (GLMM) on undated, floating chronologies indicated that the probability of fire may have been quite high in the earlier period, and that three independent variables were significant predictors in assessing the annual probability of the occurrence of fire in the study area: physiographic zone; position on slope; and linear distance to pre-colonial, native habitation sites. The GLMM also indicates that fire probabilities are not distributed uniformly in study watersheds. Trees located in close proximity to native residential sites had a high probability of being burned than those farther away (42-69% vs. 17-38%). Similarly, top of slope fire were more likely in all watersheds and physiographic zones, though varying in degree.The season of fire occurrence was determined for 85% of the fire scars. Dormant or late season fires accounted for a combined total of 87% of all fires for the entire period of record (1350-2013; 55% dormant, 33% late) - indicating that historic fires most likely took place between approximately mid-August to late March. Early season (approx. April to August) fires accounted for 13% of fires. In the 1600-1850 period, combined dormant and late season fires accounted for 91% of fires (64% and 27%, respectively), with 9% of fires occurring in the early season. During the intensive logging period (1850-1950) combined dormant and late season fires accounted for 85% of fires, and 15% in the early season. In the modern era (1950-2013), dormant and late season fires still account for the majority of fires (86%), but with a marked shift into the drier late season (mid-August - September), which now account for 43% of fires. Early season fires represent 14% of fires in this period.Though this study faced significant challenges (i.e. low sample density for earlier period specimens, large study area, experimental use of a GLMM), these data reveal interesting and potentially useful patterns of historic fire occurrence in the Santa Cruz Mountains, especially with respect to human influence over coastal fire regimes. All sources of information indicate that coastal Santa Cruz Mountains experienced far more ignitions than would be expected under a lightning-driven fire regime (roughly 4 strikes per century), and that human activity is strongly linked to fire frequency in throughout observable time periods. There is ample opportunity to improve on this data with future work in efforts to inform and refine modern approaches to resource management in this region.",ucb,,https://escholarship.org/uc/item/16w3w250,,,eng,REGULAR,0,0
312,1748,The Transit-Oriented Global Centers for Competitiveness and Livability: State Strategies and Market Responses in Asia,"Murakami, Jin",,2010,"Over the past two decades, the spatial development patterns of city-regions have increasingly been shaped by global-scale centripetal and centrifugal market forces. Complex managerial tasks and specialized producer services agglomerate in the central locations of global city-regions, whereas standardized assemble lines, wholesale inventories, and customer services stretch over the peripheral locations of global production networks. One explanation for postindustrial agglomeration is the need for face-to-face interactions and knowledge spillovers among the labor-intensive business sectors. On the other hand, the spatial concentrations of knowledge-based activities are also promoted by entrepreneurial city-statesâ€™ economic development strategies. Since the 1990s, rail transit investments and urban regeneration projects have played a pivotal role in shaping competitive and livable global centers to attract foreign direct investments and qualified international workers. Despite the growing importance of city and regional planning in the global marketplace, existing studies have provided little evidence on transit-oriented urban regeneration projects particularly in global city-regions.This dissertation examines Hong Kong, Singapore, and Tokyo as three transit-oriented global center models, wherein entrepreneurial city-states have largely integrated rail transit investments with urban regeneration projects to guide postindustrial agglomeration and spur economic development in target locations. For each of the three Asian cases, I classify types of joint development packages on the basis of built environment attributes and estimate the impacts of rail transit investments and joint development packages on market location shifts and land price changes over the last decade. The empirical findings suggest that mixed-use redevelopment projects and urban amenity improvements around terminal stations largely shift the competitive advantages of knowledge-based businesses and the lifestyle preferences of highly skilled professionals towards central locations. The hedonic price models, however, reveal that the synergetic effects of rail transit investments and urban regeneration projects are highly redistributive over the rail transit networks as well as within each station catchment area,especially where urban districts are already well-developed and development regulations are generously relaxed for commercial profits.One might argue that the Asian models represent a few extreme cases in terms of transit investment levels and urban agglomeration patterns, having very different evolutional pathways and institutional structures from other global city-regions. In response to this argument, I also attempt to illustrate specific experiences and common themes across the three Asian models and selected global city-regions that have been moving towards transit-oriented urban regeneration. The international statistics and case reviews in this dissertation suggest that there is global momentum to put greater public-private resources together into large-scale rail transit investments and transit-oriented development projects. These entrepreneurial forces tend to generate significant agglomeration impacts on knowledge-based business activities in the global marketplace, while raising transit overcapitalization and social stratification problems in the local context. The cross-cutting lessons drawn from the three Asian cases and global comparisons stress the importance of (i) evaluating urban agglomeration benefits, (ii) choosing adequate transit technologies, (iii) establishing publicprivate partnerships, (iv) applying value capture techniques and (v) ensuring local community interests in shaping â€œcompetitiveâ€ and â€œlivableâ€ transit-oriented global centers.",ucb,,https://escholarship.org/uc/item/19034785,,,eng,REGULAR,0,0
313,1749,"Screening-engineered Field-effect Photovoltaics and Synthesis, Characterization, and Applications of Carbon-based and Related Nanomaterials","Regan, William Raymond","Zettl, Alex K;",2012,"Carbon nanomaterials, and especially graphene (a 2D carbon allotrope), possess unique electronic, optical, and mechanical properties and allow access to both new physical phenomena and reinventions of familiar technologies. In the first part of this thesis (chapter 2), the low carrier density and high conductivity of graphene are used to repurpose the electric field effect (used for many decades in transistors) into a universally-applicable doping method for electrically-contacted semiconductors. This method, referred to as ""screening-engineered field-effect photovoltaics"" as the electric field doping is enabled by a carefully-designed poorly-screening electrode (e.g. graphene), is shown to open up many new low-cost and abundant semiconductors for use in high efficiency solar cells. We extend this method beyond ultrathin materials such as graphene and show that 1D nanowire electrodes made of any material also allow penetration of applied electric fields. The next part of this thesis (chapter 3) focuses on the fundamental properties of graphene -- its structure, synthesis, characterization, and manipulation -- and on using graphene as a building block for other nanostructures: grafold, graphene sandwiches and veils, and graphritos. In chapter 4, various graphene electronics are constructed and tested. Graphene field-effect transistors (FETs) and p-n junctions are fabricated to study the influence of the substrate on graphene carrier mobility and doping. Graphene nanoribbons and grafold FETs are made to investigate the effects of additional confinement on electronic transport. Chapter 5 summarizes synthesis methods and additional experiments with other nanomaterials, including dichalcogenides and chalcogenides (magnesium diboride, gallium selenide, and tin sulfide), carbon nanomaterials (carbon nanotubes and graphene), and copper oxide. Additional measurement and fabrication methods are discussed in appendix A.",ucb,,https://escholarship.org/uc/item/19d9z1mk,,,eng,REGULAR,0,0
314,1750,The Design of Stateful Serverless Infrastructure,"Sreekanti, Vikram","Hellerstein, Joseph M;",2020,"Serverless computing has become increasingly popular in the last few years because it simplifies the developer's experience of constructing and deploying applications. Simultaneously, it enables cloud providers to pack multiple users' workloads into shared physical resources at a fine granularity, achieving higher resource efficiency. However, existing serverless Function-as-a-Service (FaaS) systems have significant shortcomings around state managementâ€”notably, high-latency IO, disabled point-to-point communication, and high function invocation overheads.In this dissertation, we present a line of work in which we redesign serverless infrastructure to natively support efficient, consistent, and fault-tolerant state management. We first explore the architecture of a stateful FaaS system we designed called Cloudburst, which overcomes many of the limitations of commercial FaaS systems. We then turn to consistency and fault-tolerance, describing how we provide read atomic transactions in the context of FaaS applications. Finally, we describe the design and implementation of a serverless dataflow API and optimization framework specifically designed to support machine learning prediction serving workloads.",ucb,,https://escholarship.org/uc/item/1bw7645n,,,eng,REGULAR,0,0
315,1751,Essays in Environmental Economics,"Gallagher, Justin","Moretti, Enrico;",2011,"The first chapter of the dissertation examines the learning process that economic agents use to update their expectation of an uncertain and infrequently observed event.  The standard Bayesian updating model is restrictive in that it reflects the strong neo-classical assumption that economic agents efficiently incorporate new information with all available information when updating beliefs.  I consider the case of flooding and estimate the effect of first-hand experience on flood insurance take-up.  I compile a new nation-wide panel dataset of large regional floods and flood insurance policies in the US.  First, I show that flood insurance take-up in flooded communities increases by 9% after a flood and then steadily declines, fully dissipating after 9 years.  Floods do not affect take-up in geographically neighboring non-flooded communities unless these communities are in the same media market.  The take-up rate in non-flooded communities that share a media market with a flooded community is one-third as large as in flooded communities.  I interpret this evidence using the standard Beta-Bernoulli Bayesian learning model and a Beta-Bernoulli model that includes a forgetting/first-hand experience parameter.  I find that the standard Bayesian model can not explain both the spike in insurance in the year of a flood and the decay rate of this effect on insurance take-up in the years after the flood.  I conclude that the evidence is most consistent with a Bayesian model augmented with a forgetting/first-hand experience parameter.The second chapter of my dissertation examines the causal link between localized exposure to hazardous waste pollutants from motor vehicle exhaust and adverse human health outcomes for newborns.  I explore whether an exogenous event--the 1994 Northridge Earthquake--can be used as a quasi-experiment to test how birth outcomes change from a sudden and unexpected increase in pollution.  The Northridge Earthquake closed down portions of four busy highways in Los Angeles, CA for periods of 1-6 months.  The highway traffic was diverted onto secondary roads that previous to the earthquake had a much lower traffic volume.  The paper focuses on two health outcomes for newborns: birth weight and gestation period.  Infants born preterm or with low birth weight are less likely to survive infancy, more likely to suffer from childhood illness, and have lower future earnings.  Overall the results of this study are inconclusive due to the relatively small number of new births included in the sample design.  However, the results do suggest that a mother's race, age, and level of education are more important than proximity to a highway.  Being a minority race, a teenage mother, or not having any college education are correlated with lower birth weight.  The size of these correlations are approximately an order of magnitude larger than the point estimates for the effect of living in close proximity to a road with heavy traffic.The third chapter of the dissertation uses the housing market to develop estimates of the local welfare impacts of Superfund sponsored clean-ups of hazardous waste sites.  We show that if consumers value the clean-ups, then the hedonic model predicts that they will lead to increases in local housing prices and new home construction, as well as the migration of individuals that place a high value on environmental quality to the areas near the improved sites.  We compare housing market outcomes in the areas surrounding the first 400 hazardous waste sites chosen for Superfund clean-ups to the areas surrounding the 290 sites that narrowly missed qualifying for these clean-ups.  We find that Superfund clean-ups are associated with economically small and statistically indistinguishable from zero local changes in residential property values, property rental rates, housing supply, total population, and the types of individuals living near the sites.  These findings are robust to a series of specification checks, including the application of a regression discontinuity design based on knowledge of the selection rule.  Overall, the preferred estimates suggest that the local benefits of Superfund clean-ups are small and appear to be substantially lower than the $43 million mean cost of Superfund clean-ups.",ucb,,https://escholarship.org/uc/item/1cr812nm,,,eng,REGULAR,0,0
316,1752,Regulation of Saccharomyces cerevisiae Mating Pheromone Response: G-protein-coupled Receptor Ste2 Down-modulation by Specific Î±-Arrestins,"Alvaro, Christopher","Thorner, Jeremy;",2015,"The yeast G-protein-coupled receptor (GPCR) Ste2 is an integral plasma membrane protein that initiates response to an extracellular stimulus (the peptide mating pheromone Î±-factor) by mediating ligand-dependent activation of a cognate heterotrimeric G-protein. Prolonged pathway stimulation is detrimental, and feedback mechanisms have evolved that act at the receptor level to limit the duration of signaling and stimulate recovery from pheromone-induced G1 arrest. In the research described in this dissertation, I found that three Î±-arrestins â€” Rod1/Art4, Rog3/Art7 and Ldb19/Art1 â€” serve as adaptors to promote the ubiquitinylation-dependent internalization of Ste2 and block its ability to signal, thereby desensitizing the cells to continued stimulation. Deleting the genes encoding these three Î±-arrestins increases the sensitivity of a MATa haploid cell to mating pheromone and results in an increase in Ste2 abundance at the plasma membrane. Conversely, overexpression of either Rod1 or Rog3 enhances the rate of adaptation. To contribute to negative regulation of the mating pheromone response pathway, Ldb19 requires binding of a HECT domain class of ubiquitin ligase, Rsp5, and most likely clears misfolded Ste2 from the PM. I found that Rod1 and Rog3 contribute to clearance of the pheromone-bound state of Ste2 and negatively regulate the mating pheromone response pathway by both Rsp5-dependent and Rsp5-independent mechanisms. In addition, I identified two classes of protein kinases (Snf1/AMPK and Ypk1/SGK) that phosphorylate and inactivate Rod1. Conversely, I showed that the phosphoprotein phosphatase responsible for dephosphorylating and re-activating Rod1 is calcineurin / PP2B. Because the S. cerevisiae genome does not encode any Î²-arrestins, the findings I made and present in this dissertation are the first to show that Î±-arrestins alone are capable of negatively regulating a GPCR.",ucb,,https://escholarship.org/uc/item/1d678463,,,eng,REGULAR,0,0
317,1753,"Rising From a Placid Lake: China's Three Gorges at the Intersection of History, Aesthetics and Politics","Byrnes, Corey J.","Varsano, Paula;",2013,"Researched and written in the shadow of the recently completed Three Gorges Dam, this dissertation begins with an ""Introduction"" that describes the earliest mythology of this mountainous region, which is said to have been hand-hewn by the deity-civil servant Yu the Great, so that the waters of a cataclysmic flood could drain to the sea. This proto-governmental response to natural disaster stands at the core of all later accounts of the Gorges, helping to form an aesthetic tradition that views the landscape as not only a site of trauma but also a surface created through and primed for physical alteration. 	Chapter 1, ""Tears in the Void: Traces of the Past in Du Fu's Three Gorges Poetry,"" focuses on this tradition as manifested in the interplay of the personal and the national at a moment of grave political crisis, the An Lushan Rebellion (755-763), when the iconic poet Du Fu (712-770) sought refuge in the wilds of the Gorges. In the strikingly fragmented verse that he wrote during this period, Du's fevered visions fail to coalesce into a stable landscape in the monumental mode of Yu the Great. Instead, they flicker across the surface of the Gorges in the form of fleeting, hallucinatory traces (ji) of a fractured personal, cultural and spatial order. 	Chapter 2, ""Reinscribing the Trace: The Three Gorges in the Song Dynasty,"" shows how a new genre of travel diaries and essays (ji) from the Southern Song Dynasty (1127-1279) transforms Du Fu's traces, stabilizing and re-inscribing them as touristic landmarks through physical and literary practices of marking, recording and verifying. The authors of these texts, writing barely a generation after the cataclysmic loss of the northern half of the empire to an ethnically non-Chinese dynasty, were acutely aware of the vulnerability of the landscape as cultural topography. For them, Du Fu offered not only a sympathetic model of the loyal minister in southern exile, but also a set of geographical and historical associations that elevated the Gorges to a landscape imbued with powerful moral significance. 	Chapter 3, ""Specters of Realism and the Painter's Gaze in Jia Zhangke's Still Life,"" turns to a new aesthetic of fragmentation that, while reminiscent of Du Fu's, is finely attuned to the contemporary pressures of global capital and national development. Focusing on Jia Zhangke's 2006 film Still Life (Sanxia haoren), this chapter explores how The Three Gorges, now on the brink of inundation, serve as the ideal venue for Jia's disassembly and recombination of earlier artistic forms--portraiture, Socialist Realism and Chinese landscape painting, as well as Tang poetry and contemporary pop music--in a work that reveals precisely how cultural practices work trans-historically to constitute this particular landscape. 	While pre-modern artistic forms serve as occasional components of Jia's eclectic hybrid style, they are the very warp and weft of the American-based Chinese artist Yun-fei Ji's painting practice, the subject of Chapter 4, ""Ink in the Wound: Trauma and The Three Gorges in the Painting of Yun-fei Ji."" In his Three Gorges paintings Ji self-consciously manipulates spatial and temporal codes borrowed from classical Chinese painting to depict the dam project as a violent act of physical inscription and traumatic displacement. This chapter details how Ji imagines the landscape of The Three Gorges as a site of traumatic experience: both a raw wound that opens onto past traumas, especially the Cultural Revolution, and a frame for the physical and psychic consequences of the dam project.",ucb,,https://escholarship.org/uc/item/1dg2v9ct,,,eng,REGULAR,0,0
318,1754,Insect ecology from the scale of plant-level interactions to continent wide phylogeography,"Madsen-Steigmeyer, Tara","Tsutsui, Neil D;",2014,"This dissertation considers insect ecology from two perspectives, at two scales of space and time, using two different methodologies. In the first chapter, I investigate the direct interactions between plants and insects through plant defenses against herbivory. I explore the effect of plant exposure to natural smoke on herbivore growth, using the model system Nicotiana attenuata and Manduca sexta. Despite finding no effect in these tests, I believe that the mechanisms for potential impacts of smoke exposure on plant defenses are convincing enough to warrant further study.In the second and third chapters, I investigate the geographic patterns of genomic diversity in an herbivorous insect that underwent a dramatic host switch to become an important invasive agricultural pest. The Colorado potato beetle, Leptinotarsa decemlineata, is one of the most successful insect pests of agriculture. However, its evolutionary origins as an insect pest of potato remain unclear. I conducted reduced genome sequencing using RADseq on samples collected from across the proposed native range of Mexico and from the East Coast of the USA in the introduced range. I describe the geographic population structure of the beetle, examine the genomic diversity across the samples, and conduct a phylogenomic analysis.",ucb,,https://escholarship.org/uc/item/1dv964g3,,,eng,REGULAR,0,0
319,1755,Spatial and Temporal Variation in Mammalian Diversity of the Colorado Plateau (USA),"Stegner, Mary Allison","Barnosky, Anthony D;",2015,"Anticipating how species and ecosystems will react to continued climate change is of critical importance to biodiversity conservation and to management of the ecosystem processes on which we rely. Identifying how individual species in a community have responded in the past can be accomplished by evaluating the fossil record on a local and/or regional scale, and by examining spatial patterns of modern abundance and diversity.  Here, I explore regional patterns of mammal diversity across the Colorado Plateau (CP), examine local small mammal diversity fluctuations in fossil deposits from northern San Juan County, Utah, through the late Holocene, and assess modern spatial diversity patterns across a range of San Juan County sites.	Over the past century, extraordinary global transformations have taken place, including climate change and land conversion for human use. With these unusually rapid and extreme global changes underway, it is increasingly important to understand the extent to which designated conservation areas have protected biodiversity thus far, and to gauge their potential for continuing to do so in the future.  Chapter 1 examines the efficacy of biodiversity preservation in National Park Service (NPS) lands of the CP by using analysis of nested species assemblages to compare surveyed mammal communities to the range map predictions of which species should be present. I find that NPS lands are nested, and, although site diversity is correlated with area, elevational range, budget and visitation, a comparison between species lists compiled from surveys versus range map distributions reveals that the biogeographic patterns prevailing today cannot be distinguished from those prevailing when these NPS sites were established. These broad-stroke patterns define an important context in which to direct future conservation efforts as we attempt to divert and mitigate anthropogenic impactsâ€”past, current, and future. 	Modern patterns of diversity are the result of past events and processes that take place on the scale of decades, centuries, and millennia. Detailed paleoecological records from Quaternary deposits are remarkably useful in characterizing these long-term ecological dynamics, but only a handful of Quaternary localities that sample the small mammal community of the CP have been studied to date. In chapter 2, I describe my excavation and analyses of two fossil-bearing alcoves, East Canyon Rims 2 (ECR2) and Rone Bailey Alcove (RBA) (San Juan County, Utah), and quantify diversity and abundance change of the small mammal community as recorded in the fossil samples. Fossil localities with comparable mammal diversity have not been reported from this region previously, so these sites provide novel insight into Holocene mammal diversity in southeastern Utah.  Further, these localities contribute to our understanding of natural variation in this system by providing faunal data for a period of recent climate changeâ€”cool-wet to warm-dry.  AMS radiocarbon dates on 33 bone samples from these sites span ~4.4 ka-present, and shed light on pre-industrial faunal dynamics in the region over the course of environmental change, most notably aridification. I test for an effect of climate on community evenness and relative abundance of 10 small mammal taxaâ€”leporids, perognathines, small sciurids, arvicolines, Cynomys, Neotoma, Dipodomys, Onychomys, Peromyscus, and Thomomysâ€”and find that, in spite of considerable increases in aridity and temperature, neither is significantly correlated with relative abundance or evenness when statistically tested, but there are qualitative patterns consistent with a response to increasing aridity around 1000 years ago.	The CP is home to a diverse complement of species that are experiencing increasing temperature and drought stress today. Understanding how mammal communities might be expected to respond to impending global changes requires a baseline of information on presence, abundance, and spatial variation of species on the landscape today.  Chapter 3 describes the results of a preliminary analysis of spatial variation in the small mammals of northern San Juan County at a single point in time, with the objective of learning how species commonly preserved in the fossil record sort geographically in relation to variation in their abiotic environment. I conducted mark-recapture surveys at 8 sites in northern San Juan County, two of which were located in the immediate vicinity of ECR2 and RBA.  Over the course of one year, I compiled abundance and presence/absence data on nine species: Neotoma albigula, Onychomys leucogaster, Peromyscus maniculatus, P. truei, Dipodomys ordii, Perognathus flavescens, P. parvus, Ammospermophilus leucurus, and Tamias rufus. In chapter 3, I evaluate spatial differences in species richness using occupancy modeling and metrics of taxonomic difference, and I assess proportional and rank abundance across sites. Although the results are preliminary, some patterns are emerging: sites spanning 50km and 550m elevation range sample the same small-mammal species pool, but abundance of those species varies non-randomly, and sites are less similar in abundance than expected by random distribution of individuals.  Species evenness varies among sites, and sites with low evenness are dominated by Peromyscus maniculatus, a â€œweedyâ€ species with broad habitat requirements. This is also the first report of Perognathus parvus east of the Colorado River, suggesting recent range expansion of the species. Occupancy models indicate that presence/absence of different species is determined by different aspects of their environment, and therefore species will respond idiosyncratically to future environmental changes.  However, more survey data is necessary before these patterns can be considered robust or fully explained. In chapter 4, I compare fossil diversity at ECR2 and RBA to modern diversity at the same sites. I find that evenness reached a peak in ECR2 and RBA between ~1-1.7 ka, then began to decline between ~0.7-1 ka. Evenness of the modern community at ECR2 and RBA is also significantly lower than in the fossil record between ~0.7-4.5 ka.  The observed drop in evenness occurred prior to the onset of high-impact, post-European human land uses, like livestock grazing, and is coincident with the time when Ancestral Puebloan populations crashed due to long-term periodic droughts, suggesting a marked environmental change between ~0.7-1 ka.  Low modern evenness is consistent spatiallyâ€”modern evenness at all survey sites is lower than it is for all fossil time bins ~0.7 ka and older, suggesting that this was a landscape-level decline in diversity.  These results send a cautionary message: though the basic taxonomic integrity of the small mammal community is still present, abundance and community structure are very different today. 	Additional information on past and current diversity of the CP will improve forecasting and establish baselines against which to compare future surveys, allowing us to gauge rates and direction of change, and to prioritize conservation efforts in the future.  This study emphasizes the enormous utility of the fossil record in understanding the extent of ecological fluctuations that can be considered â€œnormalâ€ through long periods of time, information which is essential as we struggle to conserve biodiversity in a rapidly changing world.",ucb,,https://escholarship.org/uc/item/1f474437,,,eng,REGULAR,0,0
320,1756,Tropical curves and metric graphs,"Chan, Melody Tung","Sturmfels, Bernd;",2012,"In just ten years, tropical geometry has established itself as an important new field bridging algebraic geometry and combinatorics whose techniques have been used to attack problems in both fields. It also has important connections to areas as diverse as geometric group theory, mirror symmetry, and phylogenetics. Our particular interest here is the tropical geometry associated to algebraic curves over a field with nonarchimedean valuation. This dissertation examines tropical curves from several angles.An abstract tropical curve is a vertex-weighted metric graph satisfying certain conditions (see Definition 2.2.1), while an embedded tropical curve takes the form of a 1-dimensional balanced polyhedral complex in R^n. Both combinatorial objects inform the study of algebraic curves over nonarchimedean fields. The connection between the two perspectives is also very rich and is developed e.g. in [Pay09] and [BPR11]; we give a brief overview in Chapter 1 as well as a contribution in Chapter 4.Chapters 2 and 3 are contributions to the study of abstract tropical curves. We begin in Chapter 2 by studying the moduli space of abstract tropical curves of genus g, the moduli space of principally polarized tropical abelian varieties, and the tropical Torelli map, as initiated in [BMV11]. We provide a detailed combinatorial and computational study of these objects and give a new definition of the category of stacky fans, of which the aforementioned moduli spaces are objects and the Torelli map is a morphism.In Chapter 3, we study the locus of tropical hyperelliptic curves inside the moduli space of tropical curves of genus g. Our work ties together two strands in the tropical geometry literature, namely the study of the tropical moduli space of curves and tropical Brill-Noether theory. Our methods are graph-theoretic and extend much of the work of Baker and Norine [BN09] on harmonic morphisms of graphs to the case of metric graphs. We also provide new computations of tropical hyperelliptic loci in the form of theorems describing their specific combinatorial structure.Chapter 4 presents joint work with Bernd Sturmfels and is a contribution to the study of tropical curves as balanced embedded 1-dimensional polyhedral complexes. We say that a plane cubic curve, defined over a field with valuation, is in honeycomb form if its tropicalization exhibits the standard hexagonal cycle shown in Figure 4.1. We explicitly compute such representations from a given j-invariant with negative valuation, we give an analytic characterization of elliptic curves in honeycomb form, and we offer a detailed analysis of the tropical group law on such a curve.Chapter 5 is joint work with Anders Jensen and Elena Rubei and is a departure from the subject of tropical curves. In this chapter, we study tropical determinantal varieties and prevarieties. After recalling the definitions of tropical prevarieties, varieties, and bases, we present a short proof that the 4Ã—4 minors of a 5Ã—n matrix of indeterminates form a tropical basis. The methods are combinatorial and involve a study of arrangements of tropical hyperplanes. Our result together with the results in [DSS05], [Shi10], [Shi11] answer completely the fundamental question of when the r Ã— r minors of a d Ã— n matrix form a tropical basis; see Table 5.1.",ucb,,https://escholarship.org/uc/item/0nm4157r,,,eng,REGULAR,0,0
321,1757,"Identified, Misidentified, and Disidentified: Subject Formation and Reformation in American Law and Literature","Perez, Aurelio Jose","JanMohamed, Abdul R.;Sweet Wong, Hertha D.;",2010,"In this dissertation, I examine legal definitions of race within the United States and the representation and reformulation of these categories within U.S. literature.  The substrate of this dissertation is a collection of American literary and legal texts from the 17th through the 20th centuries.  I examine how these texts chronicle, represent, and often intentionally misrepresent individuals' attempts to subvert and even openly challenge delimited identifications such as `immigrant' or `slave.' Often these challenges are leveled against the normative identificatory organ of `Law,' that is, the judicial processes and legal decisions that establish and confirm these reductive identifications.  The mode of the challenges I examine is movement, or literal mobility.      When normative pathways of identification begin to fail, mobility gains importance as a means of transgressing, figuratively and literally, usually impermeable classificatory boundaries.  The idea of mutable identity is almost a truism of modern Western thought.  Less appreciated, however, is the connection between identity and location, and more pertinently, the coordination between movement and identity.  This coordination is the first focus of my dissertation.	In order to understand these identifications and mobile re-identifications, this dissertation examines the sets of conditions - historical, social, biological, and especially legal - that seek rigidly to classify individuals as well as the sets of conditions that enable mobile re-identification.  Phrased in another way: this dissertation explores both the possibilities of literal transgression of identificatory boundaries as well as the execution of such transgressions.  Alongside literary chronicles of these transgressions, I analyze a variety of legal texts that have promulgated and structured reductive methodologies of identification.  Whether negotiating slavery, Jim Crow, segregation, or post-WWII immigration politics, the examined literary texts are distinctly concerned not only with the tensions of identity, but the manners in which mobility or transit can enable self-determination through re-identification.  Conversely, the examined legal texts display the formulation and repeated revision of criteria of reductive identifications chronicled in the examined literature.  The power of law to establish interpretive methodologies that reductively identify individuals is the second focus of my dissertation.",ucb,,https://escholarship.org/uc/item/0ns9n146,,,eng,REGULAR,0,0
322,1758,Weak Lowness Notions for Kolmogorov Complexity,"Herbert, Ian-Cadoc Robertson","Slaman, Theodore A;",2013,"The prefix-free Kolmogorov complexity, K(Ïƒ), of a finite binary string Ïƒ is the length of the shortest self-delimiting program that outputs Ïƒ and is a measure of the information content of Ïƒ. There are two very natural ways to extend this notion to infinite binary strings (identified as the binary expansions of real numbers in the interval [0,1]): one can examine the initial segment complexity of a real, i.e. K(A|n)$ as a function of n, or one can examine the compressive power of A as an oracle, i.e. KA(Ïƒ) as a function of Ïƒ. Each of these approaches has a notion of minimality for reals. A real is K-trivial if the complexities of its initial segments are up to a constant just the complexities of their lengths. A real is low for K if it provides no more than an additive constant advantage to compressing any string. This dissertation examines weakenings of these notions that arise from replacing the constant bounds with slow-growing functions. The main results are that these weaker lowness notions behave very differently from the standard ones in terms of the sets of reals defined by these notions. We also include applications of these new notions to effective dimension and mutual information.",ucb,,https://escholarship.org/uc/item/0q79m72q,,,eng,REGULAR,0,0
323,1759,A Cosserat Theory for Solid Crystals â€“ with Application to Fiber-Reinforced Plates,"Krishnan, Jyothi","Steigmann, David J;",2016,,ucb,,https://escholarship.org/uc/item/04z77w,,,eng,REGULAR,0,0
324,1760,A Hierarchical Approach to the Design and Optimization of Photonics,"Michaels, Andrew","Yablonovitch, Eli;",2019,"Over the last two decades, silicon photonics has rapidly matured, leading to a growing interest in building large complex systems consisting of thousands of integrated optical components. A direct consequence of this push towards large scale integration is the need for high efficiency silicon photonic building blocks.In this work, we present a concrete path towards realizing these essential photonic building blocks. The foundation of our approach to designing photonic components is gradient-based shape optimization, which employs boundary smoothing based on high numerical precision polygons. In addition to helping us calculate accurate device sensitivities, this method affords us a great amount of flexibility when representing device geometries and enables us to incorporate design constraints directly into optimizations in a simple and intuitive way.Our approach to gradient-based optimization shares an important similarity to other forms of shape and topology optimization employed in the nanophotonics community: on its own, it is not a complete solution to designing high performance and robust devices. Due to the inherently non-convex nature of electromagnetic optimization problems, we cannot expect convex optimization to universally yield good devices without outside input. In order to overcome this obstacle, we have systematized the process of providing ""outside input"" through our hierarchical approach to design and optimization. Using a strategic combination of simple physical analysis to find good starting geometries and optimization with both coarse and fine parameterizations, we show that efficient and robust devices can be designed with minimal guesswork.Using this hierarchical approach, we demonstrate how a variety of silicon photonic components can be designed with superior performance. In particular, we design three port splitters, broadband four port splitters, fabrication-insensitive waveguide crossings, and a variety of efficient grating couplers which set a new standard for device performance. These components form the foundation for an optimized silicon photonic component library which will be important for demanding applications of the future.",ucb,,https://escholarship.org/uc/item/0rk5k87g,,,eng,REGULAR,0,0
325,1761,Causes and consequences of unsteady crustal magma transport,"Karlstrom, Leif","Manga, Michael;",2011,"Magma transport pathways through Earth's crust span 12-15 orders of magnitude in time and space, with unsteadiness at all scales. However emergent organization of this system is widespread, recorded by spatial loci of volcanism at the surface and large--scale, rapid outpourings of magma throughout the geologic record. This thesis explores several mechanisms for the organization and time evolution of magma transport, from the deep crust to the surface. A primary focus (Chapters 2-5) is the filling, stability and drainage of magma chambers, structures which function both as reservoirs feeding individual volcanic eruptions and as stalling points in the crust where magma accumulates and differentiation occurs. We show that magma chambers may dictate the spatio-temporal organization of magma rising through crust (Chapters 2-3), control the surface eruptive progression of extreme mantle melting events (Chapter 4), and actively set the size of calderas that form during shallow, crystal rich eruptions (Chapter 5). Each of these chapters explores variations on a hypothesis: interactions between magma chamber stresses and the rheology of surrounding crustal materials evolve during magma transport and this unsteady process helps determine the magnitude, location, and timing of surface eruptions. The last part of this thesis (Chapters 6-7) focuses on surface transport processes, the meandering of melt channels on the surface of glaciers and lava flows. We show that the meandering instability is a generic feature of flow over an erodable substrate, despite significantly different fluid characteristics and erosion mechanics.",ucb,,https://escholarship.org/uc/item/0s2413df,,,eng,REGULAR,0,0
326,1762,Detecting Credential Compromise in Enterprise Networks,"Javed, Mobin","Paxson, Vern;",2016,"Secure remote access is integral to the workflow of virtually every enterprise today.  It is also an avenue ripe for network infiltration---attackers who can steal network-login credentials can often readily penetrate a site's perimeter security to obtain a persistent foothold within the network.  Once inside, they can often further their access by escalating privileges and moving laterally, potentially operating for months, all the while remaining undetected under the guise of a legitimate user. Such threats can prove hugely expensive and damaging to sites, fueling APT campaigns and enormous data breaches. For example, the 2013 theft from Target of 40,000,000 credit card numbers began with attackers compromising remote-access credentials of one of its contractors, and the 2015 breach of SSNs and biometric data of millions of government employees likewise stemmed from a stolen credential.This dissertation aims to advance the state of credential compromise detection for enterprise settings. We leverage several years worth of real-world network logs from the Lawrence Berkeley National Laboratory (LBNL) in order to develop systems for detecting: (i) stealthy, distributed brute-force attacks that compromise password-based credentials by attempting a number of guesses against the site's servers---these attacks proceed in a stealthy fashion by distributing the brute-force work across an army of machines, such that each individual host only makes a few attempts, and thereby becomes hard to differentiate from failed attempts of legitimate users, and (ii) anomalous logins indicating that a user's login credentials may have been potentially compromised---either through brute-forcing attacks or broadly through other vectors (phishing attacks and credential-stealing malware). For the detection of stealthy brute-force attacks, we first develop a general approach for flagging distributed malicious activity in which individual attack sources each operate in a stealthy, low-profile manner. We base our approach on observing statistically significant changes in a parameter that summarizes aggregate activity, bracketing a distributed attack in time, and then determining which sources present during that interval appear to have coordinated their activity.  We then apply this approach to the problem of detecting stealthy distributed SSH brute-forcing activity, showing that we can model the process of legitimate users failing to authenticate using a beta-binomial distribution, which enables us to tune a detector that trades off an expected level of false positives versus time-to-detection.  Using the detector we study the prevalence of distributed brute-forcing, finding dozens of instances in an extensive eight-year dataset collected at the Lawrence Berkeley National Lab.  Many of the attacks---some of which last months---would be quite difficult to detect individually.  While a number of the attacks reflect indiscriminant global probing, we also find attacks that targeted only the local site, as well as occasional attacks that succeeded.For the detection of anomalous logins, we first extensively characterize the volumes and diversity of login activity at LBNL's network, with the goal of engineering features that with good confidence can serve as indicators of compromise.  We then develop a practical rule-based detector that leverages the global view of the network as well as historical profile of each user to flag potentially compromised credentials. On average, our detector raises 2--3 alarms per week---a reasonable analyst workload for an enterprise with several thousand users. To understand these alarms, we worked with the site operators, who deemed the top ten percent of instances suspicious enough to merit an inquiry to the affected user. Our detector successfully flagged a known compromised account and discovered an instance of a (benign) shared credential in use by a remote collaborator.In summary, this dissertation develops approaches to detect both stealthy brute-force attempts and anomalous successful logins among hundreds of thousands of logins in an enterprise network's activity. Using our detectors, we show that stealthy brute-force attacks that target password-based credentials have been around for years, and that attackers do occasionally succeed in compromising the credentials. Our work makes advances in detecting such stealthy break-ins that, if remain undetected, can prove hugely expensive for sites.",ucb,,https://escholarship.org/uc/item/0sh3b0tw,,,eng,REGULAR,0,0
327,1763,Essays on Institutions and Innovation,"Hegde, Deepak","Mowery, David C;",2010,"The three chapters of this dissertation analyze the influence of three fundamental institutions - markets, law, and politics - on the generation and commercialization of new ideas (innovation). The analyses are empirical, and apply the theoretical perspectives of economics, law, and political science.    The first chapter asks: how do real world managers deal with adverse selection and moral hazard problems in the market for ideas? To answer this question, the chapter analyzes a new sample of 505 of arm's-length contracts, negotiated during the 1995-2008 years, between inventors and developers of biomedical inventions. The statistical findings are consistent with agency theories that propose mitigating the information problems with two-part payments consisting of upfront fees and output-based royalty rates. But I also find that licenses include other types of payments (viz. minimum royalty payments and milestone payments) to address the transaction costs of verifying outputs and the uncertainty associated with developing novel inventions.The second chapter investigates political influence in the allocation of public funds for the generation of ideas. The chapter studies U.S. Congressional appropriations committee bills and documents, and argues that although appropriators do not earmark federal funds for biomedical research performers, they support allocations for those research fields that are most likely to benefit performers in their constituencies.  The econometric analysis uses data on peer reviewed grants by the National Institutes of Health during the years 1984-2003, and finds that performers in the states of certain House appropriations committee members receive 5.9-10.3% more research funds as compared to unrepresented institutions. Members appear to support funding for the projects of represented research performers in fields in which they are relatively weak, and counteract the distributive effect of the peer review process.  The third chapter (coauthored with Professors David C. Mowery and Stuart J. H. Graham) exploits the Y1995 change in U.S. patent term to understand the use of continuations by firms in the prosecution of their patents during the years 1981-2000. The findings suggest that biomedical firms use continuations to lengthen the duration of patents protecting their most valuable ideas, while electronics and semiconductor firms use the process to augment the size of their patent portfolios. Firms use different types of continuations - the Continuation Application, the Continuations-In-Part, and Divisions - for different ends. Hence, U.S. patent laws, and their reform, can benefit from a closer consideration of the type of continuation filed by applicants.",ucb,,https://escholarship.org/uc/item/0sp3n4sk,,,eng,REGULAR,0,0
328,1764,Traffic Signal Optimization with Transit Priority: A Person-based Approach,"Christofa, Eleni","Skabardonis, Alexander;",2012,"Traffic responsive signal control with Transit Signal Priority (TSP) is a strategy that is increasingly used to improve transit operations in urban networks. However, none of the existing real-time signal control systems have explicitly incorporated the passenger occupancy of transit vehicles in granting priority or have effectively addressed issues such as the provision of priority to transit vehicles traveling in conflicting directions at signalized intersections. The contribution of this dissertation is the development of a person-based traffic responsive signal control system with TSP that minimizes total person delay in a network by explicitly considering all vehicles' passenger occupancy and transit schedule delay. By using such conditions, the issue of assigning priority to transit vehicles traveling in conflicting directions is also addressed in an efficient way. In addition, the impact of these priority strategies on auto traffic is addressed by minimizing the total person delay in the network under consideration and assigning penalties for interrupting the progression of platoons on arterials. The system is first developed for isolated intersections, and then extended to arterial signalized networks. Evaluation tests for a wide range of traffic and transit operating characteristics show that significant reductions in transit passenger delay can be achieved without substantially increasing auto passenger delay. Furthermore, the system achieves lower vehicle delays compared to signal settings obtained by state-of-the-art signal optimization software. Finally, it utilizes readily deployable technologies, which provide real-time information such as sensors, Automated Vehicle Location and Automated Passenger Counter systems and can be implemented on existing infrastructure in urban multimodal networks.",ucb,,https://escholarship.org/uc/item/0t4336f3,,,eng,REGULAR,0,0
329,1765,Epitaxial Stabilization of a Morphotropic Phase Boundary in Lead-Free Ferroelectric Thin Films,"Zeches, Robert James","Ramesh, Ramamoorthy;",2011,"This dissertation details the synthesis and characterization of compressively strained epitaxially grown bismuth ferrite thin films in an effort to improve their piezoelectric properties with the goal of finding a viable lead-free alternative to lead zirconate titanate.  It is found that under compressive epitaxial strain a morphotropic phase boundary, the hallmark of high performance piezoelectrics, is stabilized in this material.  The structural characterization of these films reveals complex mixed-phase morphologies whereby nearly energetically degenerate monoclinically distorted tetragonal and rhombohedral ferroelectric phases coexist with apparent crystallographically coherent interfaces.  This mixed phase morphology is found to be responsive to perturbations both via mechanical indentation and an applied electric field, whereby a reversible phase transition is observed in the films. Electrical characterization of these films indicates a pronounced increase in their piezoelectric and ferroelectric properties.",ucb,,https://escholarship.org/uc/item/0pz9s4r8,,,eng,REGULAR,0,0
330,1766,Diffusion in SiGe and Ge,"Liao, Christopher Yuan Ting","Haller, Eugene E;",2010,"Diffusion is the most fundamental mass transport process in solids characterized by point defect-diffusing atom interactions.  In order to predict diffusion processes of impurities in a solid, the diffusion mechanisms, i.e., interactions between point defects and diffusing species, must be well understood.  While the diffusion parameters and mechanisms are well known in silicon, very limited knowledge exists for diffusion parameters and mechanisms for Ge and SiGe alloys.  As Ge and SiGe alloys are introduced in the new generations of microelectronic devices, the diffusion behavior in these materials must be studied.	The simultaneous diffusion of As, Si, and Ge in a Si0.95Ge0.05 alloy has been studied using a structure with an isotopically enriched SiGe layer.  The diffusion in both intrinsic and extrinsic conditions was carried out between 900 to 1180Â°C.  From the numerical fitting of the diffusion profiles, the diffusion mechanisms are determined.  The simultaneous As and self-diffusion have been successfully modeled as a combination of the vacancy diffusion mechanism with doubly negatively charged vacancies, the interstitial-assisted mechanism with neutral self-interstitials, and a As2V clustering process.  The diffusion mechanisms are the same as those for As in pure Si except for the clustering process, which is common for donors in pure Ge.  The effective equilibrium diffusion pre-exponential factor for As in Si0.95Ge0.05 is determined to be 129 cm2/s while the diffusion activation enthalpy is 4.27 eV.  The diffusion parameters for As in this alloy composition are very close to those in pure Si.  Future experiments to study As diffusion in different alloy compositions are proposed.  In this way, a transition from the As-in-Si-like diffusion mechanism to As-in-Ge-like diffusion mechanism can be identified.	Proton irradiation enhanced B diffusion in Ge has also been studied.  The proton irradiation introduces excess self-interstitials which are virtually non-existing under equilibrium conditions.  A molecular beam epitaxially grown structure with six B-doped layers was used for the radiation enhanced diffusion studies.  We found B diffusion is enhanced by many orders of magnitude under this non-equilibrium condition.  The effective B diffusion enthalpy under 2.5 MeV proton irradiation with 1.5 ÂµA beam current is found to be 0.48 eV for temperatures from 400 to 500Â°C.  This effective enthalpy is much lower than the 4.65 eV found under equilibrium conditions.  From the radiation enhanced B and self-diffusion experiments, we conclude that the interstitial-mediated diffusion mechanism is dominant under the proton irradiation condition.  We can also conclude that B diffusion is indeed driven by self-interstitials under equilibrium conditions.  We further propose some future experiments to help identify the exact B diffusion mechanism(s) and the charge states of the B-defect pairs in Ge.",ucb,,https://escholarship.org/uc/item/0qj1q0fk,,,eng,REGULAR,0,0
331,1767,Transmembrane Transport in Biomimetic Assemblies of One-Dimensional Nanomaterials,"Kim, Kyunghoon","Grigoropoulos, Costas P.;",2013,"The creation of biomimetic structures based on one-dimensional nanomaterials and lipid membranes will provide a unique platform for achieving functionalities of biological machines and mimicking nature at the nanoscale. Silicon nanowires (SiNW) and carbon nanotubes (CNT) are of significant interest due to the novel properties not present in bulk materials as well as characteristic dimensions comparable to the size of biological molecules. My thesis describes the creation of fabricated nanomaterials integrated with biomaterials such lipid membranes and their constitutive proteins to create biomimetic assemblies. In the first part of my dissertation, I report transmembrane carbon nanotube pores as a biological ion channel analogs. Biological ion channels in nature transport ions across cellular membranes showing two functions of gating and ion selectivity. CNT pores give structural and functional mimic of an ion channel, in part because smooth, narrow and hydrophobic inner pores of the CNT are remarkably similar to natural biological pores. First, CNTs served as a materials platform that can replicate the features of biological channels.  I successfully created ultrashort CNTs (ca. ~10nm) using lipid-assisted sonication-cutting method. Lipid molecules self-assemble on the long CNTs to form a template for sonication cutting. These short CNT pores with their length comparable to the lipid membrane thickness provide a much closer match to protein-channel dimensions. Short CNT pores were incorporated into lipid vesicles to mimic membrane ion channels and study transport properties through CNT pores. These short CNTs in a lipid membrane can transport water, protons, and small ions and reject large uncharged species. Ion rejection in CNT channels is determined by charge repulsion at the CNT rim.  Electrophoretic ion transport measurements for individual CNT pores revealed an ion conductance value of 0.63ns which is comparable to those of biological channels. CNT pores inserted in the membrane exhibited stochastic gating behavior common for biological ion channels. These fluctuations result from a spontaneous reversible ionic penetration-exclusion transition previously reported in nanofluidic transport of sub-2-nm pores. Electrophoretically-driven translocation of individual single-stranded DNA molecules through CNT pores produced well-defined ion current blockades. Overall, short CNT mimics transport properties of a biological protein channel. Since the structure and functionality of short CNT pores self-inserted in a lipid membrane resemble the Î²-barrel structure of a porin, they are termed as ""carbon nanotube porins"".In the second part, I describe synthesis of SiNWs grown via vapor-liquid-solid (VLS) mechanism. Silicon nanowires were grown on silicon substrates via chemical vapor deposition (CVD) using silane as a precursor gas and diborane for p-type doing of wires. These nanowires were utilized for a bioelectronics platform for integration of membrane protein functionality based on one-dimensional lipid bilayer. This lipid bilayer provides shielding the nanowires from the solution species and environment for proteins preserving their functionality, integrity, and even vectorality. Here, I report a hybrid lipid bilayer- silicon nanowire bioelectronic device with output controlled via light-induced proton pump protein, bacteriorhodopsin (bR). SiNW field effect transistors (FET) were fabricated via conventional micro/nanofabrication process. bR proteins were incorporated into SiNW transistors covered with a lipid bilayer shell and different ionophore molecules, valinomycin and nigericin were co-assembled to create biologically-tunable bioelectronics devices.  In this way, the devices convert photoactivated proton transport by bR protein into an electronic signal. The addition of ionophores tuned the device output by altering membrane ion permeability and the two ionophores were able to modulate different system parameters.",ucb,,https://escholarship.org/uc/item/0s24g5rv,,,eng,REGULAR,0,0
332,1768,Density Functional Perturbation Theory and Adaptively Compressed Polarizability Operator,"Xu, Ze","Lin, Lin;",2019,"Kohn-Sham density functional theory (KSDFT) is by far the most widely used electronic structure theory in condensed matter systems. Density functional perturbation theory (DFPT) studies the response of a quantum system under small perturbation, where the quantum system is described at the level of first principle electronic structure theories like KSDFT. One important application of DFPT is the calculation of vibration properties such as phonons, which can be further used to calculate many physical properties such as infrared spectroscopy, elastic neutron scattering, specific heat, heat conduction, and electron-phonon interaction related behaviors such as superconductivity . DFPT describes vibration proper- ties through a polarizability operator, which characterizes the linear response of the electron density with respect to the perturbation of the external potential. More specifically, in vibration calculations, the polarizability operator needs to be applied to d Ã— NA âˆ¼ O(Ne) perturbation vectors, where d is the spatial dimension (usually d = 3), NA is the number of atoms, and Ne is the number of electrons. In general the complexity for solving KSDFT is O(Ne3), while the complexity for solving DFPT is O(Ne4). It is possible to reduce the computational complexity of DFPT calculations by â€œlinear scaling methodsâ€. Such methods can be successful in reducing the computational cost for systems of large sizes with substantial band gaps, but this can be challenging for medium-sized systems with relatively small band gaps.In the discussion below, we will slightly abuse the term â€œphonon calculationâ€ to refer to calculation of vibration properties of condensed matter systems as well as isolated molecules. In order to apply the polarizability operator to O(Ne) vectors, we need to solve O(Ne2) coupled Sternheimer equations. On the other hand, when a constant number of degrees of freedom per electron is used, the size of the Hamiltonian matrix is only O(Ne). Hence asymptotically there is room to obtain a set of only O(Ne) â€œcompressed perturbation vectorsâ€, which encodes essentially all the information of the O(Ne2) Sternheimer equations. In this dissertation, we develop a new method called adaptively compressed polarizability operator (ACP) formulation, which successfully reduces the computational complexity of phonon12calculations to O(Ne3) for the first time. The ACP formulation does not rely on exponential decay properties of the density matrix as in linear scaling methods, and its accuracy depends weakly on the size of the band gap. Hence the method can be used for phonon calculations of both insulators and semiconductors with small gaps.There are three key ingredients of the ACP formulation. 1) The Sternheimer equations are equations for shifted Hamiltonians, where each shift corresponds to an energy level of an occupied band. Hence for a general right hand side vector, there are Ne possible energies (shifts). We use a Chebyshev interpolation procedure to disentangle such energy dependence so that there are only constant number of shifts that is independent of Ne. 2) We disentangle the O(Ne2) right hand side vectors using the recently developed interpolative separable density fitting procedure, to compress the right-hand-side vectors. 3) We construct the polarizability by adaptive compression so that the operator remains low rank as well as accurate when applying to a certain set of vectors. This make it possible for fast computation of the matrix inversion using methods like Sherman-Morrison-Woodbury.In particular, the new method does not employ the â€œnearsightednessâ€ property of electrons for insulating systems with substantial band gaps as in linear scaling methods. Hence our method can be applied to insulators as well as semiconductors with small band gaps.This dissertation also extend the method to deal with nonlocal pseudopotentials as well as systems in finite temperature. Combining all these components together, we obtain an accurate, efficient method to compute the vibrational properties for insulating and metallic systems.",ucb,,https://escholarship.org/uc/item/0s90k7tk,,,eng,REGULAR,0,0
333,1769,Effective Daylighting: Evaluating Daylighting Performance in the San Francisco Federal Building from the Perspective of Building Occupants,"Konis, Kyle Stas","Benton, Charles C;",2011,"Commercial office buildings promoted as  sustainable, energy efficient, green, or  high performance often reference use of daylight as a key strategy for reducing energy consumption and enhancing indoor environmental quality.  However, buildings are rarely studied in use to examine if the design intent of a sufficiently daylit and a visually comfortable work environment is achieved from the perspective of building occupants or how occupant use of shading devices may affect electrical lighting energy reduction from photocontrols.  This dissertation develops a field-based approach to daylighting performance assessment that pairs repeated measures of occupant subjective response using a novel desktop polling station device with measurements of the physical environment acquired using High Dynamic Range (HDR) imaging and other environmental  sensors with the objective of understanding the physical environmental conditions acceptable to occupants.  The approach is demonstrated with a 6-month field study involving (N=44) occupants located in perimeter and core open-plan office spaces in the San Francisco Federal Building (SFFB).  Over 23,100 subjective assessments paired with physical measures were analyzed to develop models of visual discomfort and shade control and to examine the assumptions of existing daylighting performance indicators.  The analysis found that existing daylight performance indicators overestimated the levels of daylight illuminance required by occupants to work comfortably without overhead ambient electrical lighting.  Time-lapse observation of interior roller shades showed that existing shade control models overestimated the frequency of shade operation and underestimated the level of facade occlusion due to interior shades.  Comparison of measured results to the daylighting objectives of the SFFB showed that available daylight enabled electrical lighting energy reduction in the perimeter zones but not in the open-plan core zones.  The results extend existing knowledge regarding the amount of daylight illuminance acceptable for occupants to work comfortably without overhead electrical lighting and for the physical variables (and stimulus intensities) associated with visual discomfort and the operation of interior shading devices.",ucb,,https://escholarship.org/uc/item/0vd3q8s2,,,eng,REGULAR,0,0
334,1770,PINs lost and PINs gained: Auxin-transport mediated patterning in the grasses,"O'Connor, Devin Lee","Hake, Sarah C;",2012,"In plants, transport mediated by the PINFORMED (PIN) family of auxin efflux carriers helps create gradients on which many developmental processes depend. Current models suggest that Arabidopsis PIN1 has two concurrent functions during leaf initiation: 1) concentrating auxin to create local maxima in the meristem epidermis, and 2) transporting auxin away from the epidermal maxima and into the internal tissues. The resulting auxin gradient is required for leaf initiation and vein patterning. I identified an angiosperm PIN clade placed sister to PIN1, here termed Sister-of-PIN1 (SoPIN1), that has likely been lost within the Brassicaceae, including in Arabidopsis, but remains in all other angiosperms sampled. I also identify a conserved duplication of PIN1 to create PIN1a and PIN1b within the grasses. I used live-cell imaging and immuno-localization to characterize the expression and localization of SoPIN1, PIN1a and PIN1b members in both maize and Brachypodium. SoPIN1 expression is highest in the epidermis and is consistently oriented toward areas where the DR5 auxin reporter is highly expressed, suggesting that SoPIN1 functions in the creation of auxin maxima. PIN1a and PIN1b localization, largely absent from the epidermis and oriented rootward in the internal tissues, suggests that these PIN proteins transport auxin after maxima formation during the canalization of leaf and stem veins. These data support the functional division of PIN proteins into maxima creation and canalization modes. In addition, the loss of SoPIN1 within the Brassicaceae suggests that PIN1 in this group may be unique amongst the angiosperms in its ability to dynamically switch between these two functional modes. I then provide a model for how the PIN1a/PIN1b duplication in the lineage leading to the grasses may relate to the novel morphological and anatomical characteristics found in monocot plants. Finally, I summarize some preliminary PIN knockdown experiments that suggest a role for PIN mediated patterning in apical dominance, meristem maintenance and leaf proximal/distal patterning.",ucb,,https://escholarship.org/uc/item/0vv3h25r,,,eng,REGULAR,0,0
335,1771,Subphonemic Teamwork: A Typology and Theory of Cumulative Coarticulatory Effects in Phonology,"Lionnet, Florian A. J.",,2016,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0wr7c976,,,eng,REGULAR,0,0
336,1772,X-ray Absorption Spectroscopy Study of Prototype Chemical Systems: Theory vs. Experiment,"Schwartz, Craig Philip","Saykally, Richard J;",2010,"Understanding the details of the intensities and spectral shapes of x-ray absorption spectra is a long-standing problem in chemistry and physics.  Here, I present detailed studies of x-ray absorption for prototypical liquids, solids and gases with the goal of enhancing our general understanding of core-level spectroscopy via comparisons of modern theory and experiment.	In Chapter 2, I investigate the importance of quantum motions in the x-ray absorption spectra of simple gases.  It is found that rare fluctuations in atomic positions can be a cause of features in the spectra of gaseous molecules.	In Chapter 3, I explore a novel quantization scheme for the excited and ground state potential surfaces for an isolated nitrogen molecule.  This allows for the explicit calculation of the ""correct"" transition energies and peak widths (i.e. without any adjustable parameters).	In Chapter 4, the importance of nuclear motion in molecular solids is investigated for glycine.  We find that the inclusion of these motions permits the spectrum to be accurately calculated without any additional adjustable parameters. 	In Chapter 5, I provide a detailed study of the hydroxide ion solvated in water.  There has been recent controversy as to how hydroxide is solvated, with two principal models invoked.   I show that some of the computational evidence favoring one model of solvation over the other has been either previously obtained with inadequate precision or via a method that is systematically biased.	In Chapter 6, the measured and computed x-ray absorption spectra of pyrrole in both the gas phase and when solvated by water are compared.  We are able to accurately predict the spectra in both cases.	In Chapter 7, the measured x-ray absorption of a series of highly charged cationic salts (YBr3, CrCl3, SnCl4, LaCl3 and InCl3) solvated in water are presented and explained.	In Chapter 8, the measured x-ray absorption spectrum at the nitrogen K-edge of aqueous triglycine is presented, including effects of various salts which can alter its solubility.  This is used to show that while x-ray absorption is sensitive to salt interactions with small peptides, it is unlikely to be a sensitive probe for overall protein structures, i.e. to distinguish beta sheet from an alpha helix at the nitrogen K-edge.	Finally, in Chapter 9 future directions are discussed.",ucb,,https://escholarship.org/uc/item/0xd515cp,,,eng,REGULAR,0,0
337,1773,Learning to Reconstruct 3D Objects,"Kar, Abhishek","Malik, Jitendra;",2017,"Ever since the dawn of computer vision, 3D reconstruction has been a core problem, inspiring early seminal works and leading to numerous real world applications. Much recent progress in the field however, has been driven by visual recognition systems powered by statistical learning techniques - more recently with deep convolutional neural networks (CNNs). In this thesis, we attempt to bridge the worlds of geometric 3D reconstruction and learning based recognition by learning to leverage various 3D perception cues from image collections for the task of reconstructing 3D objects.In Chapter 2, we present a system that is able to learn intra-category regularities in object shapes by building category-specific deformable 3D models from 2D recognition datasets enabling fully automatic single view 3D reconstruction for novel instances. In Chapter 3, we demonstrate how predicting the amodal extent of objects in images and reasoning about their co-occurrences can help us infer their real world heights. Finally, in Chapter 4, we present Learnt Stereo Machines (LSM), an end-to-end learnt framework using convolutional neural networks, which unifies a number of paradigms in 3D object reconstruction- single and multi-view reconstruction, coarse and dense outputs and geometric and semantic reasoning. We will conclude with several promising future directions for learning based 3D reconstruction.",ucb,,https://escholarship.org/uc/item/0xv175b2,,,eng,REGULAR,0,0
338,1774,Theoretical Approaches to Measuring the Welfare Effects of Asset Transfers,"Collins, Elliott","Ligon, Ethan;",2017,"Millions of low-income households have received large one-time transfers of capital as part of private and public anti-poverty programs. Economists have studied the effect of such transfers in a wide variety of settings to understand their effect on household welfare and poverty rates. The standard metric used to study these effects is aggregate consumption, measured in price-adjusted dollars per capita per day, which in turn gets used to define the global poverty lines at $2 and $1.25. In this dissertation, I discuss some of the theoretical and practical issues that these measures face, and then suggest some alternatives pulled from a model of consumer demand. I demonstrate how they can be used to expand our understanding of two programs of direct capital transfers to poor households, with the broad goal of bringing insights from economic theory farther into the practice of impact evaluation.This dissertation has as its chapters three papers, each written so that they stand alone for those interested in a particular part of the overall project. The first chapter develops a Frischian model of household expenditures, and uses this model to derive estimates of an individual householdâ€™s marginal utility in expenditures at any given time, log Î»it, which serves as the central welfare metric in each chapter. The application of this model to measure the welfare effects of asset transfers is illustrated using the early results of a pilot in South Sudan of the so-called â€œTargeting the Ultra-Poorâ€ (TUP) program. It concludes that the large asset transfers improved welfare in the short-run.1Chapter 2 uses the methods in Chapter 1 to reanalyze the first large experimental evaluation of the TUP framework in Bangladesh, previously studied in Bandiera et al. (2017). It goes on to estimate a flexible direct utility function which allows one to estimate welfare in a way that accounts for the immiserating effects of uncertainty faced by poor households. It concludes that this TUP program improved household welfare years after the transfers, and that this benefit was accompanied by a lower level of risk.Finally, Chapter 3 goes on to study the experiment in South Sudan in more detail. It compares the TUP treatment to an experimental cash transfer, and showcases the practical value of the methods in Chapter 1 by continuing to cost-effectively monitor household welfareafter the long-form panel survey was completed. It then briefly speaks to how participation in the TUP program may have affected householdsâ€™ response to the outbreak of violent civil conflict in 2014. The results show that both cash and asset transfers improved household welfare only in the short run, but the asset transfers had a sustained positive impact on householdsâ€™ total stock of assets, which may have reduced the likelihood that those households were left without recourse upon the onset of violence.",ucb,,https://escholarship.org/uc/item/0zb8s78z,,,eng,REGULAR,0,0
339,1775,High-Performance Discrete-Vortex Algorithms for Unsteady Viscous-Fluid Flows near Moving Boundaries,"Wang, Lu","Yeung, Ronald W;",2016,"A high-performance Discrete-Vortex Method (DVM) is successfully developed and implemented to directly simulate two-dimensional low to medium-Reynolds number flows around multiple arbitrarily shaped bodies undergoing either prescribed or free motions. The deterministic Viscous-Vortex-Domain (VVD) formulation is adopted to simulate vorticity diffusion. Through the use of CPU and Graphics-Processing-Unit (GPU) parallel computing, significant speedup of the simulation compared to a serial implementation on a CPU is achieved. The validity of the present DVM simulation is confirmed by comparing the present results with published ones for a variety of test cases. The current implementation of DVM has been used to study two novel flow problems of practical interest and has led to significant findings. First, the full and partial ground effects on the lift generation of a flapping (air)foil in normal hovering mode are investigated. To achieve full ground effect, the foil of chord c is made to hover above the center of a finite-sized platform of length 10c. The computed force-enhancement, force-reduction, and force-recovery regimes at low, medium, and high ground clearances are observed to be in line with existing literature. This research puts special focus on partial ground effects when the foil is hovering near the edge of the platform. Lift-modifying mechanisms not previously observed under full ground effect have been discovered. When stroke reversal of the flapping occurs near the edge of the platform, a relatively stationary strong vortex may form above the platform edge. This strong vortex can either increase or decrease the instantaneous lift force on the foil depending on the position of the foil relative to the platform edge. Further, the platform edge may lead to the formation of an additional vortex pair which increases the instantaneous lift force as the foil sweeps past the edge under certain suitable conditions. Lastly, the platform edge can lead to the formation of a reverse von KÃ¡rmÃ¡n vortex street that extends well below the stroke plane under suitable geometric arrangements.Second, the flow past a Bach-type vertical-axis wind or current turbine is simulated using the DVM at a Reynolds number of 1,500. The main purpose of the study is to evaluate the suitability of Bach-type turbines for use as micro-scale energy harvesters that can be applied to power, for example, sensor nodes of a Wireless Sensor Network. Through simulations, the maximum power coefficient of the turbine operating at a prescribed constant tip-speed ratio is found to be 0.18, which is comparable to the performance of a turbine of the same geometry at much higher Reynolds numbers. This indicates that there is only minimal performance penalty for miniaturization. The angular velocity of the turbine has a strong influence on the evolution of vortical flow structures. A new wake-capturing mechanism that boosts the performance of the turbine is discovered from the simulations for a certain range of tip-speed ratios where the vortex shed by the advancing blade helps drive the returning blade. In addition to the condition of prescribed rotation, free rotation of a steel Bach-type turbine under a steady current in water is also investigated. Significant fluctuation in angular velocity over one period of rotation is observed. This speed fluctuation is found to be detrimental to energy extraction, reducing the maximum power coefficient to approximately 0.16. This level of power-generation capability implies that such micro-scale turbines can significantly extend the life expectancy of a wireless sensor node or even maintain the node in a low-power state indefinitely.",ucb,,https://escholarship.org/uc/item/11b3z1t9,,,eng,REGULAR,0,0
340,1776,"The Effects of Intermittent Drinking Water Supply in ArraijÃ¡n, Panama","Erickson, John Joseph","Nelson, Kara L;",2016,"Over three hundred million people throughout the world receive supply from piped drinking water distribution networks that operate intermittently. This dissertation evaluates the effects of intermittent supply on water quality, pipe damage and service reliability in four study zones (one continuous and three intermittent) in a peri-urban drinking water distribution network in ArraijÃ¡n, Panama. Normal water quality in all zones was good, with 97% of routine water quality grab samples from the distribution system and household taps having turbidity < 1 NTU, total coliform and E. coli bacteria concentrations < 1 MPN / 100 mL, and â‰¥ 0.3 mg/L free chlorine residual. However, negative pressures that represent a risk for contaminant intrusion and backflow were detected in three of the four study zones, and water quality during the first flush when supply resumed after an outage was sometimes degraded. High and transient pressures that could cause pipe damage were detected in study zones with intermittent pumping, but filling and emptying of distribution pipes due to intermittent supply was not associated with transient or extreme pressures. Operational challenges, including frequent infrastructure failures, difficulty monitoring the network, and a lack of system information, resulted in unreliable supply in the intermittent zones. Continuous pressure and flow monitoring methods used in this research could be helpful tools for operators of intermittent distribution networks to provide more reliable service and identify hydraulic conditions that could lead to contaminant intrusion or pipe breaks.",ucb,,https://escholarship.org/uc/item/12v465g4,,,eng,REGULAR,0,0
341,1777,Modularity Conserved during Evolution: Algorithms and Analysis,"Hodgkinson, Luqman","Karp, Richard M.;",2013,"Modularity is a defining feature of biological systems. This dissertation presents our work on the development of algorithms to detect modularity in protein interaction networks and techniques of analysis for interpreting the results. A multiprotein module is a collection of proteins exhibiting modularity in their interactions. Multiprotein modules may perform essential functions and be conserved by purifying selection. A new linear-time algorithm named Produles offers significant algorithmic advantages over previous approaches. An algorithmic framework for evaluation is presented that facilitates evaluation of algorithms for detecting conserved modularity with respect to their algorithmic goals. Optimization criteria for detecting homologous multiprotein modules are examined, and their effects on biological process enrichment are quantified. Graph theoretic properties that arise from the physical construction of protein interaction networks account for 36 percent of the variance in biological process enrichment. Protein interaction similarities between conserved modules have only minor effects on biological process enrichment. As random modules increase in size, both biological process enrichment and modularity tend to improve, though modularity does not show this trend in small modules. To adjust for this trend, we recommend a size correction based on random sampling of modules when using biological process enrichment to evaluate module boundaries. Supporting software has been developed useful for designing high quality algorithms for detecting conserved multiprotein modularity. EasyProt is a parallel implementation of scientific workflow software designed for cloud computing that retrieves data from several sources, runs algorithms in parallel, and computes evaluation statistics. VieProt is visualization software for conserved multiprotein modularity that uses a dynamic force-directed layout and displays quality measures and statistical summaries. With high quality protein interaction data, it may be possible to use modules to improve the prediction of proteins that are orthologous to each other and that have maintained their function. We present statistical methods that may be useful for this purpose. The utility of these models will depend on anticipated improvements in protein interaction data quality.",ucb,,https://escholarship.org/uc/item/14h963bt,,,eng,REGULAR,0,0
342,1778,The Semantics of KÊ·akÌ“Ê·ala Object Case,"Sardinha, Katherine",,2017,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/14r9s7t2,,,eng,REGULAR,0,0
343,1779,Ibibio Grammar,"Kaufman, Elaine",,1968,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/16w6n07m,,,eng,REGULAR,0,0
344,1780,"Acclimation and evolution in a changing climate: Integrating physiology, transcriptomics, and genomics of a thermal specialist","Tonione, Maria","Tsutsui, Neil D;",2018,"Climate change is one of the top causes of biodiversity loss. Organisms will experience many pressures associated with climate change, one of the most obvious being increased temperature. It is therefore important to understand how animals will react to this stress. Ectotherms, such as ants, are especially sensitive to the climate as they rely on environmental temperature for everything from optimal foraging to development time. In this dissertation, I explore the individual and population level reactions to thermal stress of a cold-specialist, the winter ant, Prenolepis imparis. I also identify the role past climatic fluctuations have had in shaping this speciesâ€™ current distribution. In my first dissertation chapter, I conducted a RNA-seq analysis to identify stress-induced genes in P. imparis individuals at the transcriptome level. To identify candidate genes involved in the stress response, I induced stress by placing the ants at a low or high temperature. Then, I sequenced the transcriptome of these stressed individuals. The genes that show an increase during transcription are candidates for allowing individuals to recover from the stress. I identified a total of 709 differentially expressed genes. In the cold-stressed ants, I did not identify a strong response, indicating that the temperature we chose for trials was not cold enough. Conversely, I found a strong response to the heat. Those transcripts we found highly induced include protein folding genes, heat shock proteins, proteins associated with heat shock proteins, Ca2+ ion transport, and a few unknown genes. I also found functional categories relating to protein folding, muscles, and temperature stimulus increased in the heat-stress response. In my second dissertation chapter, I measured the short-term acclimation ability of high- and low-elevation populations of P. imparis across California. In addition, I also characterized the thermal environment both above and below ground. I found that the high-elevation sites showed increased tolerance and reduced capacity in acclimation ability relative to the low-elevation counterparts at their lower limits, suggesting an evolutionary trade-off between tolerance and acclimation ability. In addition, I found less acclimation capacity across all populations in their upper limits. I also found that the high-elevation sites experience cooler temperatures both above and below ground. The greater acclimation response at lower limits in high-elevation populations could suggest that they are better physiologically prepared to survive cooler temperatures. In my final dissertation chapter, I used phylogenetic and population genetic analyses to identify population genetic structure and historical demographic patterns across the range of P. imparis. I relate the genomic patterns to those expected as seen with in situ diversification, or maintained connectivity. I recovered five well-supported genetically isolated clades across the distribution. I also investigated gene flow between these major genetic clades and did not find evidence of gene flow between clades. High support for five major geographic lineages and lack of evidence of contemporary gene flow indicate in situ diversification across the speciesâ€™ range, probably influenced by glacial cycles of the late Quaternary.Overall, the results from this dissertation give insight into plasticity as well as the evolutionary processes that have shaped this species. My results suggest molecular pathways by which phenotypic plasticity will allow individuals to overcome heat-stress: the candidate genes provided here are a valuable resource in understanding pathways and proteins necessary for survival at unfavorable temperatures. I also report that individuals from different populations show different levels of thermal tolerance and plasticity. All the populations show less tolerance and reduced plasticity to the heat. This is troubling in the face of climate change, this limited acclimation response at the upper thermal limits suggests evolutionary constraints in heat tolerance, so major changes at the molecular level will be needed for these populations to persist in warmer environments. Finally, the entire range of this species has been profoundly affected by climatic fluctuations during the Quaternary. These fluctuations led those individuals to have separate evolutionary histories and raises the possibility that there are several unique species of P. imparis.",ucb,,https://escholarship.org/uc/item/1742b1w9,,,eng,REGULAR,0,0
345,1781,Zero-Emission Vehicle Scenario Cost Analysis Using A Fuzzy Set-Based Framework,"Lipman, Timothy Edward",,1999,"In this study, potential vehicle manufacturing costs, lifecycle costs, infrastructure support costs, and emission-related costs are compared for three potential zero-emission vehicle (ZEV) technology development and deployment scenarios. These scenarios include production of mid-sized battery electric vehicles direct-hydrogen fuel cell vehicles, and direct methanol fuel cell vehicles from 2003 to 2026, and operation of the vehicles in California's South Coast Air Basin (SCAB) from 2003 to 2043. The study focuses on potential manufacturing cost reductions for electric motors, motor controllers, battery systems, hydrogen storage tanks, and fuel cell systems, due to the combined forces of production scale economies and technological progress. Vehicle manufacturing and lifecycle costs are calculated by integrating vehicle component cost functions with a detailed vehicle performance and cost spreadsheet model. Fleet-level costs for vehicle operation, infrastructure development, and criteria pollutant and greenhouse gas emissions are calculated using a MATLAB/Simulink model developed by the author. In this regional-scale, fleet-level model, fuzzy set theory is used to characterize uncertainty in key input variables, and to propagate uncertainty through the calculation of vehicle, infrastructure, and emissions costs. Findings are that estimated ZEV purchase prices drop steadily with production volume and technological progress, but that even in future, high-volume production the estimated purchase prices for all three ZEV types remain above those of comparable conventional vehicles. However, lifecycle costs for ZEVs in some cases become competitive with those of comparable conventional vehicles, especially for direct-hydrogen fuel cell vehicles. When infrastructure and emission-related costs are considered for vehicles used in the SCAB, total lifecycle costs for direct-hydrogen fuel cell vehicles are found to be below those of even low-emission gasoline vehicles by 2026, under central case assumptions. Meanwhile, total lifecycle costs for battery EVs and direct-methanol fuel cell vehicles are found to be between those of conventional and low-emission gasoline vehicles, again in the year 2026 central case. In general, the overall level of uncertainty in the calculation of total scenario net present values is considerable, and this level of uncertainty prevents the unequivocal determination of a least-cost ZEV technology pathway for the SCAB.",ucb,,https://escholarship.org/uc/item/17r4d88f,,,eng,REGULAR,0,0
346,1782,Crowded Subjects: The Crisis of Two Souls in Early Modern England,"Lee, James J.","Kahn, Victoria;",2012,"This dissertation challenges the over-familiar Christian dualism of the corrupt body and the redeemed soul, and completely rethinks the significance of the term ""soul"" in Renaissance culture. I argue that sixteenth and seventeenth century poets and philosophers uneasily defined the human as possessing two contradictory souls, and that the anxiety provoked by the contradiction of the Aristotelian and Christian souls led to a basic redefinition of the early modern subject. The dissertation begins with an intellectual history describing how 16th century theologians came to realize that the presence of two souls in man, Aristotelian and Christian, represented a major crisis for Christian doctrine. Reading the work of Pietro Pomponazzi, Philipp Melanchthon, and John Woolton, I argue that the paradox of two souls became a problem of political obedience and a rationalization of the Christian subject's passive relation to law. The dissertation then tracks the critical response of prominent English poets, such as William Shakespeare, John Donne, and John Davies, who argued that the paradox of two souls could not be understood as a fictional justification of law and political obedience, and that 16th century theologians were telling the wrong fiction to explain the two souls. All of these English poets turned to the resources of poetic language and dramatic plot as the most potent means to understand and resolve the two souls paradox as a basis for a novel Christian poetics of freedom, and not an abject servility to law. In all of these English poets, I identify a sustained work of contesting and redefining the parameters of the word ""soul"" as the basis of the Christian subject, and a reconfiguration of the two souls paradox into a problem rendered comprehensible by literary narrative and the craft of poetry.",ucb,,https://escholarship.org/uc/item/1807s64s,,,eng,REGULAR,0,0
347,1783,Writing from the Periphery: W. G. Sebald and Outsider Art,"Etzler, Melissa Starr","Kudszus, Winfried;",2014,"This study focuses on a major aspect of literature and culture in the later twentieth century:  the intersection of psychiatry, madness and art.  As the antipsychiatry movement became an international intervention, W. G. Sebald's fascination with psychopathology rapidly developed.  While Sebald collected many materials on Outsider Artists and has several annotated books on psychiatry in his personal library, I examine how Sebald's thought and writings, both academic and literary, were particularly influenced by Ernst Herbeck's poems.  Herbeck, a diagnosed schizophrenic, spent decades under the care of Dr. Leo Navratil at the psychiatric institute in Maria Gugging.  Sebald became familiar with Herbeck via the book, Schizophrenie und Sprache (1966), in which Navratil analyzed his patients' creative writings in order to illustrate commonalities between pathological artistic productions and canonical German literature, thereby blurring the lines between genius and madness.  In 1980, Sebald travelled to Vienna to meet Ernst Herbeck and this experience inspired him to compose two academic essays on Herbeck and the semi-fictionalized account of their encounter in his novel Vertigo (1990).In this study, I reveal how Sebald incorporated Herbeck within his works over a thirty year period in order to provide a social commentary.  Sebald looks to Herbeck to examine what had become the standards of normality in Western Germany following the Second World War from a critical perspective.  Not only is Sebald's empathetic identification with the outsider Herbeck in itself a political and social act of protest, but I show how Sebald recognized in Herbeck's language an embodiment of his own viewpoints regarding (eco)politics and critical theory.  Since his understanding of Herbeck is informed by a number of disciplines, various cultural discourses assist in my clarification:  I turn to Sebald's interest in the Frankfurt School thinkers, such as Max Horkheimer and Theodor Adorno; Gilles Deleuze and FÃ©lix Guattari's ideas on minor literature; political viewpoints as posed by filmmakers such as Alexander Kluge and Werner Herzog; and also the socio-anthropological writings of Pierre Bertaux, Michel Foucault and Claude LÃ©vi-Strauss.  I furthermore provide close readings of Herbeck's poems to argue that Sebald imitates Herbeck's style.  Focusing initially on archival material, I locate within Sebald's unpublished poetry and prose several thematic, linguistic and semantic characteristics which are typical of Herbeck's poetry.  I then expand this analysis to incorporate how these features also reappear in Sebald's published novels.  In uncovering underexplored intertextuality, this study sheds new light both on Sebald's novels, since recognizing Herbeck's voice within his prose calls for a reevaluation of what is ""Sebaldian"", as well as on the broader, yet underexplored cultural movement from which I focus on Herbeck as the apex.  It also locates Sebald's key political ideas and his values concerning poetics and morality as derivatives of a particular historical and psychological discourse.",ucb,,https://escholarship.org/uc/item/1979c8pp,,,eng,REGULAR,0,0
348,1784,Learning Dependency-Based Compositional Semantics,"Liang, Percy","Klein, Dan;Jordan, Michael I;",2011,"Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts.  The core part of such a system is the semantic parser that maps questions to logical forms.  Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.Our goal is to learn a semantic parser from question-answer pairs instead, where the logical form is modeled as a latent variable.  Motivated by this challenging learning problem, we develop a new semantic formalism, dependency-based compositional semantics (DCS), which has favorable linguistic, statistical, and computational properties.  We define a log-linear distribution over DCS logical forms and estimate the parameters using a simple procedure that alternates between beam search and numerical optimization.  On two standard semantic parsing benchmarks, our system outperforms all existing state-of-the-art systems, despite using no annotated logical forms.",ucb,,https://escholarship.org/uc/item/1b1189cm,,,eng,REGULAR,0,0
349,1785,Experimental investigations of photochemically-generated organic aerosols and applications to early Earth and Mars,"Chu, Emily Faye","Boering, Kristie A.;",2013,"Aerosols in planetary atmospheres play a critical role in radiative transfer and thus also in determining the penetration depth of UV radiation and atmospheric temperatures. For early Mars and an anoxic early Earth, aerosols may have significantly influenced the stability of liquid water at the surface, as well as climate and habitability, yet the study of aerosol formation remains poorly constrained by models and experiments, making conclusions difficult. Significant progress can be made in reducing these uncertainties by providing additional laboratory constraints and tests for the photochemical and microphysical models used to generate the greatly varying predictions about aerosol formation in terrestrial-like atmospheres. Photochemistry experiments measuring gas- and condensed-phase species were conducted to determine (1) the extent to which a photochemical haze could have formed in Mars' and Earth's early atmospheres and (2) whether such aerosols might be depleted in carbon-13, providing a potentially false biosignature in organic matter in the Martian and terrestrial rock and meteorite records.To gain a greater understanding of the fundamentals of aerosol formation in the CO2-rich atmospheres of early Earth and Mars, methane (CH4) and mixtures of CH4 and carbon dioxide (CO2) were irradiated with UV light in a static stainless steel reaction chamber, leading to the formation of aerosols. HeNe laser scattering was used to detect in situ the presence and relative amounts of aerosol produced, as well as the induction time required to form aerosols from the irradiation of the gases. Online mass spectrometry measurements were used to monitor the time evolution of gas phase species and, by difference, the relative amounts of carbon in the aerosol formed during an experiment. For comparison, particle samples were also collected during the irradiation experiments for ""offline"" analysis by elemental analyzer-isotope ratio mass spectrometry (EA-IRMS); the IRMS peak areas were used to quantify the relative amounts of carbon in the aerosols. From these three types of measurements, particle production was observed to be higher as the pressure of initial reactant CH4 was increased from 5 to 200 Torr, but little sensitivity to the CH4/CO2 ratio between 0.34 and 5 for a given CH4 pressure was observed. This behavior is in contrast to what has been predicted by photochemical models, which suggest that aerosol production rates decrease as CO2 levels increase. Furthermore, aerosol formation was detectable at a CH4/CO2 ratio down to 0.25 by in situ HeNe light scattering and down to 0.34 by offline EA-IRMS analysis of collected particles. These experimental ratios are significantly lower than the threshold ratios of 0.6 to 1 predicted by photochemical models for particle production. This suggests that aerosol production in CO2-rich atmospheres with feasible sources of CH4 from mantle outgassing or methanogenic bacteria was likely to have been much more favorable than predicted by models. Further, more sophisticated photochemical models of early Earth's atmosphere will need to include reactions of organic species with oxygen in order to better predict the minimum CH4/CO2 threshold at which particles form as a function of CH4/CO2.In addition to affecting atmospheric radiative transfer, organic aerosols that formed photochemically in the atmosphere could have deposited on the surface, and would thus have been incorporated in the rock record. If such aerosols were significantly lighter in carbon-13 than the CH4 from which they formed, such a light isotopic signature might potentially be mistaken as a biomarker. To test this hypothesis, aerosols from the UV irradiation of CH4 and mixtures of CH4 and CO2 were collected and analyzed for the carbon-13 isotopic composition by EA-IRMS. Compared to the isotopic composition of the reactant CH4, the particle samples were depleted in 13C by 6Â±3 â€°. While this is likely too small to account for the large isotopic depletions in 13C observed between 2.5 to 3.0 billion years ago (which have been interpreted as the rise of methanotrophic bacteria), experimentation at lower temperatures is recommended to address whether the overall kinetic isotope effects in photochemical aerosol production might be larger at the temperatures in the upper atmosphere where CH4 photolysis would have occurred.",ucb,,https://escholarship.org/uc/item/1b76b8dp,,,eng,REGULAR,0,0
350,1786,Mirar (lo) violento: rebeliÃ³n y exorcismo en la obra de Evelio Rosero,"Martinez, Maria Juliana","Brizuela, Natalia;",2012,"This dissertation explores the work of Colombian writer Evelio Rosero (1958), whose work-like many of his nation's generation, but with a radically new aesthetic and ethic proposal-- focuses on violence and on the disappearance of people in the context of the armed conflict that has ravaged Colombia for the last thirty years.Despite having a long and consistent literary career that started in the early eighties and having received prestigious awards, Rosero continues to be almost unknown both nationally and internationally. My dissertation contends that such lack of recognition is serious and that current conversations about Colombian literature and the representation of violence more broadly cannot be done without taking into account his disruptive work. Through a careful analysis of Rosero's most representative novels -SeÃ±or que no conoce la luna, En el lejero and Los EjÃ©rcitos-- I examine the literary techniques the author uses to produce a space -both literary and political--that neither justifies nor exacerbates violence. Based primarily on the concept of the spectral put forth by Jacques Derrida in Specters of Marx, on Mieke Bal's position on political art and on Jean-Luc Nancy's construction of rebellion in Noli me tangere, I demonstrate how Rosero's novels highlight the discourses and mechanisms that put into place and even sanction the violence they supposedly lament.The dissertation is divided in three chapters. Chronologically organized, each one examines one of Rosero's most representative novels. In the introduction I contextualize Rosero's literary work within the larger efforts to represent Colombia's violent situation. I argue that by focusing on disappearance, ambiguity and spectrality Rosero avoids the most common and problematic pitfalls of such texts. I take the position that by doing so Rosero gives visibility to the many ways in which a state of violence is (re)produced and represented -both aesthetically and politically--signalling a complicity (not necessarily deliberate) between the two.The first chapter analyzes SeÃ±or que no conoce la luna. I argue that by focusing in the way los vestidos enslave and torture los desnudos due to their dual genitalia, Rosero shows the artificiality and arbitrariness of our social constructions and highlights how they are used to infringe extreme violence to a particular group of people. I contend that in the unregulated circulation of erotic desire Rosero finds a way out of this structure of abjection. The second chapter deals with the radical ""spectralization"" that takes place in En el lejero. I take the position that Rosero's emphasis on the difficulty of identifying people and spaces, and his refusal to stabilize meaning are effective tools in dismantling a system of oppression and violence while opening a space for agency and solidarity.The third and last chapter studies Rosero's most famous novel, Los EjÃ©rcitos. I read the novel's contrast between moments of intense visibility and instances of extreme obscurity and confusion as a way to underscore the violent nature of certain ways of looking at things and people. Rosero's insistence in our bonds with, and responsibility towards, what can no longer, not yet, be seen or heard is key to create a space for the political that is not based on violence and exclusion.To conclude, I argue that through Jacques Derrida's ""impure impure history of ghosts"" Rosero develops an aesthetically astonishing and politically crucial way of re-counting and accounting for the violence that a prolonged state of warfare continues to (re)produce in Latin America.",ucb,,https://escholarship.org/uc/item/1bd7w8q3,,,eng,REGULAR,0,0
351,1787,Baroclinic Critical Layers and Zombie Vortex Instability in Stratified Rotational Shear Flow,"Wang, Meng","Marcus, Philip Steven;",2016,"Without instabilities, the gas in the protoplanetary disk around a forming protostar remains in orbit rather than falling onto the protostar and completing its formation into a star. Moreover without instabilities in the fluid flow of the gas, the dust grains within the disk's gas cannot accumulate, agglomerate, and form planets. Keplerian disks are linearly stable by Rayleighâ€™s theorem because the angular momentum of the disk increases with increasing radius. This has led to the belief that there exists a large region in protoplanetary disks, known as the dead zone, which is stable to pure hydrodynamic disturbances. The dead zone is also believed to be stable against magneto-rotational instability (MRI) because the disks' cool temperatures inhibit ionization and therefore prevent the MRI.A recent study shows the existence of a new hydrodynamic instability called the Zombie Vortex Instability (ZVI), where successive generations of self replicating vortices  (\textit{zombie vortices}) may fill the disk with turbulence and destabilize it. The instability is triggered by finite amplitude perturbations, including weak Kolmogorov noise, in stratified (with Brunt-Vasala frequency N) flows, rotating with angular velocity  and shear.So far there are no observational evidences of the Zombie Vortex Instability and there are very few laboratory experiments of stratified plane Couette flow with background rotation in the literature.We perform systematic simulations exploring existence of Zombie Vortex Instability in terms of control parameters (Reynolds number, shear, rotation and stratification). We present a parameter map showing two regimes where ZVI occurs, and interpret the physics that determines the boundaries of the two regimes. We also discuss the effects of viscosity and the existence of a threshold for $Re$. Our study on viscous effect, parameter map and its underlying physics provide guidance for designing practical laboratory experiments in which ZVI could be observed.",ucb,,https://escholarship.org/uc/item/1c38b2gs,,,eng,REGULAR,0,0
352,1788,Realization of Integrated Coherent LiDAR,"Kim, Taehwan","Stojanovic, Vladimir;",2019,"LiDAR (Light Detection and Ranging) captures high-definition real-time 3D images of the surrounding environment through active sensing with infrared lasers. It has unique advantages that can compensate the fundamental limitations in camera-based 3D imaging via vision algorithms or RADARs, which makes it an important sensing modality to guarantee robust autonomy in self-driving cars. However, high price tag of existing commercial LiDAR modules based on mechanical beam scanners and intensity-based detection scheme makes them unusable in the context of mass produced consumer products.The focus of thesis is on the integrated coherent LiDAR with optical phased array-based solid-state beam steering, which has great potential to dramatically bring down the cost of a LiDAR module. It begins with an overview of LiDAR implementation options and system requirements in the context of autonomous vehicles, which leads us to conclude that beam-steering coherent FMCW LiDAR in optical C-band is indeed the best implementation strategy to realize low-cost automotive LiDARs. Motivated by this observation, a quantitative framework for evaluating FMCW LiDAR performance is also introduced to predict the design that satisfies car-grade performance requirements. Then the thesis presents the silicon implementation results from our single-chip optical phased array and integrated coherent LiDAR prototype. Our implementations leverage the 3D heterogeneous integration platform, where custom silicon photonics and nanoscale CMOS fabricated at a 300 mm wafer facility are combined at the wafer-scale to minimize the unit cost without I/O density issues. After discussing remaining challenges and possible ways to enhance the operating range and system reliability, this thesis finally addresses the problem of fundamental trade-off between phase noise and wavelength tuning in FMCW laser source, and present circuit- and algorithm-level techniques to enable FMCW measurements beyond inherent laser coherence range limit.",ucb,,https://escholarship.org/uc/item/1d67v62p,,,eng,REGULAR,0,0
353,1789,The Acquisition of Narrative Discourse: A Study in Japanese,"Clancy, Patricia",,1980,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/1dv9q9j9,,,eng,REGULAR,0,0
354,1790,"Watering the Desert: Environment, Irrigation, and Society in the Premodern Fayyum, Egypt","Haug, Brendan James","Hickey, Todd M.;",2012,"Through a study of its natural environment and irrigation system, this dissertation investigates the evolution of the landscape of Egypt's Fayyum depression across sixteen centuries, from the third century BCE to the thirteenth century CE.  From the evidence of Greek papyri, Arabic fiscal documentation, early modern travel literature, archaeology, and contemporary scientific work, I chart the changes in human relationships with earth and water over time, changes which constantly altered the inhabited and cultivated regions of the Fayyum.  My main argument throughout is that it was local agency and not state governments that continuously remade the landscape.The history of the Fayyum after the fourth century CE has long been viewed by ancient historians as one of decline from its ancient heights due to the failure of the late Roman and Muslim successor states to properly manage its irrigation system.  I locate the genesis of this narrative within nineteenth century perceptions of the docility of nature and the belief that ancient governments had achieved centralized control over the Nile and the Egyptian environment.  This anachronistic retrojection of the characteristics of the modern irrigation system has had a considerable afterlife in historical scholarship on Egyptian irrigation.	Eschewing a narrow focus on the state, this dissertation argues that that nature is a potent agent in its own right.   Ancient farmers could not control nature so they adapted to it, creating  four distinct irrigated sub-regions in the Graeco-Roman Fayyum, each tailored to the particulars of the local environment.  Our papyri stem from only one of these sub-regions, the water-scarce margins, which lay at the tail end of the irrigation system.  Here, inadequate irrigation and fertilization progressively led to soil salinization and degradation, which helped to spur the eventual abandonment of these areas.  By the medieval period, only the central floodplain remained inhabited.  Only here was sustainable agriculture under the regime of premodern technology possible.	Although the Roman state coordinated local labor on the canals, nothing could bind Fayyum villagers to the degrading margins in perpetuity.  Fourth century papyri hint that some cultivators had moved to other nomes and were prospering.  Still later documents of the sixth to eighth centuries CE reveal greatly increased settlement density in the central Fayyum.  Thus, it was local cultivators who made and remade the landscape of the Fayyum over the centuries according to their own needs.  Government could both guide and benefit from this local labor but it could never fully control it.",ucb,,https://escholarship.org/uc/item/1f48984v,,,eng,REGULAR,0,0
355,1791,In danger of undoing: The Literary Imagination of Apprentices in Early Modern London,"Drosdick, Alan","Altman, Joel B.;",2010,"With the life of the apprentice ever in mind, my work analyzes the underlying social realities of plays such as Dekker's The Shoemaker's Holiday, Beaumont's The Knight of the Burning Pestle, Jonson, Chapman, and Marston's Eastward Ho!, and Shakespeare's Henriad.  By means of this analysis, I reopen for critical investigation a conventional assumption about the mutually disruptive relationship between apprentices and the theater that originated during the sixteenth century and has become a clichÃ© of modern theater history at least since Alfred Harbage's landmark Shakespeare's Audience (1941).As a group, apprentices had two faces in the public imagination of renaissance London.  The two models square off in Eastward Ho!, where the dutiful Golding follows his master's orders and becomes an alderman, while the profligate Quicksilver dallies at theaters and ends up in prison.  This bifurcated image of apprentices arises, I argue, from the national implementation of apprenticeship as a means of social control intended to create a supervisory system over masterless men in response to a century-long expansion of vagrancy.  To reinforce this system, there arose a literature of apprenticeship, which included conduct manuals, popular ballads, prose works such as those of Thomas Deloney, and plays like those mentioned above.  However, I have found that the crown's encouragement of apprenticeship was not without its perils.  If the government wanted young men without better prospects to sign away their freedom for seven years or more, those men needed truly to believe that serving an indenture would elevate their socioeconomic station.  If that promise proved false, apprentices would abandon their posts and revert to vagrancy.  If it proved true, the widespread upward mobility enabled by apprenticeship would contribute to the formation of a proto-middle class that could exert pressure on the gentry.  Thus, apprentices posed a double threat to the traditional societal hierarchy: they were in danger of undoing old lines that divided rulers from the ruled, either by undermining normative power structures through vagrancy or by ascending the social ladder, a prospect offered to induce them not to be vagrants.Critics have long understood apprentice literature as instructive paeans to the redemptive power of honest work and the presumptive realization of individual potential.  Characters such as Golding, Deloney's Jack of Newbury, and Dekker's Simon Eyre are extolled as exemplary figures meant to be imitated by audiences and readers, their meteoric success the carrot to entice young men patiently to endure the often grueling labor of apprenticeship.  However, that carrot seemed increasingly to recede, as over half of the many thousands of apprentices who began indentures failed to complete them, producing large scale systemic frustration.  To stem the tide, a number of conduct books appeared which instructed young men merely to act like good apprentices, with an eye toward future independence and prosperity.  This performative aspect might facilitate the rise from apprentice to master, but it also helped masters keep their apprentices content.  By falsely acting in a fashion that reinforced a causal link between compliant service and inevitable success, masters could safely enjoy the cheap labor of apprentices without threat of revolt.  My work explores how suspicion of the perfidy of apprenticeship and the performance of artisanal identity quietly inheres in the very literature thought to celebrate apprenticeship by providing examples of honest labor and fit reward.",ucb,,https://escholarship.org/uc/item/1fx380bc,,,eng,REGULAR,0,0
356,1792,"The 'Cat's Paw of Dictatorship': State Security and Self-Rule in the Gold Coast, 1948 to 1957","Arnold, Chase","Kanogo, Tabitha;",2019,"On February 28th, 1948, a deadly police shooting at a veteranâ€™s demonstration in the Gold Coast sparked three days of rioting in the capital city of Accra and surrounding communities. It was the first crisis of its kind for the British colony and a clear indication of the shifting political realities of the post-war era. Though colonial rule had been in place for several generations, the people of the Gold Coast would increasingly balk at an imperial system that denied them a voice in their own government. The following nine years would witness the Gold Coastâ€™s extraordinary transition from British colony, to self-ruled territory, and eventually an independent state that renamed itself the Republic of Ghana.In the more than sixty years since Ghanaâ€™s independence in 1957, scholars and commentators alike have recognized the February riots as a turning point in Ghanaian and imperial history, signaling the new wave of decolonization that would sweep across sub-Saharan Africa in the years to follow. What has remained unknown and relatively unstudied is the fact that the riots also compelled the development of a government intelligence network in the Gold Coast. Before British officials accepted that colonial rule was as its end in West Africa, they sought to safeguard the state by providing it a domestic intelligence organization. This organization operated throughout the terminal years of British rule in the Gold Coast and succeeded in both altering the nature of the colonial state and the process of decolonization in unexpected ways.This dissertation interrogates the role of government intelligence in the Gold Coast between the years of 1948 and 1957. By examining police superintendents, Security Service officers, and colonial administrators, it reconstructs the establishment and application of intelligence resources to better understand the process and politics of decolonization in the Britainâ€™s West African empire.",ucb,,https://escholarship.org/uc/item/1h26z17f,,,eng,REGULAR,0,0
357,1793,Making Place and Nation: Geographic Meaning and the Americanization of Oregon: 1834-1859,"Moore, MacKenzie Katherine Lee","Henkin, David M;",2012,"AbstractMaking Place and Nation: Geographic Meaning and the Americanization of Oregon: 1834-1859by MacKenzie Katherine Lee MooreDoctor of Philosophy in HistoryUniversity of California, BerkeleyProfessor David Henkin, Chair""Making Place and Nation: Geographic Meaning and the Americanization of Oregon: 1834-1859"" examines the ways colonists worked to counteract the problem of Oregon's complicated geographic situation in order to naturalize the territory's membership in the nation between the first American missionary settlement and admission as the union's thirty-third state. Driven by their desire for national inclusion, colonists ascribed national meanings to local actions while also adapting national narratives to fit their immediate experience. This study uses two key concepts to spatially reconstruct the colonial experience: mental maps and vernacular geography. Colonists relied on mental mapping to navigate and to organize geographic knowledge, and this cognitive practice became part of the spatially focused community discourse dubbed vernacular geography. Locally produced geographic discourse united under one rubric the twin objectives that defined the conquest of Oregon: to civilize the landscape and eliminate the Indian presence therein, and to concurrently rewrite the map of the United States with Oregon squarely within its borders. Thus, ""Making Place and Nation"" asserts that the conquest of Oregon is best understood as a recursive process of making both place and nation.The dissertation relies on a wide variety of documentary materials, including personal and official letter correspondence, diaries and travelogues, pioneer reminiscences, petitions to local and federal government entities, newspapers, and official reports to agents of the central government. These sources reveal that colonists read both the physical transformation of Oregon's environment and the amount of independence enjoyed by native groups as indicators of the territory's potential for national incorporation The following chapters reinterpret a series of key events in the history of Oregon's colonization. Chapter One traces the role of mental mapping in establishing an American colony. Chapter Two explores colonists' conceptual tools to manage anxiety about sharing territory with independent Indian groups and their destruction in the 1847 Whitman Massacre. Chapter Three scrutinizes the production of local geographic knowledge as a method of wresting spatial control from Indians during the California and Southwest Oregon Gold Rush (1848-1853). Chapter Four analyzes the territorial dimensions of a colonial program of ethnic cleansing toward Indians in the Rogue River War (1855-56). Chapter Five investigates the extinguishment of Indian title and its relationship to the uneven implementation of the Oregon Donation Land Act during the era of removal (1856-1859).",ucb,,https://escholarship.org/uc/item/0jv7z9zn,,,eng,REGULAR,0,0
358,1794,Resource and Remaking: Organizational Mediation of Parent-School Relationships,"Chong, Seenae","Mintrop, Rick;",2018,"Engaging low-income parents of color in schools in ways that empower and respect rather than further marginalize remains a persistent challenge. Current studies on organizations which work with low-income parents of color in schools, which I term â€œparent engagement organizationsâ€ or PEOs, highlight how PEOs lend critical infrastructure and resources to activities such as disseminating information about educational practices or organizing parents for school reform. Less understood is how organizational survival, a condition specific and unique to the organization as a social actor, may influence parent engagement practices and, subsequently, how low-income parents of color are empowered to engage in schools. In this study, I move beyond technical descriptions of organizational resource brokerage in parent-school relationships to uncover how organizations, as distinct social formations, mediate parent agency in schools. Bringing insights from the sociology of organizations to bear on research that examines the possibilities of empowering low-income parents of color in schools, I used a multi-case study design to explore how the conditions of survival and sustainability shaped the ways two PEOsâ€”one educative, the other politicalâ€”fostered parent agency and positioned parents in the micro-politics of schooling.The findings suggest that organizational mediation of parent-school relationships consists of far more activities than resource brokerage. The two focal organizations dedicated resources and care to creating spaces on school sites which were welcoming and responsive to parents. However, the types of parent agency advanced by the organizations were limited to those which could also accommodate organizational needs for survival and sustainability. In the educative PEO, the parent engagement program was funded through a sub-contract with the school district. While parent engagement staff offered information and activities which addressed parentsâ€™ desire to be useful and helpful in school life, organizational identity, roles and routines, and context construed parent engagement as a compliance task that needed to be fulfilled to maintain its sub-contractor status with the district. Prioritizing good school relations was evident in the ways parent engagement staff suppressed parentsâ€™ critiques and concerns and redirected parents toward volunteer activities in peripheral school operations. In the political PEO, parents gained the skills and knowledge to advocate for and advance their interests in a variety of arenas. However, organizing was limited to district-sanctioned spaces of engagement, as pressures from collaborations with other organizations as well as district politics challenged staffâ€™s ability to stay focused on parentsâ€™ specific, expressed interests. Consequently, the power relations which upheld policy-making authority in the hands of district elites remained unchallenged. This study demonstrates the ways parent engagement organizations are socially productive actors which shape the everyday interactions among parents, teachers, and school and district leaders, thereby shaping the social life of schooling.",ucb,,https://escholarship.org/uc/item/0m45z3zx,,,eng,REGULAR,0,0
359,1795,Beyond bounded rationality: Reverse-engineering and enhancing human intelligence,"Lieder, Falk","Griffiths, Thomas L;",2018,"Bad decisions can have devastating consequences, and there is a vast body of literature suggesting that human judgment and decision-making are riddled with numerous systematic violations of the rules of logic, probability theory, and expected utility theory. The discovery of these cognitive biases in the 1970s challenged the concept of Homo sapiens as the rational animal and has profoundly shaken the foundations of economics and rational models in the cognitive, neural, and social sciences. Four decades later, these disciplines still lack a rigorous theoretical foundation that can account for peopleâ€™s cognitive biases. Furthermore, designing effective interventions to remedy cognitive biases and improve human judgment and decision-making is still an art rather than a science. I address these two fundamental problems in the first and the second part of my thesis respectively.To develop a theoretical framework that can account for cognitive biases, I start from the assumption that human cognition is fundamentally constrained by limited time and the human brainâ€™s finite computational resources. Based on this assumption, I redefine human rationality as reasoning and deciding according to cognitive strategies that make the best possible use of the mindâ€™s limited resources. I draw on the bounded optimality framework developed in the artificial intelligence literature to translate this definition into a mathematically precise theory of bounded rationality called resource-rationality and a new paradigm for cognitive modeling called resource-rational analysis. Applying this methodology allowed me to derive resource-rational models of judgment and decisionmaking that accurately capture a wide range of cognitive biases, including the anchoring bias and the numerous availability biases in memory recall, judgment, and decision-making. By showing that these phenomena and the heuristics that generate them are consistent with the rational use of limited resources, my analysis provides a rational reinterpretation of cognitive biases that were once interpreted as hallmarks of human irrationality. This suggests that it is time to revisit the debate about human rationality with the more realistic normative standard of resource-rationality. To enable a systematic assessment of the extent to which human cognition is resource- rational, I present an automatic method for deriving resource-rational heuristics from a mathematical specification of their function and the mindâ€™s computational constraints. Applying this method to multi-alternative risky-choice led to the discovery of a previously unknown heuristic that people appear to use very frequently. Evaluating human decision-making against resource-rational heuristics suggested that, on average, human decision-making is at most 88% as resource-rational as it could be.Since people are equipped with multiple heuristics, a complete normative theory of bounded rationality also has to answer the question of when each of these heuristics should be used. I address this question with a rational theory of strategy selection. According to this theory, people gradually learn to select the heuristic with the best possible speed-accuracy trade-off by building a predictive model of its performance. Experiments testing this model confirmed that people gradually learn to make increasingly more rational use of their finite time and bounded cognitive resources through a metacognitive reinforcement learning mechanism.Overall, these findings suggest thatâ€“contrary to the bleak picture painted by previous research on heuristics and biasesâ€“human cognition is not fundamentally irrational, and can be understood as making rational use of bounded cognitive resources. By reconciling rationality with cognitive biases and bounded resources, this line of research addresses fundamental problems of previous rational modeling frameworks, such as expected utility theory, logic, and probability theory. Resource-rationality might thus come to replace classical notions of rationality as a theoretical foundation for modeling human judgment and decision-making in economics, psychology, neuroscience, and other cognitive and social sciences.In the second part of my dissertation, I apply the principle of resource-rationality to develop tools and interventions for improving the human mind. Early interventions educated people about cognitive biases and taught them the normative principles of logic, probability theory, and expected utility theory. The practical benefits of such interventions are limited because the computational demands of applying them to the complex problems people face in everyday life far exceed individualsâ€™ cognitive capacities. Instead, the principle of resource-rationality suggests that people should rely on simple, computationally efficient heuristics that are well adapted to the structure of their environments. Building on this idea, I leverage the automatic strategy discovery method and insights into metacognitive learning from the first part of my dissertation to develop intelligent systems that teach people resource-rational cognitive strategies. I illustrate this approach by developing and evaluating a cognitive tutor that trains people to plan resource-rationally. My results show that practicingwith the cognitive tutor improves peopleâ€™s planning strategies significantly more than does practicing without feedback. Follow-up experiments demonstrate that this training effect transfers to more difficult planning problems in novel and more complex environments, and that this transfer effect is retained over time. This indicates that discovering and teaching resource-rational heuristics maybe a promising approach to improving human judgment and decision-making. While this approach adapts peopleâ€™s heuristics to the structure of their environment, the theory of resource-rationality suggests that human decision-making can also be improved by adapting the structure of the environment to the heuristics people already use. I illustrate this approach by developing a cognitive prosthesis for goal achievement that helps people overcome procrastination, spring into action, and achieve their goals on time.By virtue of integrating rational principles with cognitive constraints, resource-rationality provides a realistic normative standard for human reasoning and decision-making. My findings about human rationality and metacognitive learning are consistent with the view that evolution and learning adapt the mind to the structure of its environment and the constraints imposed by its limited resources. These adaptive mechanisms appear to optimize for resource-rationality, and the benefits of training with the cognitive tutor demonstrate that this adaptation can be accelerated with the help of artificial intelligence. This makes resource-rationality a promising theoretical framework for modeling and improving human cognition.",ucb,,https://escholarship.org/uc/item/0mh5z130,,,eng,REGULAR,0,0
360,1796,Essays in Asset Pricing,"Hang, Luc Kien","GÃ¢rleanu, Nicolae;",2020,"What explains the cross-sectional variation of expected returns? This dissertation contains two essays that study this question both theoretically and empirically for two popular puzzles in the asset pricing literature. In the first chapter of the dissertation, I study the asset pricing implications of learning-by-doing in a partial equilibrium model where firms optimally choose to adopt the newest technology. Firms are heterogeneous in the sense that they have different learning curves. Adopting the newest technology is costly, however, because the firms forgo the experience they accumulated in the past. The model implies that firms with (1) obsolete technology, (2) little accumulated experience, (3) low forgetting rate, or (4) high learning rate are more likely to adopt earlier, which I label as 'early innovators'. I show that these firms load more on growth options as opposed to assets-in-place, are more exposed to technological shocks, and earn a lower risk premium. The model can match the magnitude of the value premium as well as the size premium.In the second chapter of the dissertation, I document that idiosyncratic volatility and future returns are not simply negatively related. Past performance of the market predicts whether high or low idiosyncratic volatility stocks generate positive returns. A signed idiosyncratic volatility (SIV) factor, which is long high idiosyncratic volatility and short low idiosyncratic volatility following bull markets and vice versa following bear markets, produces significant positive risk-adjusted returns. A model with extrapolative agents and market segmentation can capture these facts.",ucb,,https://escholarship.org/uc/item/0mx53117,,,eng,REGULAR,0,0
361,1797,Determinants of Cord Blood DNA Methylation Variability in a Mexican-American Birth Cohort,"Yousefi, Paul Darius","Holland, Nina;",2016,"Epigenetic mechanisms, particularly DNA methylation, are a possible link between environmental and biological determinants of health.  As the DNA methylome undergoes rearrangement in utero and is susceptible to environmental insults, it may be a mechanism explaining the developmental origins of human disease with public health importance. However, the epidemiologic studies needed to identify the role DNA methylation plays mediating environmental exposure disease risk still face several obstacles. This dissertation addresses knowledge gaps impeding the rigorous adoption of genome-scale measures of site-specific DNA methylation, like the Illumina Infinium HumanMethylation450 (450K) BeadChipÂ®, into epidemiologic study designs. We then investigate the impact of prenatal exposure to polybrominated diphenyl ethers (PBDEs) on DNA methylation of children at birth. PBDEs are a class of flame retardant chemicals widely used in U.S. consumer products over the last 40 years that have previously been associated with adverse neurobehavioral outcomes, obesity, and other effects.Specifically, we aimed to:1)	Identify and minimize sources of technical variation for site-specific DNA methylation measured by 450K BeadChip assay in epidemiologic studies2)	Characterize sources of biological variation due to host factors (e.g. blood cell composition and sex) in measures of whole blood DNA methylation at birth3)	Determine whether prenatal exposure to PBDEs is associated with differential methylation patterns of CpG sites in umbilical cord bloodOur results identified that the newly proposed All Sample Mean Normalization (ASMN) procedure performed consistently well, both at reducing batch effects and improving replicate comparability compared to several other leading normalization methods. It can be successfully implemented in epidemiologic studies to enhance 450K DNA methylation data preprocessing.In our examination biological variation, we found that a standard approach in epigenome-wide analysis â€“ minfi white blood cell composition estimation â€“ did not correlate well with white cell counts from newborns (Ï = -0.05 for granulocytes; Ï = -0.03 for lymphocytes), but improved substantially (Ï = 0.77 for granulocytes; Ï = 0.75 for lymphocytes) in older children likely due to increasing similarity with minfiâ€™s adult reference data as children aged. This suggests that minfi may not currently be appropriate for analysis involving newborns or young children. Additionally, results on DNA methylation differences by sex identified 3,031 differentially methylated positions (DMPs) and 3,604 sex-associated differentially methylated regions (DMRs) on autosomes that were mostly hypermethylated in girls compared to boys.  Our hits were significantly enriched for gene ontology terms related to nervous system development and behavior.Finally, we investigated the impact of exposure to PBDEs during the highly susceptible prenatal period on DNA methylation of Mexican-American children enrolled in the Center for Health Assessment of Mothers and Children of Salinas (CHAMACOS) at birth. We identified between 6 and 48 DMRs in umbilical cord blood associated with different measures of prenatal PBDE exposure. BDEâ€™s-47, -99 and Î£4BDE had fewer (from 6 to 9), mostly hypomethylated DMRs.  Prenatal BDE-100 and -153 levels were associated with more DMRs (11 and 48 respectively) and the majority hypermethylated.  The PBDE-DMRs we found were located in genes (e.g. NRBP1, CDH9, NTN1, S100A13) involved in biologically relevant functions (including axon guidance and tumor suppression) given the health effects observed in association with BDE exposure to date.",ucb,,https://escholarship.org/uc/item/0p1563vg,,,eng,REGULAR,0,0
362,1798,Essays on Product Strategies through Consideration of Individual Distributions,"Perry, Hagit","Villas-Boas, Miguel;",2016,"Marketing literature and practitioners are in agreement that it is essential for brands in competitive markets to identify segments that should be targeted and to build rational product strategies that target these segments. It is essential because most markets include consumers with heterogeneous preferences and precise segmentation and targeting creates product differentiation, which prevents direct competition and allow the market to reach an optimal profit optimization equilibrium. In this consumer markets' era, defined by practitioners as the big data era, consumers' individual transactions and actions, which reveal their preferences, became highly available to marketers. This allows marketers to greatly improve their targeting and to optimize their profits through that. This dissertation contains three essays that examine optimal products strategies with consideration of individual distributions. Through the models that are built and estimated, individual preferences are identified. Following that, individuals are aggregated into clustered segments, and clear optimization strategy is designed. All the essays build and discuss structural models and estimation strategies. Each estimation uses unique datasets that were selected and organized carefully for the purpose of robust identification of the varied effects that are examined and analyzed. Each essay identifies and considers the individual distributions in the analyses. Altogether, the essays provide a deeper understanding of how to consider individual distributions in varied settings and marketing needs that marketers face frequently. Chapter 1 examines the theory of trying, forgetting, and sales in empirical settings. This is an important model as there are many markets where consumers need to try products for realizing their fit, however after trying, some consumers may forget the fit over time through learning processes of competitive information and other processes. The theory shows that the trying and forgetting model predicts that sales will occur periodically according to the magnitudes of the effects as the sales are used by the brands as product-fit reminders to the targeted consumers. For the empirical examination of this theory, a model that includes trying and forgetting effects within the standard demand side model is built and estimated. The model allows consumers to have heterogeneous tastes and includes treatment for possible endogeneity. Using the demand estimation and including an individual level distribution estimation, the population is divided into segments. Consumers are divided by their utilities for products as it is optimal for firms to target with the regular price the segments that favor the and when they launch sales, they may target more segments as the trying experience may affect their utility and make them be included in the main segment that this firm targets. This segmentation of the data makes it possible to find the equilibrium in which each firm optimizes profits and the market does not enter a situation of direct competition and a Bertrand game as the firms focus on the segments that favor them and launch temporal sales to introduce or remind consumers of the products fit. This allows the identification prices strategies that optimize profits. Chapter 1 also builds a novel dynamic game supply side model together with simulation strategy and technique for that. This is a major contribution as it finds the equilibrium of a multi agent, segments, states, and periods dynamic game for these common settings where firms need to design a long-term, per period, pricing menu as they cannot change their product pricing often. The results of the estimation and simulation show that the trying and forgetting effects are highly significant on the demand side, but are not used well by some brands through their introduction period and afterward, which greatly and negatively influence their market share and long-term profits. Chapter 2 examines a method of finding individual level preference for attributes across products and the importance that it can have on policy makers, marketers, and consumers. It specifically discusses the case of reducing overweight in the population through finding the willingness to pay for the fat attribute of products among consumers that consistently buy fattier products at varied categories and introducing these consumers to products that are healthier for them through promotions on those products. This is an important question as overweight is was recognized as a global epidemic and thus researchers and policy makers are consistently looking for solutions with no consistent finding yet as neither macro taxes of attributes such as sugar or fat nor or macro subsidies of healthier products were feasible, effective, or efficient. It shows that the standard model does not allow targeted and effective promotions to these consumers as there is a gap in willingness to pay for fat through the population compared to the targeted group. However, using the estimation of the individual level distributions, this part shows that it is possible to convert this segment of consumers to choose healthier products through small magnitude promotional pricing. Chapter 3 examines a case that is opposite to the previous chapters. While in the previous chapters the segments were revealed through the estimation and individual distribution estimation methods. The data in this chapter saliently reveals that 20\% of consumers increased their per unit spend in a durable goods category at the first months of the US sub-prime recession of 2008. This hints that a large portion of the consumers became price loving at the beginning of one of the most difficult periods of the US economy.  This is clearly the opposite to the expectation, thus chapter examines the data carefully and suggest varied models. Finally, it shows that in this case, a well specified demand model can identify the reasons for the initial confusion coming from the data. Altogether, the essays examine frequent market settings that were not examined before and provide models together with estimation strategies and methods, which allow better optimization of product strategies through the consideration of individual level distributions and through segmenting the population accordingly.",ucb,,https://escholarship.org/uc/item/0p19j5rt,,,eng,REGULAR,0,0
363,1799,Towards Informed Exploration for Deep Reinforcement Learning,"Tang, Haoran","Sethian, James A;Abbeel, Pieter;",2019,"In this thesis, we discuss various techniques for improving exploration for deep reinforcement learning. We begin with a brief review of reinforcement learning (RL) and the fundamental  v.s. exploitation trade-off. Then we review how deep RL has improved upon classical and summarize six categories of the latest exploration methods for deep RL, in the order increasing usage of prior information. We then explore representative works in three categories discuss their strengths and weaknesses. The first category, represented by Soft Q-learning, uses regularization to encourage exploration. The second category, represented by count-based via hashing, maps states to hash codes for counting and assigns higher exploration to less-encountered states. The third category utilizes hierarchy and is represented by modular architecture for RL agents to play StarCraft II. Finally, we conclude that exploration by prior knowledge is a promising research direction and suggest topics of potentially impact.",ucb,,https://escholarship.org/uc/item/0pg0t84b,,,eng,REGULAR,0,0
364,1800,"Nesting the Neglected ""R"" A Design Study: Writing Instruction within a Prescriptive Literacy Program","Morizawa, Grace Hisaye","Mintrop, Heinrich;",2014,"Teaching writing has long been neglected as in schools. Findings from the National Assessment of Educational Progress (NAEP) indicated that most students have basic writing skills, but cannot write well enough to meet the needs of employers or for college. The writing programs in prescriptive literacy programs, which were adopted to ensure student achievement have not proven to be effective for developing proficient student writers. This design study is an attempt to provide teachers trained to teach in a prescriptive literacy program with the writing content and pedagogical knowledge necessary to engage elementary students in writing as a complex, intellectual activity so that they become proficient writers.From the literature on effective writing instruction and on teacher learning, I developed a theory of action to guide the design. A key feature of the design was to situate teacher learning in the context of a study group led by a facilitator with knowledge about writing instruction. The design emphasized teachers learning from writing themselves, reviewing student work, learning effective strategies and procedures of writing instruction, and developing knowledge through collaborative talk and reflections. Seven teachers, Grades 2 to 5, from a Title I urban school that required teachers follow the script of Open Court Reading (OCR) participated in the study. At the time of this study a window of opportunity had opened up to modify the OCR writing component.I framed teacher learning in two dimensions--Dimension 1: Instructional Strategies and Procedures, and Dimension 2: Writing as a Process. I investigated the impact of the design and the process of the design's development. Overall teachers' knowledge about writing content increased; their knowledge about writing pedagogy increased to a lesser degree; however their level of growth varied. Moreover growth in the elements of instructional strategies also varied. Growth ranged from 15% for teacher modeling writing and 109% for teacher referring to literature to teach writing strategies. Thus, I found the design basically sound but recommended modifications for future iterations.",ucb,,https://escholarship.org/uc/item/0pq0p9tb,,,eng,REGULAR,0,0
365,1801,Exploring Self-Assembly and Photomechanical Switching Properties of Molecules at Surfaces,"Cho, Jongweon","Crommie, Michael F;",2010,"The possible reduction of functional devices to molecular length scales provides many exciting possibilities for enhanced speed, device density, and new functionality. Optical actuation of nanomechanical systems through the conversion of light to mechanical motion is particularly desirable because it promises reversible, ultra-fast, remote operation. Past studies in this area have mainly focused on solution-based molecular machine ensembles, but surface-bound photomechanical molecules are expected to be important for future applications in molecular machines, molecular electronics, and functional surfaces. Cryogenic ultra-high-vacuum scanning tunneling microscopy has been employed to study the surface-based photomechanical switching properties of a promising class of photomechanical molecule called azobenzene.In the case of tetra-tert-butyl-functionalized azobenzene (TTB-AB) molecules adsorbed on Au(111) reversible switching by means of ultraviolet and visible excitation is experimentally observed at the single-molecule level. The presence of the metallic surface leads to a significant change of the molecular photoswitching properties: (i) photoswitching cross section is significantly reduced compared to the molecules in solution environment, (ii) photoswitching directionality is strongly affected. (iii) correlation between molecular ordering, electronic structure, and photoswitching capability is observed. (iv) new photoswitching dynamical mechanisms become operative.The results presented in this thesis provide insight into our understanding of photoswitching and adsorption properties of surface-bound molecules and elucidate the important role of molecule-surface interactions and molecule-molecule interactions.",ucb,,https://escholarship.org/uc/item/0q10c1m9,,,eng,REGULAR,0,0
366,1802,Studies on the Organometallic Reactivity of Gold(III),"Wolf, William James","Toste, F. Dean;",2016,"The work contained in thesis represents our efforts towards a fundamental understanding of gold in the context of redox cycling. The use of late transition metals in catalysis typically relies on two or more readily accessible oxidation states for bond breaking and bond forming events. However, the heaviest coinage metal does not enjoy such a wealth of reactivity and has been comparatively unexplored.Chapter 1 summarizes the research by our group and others in advancing the concept of redox active gold catalysis. Two strategies that differ on the use sacrificial oxidants to affect catalysis are described, as well as our efforts to improve on the performance of bimetallic gold catalysts through a ligand design approach. A brief survey of fundamental reactivity of Au(I) towards oxidative addition is also presented, to provide context for our studies as described in the subsequent chapters.Chapter 2 describes our studies on the C(sp2)â€“C(sp2) reductive elimination of biaryls from Au(III). Low temperature NMR studies revealed and characterized an unexpectedly rapid process that is mechanistically distinct from previous studies on C(sp3)â€“C(sp3) reductive elimination from Au(III). In addition, our preliminary efforts towards the development of a gold-catalyzed biaryl cross-coupling were informed by these studies on reductive elimination.Chapter 3 focuses on our investigations of a photochemical oxidative addition of CF3I to Au(I) complexes. This type of transformation was initially described for the oxidation of phosphine-supported Au(I) alkyl complexes; however the use of phosphine-supported Au(I) aryl complexes allows for the isolation and characterization of organometallic Au(III) complexes containing the â€“CF3 moiety. These complexes are also capable of C(sp2)â€“CF3 bond forming reductive elimination upon halide abstraction at low temperature and represent a unique class of compound in this regard.Chapter 4 builds on the results from chapter 3 and focuses on the effects of halide ligands on the C(sp2)â€“CF3 reductive elimination from Au(III) at elevated temperature. A series of Au(III) complexes containing each member of the halide family (I, Br, Cl, and F) were prepared and the effects of these ancillary ligands on the selectivity of C(sp2)â€“CF3 vs. C(sp2)â€“X (X = halide) elimination from Au(III) were characterized. The selectivity can be tuned by choice of halide ligand, and this phenomenon is attributed to increasing Auâ€“X bond strengths which correlate with an increase in selectivity for C(sp2)â€“CF3 elimination.",ucb,,https://escholarship.org/uc/item/0qj288fg,,,eng,REGULAR,0,0
367,1803,The role of RFRP-3 in chronic stress induced reproductive dysfunction and astrocyte communication,"Geraghty, Anna Christine","Kaufer, Daniela;",2015,"Though it is well established that chronic stress induces female reproductive dysfunction, whether stress negatively impacts fertility and fecundity when applied prior to mating and pregnancy has not been well explored.  My dissertation has investigated the mechanism behind stress-induced infertility in female rodents, as well as the long-term effects of chronic stress on reproductive success. Using naturally cycling female rats, Chapter 2 looks at changes in reproductive hormones and behavior after chronic immobilization stress. I also examined the long-term repercussions of chronic stress, allowing animals recovery from the stressors and looking at later mating and pregnancy success. My research has focused primarily on the role of a reproductive inhibitory hormone, RFRP-3, a hypothalamic peptide modulated by high stress. However, in Chapter 3, I investigate a new mechanism for RFRP-3 outside of that role, in hippocampal astrocytes. I see that RFRP-3 may also mediate an effect on how astrocytes connect and communicate with each other within the hippocampus. This opens a new intriguing line of research for novel roles of RFRP-3 outside of reproduction. These studies show that chronic stress has long-term effects on pregnancy success, even post-stressor, that are mediated by RFRP3, and point to RFRP3 as a potential clinically-relevant single target for stress-induced infertility.",ucb,,https://escholarship.org/uc/item/0r69d1cb,,,eng,REGULAR,0,0
368,1804,Design Considerations for CMOS-Integrated Hall-effect Magnetic Bead Detectors for Biosensor Applications,"Skucha, Karl","Boser, Bernhard E;",2012,"This dissertation presents a design methodology for on-chip magnetic bead label detectors based on Hall-effect sensors to be used for biosensor applications.  Signal errors caused by the label binding process and other factors that place constraints on the minimum detector area are quantified and adjusted to meet assay accuracy standards. The methodology is demonstrated by designing an 8,192 element Hall sensor array implemented in a commercial 0.18 um CMOS process with single mask post-processing. The array can quantify a one percent surface coverage of 2.8 um beads in thirty seconds with a coefficient of variation of 7.4%. This combination of accuracy and speed makes this technology a suitable detection platform for biological assays based on magnetic bead labels.",ucb,,https://escholarship.org/uc/item/0rk742f5,,,eng,REGULAR,0,0
369,1805,The Consequences of the Vietnam War on the Vietnamese Population,"Mizoguchi, Nobuko","Lee, Ronald D.;",2010,"The purpose of this dissertation is to examine the demographic and socioeconomic consequences of wars, using the case of the Vietnam War and its effects on the Vietnamese population. Using mainly the 1989 and 1999 census microdata, it focuses on the effects of the last ten years of the Vietnam War (or the ""American War"") from 1965 to 1975, characterized by the escalation of the war with a large presence of American troops in Vietnam and extensive aerial bombings by the United States.  The dissertation consists of two descriptive chapters and two analytical chapters. In the first descriptive chapter, I summarize existing estimates of mortality in Vietnam covering the period before, during, and after the war. I find evidence of increased mortality among young men during wartime, but raised mortality among children and the general population is not observed. Next, I examine whether the Vietnamese population age and sex structure show evidence of the war's imprints. Indeed, the 1989 and 1999 Vietnamese censuses reveal that the war left a mark on the cohorts that were in their 20s and 30s during 1965-1975, by reducing their numbers relative to their surrounding cohorts and by skewing the sex ratios. In the first analytical chapter, I examine marriage patterns in Vietnam between 1979 and 1999 using census data. Using a marriage squeeze index that applies the age-specific probability of first marriage estimated using the Coale-McNeil marriage model to the population, I show that Vietnam experienced a severe marriage squeeze in 1979 and 1989, but the squeeze had been alleviated by 1999. Furthermore, the dissertation investigates the relationship between the marriage squeeze and two war-related causes of the squeeze: excess male mortality and emigration. While the relationship between excess male mortality and the marriage squeeze was not observed, the results indicate that disproportionate male emigration is likely to be a major factor in bringing about the marriage squeeze. Lastly, the dissertation explores the long-term effects of early-life exposure to the war, examining educational attainment, literacy, marriage, and employment outcomes of those who experienced the war as infants and in utero, using the difference-in-differences technique. Separate analyses were conducted for North and South Vietnam. The results reveal adverse effects of early-life exposure to the war on marriage and employment in the north and on employment in the south. Mixed results are seen on literacy and educational outcomes.",ucb,,https://escholarship.org/uc/item/0sh7j7s9,,,eng,REGULAR,0,0
370,1806,Threshold-based resurfacing policies in pavement management to minimize costs and greenhouse gas emissions,"Ogwang, Allan","Madanat, Samer;Horvath, Arpad;",2016,"There is an increasing need for the reduction of greenhouse gas (GHG) emissions resulting from pavement maintenance activities, which account for millions of tons of GHG emissions annually. By optimizing pavement resurfacing activities, there is potential for reducing the carbon footprint associated with pavements and users of pavements. We propose a framework for estimating the relationship between GHG emissions from pavement resurfacing activities and pavement cracking-threshold policies. Cracking threshold is defined herein as the maximum percent cracking level a pavement is allowed to reach before an asphalt overlay is applied. In this framework, a probabilistic model capable of predicting both crack initiation and progression over time for individual pavement segments is formulated. The model is applied to a population of pavement segments and, given a cracking-threshold value, can predict the amount of GHG emissions and costs incurred due to resurfacing activities over a specified planning horizon. The model also predicts the corresponding user costs and emissions. In order to obtain the relationship between cracking threshold and GHG emissions, the cracking threshold is varied within a practical range of values. We obtain the corresponding resurfacing interval from which GHG emissions values are computed. The dataset used in the case study is obtained from the Washington State Department of Transportation (WSDOT) in the United States. The results show that the optimal cracking thresholds for minimizing costs and GHG emissions are close to each other, and are both higher than those used currently by WSDOT.",ucb,,https://escholarship.org/uc/item/0sq015x4,,,eng,REGULAR,0,0
371,1807,"Neutral and Cationic Vanadium Bisimido Complexes: Their Synthesis, Characterization, and Application in the Binding, Activation, and Catalytic Functionalization of Small Molecules","La Pierre, Henry Storms","Arnold, John;Toste, F. Dean;",2011,"Chapter 1: The syntheses of neutral (halide and aryl) and cationic vanadium bisimides are described. Characterization of the complexes by X-ray diffraction, 13C NMR, 51V NMR, and V L2,3-edge NEXFAS provide a description of the electronic structure in comparison to group 6 bisimides and the bent metallocene isolobal analogs.Chapter 2: Under 1 atm of H2, [V(NtBu)2(PMe3)3][Al(PFTB)4], 1.10, (PFTB = perfluoro-tert-butoxide), was shown to catalytically semihydrogenate alkynes to (z)- alkenes. Synthetic and DFT studies, in combination with H2/D2 crossover and PHIP NMR experiments, indicate that H2 is activated by [1,2]-addition to 1.10 and upon the insertion of alkyne into the V-H bond of the vanadium hydrido amide A, the product alkene and 1.10 are generated by the [1,2]-Î±-NH-elimination of the alkenyl ligand.Chapter 3: A series of carbon monoxide, isocyanide, and nitrile complexes of [V(PR3)2(NtBu)2][Al(PFTB)4],	(R	=	Me,	Et)	were	prepared. [V(PMe3)3(NtBu)2][Al(PFTB)4], (PFTB = perfluoro-tert-butoxide) reacts with 2,6- xylylisocyanide (CNXyl) or acetonitrile to afford complexes 3.1 and 3.2. Complex 3.1 was crystallographically characterized revealing a C-N bond length of (1.152(4) AÌŠ), and IR studies showed a C-N stretching frequency of 2164 cm-1. Treatment of [V(PEt3)2(NtBu)2][Al(PFTB)4] with CNXyl yielded the desired isocyanide complex in 60% yield with a C-N stretching frequency of 2156 cm-1. The desired d0 vanadium bisimido, carbonyl complex was achieved via the exposure of 1.11 to 1 atm of CO. Complex 3.4 has a C-O stretching frequency of 2015 cm-1 (CaF2 solution cell). Isotopic labeling with 13CO reveals a stretching frequency of 1970 cm-1, which confirms the assignment of the complex as a terminal Î·1-CO complex and which is also implied by its NMR data in comparison to the other crystallographically characterized compounds presented here. The 13C{31P}{1H} NMR spectrum of 3.4-13C reveals a broad singlet at 228.36 ppm implying deshielding of the carbonyl carbon. This datum, in conjunction with the shielded vanadium NMR shift of -843.71 ppm, suggests Ï€ back-bonding is operative in the bond between carbon monoxide and 1.11. This model was further confirmed by DFT analysis of the model complex [V(Î·1-CO)(PMe3)2(NtBu)2]+, 3.5, which reveals that the basis of the reduced stretching frequency in 3.4 is Ï€ back-bonding from the 2b1 and 1b2 orbitals of 1.11.",ucb,,https://escholarship.org/uc/item/0sx8j7g5,,,eng,REGULAR,0,0
372,1808,Coordination Chemistry of Siderophores: The Intersection of Bacterial Iron Acquisition and Host Defense,"Allred, Benjamin Earl","Raymond, Kenneth N;",2013,"Bacterial pathogens acquire the iron they need for survival and growth in a host by using siderophores. The structures of siderophores are specialized for binding ferric ion with high affinity. Siderophore structures are also specialized to specifically interact with the proteins that mediate siderophore function. These proteins include the bacterial proteins involved in siderophore uptake and utilization, as well as host proteins that inhibit bacterial iron acquisition by intercepting siderophores.The interactions between siderophores and iron underlie biological function. The fundamental coordination chemistry of catecholate and hydroxamate siderophores affects protein interactions during siderophore uptake and host defense. Chapter 1 reviews previous studies on siderophore coordination chemistry and the effects it has on protein interactions and biology with emphasis on research from the Raymond laboratory.The human protein siderocalin defends against siderophore-mediated iron acquisition. Human siderocalin recognizes the metal center of catecholate siderophores, including enterobactin, with high affinity. Several pathogens modify the catecholate metal binding units to make stealth siderophores that are not recognized by human siderocalin. As presented in Chapter 2, the pathogens Vibrio fluvialis and Vibrio cholerae use the siderophores fluvibactin and vibriobactin, respectively, which have catechol-oxazoline metal-binding units. The catechol-oxazoline had been proposed to be a stealth mechanism, but the results herein presented clearly demonstrate that it is not a stealth mechanism. Catechol-oxazoline coordinates iron in either a catecholate mode or a phenolate-oxazoline mode. The phenolate-oxazoline mode is not recognized by siderocalin while the catecholate mode is. Siderocalin stabilizes the catecholate mode sufficiently to cause a shift from the phenolate-oxazoline mode at physiological pH. The high affinity recognition of the ferric triscatecholate metal center allows siderocalin to defend against iron acquisition by a large number of bacterial siderophores.Siderocalin defense has been identified in hosts other than humans. Ex-FABP is a protein found in chickens that has high structural homology to human siderocalin. Chapter 3 reports that Ex-FABP binds many of the same siderophores that are bound by human siderocalin. Unlike human siderocalin, the binding pocket of Ex-FABP is expanded to allow it to bind glucosylated enterobactin. Many of the pathogens specific to chickens use glucosylated enterobactin siderophores known as salmochelins. Salmochelins are stealth siderophores in humans. The siderophore recognition of Ex-FABP demonstrates that siderophore binding proteins may be a general host defense mechanism, and that siderocalins have adapted to the pathogens most frequently encountered by the host. Siderocalin defense and stealth siderophores are at the edge of the arms race for iron.Siderophores carry iron into a bacterial cell through specific transport systems. Once inside the cell, the iron must be removed from the siderophore. Bacteria that use ferric enterobactin remove the iron by hydrolyzing the backbone with an esterase followed by reduction of the ferric ion. Hydrolysis is necessary because the high stability of intact ferric enterobactin prevents biological reduction and iron release. V. cholerae had been reported to use ferric enterobactin, but it does not have an esterase to hydrolyze the backbone. Chapter 4 reports that V. cholerae does not use intact ferric enterobactin, but that it most likely uses ferric complexes of enterobactin hydrolysis products.Uptake of ferric siderophores relies on specific cell membrane receptors. Many siderophore receptors recognize the apo-siderophores as well as the ferric complexes. Binding apo-siderophores does not directly deliver iron to the bacteria, but it plays a role in the uptake mechanism. Chapter 5 describes YxeB, the ferrichrome/ferrioxamine receptor of Bacillus cereus. YxeB transports the siderophores using a Gram-positive siderophore-shuttle in which metal exchange between a ferric siderophore and the bound apo-siderophore is facilitated by the receptor. Metal exchange is not required for uptake, but the siderophore-shuttle is faster than transport without metal exchange.Metal exchange, iron release, and the sterics and electronics of the metal center are coordination chemistry principles that influence the interactions between siderophores and proteins. Proteins usher siderophores through the biological functions of removing iron from the host, passing through the bacterial membrane, and releasing iron to the cell. Therefore, siderophores act as the intermediaries between ferric ion and the biology of bacterial iron uptake.",ucb,,https://escholarship.org/uc/item/0tc2390h,,,eng,REGULAR,0,0
373,1809,Design of the Adaptive Cruise Control Systems: An Optimal Control Approach,"Kim, Sanggyum","Tomizuka, Masayoshi;",2012,"Modern automobiles are equipped with various driver assistance functions which enhance safety and relieve driver fatigue. With the recent development of sensor technology, the Adaptive Cruise Control (ACC) system has been put into practice. This thesis investigates several aspects for the ACC system including (1) smooth reaction of the host vehicle to the cutting in and out of lead vehicles, (2) real-time optimal profile generation for stop-and-go motions, (3) optimal feedback controller design, and (4) extension to Cooperative Adaptive Cruise Control (CACC) systems.The ACC system should maintain an appropriate relative distance to the lead vehicle and should also maintain the desired speed set by the driver if there is no lead vehicle or if the speed of the lead vehicle is faster than the desired speed. Also, it should react smoothly when the lead vehicle cuts out or if a new lead vehicle cuts in from a side lane. This thesis introduces the virtual lead vehicle scheme to prevent the switching between the distance control and the speed control. By controlling the motion of the virtual lead vehicle to be smooth, the scheme could provide smooth reaction of the host vehicle to the cutting in and out of lead vehicles. Linear Quadratic (LQ) optimal control scheme is utilized to find the control gains for the virtual lead vehicle and the host vehicle. Variable weights are utilized in LQ for the virtual lead vehicle. With the variable weights, the motion of the virtual lead vehicle is controlled to be smooth when there is no safety threat while ensuring that the virtual lead vehicel is still responsive and fast when a dangerous situation occurs. ACC with Stop-and-Go and the Cooperative Adaptive Cruise Control (CACC) system are extensions of the conventional ACC system. Stop-and-Go system is targeted to be used in urban driving situation where the lead vehicle can stop completely. In that case, the Stop-and-Go system should have a capability to stop the host vehicle completely. The constant time-headway policy used to find the appropriate relative distance causes undesirable motion for a complete stop. In this thesis a sliding controller is utilized to control the complete stopping motion. To find the optimal stopping trajectory, a constrained Quadratic Programming (QP) problem is solved. A constrained QP is also used to find the optimal velocity profile when the stopped vehicle is to resume motion. Multi-resolution formulations and the Lemke algorithm are utilized to find the optimal trajectories in real time. The CACC system utilizes wireless communication so that the vehicles in the network can share information with other vehicles. In this thesis, a centralized controller is designed by LQ optimal control scheme and potential benefits and problems are addressed. A Kalman filter with variable measurement noise covariance is introduced to compensate the lost data through the wireless network associated with the CACC system. The proposed control schemes have been verified through simulations.",ucb,,https://escholarship.org/uc/item/0v5399z9,,,eng,REGULAR,0,0
374,1810,The Role and Mechanism of Meiotic Chromosome Motion in C. elegans,"Wynne, David","Dernburg, Abby F;",2010,"Proper meiotic chromosome segregation in C. elegans requires homolog pairing, synapsis, and recombination.  The mechanisms underlying homologous chromosome pairing remain poorly understood.  In C. elegans, as in many other eukaryotes, pairing is accompanied by a global rearrangement of chromosomes.  Work from the Dernburg lab and others has found that this rearrangement is driven through the association of special chromosome regions known as Pairing Centers (PCs) with nuclear envelope proteins and cytoskeletal components (Phillips et al. 2005, Sato et al. 2009).  Using fluorescent markers for nuclear envelope attachment sites and Pairing Centers, I analyzed prophase chromosome dynamics through real-time imaging and quantitative motion tracking. My results reveal a dramatic increase in chromosome motion at the onset of chromosome pairing that persists after homologous loci are paired. I show that this increased mobility correlates with the formation of NE patches, and that the increase in motion that accompanies meiotic entry is abrogated by knockdown of cytoplasmic dynein.  These rapid motions are also sensitive to depolymerization of microtubules by colchicine, but are not affected by treatment with Latrunculin A.  In addition, fluorescent labeling of whole chromosomes suggests that meiotic chromosome motion is driven primarily by the PC end of chromosomes and that the chromosome is quite flexible.  These data support a model in which meiotic chromosome motion is promoted by a small number of fast, microtubule-dependent, motor-driven movements that augment the smaller, likely diffusive motions seen prior to meiosis. The observation that fast motions persist well after pairing is completed suggests additional roles in chromosome synapsis or recombination, and are consistent with the idea that rapid motions function to destabilize inappropriate, non-homologous interactions.",ucb,,https://escholarship.org/uc/item/0vm8h6zf,,,eng,REGULAR,0,0
375,1811,"Biochar for sustainable agricultural intensification: technical/economic potential, and technology adoption","Crane-Droesch, Andrew","Levine, David;Kammen, Daniel;",2015,"Growing population, changing climate, and human development will require sustainable agricultural intensification in sub-Saharan Africa -- a region where most people still live in rural areas, and rural poverty remains severe.  While the 20th century saw massive increases in agricultural productivity over most of the world, sub-Saharan Africa was largely bypassed.  In many respects, catching up in the 21st century poses more difficult challenges than were faced in the 20th -- with the novel challenges of climate change, soil degradation, and the closing agricultural frontier being chief among them.  Solutions are required that profitably improve productivity, strengthen and/or rebuild soil fertility, and do so within the limits imposed by a warming and carbon-constrained world.  This will be needed if the region is to shake off the stagnation that has characterized it for the past century, and contribute to solving to the global challenges posed by the coming century.This dissertation focuses on one potential solution -- biochar -- and follows it from agronomic efficacy, to preliminary economic analysis, to rigorous trial in the field.  As such, it seeks to be an example of the interdisciplinary approach that I argue is needed to guide the development, deployment, and scaling of solutions to problems in the environment/development space.The first chapter is a meta-analysis of crop yield response to biochar.  Using data from 84 studies, I (and co-authors) employ meta-analytical, missing data, and semiparametric statistical methods to explain heterogeneity in crop yield responses across different soils, biochars, and agricultural management factors, and then estimate potential changes in yield across different soil environments globally.  We find that soil cation exchange capacity and organic carbon were strong predictors of yield response, with low cation exchange and low carbon associated with positive response.  We also find that yield response increases over time since initial application, compared to non-biochar controls.  High reported soil clay content and low soil pH were weaker predictors of higher yield response.  No biochar parameters in our dataset -- biochar pH, percentage carbon content, or temperature of pyrolysis -- were significant predictors of yield impacts.  Projecting our fitted model onto a global soil database, we find the largest potential increases in areas with highly weathered soils, such as those characterizing much of the humid tropics.  Richer soils characterizing much of the world's important agricultural areas appear to be less likely to benefit from biochar.The second chapter is a preliminary economic analysis of biochar's potential in two contexts -- rural western Kenya, and northern Vietnam.  Using recall-based datasets from smallholder farmers, I (and co-authors) estimate yields as a function of biochar and fertilizer use.  We find an positive association between biochar use and average yields in Kenya, but no correlation in Vietnam.  We then use these estimates to calculate optimal input mixes under hypothetical biochar and carbon prices, given heterogeneity in response both to biochar and fertilizer, and heterogeneous budget constraints.  In Kenya, we find that biochar is more-likely-than-not to be profitable to adopt for 23\% of our sample if unsubsidized and available at its current sale price of around \$188/ton, while a hypothetical carbon subsidy of \$100/ton CO$_2$e increases this proportion to 47\%, though these proportions are not different from zero at 95\% confidence.  Because of limited short-term complementarity between biochar and inorganic fertilizer, we estimate that biochar adoption would change profits little, given budget constraints for agricultural inputs.  We conclude that carbon subsidies may have a marginal impact on biochar's profitability in Western Kenya, but that further research is needed to improve the precision of these estimates, extend them to account for any longer-term changes in soil characteristics that might impact biochar's profitability, and account for any potential biases stemming from time-varying variables that not measured or modeled in the context of this study.The third chapter reports the results of a Kenyan field experiment on adoption and impact of biochar, which was motivated by the encouraging findings of the previous two studies.  In addition to technical efficacy, I sought to determine what mix of policies might most effectively speed biochar dissemination, given the slow pace of technological change in African agriculture over the past several decades.  I randomly assigned prices, demonstrations, and risk-free trials.  Yields increased by 37\% and 50\% in the first two seasons, and response to inorganic fertilizer improved.  However, uptake was 2.6\% and 10\% respectively.  Farmers were highly price-sensitive. Social network effects were marginally significant, but positive at low penetration and negative at high penetration.  Given uptake well below the social optimum, subsidies for biochar appear justified from a social cost/benefit standpoint.  The dissertation closes with a short discussion of lessons learned, and ways forward for further applied research.",ucb,,https://escholarship.org/uc/item/0xv3n1tw,,,eng,REGULAR,0,0
376,1812,Enhancing the Language Skills of Toddlers with Severe Communication Difficulties Who Benefit from AAC: A Comparison of Two Language Intervention Approaches,"Solomon-Rice, Patti","Soto, Gloria;Holloway, Susan;",2010,"Toddlers who demonstrate significant speech impairments and use augmentative and alternative communication (AAC) often demonstrate concomitant expressive language impairments.  These toddlers receive AAC intervention due to difficulties with speech production at the motor level, and receive language intervention due to difficulties with vocabulary development at the linguistic level.  One language intervention approach with a large base of evidence supporting efficacy in young children who have language delays, but do not use AAC, is responsivity education combined with focused stimulation.  Another language intervention approaches with a large base of evidence supporting efficacy in young children who use AAC is responsivity education combined with aided AAC modeling.  However, the effectiveness of responsivity education and focused stimulation has not been systematically studied, nor has the approach been compared to responsivity education and focused stimulation with the addition of AAC modeling in toddlers who use AAC.A challenge of language intervention is to provide effective and efficient treatment for the expressive language impairment.  To address this critical challenge with toddlers who use AAC, the current investigation compared the effectiveness and efficiency of these language intervention approaches in teaching new vocabulary production to toddlers between the ages of two and three who received AAC intervention.  Specifically, the study examined the possible added value of including AAC modeling to an intervention involving responsivity education and focused stimulation.  The study used an adapted alternating treatment design across participants.  Four 2-year-old toddlers, who used AAC to communicate, participated in the study.  One toddler used a visual grid, manual sign, and word approximations/words to communicate; one toddler used manual sign and word approximations/words to communicate; one toddler used a communication board, manual sign, and word approximations/words to communicate; and a final toddler used a flip chart and word approximations/words to communicate.The results of the study provided preliminary evidence that the language intervention approach of responsivity education and focused stimulation was more effective for improving the vocabulary production in three of four participants in the study.  The added value of including AAC modeling in the intervention resulted in more effective vocabulary production with the fourth participant.  In addition, the results of the investigation provided preliminary evidence that the language intervention approach of responsivity education and focused stimulation was more efficient for improving the vocabulary production in two of the four participants in the study.  The added value of AAC modeling in the intervention resulted in more efficient vocabulary production for the third participant.  Neither intervention approach was efficient with the fourth participant.  Possible factors affecting the results of the study, theoretical and clinical implications, limitations, and future directions for research are discussed.",ucb,,https://escholarship.org/uc/item/0zc4c0qd,,,eng,REGULAR,0,0
377,1813,Engineering Lipid Vesicles for Cellular Reconstitution,"Richmond, David","Fletcher, Daniel A;",2011,"The reductionist approach to modern cell biology aims to identify the individual molecules and interactions that give rise to complex biological activity.  A complementary constructionist approach, known as reconstitution, aims to recapitulate biological structures and functions from basic building blocks in order to show which components are essential and how biophysical constraints, such as membrane boundaries, influence organization and activity. My dissertation research has applied and extended this `bottom-up' approach to study the role of membrane mechanics in the formation of cellular filopodia and to develop new tools for reconstituting processes encapsulated within membranes and for engineering cell-like devices.We investigated the mechanics of actin-membrane interactions by studying dendritic actin networks grown on the surface of giant unilamellar vesicles.  In this minimal system, we observed the formation of parallel filament protrusions arising from the highly branched dendritic actin network, notably in the absence of bundling proteins. We confirmed through a simple theoretical model that a lipid bilayer can drive the emergence of bundled actin filament protrusions from branched actin filament networks, thus playing a role normally attributed to actin-binding proteins.  This revealed a critical role for the membrane in organizing actin filaments at the plasma membrane.This work motivated the development of a technique for encapsulating protein contents in the lumen of lipid vesicles in order to emulate the biophysical boundary conditions of real cells.  We demonstrated the use of a microfluidic jet to form lipid vesicles with controlled contents by deforming a planar bilayer. These vesicles mimic an essential organizational feature of cells - encapsulation within a lipid membrane - and provide a platform for more complex cellular reconstitution.  Subsequently, we adapted this technique to a pulsed inkjet-based device, enabling greater control of vesicle size and improved throughput.  Using this inkjet-based device for vesicle formation, we were able to control membrane properties such as asymmetric lipid composition and insertion of membrane proteins, which are essential for numerous cellular processes.  We demonstrated the applicability of this technique by reconstituting SNARE-mediated membrane fusion in a geometry that mimics exocytosis.In summary, this work has provided new insight into the role of lipid bilayer mechanics on the reconstitution of cellular protrusions and developed a novel technique that enables formation of lipid vesicles with controlled contents and membrane properties.  Further development of this technique will enable advanced reconstitution experiments and construction of functional cell-like devices for medical and biomaterials applications.",ucb,,https://escholarship.org/uc/item/0hq5d9jr,,,eng,REGULAR,0,0
378,1814,"The Home, the Map and the Garden: Literary Space as Monumental Space in Kofman, Perec and Rodoreda","Scala, Suzanne Anna","Alter, Robert;",2014,"This dissertation explores how three authors, Sarah Kofman, Georges Perec and MercÃ¨ Rodoreda, create textual monuments to divisive historical events. I look most closely at Kofman's Rue Ordener, rue Labat, Perec's W ou le souvenir d'enfance and Rodoreda's El carrer de les CamÃ¨lies. We can think of these texts as textual monuments because they emphasize the spatial aspect of their narratives and because they are concerned with memory.The first chapter of the dissertation considers how, in Rue Ordener rue Labat, Sarah Kofman is able to write a memoir that recognizes the specificity of her experience while at the same time letting the reader into the story. I employ Emmanuel Levinas' concept of the intersubjective space of the home to understand how Kofman, at both the level of content and structure, allows the reader to co-create the textual space.In the second chapter I turn to Georges Perec's W ou le souvenir d'enfance. Perec, unable to recall his childhood memories, instead creates a memoir structured by the ""fragile intersection"" of several texts. The textual monument he creates is not monolithic, but rather takes its form from the multiple paths between nodes of meaning.The third chapter considers MercÃ¨ Rodoreda's novella, El carrer de les CamÃ¨lies. Rodoreda uses the motifs of the garden and the cemetery to advocate for a textual monument that valorizes what Gilles Deleuze and FÃ©lix Guattari would call ""rhizomatic"" connections between people and symbols, rather than a monument that concentrates on roots and the past.The Conclusion considers the important common features of the three texts, namely the idea that memory is constructed by a person in the present and that this construction, while it can be experienced as tragic for the person remembering, can also be liberating. Future work in this area would seek to establish a relationship between poststructuralist theory, narratology and the emphasis on space common to both textual and physical monuments in the twentieth century.",ucb,,https://escholarship.org/uc/item/12v6k71d,,,eng,REGULAR,0,0
379,1815,The Role of Mitochondrial Deacetylase SIRT3: Delivering Benefits of Calorie Restriction and Promoting Adult Stem Cell Function,"Brown, Katharine van Dyke","Chen, Danica;",2012,"With the increase in the aging population, there has been a growing interest inunderstanding the process of age-related physical decline and increased disease risk.Research in model organisms has shown that aging, far from being a spontaneousdevelopment, is actually a controlled process, with molecular mechanisms that can alterthe pace of cellular and tissue decline. The aim of this dissertation work was to gaininsight into the molecular mechanisms that can control this aging rate.We found that SIRT3, a mitochondrial NAD+-dependent deacetylase, performs a vital roleduring calorie restriction in mice to decrease oxidative damage in tissues. These resultsled us to identify superoxide dismutase 2(SOD2) as a target protein of SIRT3, andconfirmed that SIRT3 can deacetylate SOD2 at two critical lysine residues (K53 and K89).Deacetylation of these residues on SOD2 leads to an increase in SOD2 detoxificationactivity. Furthermore, SOD2 is more deacetylated in the tissues of mice on calorierestriction, but this effect is abrogated in mice that are deficient for SIRT3. These resultsled us to develop a model whereby calorie restriction upregulates SIRT3 expression andactivity, leading to an increased deacetylation of SOD2. The increased activity level ofthe deacetylated SOD2 has the effect of decreasing oxidative damage in tissues.Concomitantly, we found that SIRT3 is required for the switch to fatty acid utilization duringcalorie restriction. SIRT3 KO mice on calorie restriction have reduced beta- oxidation, lowerlong chain-acyl CoA dehydrogenase activity (LCAD), and a preference for glucose uptakeand carbohydrate metabolism. Our findings indicate that other molecular adaptations thatoccur during calorie restriction are insufficient to compensate for a SIRT3 deficiency in thismetabolic context.We also present our findings that SIRT3 is highly expressed in hematopoietic stem cells(HSCs) as compared to differentiated hematopoietic cells, which led us to explore the roleSIRT3 was playing in this stem cell population. Our results indicate that SIRT3 is required as a stress-responsive protein that can protect stem cell function during conditions ofoxidative stress. These conditions can include serial transplant, chemical treatment, orthe increased oxidative stress associated with aging. Furthermore, we show that SIRT3expression is decreased in HSCs from aged mice, and enforced expression of SIRT3 canimprove the function of aged HSCs, suggesting the potential for rejuvenation of agedHSCs.Our studies confirm the role that SIRT3 plays in protecting cells and tissues from oxidativestress, as well as offer. Although to date, no lifespan data has been published on SIRT3KO mice, the results presented here indicate that a relatively shortened lifespan orhealthspan would not be unexpected. These findings also open avenues forunderstanding the role of SIRT3 in stem cell biology, both other stem cell types, andpotentially in human stem cell systems.",ucb,,https://escholarship.org/uc/item/13r9x81g,,,eng,REGULAR,0,0
380,1816,Socio-economic and Engineering Assessments of Renewable Energy Cost Reduction Potential,"Seel, Joachim","Borenstein, Severin;",2017,"This dissertation combines three perspectives on the potential of cost reductions of renewable energy â€“ a relevant topic, as high energy costs have traditionally been cited as major reason to vindicate developments of fossil fuel and nuclear power plants, and to justify financial support mechanisms and special incentives for renewable energy generators. First, I highlight the role of market and policy drivers in an international comparison of upfront capital expenses of residential photovoltaic systems in Germany and the United States that result in price differences of a factor of two and suggest cost reduction opportunities. In a second article I examine engineering approaches and siting considerations of large-scale photovoltaic projects in the United States that enable substantial system performance increases and allow thus for lower energy costs on a levelized basis. Finally, I investigate future cost reduction options of wind energy, ranging from capital expenses, operating expenses, and performance over a projectâ€™s lifetime to financing costs. The assessment shows both substantial further cost decline potential for mature technologies like land-based turbines, nascent technologies like fixed-bottom offshore turbines, and experimental technologies like floating offshore turbines. The following paragraphs summarize each analysis:International upfront capital cost comparison of residential solar systemsResidential photovoltaic (PV) systems were twice as expensive in the United States as in Germany (median of $5.29/W vs. $2.59/W) in 2012. This price discrepancy stems primarily from differences in non-hardware or â€œsoftâ€ costs between the two countries, of which only 35% be explained by differences in cumulative market size and associated learning. A survey of German PV installers was deployed to collect granular data on PV soft costs in Germany, and the results are compared to those of a similar survey of U.S. PV installers. Non-module hardware costs and all analyzed soft costs are lower in Germany, especially for customer acquisition, installation labor, and profit/overhead costs, but also for expenses related to permitting, interconnection, and inspection procedures. Additional costs occur in the United States due to state and local sales taxes, smaller average system sizes, and longer project-development times. To reduce the identified additional costs of residential PV systems, the United States could introduce policies that enable a robust and lasting market while minimizing market fragmentation. Regularly declining incentives offering a transparent and certain value propositionâ€”combined with simple interconnection, permitting, and inspection requirementsâ€”might help accelerate PV cost reductions in the United States. Performance analysis of large-scale solar installations in the United StatesThis paper presents the first known use of multi-variate regression techniques to statistically explore empirical variation in utility-scale PV project performance across the United States. Among a sample of 128 utility-scale PV projects totaling 3,201 MWAC, net capacity factors in 2014 varied by more than a factor of two. Regression models developed for this analysis find that just three highly significant independent variables â€“ the level of global horizontal irradiance (GHI), the use of single-axis tracking, and the inverter loading ratio (ILR) â€“ can explain 92% of this project-level variation (with GHI alone able to explain 71.6%). Adding the commercial operation year as a fourth independent variable and three interactive variables (tracking x GHI, tracking x ILR, GHI x ILR) improves the model further and reveals interesting relationships (e.g., the performance benefit of tracking increases with a higher GHI but diminishes with a higher ILR). Taken together, the empirical data and statistical modeling results presented in this paper can provide a useful indication of the level of performance that solar project developers and investors can expect from various project configurations in different regions of the United States. Moreover, the tight relationship between fitted and actual capacity factors should instill confidence among investors that the utility-scale projects in this sample have largely performed as predicted by our models, with no significant outliers to date. Holistic assessment of future cost reduction opportunities of wind energy applicationsWind energy supply has grown rapidly over the last decade. However, the long-term contribution of wind to future energy supply, and the degree to which policy support is necessary to motivate higher levels of deployment, dependsâ€”in partâ€”on the future costs of both onshore and offshore wind. Here, I summarize the results of an expert elicitation survey of 163 of the worldâ€™s foremost wind experts, aimed at better understanding future costs and technology advancement possibilities. Results suggest significant opportunities for cost reductions, but also underlying uncertainties. Under the median scenario, experts anticipate 24â€“30% reductions by 2030 and 35â€“41% reductions by 2050 across the three wind applications studied. Costs could be even lower: experts predict a 10% chance that reductions will be more than 40% by 2030 and more than 50% by 2050. The main identified drivers for near term cost reductions are rotor-related advancements and taller towers for onshore installations, fixed-bottom offshore turbines can benefit from an upscaling in generator capacity, streamlined foundation design and reduced financing costs, while floating offshore turbines require further progress in buoyant support structure design and installation process efficiencies. Insights gained through this expert elicitation complement other tools for evaluating cost-reduction potential, and help inform policy, planning, R&D, and industry strategy.",ucb,,https://escholarship.org/uc/item/14h9t4pj,,,eng,REGULAR,0,0
381,1817,"Deconstruct, Imagine, Build: Bringing Advanced Manufacturing to the Maker Community","Lo, Joanne","Paulos, Eric;",2016,"Physical prototypes serve as a common starting point for the process of innovation, improvement of an existing product, and experimentation of new interactions. As the shapes, forms, and functions of the electronic landscape rapidly evolve, fabrication and prototyping methods need to keep up with the changing needs as well. This dissertation contributes concepts and techniques that answer two research questions: 1. What type of prototyping processes and tools could support the rapidly evolving field of interactive technology? 2. How can these prototyping processes and tools be selected to add value to the broader community - one that includes engineers, designers, and hobbyists?  In this thesis, I will demonstrate that by using concepts inspired by various advanced manufacturing fields - such as MEMS, structural electronics, and flexible electronics - novel interaction modalities can be prototyped with commercially accessible materials. Electronics presented in this dissertation include circuit boards with mechanically functional shapes, non-emissive textile displays, and on-skin electronic devices. Moreover, this thesis also describes a web-based digital tool that allows users to free-form sketch basic circuits and also provides step-by-step fabrication and debugging guidance. Using this tool, users will be able to sketch, design, and prototype electronics with materials such as silver/graphite pen, conductive thread, paper, and fabric.  We hope that this thesis will inspire the community to create innovative interactions that utilize readily available prototyping tools.",ucb,,https://escholarship.org/uc/item/15h9z66f,,,eng,REGULAR,0,0
382,1818,"Irish on the Air: Media, Discourse, and Minority-Language Development","Cotter, Colleen",,1996,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/1623s55p,,,eng,REGULAR,0,0
383,1819,Mechanistic Studies of Biomimetic Reactions by Synthetic Enzyme Mimics,"Hart-Cooper, William Michael","Raymond, Kenneth N;Bergman, Robert G;",2015,"AbstractMechanistic Studies of Biomimetic Reactions by Synthetic Enzyme MimicsByWilliam Michael Hart-CooperDoctor of Philosophy in ChemistryUniversity of California, BerkeleyProfessor Kenneth N. Raymond, Co-chairProfessor Robert G. Bergman, Co-chairChapter 1. A brief introduction to common synthetic host structures and justification for the work described herein is provided.Chapter 2. The development of 1 and related hosts as a new class terpene synthase mimics that catalyze intramolecular Prins cyclizations. The property of water exclusion is observed. Host 1 is also shown to compensate for the gem-disubstituent effect. Chapter 3. The development of new terephthalamide hosts enabled an investigation of the effect of host structure on the enantio- and diastereoselectivity of these reactions, as well as a simple kinetic analysis. Rate accelerations and turnover numbers are notably high. Chapter 4. The mechanism of proton transfer in an archetypal enzyme mimic is studied using amide hydrogen deuterium exchange (HDX) kinetics. Collectively, these data shed light on the role of acid, base and water-mediated proton transfer in a synthetic active site with relevance to proton-mediated catalysis. Moreover, the emergent mechanism of solvent-occupied proton transfer raises the prospect of designable hosts with properties that are unique to the integration of their partsChapter 5. A short overview is provided, which places the results of chapters 2-4 in context with some broader goals of biomimetic supramolecular chemistry.",ucb,,https://escholarship.org/uc/item/1890r9bs,,,eng,REGULAR,0,0
384,1820,The Evolution of Social Monogamy and Biparental Care in Stomatopod Crustaceans,"Wright, Mary Louisa","Caldwell, Roy L;",2013,"ABSTRACTThe Evolution of Social Monogamy and Biparental Care in Stomatopod CrustaceansBy Mary Louisa WrightDoctor of Philosophy in Integrative BiologyUniversity of California, BerkeleyProfessor Roy L. Caldwell, Chair	Although social monogamy and biparental care have been extensively studied in birds, mammals, and fish, the evolutionary origins and maintenance of these phenomena are not well-understood, particularly in invertebrate taxa. The evolution of social monogamy is of interest because current theory predicts that both males and females will usually gain more fitness from mating with multiple partners. Furthermore, biparental care should only occur when males and females both gain more fitness benefits from providing parental care than from investing time and energy into mate searching.  Given these expectations, under what environmental and social conditions will social monogamy and biparental care arise and do the same conditions maintain monogamy and biparental care on an evolutionary time scale? Long-term social monogamy, which occurs when a male and female pair for longer than a single breeding cycle, has been reported in eight genera of Lysiosquilloid stomatopods. Furthermore, the Lysiosquilloidea also contains the only marine crustacean genus (Pullosquilla) in which biparental care has been systematically studied. This dissertation examines the evolutionary maintenance and origins of both biparental care and long-term social monogamy in the Lysiosquilloidea, using experimental manipulations, ecological surveys, and comparative, phylogenetically-based methods.	 Chapter 1: The maintenance of biparental care  I examined the fitness costs and benefits of biparental care in the stomatopod  Pullosquilla thomassini  using an experimental manipulation of the number and sex of care providers. In the absence of any care, egg clutch survival and growth decreased. However, neither the number, nor the sex of the care providers had a significant effect on changes in egg clutch mass. Parental care treatment did not affect ovary size, the total number of eggs in a clutch, or egg size. Thus, while parental care increases production of offspring, uniparental care by either sex is sufficient to achieve this goal. Males providing uniparental care lost more weight than those providing biparental care or no care. This may lead to sexual conflict over female desertion. These results suggest that biparental care is not evolutionarily maintained in  P. thomassini  by increasing the number of offspring hatching in an egg clutch. Instead, I hypothesize that biparental care may be evolutionarily maintained in  P. thomassini  by increasing the rate of egg clutch production and facilitating double-clutching. Chapter 2: The effects of environmental and demographic variation on pairing behaviors I examined the effects of environmental and demographic variation on pairing behaviors, egg clutch production, and burrow distribution in two sympatric stomatopod crustaceans,  P. litoralis  and  P. thomassini . These small (<16mm) stomatopods are found as heterosexual pairs in U-shaped burrows in coral patch reef ecosystems. Coral patch reef ecosystems consist of coral heads of varying sizes separated by sand flats; in this heterogeneous environment, the coral heads host high levels of invertebrate and fish diversity and abundance in comparison to the sand flats. I hypothesized that environmental heterogeneity in the coral back reef environment would affect pairing behaviors, egg clutch production, and burrow distribution of  Pullosquilla  species due to gradients in food abundance or predation by fish. I tested this hypothesis on demographic and environmental data collected from a survey of  Pullosquilla  species collected in a patch reef in Moorea, French Polynesia. My findings indicate that proximity to coral heads is an important factor in structuring the demography and pairing behaviors of  P. litoralis, but not its congener, P. thomassini. The directionality of the relationships between proximity to coral heads and several demographic traits suggests that gradients in fish predation are responsible for these patterns in P. litoralis. This suggests that selective pressures from fish predation may play an important role in the maintenance of pairing behaviors in P. litoralis. Determining the causes of differences in demographic patterns and pairing behaviors of P. litoralis and P. thomassini may yield a better understanding of the evolutionary maintenance of social monogamy in stomatopod crustaceans. Chapter 3: The evolutionary origins of long-term social monogamy in stomatopods I examined two hypotheses for the evolutionary origins of long-term social monogamy in stomatopod crustaceans using comparative, phylogenetically-based methods. One of the most commonly posited explanations for the evolution of social monogamy is that biparental care is required to successfully raise offspring. A prediction of this hypothesis is that biparental care should evolve in a clade before or at the same time as social monogamy. I tested this prediction by reconstructing ancestral states of social monogamy and biparental care on a Maximum Likelihood tree of 66 stomatopod species and found that long-term social monogamy evolved before biparental care in the Lysiosquilloid stomatopods. This indicates that a need for biparental care did not lead to the origin of social monogamy in this clade. Based on my finding that predation influences pairing behaviors in P. litoralis (Chapter 2) and the observation that all known socially monogamous stomatopods are sit-and-wait predators, I propose an alternative hypothesis for the origin of social monogamy in stomatopods. Sit-and-wait predation evolves as a strategy to maximize energy intake while minimizing predation risk when a lineage lives in an environment where both prey items and potential predators are abundant. I therefore hypothesized that a suite of behaviors, including burrowing, sit-and-wait predation, and social monogamy, that allowed stomatopods to escape high levels of predation evolved in the Lysiosquilloidea.  I tested two predictions of this hypothesis: 1) social monogamy should evolve more often in burrow-dwellers living in soft-bottom substrates and 2) the evolution of long-term social monogamy should be correlated with the evolution of sit-and-wait predation. I tested this hypothesis on a Maximum Likelihood phylogeny of 66 stomatopod species using ancestral state reconstructions and Pagel's (1994) test of correlated trait evolution and found that burrowing, sit-and-wait predation, and social monogamy evolved sequentially in the Stomatopoda. Long-term social monogamy may have evolved as a way of further maximizing the fitness benefits of the sedentary lifestyle associated with sit-and-wait predation. This novel evolutionary route to long-term social monogamy may be associated with the shallow benthic marine environments that most Lysiosquilloids inhabit.ConclusionsThe findings of my thesis emphasize the importance of studying a diversity of taxa and environments when trying to understand the evolution of important behavioral traits. For example, it is often assumed that when biparental care is widespread in a species, it increases the viability of the current brood of offspring. However, in P. thomassini there is no evidence that biparental care increases either the survival or development of embryos (Chapter 1). Additionally, the evolution of social monogamy in many animals is attributed to a need for biparental care. The Lysiosquilloid stomatopods appear to provide a counter-example in which social monogamy likely facilitated the evolution of biparental care and other form of paternal effort (Chapter 3). Instead, my findings support the hypothesis that long-term social monogamy and sit-and-wait predation may have evolved to decrease mortality from predation during foraging and mate searching. The role of the risk of predation during mate searching in the evolution of social monogamy has received relatively little attention in the large body of literature on mating system evolution, but it appears that predation plays an important role in determining pairing behaviors and burrowing distributions in P. litoralis (Chapter 2). Taken as a whole, these findings provide compelling justification for studying the evolution of behaviors in a wide diversity of taxa.",ucb,,https://escholarship.org/uc/item/18q929n4,,,eng,REGULAR,0,0
385,1821,"Flight Delays, Capacity Investment and Welfare under Air Transport System Equilibrium","Zou, Bo","Hansen, Mark M;",2012,"Infrastructure capacity investment has been traditionally viewed as an important means to mitigate congestion and delay in the air transportation system. Given the huge amount of cost involved, justifying the benefit returns is of critical importance when making investment decisions. This dissertation proposes an equilibrium-based benefit assessment framework for aviation infrastructure capacity investment. This framework takes into consideration the interplays among key system components, including flight delay, passenger demand, flight traffic, airline cost, and airfare, and their responses to infrastructure capacity investment. We explicitly account for the impact of service quantity changes on benefit assessment. Greater service quantity is associated with two positive feedback effects: the so-called Mohring effect and economies of link/segment density. On the other hand, greater service quantity results in diseconomies of density at nodes/airports, because higher traffic density at the airport leads to greater airport delays. The capacity-constrained system equilibrium is derived from those competing forces.Two approaches are developed to investigate air transport system equilibrium and its shift in response to infrastructure capacity expansion. In Chapter 2, we first view the system equilibrium from the airline competition perspective. We model airlines' gaming behavior for airfare and frequency in duopoly markets, assuming that airlines have the knowledge of individuals' utility structure while making decisions, and that delay negatively affects individuals' utility and increases airline operating cost. The theoretical airline competition model developed in Chapter 2 provides analytical insights into the interactions among various system components. Under a symmetric Nash equilibrium, we find that the presence of flight delay increases passenger generalized cost and discourages air travel. Airlines would not pass delay cost entirely onto passengers through higher fare, but also account for the impact of service degradation on passenger willingness-to-pay and consequently passenger demand. To avoid exorbitant flight delays, airlines would use larger aircraft, meanwhile taking advantage of economies of aircraft size. The resulting unit cost reduction partially offsets operating delay cost increase. The equilibrium shift triggered by capacity expansion reduces both schedule delay and flight delay, leading to lower passenger generalized cost and higher demand, despite slightly increased airfare. Airlines will receive larger profit, and consumer welfare will increase, as a result of the expansion. Although delay reduction is less than expected because of induced demand, the overall benefit, which encompasses reduction in both schedule delay and flight delay, would be much greater than estimated from a purely delay-based standpoint. The equilibrium analysis can be alternatively approached from a traveler-centric perspective. The premise of an air transport user (i.e. traveler) equilibrium is that each traveler in the air transportation system maximizes his/her utility when making travel decisions. The utility depends upon market supply and performance characteristics, consisting of airfare, flight frequency, and flight delay. The extent of airline competition is implicitly reflected in the determination of airfare and flight frequency. Given the limited empirical evidence of the delay effect on air transportation system supply, two econometric models for airfare and flight frequency are estimated in Chapter 3. We find positive delay effect on fare, which should be interpreted as the net effect of airlines' tendency to pass delay cost to passengers while also compensating for service quality degradation. Higher delay discourages carriers from scheduling more flights on a segment. Both delay effects, however, are relatively small. The estimated fare and frequency models, together with passenger demand and airport delay models presented in Chapter 4, are integrated to formulate the air transport user equilibrium as fixed point and variational inequality problems. We prove that the equilibrium existence is guaranteed; whereas equilibrium uniqueness cannot be guaranteed. We apply the user equilibrium to a fully connected, hypothetical network with the co-existence of direct and connecting air services. Using a simple, heuristic algorithm, we find that the equilibrium is insensitive to initial demand values, suggesting that there may be a single equilibrium for this particular model instance. Hub capacity investment attracts spoke-spoke passengers from non-stop routes, and generates new demand on hub-related routes. At the market level, hub capacity expansion would result in greater total demand and consequently passenger benefits in almost all markets--except for ones where a predominant portion of passengers choose non-stop routes due to extremely high circuity for one-stop travel. In the latter set of markets, after capacity expansion passenger demand and benefits would be both reduced. This counter-intuitive result carries important implications that capacity increase does not necessarily benefit everyone in the system. Similar to the findings from the airline competition model, with changes in flight delay, schedule delay, airfare, and total demand, the user equilibrium model yields much higher passenger benefits from capacity investment than the conventional method; whereas hub delay saving is offset by traffic diversion and induced demand. With continuous capacity investment, the air transportation network will witness substantial changes in service supply and traffic patterns.",ucb,,https://escholarship.org/uc/item/19069773,,,eng,REGULAR,0,0
386,1822,Time-Domain Ultra-Wideband Synthetic Imager in Silicon,"Arbabian, Mohammad Amin","Niknejad, Ali M;",2011,"Low-cost and portable medical devices will play a more significant role in wellness, healthcare and medicine. While consumer electronics have become ubiquitous and inexpensive, medical devices, by contrast, are still primarily found only in hospitals. There is a great potential benefit in using techniques developed in the consumer electronic industry and applying them to the healthcare market. To do this, substantial innovation is required to develop new sensors and devices that are fundamentally less invasive and use profoundly different physical phenomena to address medical applications. This research aims at designing a non-invasive, low-cost, and portable imaging device for cancer screening. Detection in early stages has proven to be essential for reducing the mortality rate in cancer. This requires pursuit of modalities that could be widespread and are safe to be used for more frequent screening. This research uses the available contrast in microwave frequencies to detect abnormalities.Conceptualization, architectural and system-level design, and finally implementation of the system called TUSI, Time-Domain Ultra-Wideband Synthetic Imager, are addressed. Using an array of closely controlled radiating silicon chips, acting as transceivers in microwave /mm-wave frequencies, this device transmits short ""beam-steered"" pulses and picks up reflections from tissue abnormalities (e.g. cancerous tissue). By processing the data from multiple transceivers, a larger aperture is synthesized. In essence, this imager probes the ""electrical"" properties of the tissue. Various challenges related to generating, controlling, transmitting, and detecting these coherent ultra-short pulses are examined and new solutions proposed. A pixel-scalable integrated transceiver consisting of elements from antenna-to-antenna is designed and implemented in a SiGe BiCMOS process.",ucb,,https://escholarship.org/uc/item/19f071g6,,,eng,REGULAR,0,0
387,1823,Towards Secure and Privacy-Preserving Online Social Networking Services,"Gong, Zhenqiang","Song, Dawn;",2015,"Online social networking services (e.g., Facebook, Twitter, and Blogger) bring new benefits to almost all aspects of our lives. They have completely transformed how we communicate with each other, how we process information, and how we diffuse social influence. However, these social networking services are also plagued by both conventional and emerging threats to security and privacy. For instance, two fundamental security risks are 1) usersâ€™ accounts are compromised by attackers or get lost and 2) attackers create massive fake (or Sybil) accounts to launch various malicious activities. In this thesis, we first design secure and usable account recovery methods based on usersâ€™ trusted friends to recover compromised or lost user accounts. Second, we construct a scalable semi-supervised learning framework, which is based on probabilistic graphical model techniques, to detect Sybil accounts. Third, we demonstrate that diverse private information (e.g., private user demographics and hidden social connections) can be inferred with high accuracies from data that is publicly available on social networking sites, which has implications for the design of privacy-preserving online social networking services.",ucb,,https://escholarship.org/uc/item/1b14t6kq,,,eng,REGULAR,0,0
388,1824,Neuromechanics of Maneuverability: Sensory-Neural and Mechanical Processing for the Control of High-Speed Locomotion,"Mongeau, Jean-Michel","Full, Robert J;",2013,"Maneuverability in animals is unparalleled when compared to the most maneuverable human-engineered mobile robot. Maneuverability arises in part from animals' ability to integrate multimodal sensory information with an ongoing motor program while interacting within a spatiotemporally complex world. Complicating this integration, actions from the nervous system must operate through the mechanics of the body. Since sensors and muscles are fused to a mechanical frame, mechanical processing occurs at both at the sensory (input) and motor (output) levels. To reveal the basic organization of the neural and mechanical parts of organisms during locomotion, I studied high-speed sensorimotor tasks in a remarkably maneuverable insect, the cockroach, which integrates sensory information to navigate through irregular, unpredictable environments. Animals can expend energy to acquire information by emitting signals or moving sensory structures. However, it is not clear if the energy from locomotion, itself, could permit a different form of sensing, in which animals transfer energy from movement to reconfigure a passive sensor. In the first chapter, I demonstrate that cockroaches can transfer the self-generated energy from locomotion to actively control the state of the antenna via passive mechanical elements, with important effects on body control. This chapter advances our current understanding of sensorimotor integration during rapid running by showing how the whole body, not just the sensor, can participate in sensory acquisition.Information flow from individual sensory units operating on locomotion-driven appendages to the generation of motor patterns is not well understood. The nervous system must rapidly integrate sensory information from noisy channels while constrained by neural conduction delays. When executing high-speed wall following using their antennae, cockroaches presumably integrate information between self and obstacles to generate appropriate turns, preventing collisions. Previous work on modeling high-speed wall following within a control theoretic framework predicted that a sensory controller for antenna tactile sensing of wall position (P) and the derivative of position (D) was sufficient for control of the body. I hypothesized that individual mechanoreceptive units along the antenna were tuned to enable stable running. Extracellular multi-unit recordings revealed P and D sensitivity and variable-latency responses, suggesting the antenna may function as a delay line. In the second chapter, I show how individual sensor units distributed on the antenna precondition neural signals for the control of high-speed turning.Since sensors of animals are embedded within the body, they must function through the mechanics of the body. In Chapter 3, I studied mechanical properties of the primary tactile sensors of cockroaches, the antennae, using experimental and engineering approaches. I revealed how both the static and dynamic properties of the antenna may influence sensory acquisition during quasi-static and dynamic sensorimotor tasks. Further elucidation of antennal mechanical tuning will lead to new hypotheses, integrating distributed mechanosensory inputs from a dynamic sensory appendage operating on a moving body. During rapid escape from predators, the neuromechanical system of animals is pushed to operate closer to its limits. When operating at such extremes, small animals are true escape artists benefiting from enhanced maneuverability, in part due to scaling. In Chapter 4, I show a novel neuromechanical strategy used by the cockroach P. americana and the gecko H. platyrus which may facilitate their escape when encountering a gap. Both species ran rapidly at 12-15 body lengths-per-second toward a ledge without braking, dove off the ledge, attached their feet by claws like a grappling hook, and used a pendulum-like motion that can exceed one meter-per-second to swing around to an inverted position under the ledge, out of sight. In cockroaches, I show that the behavior is mediated by a rapid claw-engagement reflex initiated during the fall. Finally, I show how the novel behavior has inspired the design of a small, hexapedal robot that can assist rescuers during natural and human-made disasters.",ucb,,https://escholarship.org/uc/item/1b79g7xw,,,eng,REGULAR,0,0
389,1825,Bacterial indole-3-acetic acid production: a key mediator of plant-microbe interactions between Phaseolus vulgaris and the foliar epiphyte Pantoea agglomerans 299R,"Powell, Tracy Kathleen","Lindow, Steven E;",2011,"The phyllosphere epiphyte Pantoea agglomerans 299R synthesizes indole-3-acetic acid (IAA), an important plant hormone. IAA production was previously shown to confer a small but significant fitness advantage to Pa299R cells inoculated onto bean (Phaseolus vulgaris) leaves, but the mechanism by which bacterial IAA exerts this effect is unknown. In this work, we investigated several hypotheses regarding how bacterial IAA enhances the growth and survival of leaf epiphytic microbes such as Pa299R.We first tested the hypothesis that bacterial IAA enhances the availability of plant sugars to phyllosphere bacteria, thereby relieving carbon limitation of bacterial growth on the leaf surface (Chapter Two). We inoculated sucrose- and fructose-inducible biosensor bacteria onto bean leaves, and investigated the effects of auxin availability on sugar sensing. We manipulated auxin availability in two ways: first, we compared sugar sensing by wild-type Pa299R (IAA+) biosensors to that of the isogenic, IAA- mutant PaMX149; and second, we compared sugar sensing by PaMX149 bacteria inoculated with exogenous NAA to bacteria inoculated without NAA. The presence of auxin--whether endogenously synthesized or exogenously applied--was associated with a significant decrease in sucrose sensing, and a small but significant increase in fructose sensing. Additionally, in vitro assays demonstrated that Pa299R initiates growth more rapidly when using glucose or fructose as a sole carbon source than when using sucrose. Together, this suggests that IAA biosynthesis may function as a resource conversion strategy to enhance carbon acquisition and rapid bacterial growth in an oligotrophic, environmentally dynamic phyllosphere.In addition to its role in carbon acquisition, we also investigated several alternative hypotheses of bacterial IAA production in the phyllosphere (Chapter Three). We demonstrated that bacterial IAA exerts no measurable impact on resistance of Pa299R to environmental stress. We also found that bacterial IAA does not affect the autofluorescence of substrate leaf epidermal cells that support large bacterial aggregates of Pa299R. Finally, we describe unsuccessful efforts to determine the role of bacterial IAA in another foliar epiphyte and compatible bean pathogen, Pseudomonas syringae pv. syringae B728a.",ucb,,https://escholarship.org/uc/item/1bf1b5m3,,,eng,REGULAR,0,0
390,1826,Supranational Citizenship: (Im)mobility and the Alternative Birth Movement in Mexico,"Vega, Rosalynn Adeline","Scheper-Hughes, Nancy;Briggs, Charles L;",2016,"My research analyzes how the remaking anew of traditionâ€”the return toâ€œtraditionalâ€ birthing arts (home birth, midwife-assisted birth, water birth, â€œnaturalâ€ birth)â€”has resulted in the commodification of indigenous culture and the re-inscription of racial inequalities on the one hand, and, despite feminist rhetoric about womenâ€™s liberation from (masculine) biomedical hegemony, the reconfiguring of parent-child bonds in ways that again place the burden of correctly producing future bioconsumers on womenâ€™s shoulders. I focus on the extremes of contemporary Mexican societyâ€”disenfranchised indigenous families and members of the global meritocracyâ€”and in doing so, I demonstrate how citizenship retains value for some while being rendered an inadequate analytical frame for others. More specifically, I argue that the privileged do not position themselves as citizens through claims to public resources; instead, they accumulate cultural capital through privatized services. Through an examination of processes of racialization and patterns of bioconsumption, I critique the broad application of the concept of citizenship, and make a case for the consideration of the bioconsumer (individuals for whom market-based consumption of medical services plays a formative role in how their identities are syncretically portrayed and perceived). The main stakes of bioconsumption are the presentation of self and accrual of cultural capital. Furthermore, I demonstrate how what is under negotiation in the alternative birth movement is the social-moral body onto which identities get mapped. Close ethnographic study reveals how the so-called â€œhumanizationâ€ of birth and the reduction of maternal and infant mortality are distant projects that are collapsed onto one another and produce an emerging ideology of â€œgood parenthood.â€ In this ideology, children represent parentsâ€™ stake in the contemporary global meritocracy. This work therefore uncovers how traditional ways of birthing are being destroyed and reinvented through racialized class privilege, which is based on a model of neoliberal consumption that simultaneously promotes â€œhumanityâ€ and reinforces inequality by infusing transnational movements with reified class logics and racialization.  I thus consider Mexican traditional midwivery as a unique lens for examining how indigeneity becomes an object of consumption via ethnomedical piracy within a transnational racialized economy.  Through 28 months of in-depth, multi-sited research across Mexico, from October 2010 to November 2013, this dissertation analyzes the physical and social mobility of some individuals, and the relative immobility of others, through the lens of humanized birth. In writing this dissertation, I aim to make the following interventions:First, I disrupt notions of citizenship-making by placing under the same lens those for whom citizenship is always just out of reach and those for whom citizenship is not a concern as their privileged access to privatized markets allows for a supra-state existence. I offer the concept of â€œsupranationalismâ€ as a contribution to emerging literature that rethinks states as theÂ consolidationÂ of territory and government, thus opening up other ways of conceptualizing polity and geography.  I use the topic of birth to provide ethnographic evidence of how biopower is not only imposed by states upon citizen-subjects; but by powerful, extra-governmental, social and economic forces operating in the context of neoliberalism, thus resulting in real material consequences and shaping health outcomes.Â  Second, I deploy the concepts of racialization and power when I critique the ways in which the global alternative birth movement inadvertently appropriates and commodifies indigenous culture. When â€œindigeneityâ€ is invoked in the realm of so-called â€œhumanizedâ€ birth, the object is fetishized, separated entirely from its cultural, socioeconomic, and geographical context, and repackaged for popular consumptionâ€”leading me to rethink the relationship between neoliberal citizenship and consumerism. I use the example of midwifery in Mexico to examine racialized identities-turned-merchandise, with real effects for the bodies of women. Building upon studies that explore the political economy of the body under contemporary global capitalism, I use a transnational context to analyze the political economy of identities vis-Ã -vis the body.In this dissertation, I examine the disparate and unequal distribution of â€œtraditional,â€ and ethnomedical forms of â€œnaturalâ€ birth among social collectivities. The dissertation examines the mobility of humanized birth practitioners and participants who travel across borders to contribute to ideology and practices being produced transnationally, while comparatively immobile women are socially situated in ways that preclude their participation in medical migrations. Thus, while my ethnographic research provides detailed examples of how humanized birth is reshaped and reconstituted in sites that bear stark contrast to the social and geographic locations where the humanized birth model was originally produced, I am more concerned with how politic economic terrains are not only traversed, but are themselves transformed by medical migration. Finally, I complexify notions of feminist liberation by asking how humanized birth may be the first step within a new regime of pressures and â€œrequirementsâ€ presented by modern-day â€œgood parenting.â€ I resist viewing children only as commodities; however, I do argue that children represent parentsâ€™ stake in our contemporary meritocracyâ€”a system that naturalizes extreme inequality by allowing us to believe in democratic structures and the idea that education and proper preparation will open doors for children to a brilliant future.Â  Furthermore, I suggest that meritocratic structures exert pressure in the womb.Â  The difference between a privatized and public childhood begins in vitro with prenatal care.",ucb,,https://escholarship.org/uc/item/1bx9c6p8,,,eng,REGULAR,0,0
391,1827,The Role of the Underground Economy in Social Network Spam and Abuse,"Thomas, Kurt","Paxson, Vern;",2013,"Online social networks have emerged as real-time communication platforms connecting billions of users around the globe. Implicit to the interactions within an online social network is the notion of trust; users create relationships with their friends and valued media outlets, in turn receiving access to content generated by each relationship. This trust however comes with a price. On the heels of the widespread adoption of online social networks, scams, phishing, and malware attacks conducted by criminals have become a regular occurrence. Such attacks exploit the trust users place in their relationships and the integrity of information found in online social networks. The threat criminals pose to online social networks is exacerbated by the emergence of an underground economy---a digital network of criminals who buy and sell goods that directly enable the abuse of online social networks. Such services empower other miscreants to penetrate online social networks and engage with victims, while at the same time abstracting away the complexities of circumventing existing protection mechanisms employed by online social networks to hinder spam and abuse. In this dissertation, we empirically analyze in both breadth and depth the range of threats currently targeting online social networks through the lens of Twitter. We map out the support infrastructure that is critical to online social network abuse, characterize the tools and techniques used to disseminate malignant content, and evaluate how such attacks ultimately realize a profit for the attackers involved. In the process, we argue that the for-profit infrastructure provided by the underground economy in the form of fake accounts and affiliate programs has become a fundamental weak point of abuse. Defenders should concentrate their efforts on disrupting these resources rather than fighting the subsequent, multifaceted abuse it enables such as scams, phishing, malware, and political attacks. To aid in this effort, we develop two new strategies for preventing abuse in social networks. Our first defense identifies abusive links in online social networks (or any web service) before they are distributed to recipients. At its heart, this technique identifies common HTML content generated by affiliate programs and criminal hosting infrastructure which act as a buttress for the abuse ecosystem. Our second defense relies on directly engaging with the underground economy that fuels online social network abuse to understand how millions of fake accounts are registered in an automated fashion. We leverage this understanding to detect abusive accounts at the time of their registration, preventing criminals from ever interacting with the legitimate users of online social networks.In summary, this dissertation provides a data-driven analysis of spam and abuse on Twitter. We demonstrate that existing solutions for protecting online social networks fail to protect the millions of users that now rely on the technology as a global communication platform, exposing users to scams, phishing, malware, and even political censorship. By adopting the solutions presented in this dissertation, online social network operators can effectively defend both the ingress points of abuse---fraudulent and compromised accounts---and the egress points of abuse---spam links that direct victims to spamvertised products, fake software, clickfraud, banking theft, and malware that converts a victim's machine into a commodity for the underground economy. Such solutions afford online social network providers an opportunity to strike at the critical infrastructure that criminals rely on in order to monetize and abuse online social networks.",ucb,,https://escholarship.org/uc/item/1cs1x8pw,,,eng,REGULAR,0,0
392,1828,Tuning Hardware and Software for Multiprocessors,"Mohiyuddin, Marghoob","Wawrzynek, John;",2012,"Technology scaling trends have enabled the exponential growth of computing power. However, the performance of communication subsystems scales less aggressively. This means that an application constrained by memory/interconnect performance will not be able to use the available computing power efficiently---in fact, technology scaling will make this efficiency even worse. This problem can be alleviated if algorithms minimize communication. To this end, we describe communication-avoiding algorithms and highly optimized implementations of a sparse linear algebra kernel called ``matrix powers''. Results show up to 2.3x improvement in performance over the naive algorithms on modern architectures. Our multi-core implementation of matrix powers enables us to develop a communication-avoiding iterative solver for sparse linear systems which is up to 2.1x faster than a conventional Generalized Minimal Residual method (GMRES) implementation. Another problem plaguing the supercomputer industry is the power bottleneck---power has, in fact,  become the pre-eminent design constraint for future high-performance computing systems which is why computational efficiency is being emphasized over simply peak performance. Static benchmark codes have traditionally been used to find architectures optimal with respect to specific metrics. Unfortunately, because compilers generate sub-optimal code, benchmark performance can be a poor indicator of the performance potential of architecture design points. Therefore, we present hardware/software co-tuning as a novel approach for system design. In co-tuning, traditional architecture space exploration is tightly coupled with software auto-tuning for delivering substantial improvements in area and power efficiency. We demonstrate co-tuning by exploring the parameter space of a Tensilica's Xtensa-based multi-processor running three of the most heavily used kernels in scientific computing, each with widely varying micro-architectural requirements: sparse matrix vector multiplication, stencil-based computations, and general matrix-matrix multiplication. Resultsdemonstrate that co-tuning improves hardware area and power efficiency by up to 3x and 2.4x respectively.",ucb,,https://escholarship.org/uc/item/1dg4w04b,,,eng,REGULAR,0,0
393,1829,"Immigration through Education: The Interwoven History of Korean International Students, US Foreign Assistance, and Korean Nation-State Building","Cho, Jane Jangeun","Klein, Kerwin L.;",2010,"This dissertation identifies Korean international students as immigrants, as conduits of knowledge transfer, and as agents of change.  Part of the American Cold War policy was to establish Korea's higher educational institutions with a core group of US-educated people.  Figuring prominently in this story is the US government's use of foreign assistance as a diplomatic tool to build its influence abroad.  The Korean government readily accepted the aid but imprinted its designs on the American blueprint to reflect its own goal of building a modern nation-state.  American universities under contract with the US government assisted the redesign of key departments at Seoul National University (SNU) and the establishment of Korea Advanced Institute of Science (KAIS).  Planned as model universities or paradigms for other Korean institutes of higher education, both national institutes became the standard bearers of ""modern"" knowledge.   Both projects favored US-educated Koreans.  To this end, the majority of the faculty members in the departments selected for restructuring at SNU was sent to the US to be trained and the overwhelming majority of KAIS' inaugural faculty members held doctoral degrees from the United States.  The benefits and prestige associated with an American education in the Korean society contributed to a positive cultural representation of the US as a whole.  This caused a growing number of Koreans to immigrate to the US to pursue their studies.  These international students were central to Korean American immigration.  They were information brokers, the first links to chain migration, and contributors to the changing racial and ethnic make-up of the American population in the twentieth century.",ucb,,https://escholarship.org/uc/item/1dn32819,,,eng,REGULAR,0,0
394,1830,"Corporation, People, and Government: A Look at the Rise of the Waste Management Corporation from Rural California to the Rest of the World","Asmatey, Yalda","Nader, Laura;",2013,"This research project is a study of Kettleman City, California, home to the largest Class I toxic waste dump in the western United States, owned and operated by the public corporation Waste Management Inc. (WMI).  The story of Kettleman City is a cautionary tale of hubris that warns of the consequences of the complete disregard for the natural environment and the tolerance for corporation's profit-generating schemes that harm human health and the ecosystem. Divided into three parts, the project expands scholarship on the anthropology of disaster, the study of corporations in the United States within a framework of environmental justice, and the controlling processes underlying the dominant paradigms. The first part of the dissertation examines government and corporate neglect and acquiescence to the incremental degradation and devastation of California's environment since the mid-nineteenth century involving the displacement and extermination of Native Americans and the Tulare Lake Basin, the killing and contamination of migratory birds in the Kesterson Wildlife Refuge, and the corruption and power of the agricultural industry.  This history lays the groundwork for WMI selecting Kettleman City as the site for the largest toxic waste dump west of the Mississippi.  Since the 1990s, the town's largely Latino population has been fighting against the dump and for the environmental safety of its residents, and many credit them for launching the environment justice movement in the western United States. Using ethnographic and archival methods, I examine the history of Kettleman City and the opposition of its residents to the waste facility and the recent discovery of elevated rates of birth defects and infant deaths since 2007. In the second part of this study I examine how our corporatized, industrial society has made landfills and other environmental injustices permanent fixtures in our society and how, as consumers, we have become conditioned to disregard waste as anything but normal. This research complicates the categorization of what constitutes a ""disaster"" and finds that not only are landfills certain to cause serious future catastrophes, but that unlike other disasters that are abrupt and uncontrollable, landfill disasters are avoidable ""ticking time bombs."" We accept them because of ideological convictions supported by science, technology, and government oversight that evidently accepts accidents, spills, and site contaminations as natural, inevitable or necessary byproducts.  I document the long list of serious violations at the WMI facility in Kettleman City and the subsequent fines levied by regulators and out-of-court settlement deals. In short, I argue that our history and culture has created a society with a high tolerance for corporate environmental degradation for the perceived benefit of economic progress. In the final part of this study I unveil the socio-historical, political, and economic processes responsible for a culture of wastefulness.  In the nineteenth century, Americans were aware of what they used and purchased. Americans valued thrift, and since recycling was very common, people produced little waste. The Industrial Revolution led to the rise of corporations by the twentieth century and to a growing advertising industry that promoted hyper-consumption. This, in turn, created demand for the waste hauling and disposal industry. I document how WMI, a multibillion dollar transnational corporation, by the 1980s and 1990s had grown into a powerful institution, and now maintains a monopoly over the American waste industry and beyond.  I explore the history of WMI and examine how the company has successfully influenced political, economic, and cultural spheres of American society to build and sustain its empire.  WMI's success lies in its power to influence public perceptions of waste and waste services: a power that has gone unquestioned for far too long.",ucb,,https://escholarship.org/uc/item/1f495960,,,eng,REGULAR,0,0
395,1831,Induction of lignocellulose degrading enzymes in Neurospora crassa by cellodextrins,"Znameroski, Elizabeth A.","Cate, Jamie HD;Glass, N. Louise;",2012,"Neurospora crassa colonizes burnt grasslands in the wild and metabolizes both cellulose and hemicellulose from plant cell walls. When switched from a favored carbon source such as sucrose to cellulose, N. crassa dramatically upregulates expression and secretion of a wide variety of genes encoding lignocellulolytic enzymes. However, the means by which N. crassa and other filamentous fungi sense the presence of cellulose in the environment remains unclear. Here, I show that a N. crassa mutant carrying deletions of two genes encoding predicted extracellular Î²-glucosidase enzymes and one intracellular Î²-glucosidase enzyme (d3Î²G) lacks Î²â€glucosidase activity, but efficiently induces cellulase gene expression and cellulolytic activity in the presence of cellobiose as the sole carbon source. These data indicate that cellobiose, or a modified version of cellobiose, functions as an inducer of lignocellulolytic gene expression and activity in N. crassa. In addition, I have identified two cellodextrin transporters involved in sensing cellulose. A N. crassa mutant carrying deletions for both transporters is unable to induce cellulase gene expression in response to crystalline cellulose. Furthermore, a mutant lacking Î²- glucosidase enzymes and transporters (d3Î²GdT) does not induce cellulase gene expression in response to cellobiose.",ucb,,https://escholarship.org/uc/item/1fx3x04k,,,eng,REGULAR,0,0
396,1832,"Elite Theban Women of the Eighth-Sixth Centuries BCE in Egypt: Identity, Status and Mortuary Practice","Li, Jean","Redmount, Carol A;",2010,"The roles and status of women in ancient Egyptian society remain imperfectly defined particularly in the Third Intermediate and Late Periods.  Egyptology has generally examined women from the perspective of fertility and sexuality, thus defining the social roles of women as wives and mothers who derived their status from their male associations.  This dissertation discusses women's roles by investigating the ways in which elite Theban women constructed and displayed their identities in their mortuary practices during the eighth-sixth centuries BCE (Dynasties 22/25-Dynasty 26). In Thebes, the archaeological remains of the eighth-sixth centuries demonstrate conspicuous identity displays by men, but where and how women fit into this period of ""big personalities"" has not been analyzed in detail. This dissertation argues that the eighth-sixth centuries BCE was not a time of decay, as it is traditionally characterized in Egyptology, but instead a dynamic era in which its cultural products, especially mortuary practices, exhibited a creative tension between tradition and innovation.  Identity construction by the ancient Egyptians during a time of rapid socio-political change is manifested in this tension of tradition and innovation. Women featured prominently in the innovations of cultural practices such as kingship, religion, art and mortuary practices, which suggest that they fully participated in the societal-wide preoccupation of identity construction. Therefore, the eighth-sixth centuries BCE provides a rare opportunity to examine the nuances of elite female identity constructions.The material evidence for elite Theban women derives primarily from mortuary contexts.  Therefore, this dissertation uses the mortuary practices of elite Theban women in the eighth-sixth as its evidentiary core.  Relevant mortuary evidence was compiled into two databases: the Tombs and Contents corpus and the Funerary Objects corpus.  The first contains information on the Theban tombs and their contents that attested to the presence of women or belonged to women.  The Funerary Objects corpus contains information on unprovenanced mortuary objects belonging to women that are attributed stylistically to Thebes.  The information in these databases was analyzed for patterns in the allocation of titles, the spatiality of tombs and distribution and type of funerary objects. Furthermore, this project used different theoretical lenses of memory, landscape, gender and identity to analyze elite female mortuary practices in Thebes.  The application of these theoretical lenses to the mortuary data revealed the ways elite women created and displayed important elements of their status and identity in death. The results of the holistic analysis of elite female mortuary practices reveal that elite Theban women of the eighth-sixth centuries operated as active agents to more forcefully express their identities, especially status, albeit within the traditional societal modes and boundaries.  Elite female strategies of identity construction were polysemic and complex.  Elite female mortuary practices suggest, that, in contrast to traditional Egyptological understanding of women, elite Theban women of the eighth-sixth centuries did not derive their status and identity solely from their male relatives.  Instead, their burial practices often reveal a concern with their own status independent of male associations. Elite Theban women's concern for the display of their identity independent of men has implications for a number of issues concerning the social status of women in ancient Egypt, including the issue of mandatory celibacy of women in the Amen clergy. Another implication of this work is that Egyptology needs to expand beyond traditional frameworks of gender when analyzing women. By analyzing groups of women in their individual historical and socio-cultural contexts, this dissertation expands discussions of ancient Egyptian women beyond the monolithic categories of mother and wife. The archaeological analysis of the burial practices of elite Theban women of the eighth-sixth centuries suggests that ancient Egyptian women were active participants and contributors in societal trends of identity constructions. Elite female strategies of identity construction demonstrate complexities of identity conceptions by women that extend beyond the traditional scholarly characterizations that developed women's identities solely by reference to men.",ucb,,https://escholarship.org/uc/item/1gh4d9j2,,,eng,REGULAR,0,0
397,1833,Times of the Event: On the Aesthetico-Political in West Germany and Austria circa 1968,"Weiner, Andrew Stefan","Silverman, Kaja;",2011,"In what ways do the cultural fields of aesthetics and politics transform each other, and how does this relation change under specific historical circumstances? How is it possible to evaluate the history of activities that occur at the intersection of aesthetics and politics? How can we account for the way in which such practices are able to alter the very criteria by which they might be recognized or judged? This dissertation responds to such questions by tracing the emergence of a distinct field of cultural production in West Germany and Austria in the years leading up to and through 1968, designating this field as ""aesthetico-political."" It seeks to determine the various conditions of possibility for these emergent forms of activity, and to examine the different models of political agency, aesthetic experience, and subjectivation they proposed. The analysis focusses on four distinct transformations: politicizations of the aesthetic (a term including but not limited to the arts); aestheticizations of politics, especially through the mediatization of the public sphere; the reorientation of artistic practices away from modernist models; and the ascendance of New Left movements. The dissertation's test cases primarily concern activities that I term ""events"": durational occurrences taking place in the space between aesthetics and politics.",ucb,,https://escholarship.org/uc/item/1gv4c1cd,,,eng,REGULAR,0,0
398,1834,Charge transport in metal oxide nanocrystal-based materials,"Runnerstrom, Evan Lars","Xu, Ting;Milliron, Delia;",2016,"There is probably no class of materials more varied, more widely used, or more ubiquitous than metal oxides. Depending on their composition, metal oxides can exhibit almost any number of properties. Of particular interest are the ways in which charge is transported in metal oxides: devices such as displays, touch screens, and smart windows rely on the ability of certain metal oxides to conduct electricity while maintaining visible transparency. Smart windows, fuel cells, and other electrochemical devices additionally rely on efficient transport of ionic charge in and around metal oxides. Colloidal synthesis has enabled metal oxide nanocrystals to emerge as a relatively new but highly tunable class of materials. Certain metal oxide nanocrystals, particularly highly doped metal oxides, have been enjoying rapid development in the last decade.  As in myriad other materials systems, structure dictates the properties of metal oxide nanocrystals, but a full understanding of how nanocrystal synthesis, the processing of nanocrystal-based materials, and the structure of nanocrystals relate to the resulting properties of nanocrystal-based materials is still nascent. Gaining a fundamental understanding of and control over these structure-property relationships is crucial to developing a holistic understanding of metal oxide nanocrystals. The unique ability to tune metal oxide nanocrystals by changing composition through the introduction of dopants or by changing size and shape  affords a way to study the interplay between structure, processing, and properties.This overall goal of this work is to chemically synthesize colloidal metal oxide nanocrystals, process them into useful materials,  characterize charge transport in materials based on colloidal metal oxide nanocrystals, and develop ways to manipulate charge transport. In particular, this dissertation characterizes how the charge transport properties of metal oxide nanocrystal-based materials depend on their processing and structure. Charge transport can obviously be taken to mean the conduction of electrons, but it also refers to the motion of ions, such as lithium ions and protons. In many cases, the transport of ions is married to the motion of electrons as well, either through an external electrical circuit, or within the same material in the case of mixed ionic electronic conductors. The collective motion of electrons over short length scales, that is, within single nanocrystals, is also a subject of study as it pertains to plasmonic nanocrystals. Finally, charge transport can also be coupled to or result from the formation of defects in metal oxides. All of these modes of charge transport in metal oxides gain further complexity when considered in nanocrystalline systems, where the introduction of numerous surfaces can change the character of charge transport relative to bulk systems, providing opportunities to exploit new physical phenomena.Part I of this dissertation explores the combination of electronic and ionic transport in electrochromic devices based on nanocrystals. Colloidal chemistry and solution processing are used to fabricate nanocomposites based on electrochromic tin-doped indium oxide (ITO) nanocrystals.  The nanocomposites, which are completely synthesized using solution processing, consist of ITO nanocrystals and lithium bis(trifluoromethylsulfonyl)amide (LiTFSI) salt dispersed in a lithium ion-conducting polymer matrix of either poly(ethylene oxide) (PEO) or poly(methyl methacrylate) (PMMA). ITO nanocrystals are prepared by colloidal synthetic methods and the nanocrystal surface chemistry is modified to achieve favorable nanocrystal-polymer interactions. Homogeneous solutions containing polymer, ITO nanocrystals, and lithium salt are thus prepared and deposited by spin casting. Characterization by DC electronic measurements, microscopy, and x-ray scattering techniques show that the ITO nanocrystals form a complete, connected electrode within a polymer electrolyte matrix, and that the morphology and properties of the nanocomposites can be manipulated by changing the chemical composition of the deposition solution. Careful application of AC impedance spectroscopy techniques and DC measurements are used to show that the nanocomposites exhibit mixed ionic and electronic conductivity, where electronic charge is transported through the ITO nanocrystal phase, and ionic charge is transported through the polymer matrix phase. Additionally, systematic changes in ionic and electronic conductivity with morphology are measured. The synthetic methods developed here and understanding of charge transport ultimately lead to the fabrication of a solid state nanocomposite electrochromic device based on nanocrystals of ITO and cerium oxide.Part II of this dissertation considers electron transport within individual metal oxide nanocrystals themselves. It primarily examines relationships between synthetic chemistry, doping mechanisms in metal oxides, and the accompanying physics of free carrier scattering within the interior of highly doped metal oxide nanocrystals, with particular mind paid to ITO nanocrystals. Additionally, synthetic methods as well as metal oxide defect chemistry influences the balance between activation and compensation of dopants, which limits the nanocrystals' free carrier concentration. Furthermore, because of ionized impurity scattering of the oscillating electrons by dopant ions, scattering must be treated in a fundamentally different way in semiconductor metal oxide materials when compared with conventional metals. Though these effects are well understood in bulk metal oxides, further study is needed to understand their manifestation in nanocrystals and corresponding impact on plasmonic properties, and to develop materials that surpass current limitations in free carrier concentration and mobilities. In particular, efforts to address these limitations by developing new nanocrystal materials (with careful consideration of structure-property relationships) are described. Synthetic control of nanocrystal shape is also explored. Each of these topics have implications in determining the properties of localized surface plasmon resonances (LSPRs) in these nanocrystals. Part II culminates as the defect chemistry of metal oxides is identified as a major factor influencing LSPR and charge transport in doped metal oxide nanocrystals. Aliovalent dopants and oxygen vacancies act as centers for ionized impurity scattering of electrons, and this electronic damping leads to lossy, broadband LSPR with low quality factors, limiting applications that require near field concentration of light. However, the appropriate dopant can mitigate ionized impurity scattering. Herein, the synthesis and characterization of a novel doped metal oxide nanocrystal material, cerium-doped indium oxide (Ce:In2O3) is described. Ce:In2O3 nanocrystals display tunable mid-infrared LSPR with exceptionally narrow line widths and the highest quality factors observed for nanocrystals in this spectral region. Drude model fits to the spectra indicate that a drastic reduction in ionized impurity scattering is responsible for the enhanced quality factors, and high electronic mobilities reaching 33 cm2/Vs are measured optically, well above the optical mobility for ITO nanocrystals. The microscopic mechanisms underlying this enhanced mobility are investigated with density functional theory calculations, which suggest that scattering is reduced because cerium orbitals do not hybridize with the In orbitals that dominate the bottom of the conduction band. Ce doping may also reduce the equilibrium oxygen vacancy concentration, further enhancing mobility. Absorption spectra of single Ce:In2O3 nanocrystals are used to determine the dielectric function, and simulations predict strong near field enhancement of mid-IR light, especially around the vertices of Ce:In2O3 nanocubes.Part III examines how the defect chemistry of metal oxides can be used to manipulate not only electronic transport, but also ionic transport in materials that are relevant for high temperature electrochemistry. Over the past few years, the observation of unexpected but significant proton conductivity in porous, nanocrystalline ceramics has generated substantial scientific interest mirroring the excitement surrounding ionic conduction in other nanostructured or porous materials. Numerous studies, to varying degrees of success, have attempted to describe or control the mechanisms that enable proton motion in nanocrystalline ceramics. Here, colloidally synthesized ceramic nanocrystals of cerium oxide (CeO\textsubscript{2}) and titanium oxide (TiO\textsubscript{2}) are utilized to systematically study how grain size, microporosity, and composition influence proton conduction. By measuring the temperature-dependent impedance of porous thin films of these nanocrystals under dry and wet atmospheres, it was found that both CeO2 and TiO2 display significant proton conductivity at intermediate temperatures between 100C and 350C. Furthermore, oxygen activity strongly impacts proton transport; using oxygen as a carrier gas drastically reduced the proton conductivity by up to 60 times. Together, these results suggest that the most likely source of mobile protons in these systems is dissociative adsorption of water at surface oxygen vacancies, with composition, nanocrystal size, and oxide defect equilibria influencing the surface activity toward this reaction and hence the proton conductivity.",ucb,,https://escholarship.org/uc/item/1h2907bc,,,eng,REGULAR,0,0
399,1835,"Block Copolymer Electrolytes: Thermodynamics, Ion Transport, and Use in Solid-State Lithium/Sulfur Cells","Teran, Alexander Andrew","Balsara, Nitash P;",2013,"Nanostructured block copolymer electrolytes containing an ion-conducting block and a modulus-strengthening block are of interest for applications in solid-state lithium metal batteries. These materials can self-assemble into well-defined microstructures, creating conducting channels that facilitate ion transport.  The overall objective of this dissertation is to gain a better understanding of the behavior of salt-containing block copolymers, and evaluate their potential for use in solid-state lithium/sulfur batteries.  Anionically synthesized polystyrene-b-poly(ethylene oxide) (SEO) copolymers doped with lithium bis(trifluoromethanesulfonyl)imide (LiTFSI) salt were used as a model system.  This thesis investigates the model system on several levels: from fundamental thermodynamic studies to bulk characterization and finally device assembly and testing.First, the thermodynamics of neat and salt-containing block copolymers was studied. The addition of salt to these materials is necessary to make them conductive, however even small amounts of salt can have significant effects on their phase behavior, and consequently their ion-transport and mechanical properties.  As a result, the effect of salt addition on block copolymer thermodynamics has been the subject of significant interest over the last decade. A comprehensive study of the thermodynamics of block copolymer/salt mixtures over a wide range of molecular weights, compositions, salt concentrations and temperatures was conducted. The Flory-Huggins interaction parameter was determined by fitting small angle X-ray scattering data of disordered systems to predictions based on the random phase approximation (RPA).    Experiments on neat block copolymers revealed that the Flory-Huggins parameter is a strong function of chain length.  Experiments on block copolymer/salt mixtures revealed a highly non-linear dependence of the Flory-Huggins parameter on salt concentration.  These findings are a significant departure from previous results, and indicate the need for improved theories for describing thermodynamic interactions in neat and salt-containing block copolymers.Next, the effect of molecular weight on ion transport in both homopolymer and copolymer electrolytes were studied over a wide range of chain lengths.  Homopolymer electrolytes show an inverse relationship between conductivity and chain length, with a plateau in the infinite molecular weight limit.  This is due to the presence of two mechanisms of ion conduction in homopolymers; the first mechanism is a result of the segmental motion of the chains surrounding the salt ions, creating a liquid-like environment around the ion while the second mechanism of ion conduction is attributed to diffusion of the entire polymer chain with coordinated ions.  Equilibrated block copolymer electrolytes exhibit a non-monotonic dependence on molecular weight, decreasing with increasing molecular weight in the small molecular weight limit before increasing when molecular weight exceeds about 10 kg mol-1The effect of morphology on ion transport was studied by conducting simultaneous impedance and X-ray scattering experiments as the block copolymer electrolyte transitioned from an ordered lamellar structure to a disordered phase.  The ionic conductivity increased discontinuously through the transition from order to disorder.  A simple framework for quantifying the magnitude of the discontinuity was presented.  Finally, block copolymer electrolytes were examined specifically for use in high energy density solid state lithium/sulfur batteries.  Such materials have been shown to form a stable interface with lithium metal anodes, maintain intimate contact upon cycling, and have sufficiently high shear moduli to retard dendrite formation.  Having previously satisfied the concerns associated with the lithium metal anode, the compatibility of the sulfur cathode was explored.  The sulfur cathode presents many unique challenges, including the generation of soluble lithium polysulfides (Li2Sx, 2 â‰¤ x â‰¤ 8) during discharge.  The solubility of such species in block copolymers and their effect on morphology was examined.  The lithium polysulfides were found to exhibit similar solubility in the block copolymers as in typical organic electrolytes, however induced unusual and unexpected phase behavior in the block copolymers. Inspired by successful efforts to physically confine the soluble lithium polysulfides via nanostructured carbon-sulfur composites in the cathode, our nanostructured block copolymer electrolytes were employed in full electrochemical cells with a lithium metal anode and sulfur cathode.  Different cathode compositions, electrolyte additives, and cell architectures were tested.  Surprisingly, the polysulfides diffused readily from the cathode through the block copolymer electrolyte, and the normally robust SEO|Li metal interface was detrimentally affected their presence during cycling. The polysulfides appeared to change the mechanical properties of the electrolyte such that intimate contact with the lithium metal was lost.  Several promising strategies to overcome this problem were investigated and offer exciting avenues for improvement for future researchers.",ucb,,https://escholarship.org/uc/item/0jg0n51n,,,eng,REGULAR,0,0
400,1836,"In the Public's Interest: Evictions, Citizenship and Inequality in Contemporary Delhi","Bhan, Gautam","Roy, Ananya;",2012,"Millennial Delhi is a city whose landscape has been scarred by a series of evictions of the homes of some of its most vulnerable citizens. These evictions are different not just in degree but in kind from those that have come before. Evictions at this scale last occurred in Delhi during what is known as the Emergency from 1975-77 when democratic and fundamental rights were suspended. Unlike evictions within the Emergency, however, contemporary evictions have occurred through democratic processes rather than in their absence- they mark a different set of negotiations, legitimations, processes as well as horizons of resistance. A further factor makes contemporary evictions distinct: they were ordered not by the sarkar -the institutions of the executive across local, state and federal scales that govern the national capital - but by the adalat, the Judiciary. They were, in fact, ordered by the Delhi High Court and the Supreme Court of India within a unique judicial innovation in India called the Public Interest Litigation that had been established, ironically, to enable the poor to access justice in the highest courts of the land. To understand how the evictions of the poor can be read as acts in the ""public interest,"" this dissertation argues that we must first locate the basti in the particularity of the production of space in Delhi. The Hindustani word ""basti"" comes from basna which means to settle or inhabit. It is the term used most often by the poor to describe their homes that are often marked by some measure of physical, economic, and infrastructural vulnerability. The basti is often reduced to the slum, a marker of illegal occupation of land and, more broadly, the dysfunctional landscape of the megacities of the global South. Yet this dissertation argues that more than just a `slum,' built environment, material housing stock, or planning category, a basti is, in fact, a territorialisation of a political engagement within which the poor negotiate their presence in as well as right to the city. It is a spatial manifestation of the negotiations of citizenship. Its eviction then represents not just the demolition of a built environment but the transformation of precisely this political engagement- an erasure of the poor's presence within and right to the city. Put another way, contemporary evictions represent an altered urban politics where a set of familiar referents- development, order, governance, citizens, and the public- are redefined to not only enable evictions but also to see them as acts of good governance, order and planning. Read this way, evictions allow us to access the central theoretical and ethical concern of this dissertation: the politics of the production and reproduction of poverty and inequality in the contemporary Indian city and the negotiations of citizenship that underlie it. Broadly, this dissertation argues that evictions make visible make visible a juridicalisation of politics in the Indian city. This juridicalisation is marked by the emergence of new frameworks, discourses and practices in urban politics that instantiate themselves in the city through the judiciary rather within the more familiar institutional compacts between institutions of representative government and urban residents. The juridicalisation of politics marks the expansion of the jurisdiction not just of the courts but also of the realm of the law within urban politics. As the sphere of authority of the Courts widens in the city, a series of questions, concerns, interventions, processes and debates within urban politics come to be come to seen, articulated, and addressed as juridical questions - they speak and are spoken about within the frameworks of law. Following its concern with the politics of poverty, inequality and citizenship, the dissertation traces juridicalisation along one particular vector: it shows how evictions were made to make ""legal sense"" within public interest litigations. Four key frameworks thus emerge: (a) planned illegalities; (b) planned development and/as crisis; (b) the impoverishment of poverty; and (c) the juridicalisation of resistance. The dissertation first constructs a spatial history of inhabitation in the city to challenge the assumed relationships between ""illegality,"" planning and the settlements of the poor, arguing that the ""illegal"" production of urban space in Delhi comprises not just the `slum' but the production of illegal housing by the middle and upper middle classes as well. It does so by problematizing the familiar and commonsensical narrative of the ""failure of planning"" in the Indian city and showing that the traces of planning ensure that the city may not be as it was planned but it is an outcome of planning. It argues that illegality is the dominant mode of the production of housing in Delhi and that it is within illegalities that the production of urban space in the city must be understood. Questions of urban politics must thus look not at the dichotomy of the legal-illegal but instead at the ways in which planning and planned development produce illegality. Equally, they must interrogate the processes by which particular kinds of urban practices and actors are framed as ""illegal"" relative to others and what work such a framing is meant to do. Having established the relationship between illegality, planning and planned development in the city empirically, the dissertation then analyses a body of case law in the Delhi High Court and the Supreme Court of India to show that the Courts misrecognise illegality in their twin understandings of ""encroachment"" and ""encroacher"" when they portray the former as the visible manifestation of what they see as the crisis of the city and the latter as one of the actors primarily responsible for this crisis. Showing how the courts use narratives of the failure of ""planned development"" and what they call ""Government"" to justify their interventions into the city, the dissertation describes their attempt to make the city into a governable space using the ""Plan in its legal position"" to represent an idealized spatial order. Intervening in the crisis of the city towards this idealized order thus becomes not only the primary definition of public interest but also an ethico-moral imperative that acts as a rationality of judicial government.Further, the dissertation argues that the case-law on evictions makes visible the impoverishment of poverty, drawing upon Upendra Baxi's concept of impoverishment as a dynamic process of public decision-making in which it is considered just, right and fair that some people may become or stay impoverished. The Courts enable impoverishment by through the creation of the category of the ""encroacher"" that binds the identity of the poor to a spatial illegality and becomes the basis of a disavowal of their rights. Additionally, through the discursive erasure of the vulnerability of the poor and the emergence of a new ""urban majority"" as the subject of urban politics, they transform the poor into improper citizens thereby legitimizing a regime of differentiated citizenship. Using interviews with activists in urban social movements in Delhi, the dissertation further shows how the emergence of the judiciary as the site and object of resistance has resulted in the juridicalisation of resistance:  the impact of the presence of the Court within the calculus of negotiation and confrontation as modes of engagement and resistance to evictions. The presence of the Court challenges the choice of strategies of urban social movements, introduces new actors and decision-making processes into movement spaces, alters the content of right-claims and forecloses certain kinds of claimants just as it shapes the political identity and history of basti and its residents themselves. Finally, in conclusion, the dissertation explores how new forms and claims to the city can emerge in response to these challenges that will be not just impassioned, but equitable and effective.",ucb,,https://escholarship.org/uc/item/0nd455cm,,,eng,REGULAR,0,0
401,1837,Sensing and Harvesting Pressure Fluctuations in Harsh Environments,"Beker, Levent","Lin, Liwei;Pisano, Albert P;",2017,"Pressure is one of the most frequent and important parameters utilized in many applications to provide critical information about the operation of a system as well as a potential mechanical energy source.  Among various pressure-related devices, very few function in harsh environment which includes high temperature, high pressure, highly corrosive, or biofouling conditions. In the first part of this dissertation, a high temperature pressure sensor with a novel concentric ring-circle-shape design is proposed specifically for the geothermal applications with detailed modeling, fabrication, and characterization results.  A concentrically matched ring-circular capacitance pressure sensor design is proposed to tackle the common mode noise problem of km-long cables in geothermal wells. Furthermore, to provide a high temperature and corrosive environment survivability, silicon carbide is utilized as it has superior material properties as compared to silicon in harsh environments. Experimentally, it was shown that the novel design can provide differential capacitance output at temperatures of 180â„ƒ and capacitances ranging between 0.14 pF to 1.45 pF were observed from a sensor designed to operate under 1 MPa. In the second part of this dissertation, an energy harvester application is proposed to make the use of pressure fluctuations within the lateral ventricles of the brain which is a biofouling environment. This time, a concentric ring-circular design is utilized to convert the mechanical pressure fluctuations to electrical energy efficiently. The harvesters were fabricated using aluminum nitride as the piezoelectric material and the increase in efficiency of the proposed design was shown as characterized by both in-air and underwater tests. A 3D-printed lateral ventricle mockup setup was used to mimic the operation of the harvester in lateral ventricles and the harvester with a diameter of 2.5 mm generated a power of 0.62 nW. Moreover, considering the potential issues in deployment and mounting, a flexible harvester was developed using PVDF and PET materials and under water characterization the harvester with 3 mm diameter resulted in power of 1.75 nW.",ucb,,https://escholarship.org/uc/item/0j89g6mb,,,eng,REGULAR,0,0
402,1838,Host factor regulation of Toxoplasma gondii growth and differentiation,"Weilhammer, Dina","Sha, William C;",2010,"Toxoplasma gondii is an obligate intracellular protozoan parasite that infects a wide range of mammalian and avian hosts, including up to one third of the human population.  Immunocompetent individuals clear acute infection with the parasite, however chronic infection is always established and persists for the life of the infected host.  Reactivation of chronic infection in immunocompromised individuals, and vertical transmission from mother to fetus during acute infection can cause devastating neural pathology.  Acute and chronic infection are associated with two distinct forms of the parasite: tachyzoites, which are fast replicating and responsible for acute infection and bradyzoites, which are slow replicating and establish chronic tissue cysts.  During the course of infection, tachyzoites differentiate into bradyzoites. While tachyzoites can infect and replicate within virtually any nucleated cell, bradyzoite cysts typically only develop within neural and muscle tissue.  Despite the critical importance of bradyzoite development to T. gondii  pathogenesis, the factors responsible for this tissue tropism are unclear.  Specifically, there are few defined molecular characteristics of the host cell that have been shown to regulate parasite growth and differentiation as bradyzoites.  Using an optimized in vitro system of bradyzoite induction, we have identified several new mechanisms by which T. gondii stage conversion is regulated in vitro.  First, we have shown that enhancement of host cell glycolysis can support continued tachyzoite growth under metabolic stress conditions and thus inhibit bradyzoite conversion in a cell-intrinsic manner.  Second, we have shown that cell lines that are intrinsically resistant to conversion, either basally or due to the induction of glycolysis, surprisingly release soluble mediators that inhibit conversion in trans.  Finally, we have defined a new metabolic function for host Akt in T. gondii differentiation.  These results suggest two new hypotheses as to how growth and differentiation of T. gondii may be regulated in vivo.  One, the preferential encystment seen in highly glycolytic tissues may be a result of the ability of these tissues to sustain enhanced tachyzoite growth and increased parasite load, and if conversion from tachyzoites to bradyzoites is spontaneously occurring at some level in vivo, increased parasite load may lead to a higher incidence of cyst development in these tissues.  Two, cells may broadly be releasing inhibitory mediators, making many tissues inhospitable to conversion, and thus restricting cyst development to particular tissues.  While the in vivo significance of these hypotheses remains to be investigated, they provide a new metabolic framework within which the mechanisms that regulate tachyzoite to bradyzoite conversion in vivo can be investigated.",ucb,,https://escholarship.org/uc/item/0kv4540j,,,eng,REGULAR,0,0
403,1839,Economic perturbations and fetal growth: A multilevel analysis of exposure to labor market insecurity during gestation and birth weight for gestational age,"Margerison-Zilko, Claire E.","Ahern, Jennifer;",2011,"Economic perturbations and fetal growth: A multilevel analysis of exposure to labor market insecurity during gestation and birth weight for gestational age byClaire E. Margerison-ZilkoDoctor of Philosophy in EpidemiologyUniversity of California, BerkeleyProfessor Jennifer Ahern, ChairBackground. Epidemiologic research has made important strides in identifying individual-level risk factors for adverse birth outcomes such as low birth weight and preterm birth, which carry high clinical, social, and economic costs.   Despite this accumulated knowledge, we remain unable to explain differences in the distribution of birth outcomes between populations and within populations across space and time, suggesting the need to consider macro-level, ecologic determinants of birth outcomes.  In this dissertation, I developed a conceptual model based on ecologic and evolutionary theory and research that proposes that unexpected changes to the human ecology, i.e. perturbations, may result in unexpected behavioral and biological responses in humans and that such responses will be conserved by natural selection if they are adaptive.  Based on this framework, I hypothesized that reductions in fetal growth would occur in response to maternal exposure to ecological perturbations--specifically, perturbations to labor market security--during gestation.Methods.  I examined the association between maternal exposure to state-level labor market perturbations during each trimester of gestation and fetal growth, as measured by birth weight for gestational age percentile.  The study population included 6,715 gestations and births between 1982 and 2000 to women enrolled in the National Longitudinal Survey of Youth 1979 (NLSY79).  I calculated birth weight for gestational age percentiles using national reference data and categorized births <10th percentile as small for gestational age (SGA).I defined perturbations to labor market security as months in which the state unemployment rate was higher than its statistically expected value (i.e., unexpectedly high labor market insecurity) and months in which the state unemployment rate was lower than its statistically expected value (i.e., unexpectedly high security).  I derived statistically expected values using ARIMA modeling methods to account for autocorrelation.  Gestations in the NLSY79 were classified as either exposed or unexposed to labor market insecurity or security in the first, second, and third trimester if one of these labor market perturbations occurred in the maternal state of residence during that trimester.I used linear and logistic regression models to examine the association between labor market perturbations in each trimester and birth weight percentile and odds of SGA.  I also examined whether any observed associations differed by maternal race/ethnicity, childhood socioeconomic status, educational attainment, marital status, employment status, or poverty status.  Finally, I explored whether any observed associations were mediated by individual economic change (i.e., changes in maternal employment status or household income) or maternal pregnancy behaviors (i.e., smoking, first trimester utilization of prenatal care, or net gestational weight gain).  If associations were mediated by one these factors, I calculated the proportion of the total association explained by that factor.Results.  Exposure to labor market insecurity in the first trimester was significantly associated with a decrease in birth weight for gestational age of 4.05 percentile points (95% CI = -6.87, -1.22) and higher odds of SGA (OR = 1.50, 95% CI = 1.21, 1.86).  Exposure to labor market insecurity in the second and third trimesters was not significantly associated with either outcome.  Exposure to labor market security was not associated with birth weight for gestational age percentile or SGA.  The association between exposure to labor market insecurity in the first trimester and birth weight percentile differed significantly by maternal childhood SES, educational attainment, and employment status but not by race/ethnicity, marital status, or poverty status.  Exposure to labor market insecurity in the first trimester was associated with decreases in birth weight percentile of 5.52 (95% CI = -10.0, -1.04) and 8.66 points (95% CI = -14.04, -3.29) among women with average or high childhood SES, respectively, while the association was not significant among women with low childhood SES.  Exposure to labor market insecurity in the first trimester was associated with a decrease in birth weight percentile of 9.22 points (95% CI = -15.77, -2.88) among women with <12 years educational attainment, while the association was not significant among women with 12 years or >12 years educational attainment.  Exposure to labor market insecurity was associated with a decrease in birth weight percentile of 7.10 points (95% CI = -12.33, -1.87) and 10.27 points (95% CI = (-18.82, -1.71) among women keeping house and out of the labor force, respectively, while the association was not significant among employed and unemployed women.My exploration of mediation by individual economic change and maternal pregnancy behaviors found that approximately 11% of the association between exposure to labor market insecurity in the first trimester and birth weight percentile was explained by net maternal gestational weight gain.  The association also differed significantly by maternal smoking, with the association only significant among smokers.  No other individual economic change or maternal pregnancy behaviors mediated greater than one percent of the association.Conclusions.  Findings support my hypothesis that fetal growth responds to a contemporary ecological perturbation, i.e., unexpectedly high labor market insecurity.  Exposure to this perturbation appears to have more impact on fetal growth if it occurs in the first trimester of gestation.  The finding that associations between exposure to labor market insecurity and birth weight percentile were stronger among women with high childhood SES, <12 years education, and those keeping house or out of the labor force suggests that these women may be more vulnerable to economic perturbations.  Although further research on mediation is needed, initial findings suggest that maternal gestational weight gain may represent one pathway through which economic perturbations affect fetal growth.",ucb,,https://escholarship.org/uc/item/0nd917g7,,,eng,REGULAR,0,0
404,1840,Curved and anisotropic unstructured mesh generation and adaptivity using the Winslow equations,"Fortunato, Meire","Persson, Per-Olof;",2016,"High-order methods are receiving considerable interest from the computational community because they can achieve higher accuracy with reduced computational cost compared to traditional low-order approaches. These methods generally require unstructured meshes of non-inverted curved elements, and the generation of high-order curved meshes in a robust and automatic way is an important and challenging open problem.We present a method to generate high-order unstructured curved meshes by solving the classical Winslow equations using a new continuous Galerkin finite element discretization. This formulation appears to produce high quality curved elements, which are highly resistant to inversion. In addition, the corresponding nonlinear equations can be solved efficiently using Picard iterations, even for highly stretched boundary layer meshes. Another challenge that mesh-based methods face is that the discretization of the domain is usually generated before the solution is known, which can lead to large numerical errors or non-convergent schemes. A tool that can be used to overcome this problem is mesh adaptivity. We use the Winslow variable diffusion equations -- which are a variation of the classical form -- to perform curved and anisotropic unstructured mesh adaptivity. We use a range of numerical examples to validate our models including complex geometries and stretched boundary layers. We demonstrate the high quality of the generated meshes and the performance ofthe nonlinear solver. Finally, we present an example of mesh adaptivity for shock capturing when solving the Euler equations of gas dynamics for supersonic flow.",ucb,,https://escholarship.org/uc/item/0zc4s5sv,,,eng,REGULAR,0,0
405,1841,Optimization for Urban Mobility Systems,"Lee, Jiung","Shen, Zuo-Jun;",2019,"In the recent decades, new modes of transportation have been developed due to urbanization, highly dense population, and technological advancement. As a result, design and operation of urban transportation have become increasingly important to better utilize the resources and efficiently meet demand. This dissertation was motivated by two problems on optimizing design and control of urban transportation. In the first one, we consider a problem of dynamically matching heterogeneous market parcitipants so as to maximize the total number of matching, which was motivated by practices of ride-sharing platforms. In the other problem, we study efficient design of elevator zoning system in high-rises with uncertainty in customer batching.In Chapter 1, we consider a multiperiod stochastic optimization of a market that matches heterogeneous and impatient agents. The model was mainly motivated from carpooling products run by ride-sharing platforms such as Uber and Lyft, and kidney exchange market, where market participants are heterogeneous in terms of how likely they can be matched with others. In the case of a ride-sharing platform, one of the key operational decisions for carpooling is to efficiently match riders and clear the market in a timely manner. In doing so, the platform needs to take into account the heterogeneity of riders in terms of their trip types(e.g origin-destination pair) and different matching compatibility. For example, some customers may request rides within San Francisco, while others may request rides from San Francisco to outside the city. Since picking up and dropping off a customer within the city can be done within relatively short amount of time, those who want to travel within the city can be matched with any other riders for carpooling. However, the destinations of those who want to travel to outside the city may be very different, and in order to maintain customers' additional transit time due to carpooling, it is likely that they can be only matched with those who want to travel within the city. In the case of kidney exchange where market participants arrive in the form of patient-donor pair, pairs with donor who can donate her kidney to most of patients (for example, blood type O) and patient who can get kidney from most of donors (for example, blood type AB) can be easily matched to other pairs. The opposite case would be hard-to-match pair that is incompatible for matching with most of other pairs. Our model is an abstraction of these two motivating examples, and considers two types of agents: easy-to-match agents that can be matched with either type of agents, and hard-to-match agents that can be only matched with easy-to-match ones. We first formulate a dynamic program to solve for optimal matching decisions over infinite time horizon in a discrete time setting, and characterize structure of optimal stationary policies. Inspired by practices in kidney exchange where the market is cleared for every fixed time interval, we connect the discrete time model to a continuous time setting by investigating the effect of the length of matching intervals on the matching performance. Results from numerical experiments indicate certain patterns in the relationship between the length of matching intervals and the maximum number of matching achieved, and provides valuable insights for future direction of research. In Chapter 2, we consider a zoning problem for elevator dispatching systems in high-rises. In practice, zoning is frequently used to improve efficiency of elevator systems. The idea of zoning is to prevent different elevators from stopping at common floors, which may result in long service times of elevators and thus long waiting times of customers. Our goal is to provide a mathematical framework that can help a system planner decide optimal zoning design with some performance guarantee. To this end, we focus on uppeak traffic situation during morning rush hour, which is in general the heaviest traffic during the day. The performance in the uppeak traffic situation can be considered as the system's capacity, because if the system can handle uppeak traffic well, it can also serve other types of traffic with good performance. Thus, the performance measure in the uppeak traffic situation can be used as a metric to choose the optimal zoning configuration. One of the components that complicate the problem is customer batching, on which the system may not have a control. In view of this, we formulate an adversarial optimization problem that can measure the system performance of different zoning decisions. By considering the heaviest traffic situation of the day and using the adversarial framework, we provide a model that can be used for capacity planning of elevator systems. We formulate mixed-integer linear program(MILP)s to find the optimal zoning configuration. To solve the MILPs, we show that we can use simple greedy algorithms and solve smaller linear programs. We also provide a few illustrative examples as well as numerical experiments to verify the theoretical results and obtain insights for further analysis.",ucb,,https://escholarship.org/uc/item/0zj695rp,,,eng,REGULAR,0,0
406,1842,Finding Yourself in a Book: Marginalized Adolescent Identity Development and Literary Engagements,"Johnston, Anthony","Freedman, Sarah W;Pearson, P. David;",2014,"This dissertation examines the identities of ""marginalized"" adolescents as they engage in literacy-based activities.  Using ethnographic and qualitative research methods (including surveys/questionnaires, audio recorded interviews, video recorded observations, classroom artifacts, and observational notes), a multi-case study occurred over six months.  The study took place at South Bay High, a small public charter school, located in a poor and working class neighborhood of major city in Northern California, serving non-dominant youth.  Twenty two juniors, and of these, six focal participants, elected to participate in the study, which took place in their English 11 class.  The study utilizes socio-cultural theories of learning and identity, transactional theories of pedagogy, and applies figured worlds and positional identity theory in its analysis.  This work is in conversation with a growing genre of scholarship referred to as literacy and identity studies (Moje, 2009).  The relative fragility and durability of a student's academic identity is considered.  In addition to examining individual identities, this work also takes up the collective classroom identity as a site for examination.  By taking into account local histories of cultural and social contextual matters, and by examining classroom culture (i.e., norms, discourses, routines), the classroom studied offers the first case studied.  Specifically, I consider the effect of ideologically divergent approaches to literacy instruction on the academic identities of the collective.Adolescence is a time when young people are in search of narratives and discourses to offer understandings of the past, security in the present, and imagined trajectories towards the future.  How one comes to see oneself (and one's future) is often determined by the narratives made available - from peers, media, families, schools, and other institutions.  Non-dominant youth have less access to identity resources imbued with social and academic capital from which to construct identities or imagined futures.  The second findings chapter follows the focal participants as they take up literacy-based resources as they engage in processes of authoring the self.The figured world of the high school classroom has a limited amount of roles for students to occupy.  Often students are labeled and treated in ways that position them on a relative scale of academic potential and social behavior.  Once students become positioned in particular ways (i.e., as the class clown, teacher's pet, slacker) they often accept these positionings and come to define themselves in relatively fixed terms.  However, in an ELA class, literacy can serve as a medium for students to ""try on"" identities not always available to them in other spaces.  The third findings chapter looks at how focal participants were positioned and at the positioning events that serves to either solidify or disrupt seemingly fixed identities.Implications of the study include: Instructional practices that treat ELA classrooms as spaces for interpretations not only of texts but also in ways that provide insights into students own lives.  An examination of the multiple competing forces present in classrooms, from federal and state-mandated testing to the teacher's pedagogical stance, illustrates the complexity of classroom spaces, particularly in classrooms for students who have traditionally been underserved by schooling as an institution.  The need to examine the spectrum of diversity among non-dominant youth so that young people are not further reduced or essentialized by progressive instructional methods is also considered.",ucb,,https://escholarship.org/uc/item/11c0q3mc,,,eng,REGULAR,0,0
407,1843,Learning from others through testimony and statistics,"Hu, Jane","Xu, Fei;",2014,"In learning about the world, children have at least two types of information available to them: information they learn from their personal experiences, and information they receive from others. This dissertation examines how children use both of these types of information to make inferences about others. In chapter 1, I discuss the role of this work in the context of previous developmental psychology research. In chapter 2, I present a set of empirical studies in which children inferred an agent's graded preferences from observing his choice actions. In chapter 3, I present a second set of empirical studies, in which children's endorsement of majority testimony differs based on domain type and amount of personal experience available. In chapter 4, I present a third set of empirical studies in which children assessed informants' knowledge sources and chose options endorsed by informants who received their knowledge from more reliable sources of knowledge. In chapter 5, I discuss the implications of this work and suggest future directions. Overall, the empirical work included in this dissertation suggests preschoolers' inferences abilities are more sophisticated than previously demonstrated: they can use contextual information in conjunction with statistical information, and can use that to make inferences about others' mental states and knowledge.",ucb,,https://escholarship.org/uc/item/11m7q656,,,eng,REGULAR,0,0
408,1844,Design of Minimally Actuated Legged Milli-Robots Using Compliant Mechanisms and Folding,"Hoover, Aaron Murdock","Fearing, Ronald S.;",2010,"This thesis explores milli- and meso-scale legged robotdesign and fabrication with compliant mechanisms. Our approachmakes use of a process that integrates compliant flexure hingesand rigid links to form parallel kinematic structures through thefolding of flat-fabricated sheets of articulated parts. Usingscrew theory, we propose the formulation of an equivalent mechanismcompliance for a class of parallel mechanisms, and we use that compliance to evaluate a scalar performance metric based on the strain energy stored in a mechanism subjected to an arbitrary load. Results from the model are supported by experimental measurements of arepresentative mechanism. With the insight gained from the kinematic mechanism design analysis, we propose and demonstrate compliant designs for two six-legged robots comprising the robotic, autonomous, crawling hexapod (RoACH) family of robots. RoACH is a two degree of freedom, 2.4 gram, 3 cm long robot capable of untethered, sustained, steerable locomotion. RoACH's successor, DynaRoach,is 10 cm long, has one actuated degree of freedom and is capable of running speeds of up to 1.4 m/s. DynaRoACH employs compliant legs to help enable dynamic running and maneuvering and is three orders of magnitude more efficient than its milli-scale predecessor. We experimentally demonstrate the feasibility of a biologically-inspired approach to turning control and dynamic maneuvering by adjusting leg stiffness. While the resultagrees qualitatively with predictions from existing reduced order models, initial data suggest the full 3-dimensional dynamics play an importantrole in six-legged turning.",ucb,,https://escholarship.org/uc/item/11t4p9r2,,,eng,REGULAR,0,0
409,1845,"The role of the ion channel, TRPA1, in itch transduction in the mammalian peripheral nervous system.","Wilson, Sarah Ruth","Bautista, Diana M;",2015,"Itch is defined as an unpleasant sensation that evokes a desire to scratch. In contrast to acute itch that is transient, chronic itch is a persistent, debilitating condition for which there are few treatment options. Chronic itch accompanies a number of skin diseases and systemic conditions, as well as a variety of neurological disorders. However, little is known about the molecules and cell types that mediate acute or chronic itch in primary sensory neurons and skin.We have established an essential role for the ion channel, TRPA1, in multiple forms of both acute and chronic itch. We have shown that TRPA1 is required for neuronal activation and itch behaviors in mice downstream of acute exogenous and endogenous itch compounds. Similarly, TRPA1 is required for itch behavior and itch-evoked expressional changes in both sensory neurons and skin in a mouse model of dry skin. We have also identified a novel itch-causing compound: Thymic Stromal Lymphopoietin (TSLP). Numerous studies suggest that the epithelially-derived cytokine TSLP acts as a master switch that triggers both the initiation and maintenance of the chronic itch disease atopic dermatitis. Our work demonstrates that TSLP activates sensory neurons directly and leads to acute itch behaviors. TRPA1 is required for both TSLP-evoked neuronal activation and itch behaviors. Taken together, our work shows that TRPA1 is a master regulator of itch signaling.",ucb,,https://escholarship.org/uc/item/174754z9,,,eng,REGULAR,0,0
410,1846,Electrical and Optical Enhancement in Internally Nanopatterned Organic Light-Emitting Diodes,"Fina, Michael Dane","Mao, Samuel S;Greif, Ralph;",2012,"Organic light-emitting diodes (OLEDs) have made tremendous technological progress in the past two decades and have emerged as a top competitor for next generation light-emitting displays and lighting. State-of-the-art OLEDs have been reported in literature to approach, and even surpass, white fluorescent tube efficiency. However, despite rapid technological progress, efficiency metrics must be improved to compete with traditional inorganic light-emitting diode (LED) technology. Organic materials possess specialized traits that permit manipulations to the light-emitting cavity. Overall, as demonstrated within, these modifications can be used to improve electrical and optical device efficiencies. This work is focused at analyzing the effects that nanopatterned geometric modifications to the organic active layers play on device efficiency.In general, OLED efficiency is complicated by the complex, coupled processes which contribute to spontaneous dipole emission. A composite of three sub-systems (electrical, exciton and optical) ultimately dictate the OLED device efficiency. OLED electrical operation is believed to take place via a low-mobility-modified Schottky injection process. In the injection-limited regime, geometric effects are expected to modify the local electric field leading to device current enhancement. It is shown that the patterning effect can be used to enhance charge carrier parity, thereby enhancing overall recombination. Current density and luminance characteristics are shown to be improved by OLED nanopatterning from both the model developed within and experimental techniques.Next, the optical enhancement effects produced by the nanopatterned array are considered. Finite-difference time-domain (FDTD) simulations are used to determine positional, spectral optical enhancement for the nanopatterned device. The results show beneficial effects to the device performance. The optical enhancements are related to the reduction in internal radiative quenching (improved internal quantum efficiency) and improvement in light extraction (improved outcoupling efficiency). Furthermore, the electrical model is used to construct a positional radiative efficiency map that when combined with the optical enhancement reveals the overall external quantum efficiency enhancement.",ucb,,https://escholarship.org/uc/item/19f58436,,,eng,REGULAR,0,0
411,1847,"The Post-American Novel: 9/11, the Iraq War, and the Crisis of American Hegemony","Page, Gabriel","Jones, Donna V.;",2018,"This dissertation proposes a new analytical category for thinking about a subset of post-9/11 Anglophone novels that are engaged with the political aftermath of 9/11. I designate this category the post-American novel, distinguishing it from the category of 9/11 fiction. While the 9/11 novel is a sub-genre of national literature, focusing on the terrorist attacks as a national trauma, the post-American novel is a transnational literary form that decenters 9/11, either by contextualizing the terrorist attacks in relation to other historical traumas or by shifting focus to the â€œWar on Terror.â€ I theorize the post-American novel as the literary expression of international opposition to the 2003 U.S. invasion of Iraq. International opposition to the Iraq War exposed the fractures in American global hegemony, and so I define the post-American novel as the historically-engaged novel of the crisis of American hegemony. I develop this argument through a detailed analysis of the political aesthetics of some representative post-American novels by four international writers. In various ways, these novels all diagnose post-9/11 American society from an international perspective and subvert the myth of American exceptionalism, though their forms of cultural and political critique are more far-ranging than this. The category of the post-American novel is not meant to herald the end of American literature. And though I peg its emergence to the second Iraq War, the category is flexible enough to encompass a range of contemporary novels by international writers who explore the role of the United States in a changing world.",ucb,,https://escholarship.org/uc/item/1cs8p0b4,,,eng,REGULAR,0,0
412,1848,Fabrication Process Development for High-Purity Germanium Radiation Detectors with Amorphous Semiconductor Contacts,"Looker, Quinn","Vetter, Kai;",2014,"High-purity germanium (HPGe) radiation detectors are well established as a valuable tool in nuclear science, astrophysics, and nuclear security applications.  HPGe detectors excel in gamma-ray spectroscopy, offering excellent energy resolution with large detector sizes for high radiation detection efficiency.  Although a robust fabrication process has been developed, improvement is needed, especially in developing electrical contact and surface passivation technology for position-sensitive detectors.  A systematic study is needed to understand how the detector fabrication process impacts detector performance and reliability.  In order to provide position sensitivity, the electrical contacts are segmented to form multiple electrodes.  This segmentation creates new challenges in the fabrication process and warrants consideration of additional detector effects related to the segmentation.A key area of development is the creation of the electrical contacts in a way that enables reliable operation, provides low electronic noise, and allows fine segmentation of electrodes, giving position sensitivity for radiation interactions in the detector.  Amorphous semiconductor contacts have great potential to facilitate new HPGe detector designs by providing a thin, high-resistivity surface coating that is the basis for electrical contacts that block both electrons and holes and can easily be finely segmented.  Additionally, amorphous semiconductor coatings form a suitable passivation layer to protect the HPGe crystal surface from contamination.  This versatility allows a simple fabrication process for fully passivated, finely segmented detectors.  However, the fabrication process for detectors with amorphous semiconductors is not as highly developed as for conventional technologies.  The amorphous semiconductor layer properties can vary widely based on how they are created and these can translate into varying performance of HPGe detectors with these contacts.  Some key challenges include minimizing charge injection leakage current, increasing the long-term stability of the contacts, and achieving good charge collection properties in segmented detectors.A systematic study of contact characteristics is presented where amorphous germanium (a-Ge) and amorphous silicon (a-Si) contacts are sputtered with varying sputter gas hydrogen content, sputter gas pressure, and amorphous film thickness.  A set of about 45 detectors fabricated from 11 different crystal samples were analyzed for electron barrier height and effective Richardson constant.  Most of these detectors were subjected to as many as 10 temperature cycles over a period of up to several months in order to assess their long-term stability.  Additionally, 6 double-sided strip detectors were fabricated with a-Ge and a-Si contacts in order to study their inter-electrode charge collection properties.  An attempt is made to relate fabrication process parameters such as hydrogen content, sputter pressure, and film thickness to changes observed in detector performance and assess the level of reproducibility using the current methods.Several important results and conclusions were found that enable more reliable and highly performing detectors with amorphous semiconductor contacts.  Utilizing the new information should enable consistent production of finely segmented detectors with excellent energy resolution that can be operated reliably for a long period of time.  The passivation process could impact planar detectors as well as other designs, such as the p-type point contact detector.  It is demonstrated that the long-term stability of amorphous semiconductor contacts is primarily dependent on the time the detector is at room temperature rather than the number of temperature cycles.  For a-Ge contacts, higher sputter pressure yields a more stable process that changes little with time, giving a reliable hole-blocking contact.  The a-Si contacts form a good electron-blocking contact with decreasing leakage current over time.  Both materials, when 7% hydrogen is included in the argon sputter gas, show acceptable levels of inter-electrode charge collection to be useful for strip electrode detectors.",ucb,,https://escholarship.org/uc/item/1d14c7t8,,,eng,REGULAR,0,0
413,1849,"The Comic Bildungsroman: Evelyn Waugh, Samuel Beckett, and Philip Roth","Seidel, Matthew David","Falci, Eric;",2010,"This dissertation argues that the relationship between comedy and the Bildungsroman is symbiotic rather than subversive, indicative of a fundamental affinity between mode and genre. The Bildungsroman is a genre supremely anxious about the social, professional, and romantic definition its heroes seek, an anxiety that leaves it highly vulnerable to the incursions of comedy. Definition is about limits, ends, bounds, and stability. I argue that comedy attacks all these things mercilessly, and finds in the Bildungsroman's preoccupation with definition, limits, and bounds a fertile ground for its own forces of indefinition, limitlessness, and boundlessness. Therefore, small, sometimes trivial examples of comic indefinition can be traced back to the larger definitional stakes of the Bildungsroman form. The comic twentieth-century novels I take up, Evelyn Waugh's Decline and Fall and The Loved One, Samuel Beckett's Murphy and Company, and Philip Roth's Portnoy's Complaint and Sabbath's Theater, feed on the Bildungsroman's ever-present, latent comedy. Comic Bildungsromans, anti-Bildungsromans, parodic Bildungsromans: a rose is a rose is a rose.  Whatever the name, the comic Bildungsroman doesn't so much distort the image of the Bildungsroman as reflect its truest form.",ucb,,https://escholarship.org/uc/item/1fd6q58p,,,eng,REGULAR,0,0
414,1850,"Stable isotope fractionation and trace element partitioning in marine, terrestrial, and laboratory-synthesized carbonates","Mitnick, Elizabeth Horner","DePaolo, Donald J;",2018,"This dissertation is about trace element and isotope fractionation in natural and laboratory-synthesized calcite and aragonite. Quantifying various isotope (Î´13C and Î´44Ca) and trace element (Sr/Ca and Mg/Ca) ratios in carbonates allows us to probe the kinetics of various reactions at the mineral-fluid interface. Understanding the way in which these signals depend on various parameters (e.g., precipitation rate, temperature, fluid composition) elucidates molecular-scale fundamentals of crystal growth. Similarly, this framework allows us to use these isotopic and trace element probes as proxies for interpreting natural carbonates, and in many cases, understand ancient environments. Furthermore, by identifying the nature of carbonate growth in solutions of varying chemistry and temperature, we can best harness the ability of these minerals to sequester CO2 and fluid contaminants in response to anthropogenic pollution and climate change.  In Chapter 2, I use the Ca and Sr concentrations of marine sedimentary pore fluids to estimate rates of authigenic carbonate precipitation in modern sediments. Using the extensive Ocean Drilling Program database, these rates of authigenic carbonate production can be compared to coeval rates of organic carbon and biomineralized carbonate deposition, thereby quantifying relative fractions of each contributing carbon flux. While there are many sedimentary environments in which the relative authigenic carbonate fraction is significant in comparison to the organic and biomineralized fractions, the Î´13C of these carbonates is almost identical to the biomineralized fraction in nearly all cases. Within the context of a carbon isotope mass balance framework, I demonstrate that the biomineralized and authigenic carbonate fractions can be suitably accounted for via a single inorganic carbonate pool. This finding has implications for interpreting the geologic record, indicating that some of the fluctuations in Î´13C over the course of Earth history cannot be explained by the ongoing and extensive production of authigenic carbonate. I explore the few marine sedimentary environments that do produce authigenic carbonate with Î´13C distinct from that of the biomineralized fraction, and thereby could be possible exceptions to the main finding of this work. The processes responsible for creating this isotopically anomalous authigenic carbonate are not thought to be steady-state phenomena throughout most of Earth history. The production of authigenic carbonate in these environments therefore suggests that Î´13C variations in the sedimentary record could not have been the result of authigenic carbonate formation. Rather, the formation of authigenic carbonate with a Î´13C composition distinct from that of biomineralized carbonate would be a consequence of a non-steady state perturbation to the carbon cycle. Consequently, these exceptional environments do not change the finding that a single inorganic carbon reservoir can be used to encapsulate the ongoing biomineralized and authigenic fluxes in marine sediments from an isotopic mass balance standpoint.	In Chapter 3, I study the formation of calcite and aragonite from the travertine-precipitating hot springs of Bridgeport, California. These travertines form as an elongate fissure ridge of intergrown calcite and aragonite, precipitating along the edges of a thin fluid conduit with systematically changing fluid pH, temperature, and CO2 degassing. The ability to readily measure modern precipitates from predictably evolving ambient fluids makes this system a unique natural laboratory in which to probe kinetics of carbonate growth in a high-temperature (>60Â°C) system. While travertines around the world have been the subject of a substantial body of literature, only one other study has reported the Ca isotope fractionation that occurs between travertine carbonates and fluid. In this work, Î´13C, Î´44Ca, Sr/Ca and Mg/Ca ratios in bulk carbonates and fluids are measured. Although the Sr/Ca ratios in the bulk solid are much higher than expected based on experimental studies, I argue that they may not be anomalous for thermogene travertines. The spatially evolving Î´13C and Sr/Ca composition of the fluid and solid indicates decreasing aragonite content down-conduit, suggesting that the Ca isotope fractionation will reflect the relative change in calcite and aragonite fractions. Although aragonite is expected to produce a larger magnitude fractionation than calcite, we do not observe any trend with distance from the inlet corresponding to the systematically changing mineral fractions. This observation can be explained by ongoing recrystallization of carbonate in the porous conduit walls, demonstrating the complex nature of multi-mineral travertine deposits and the impact of diagenetic alteration on Ca isotope systems in nature. Finally, in Chapter 4, I probe calcite growth at the smallest spatial and temporal scales studied in this dissertation. I present results from an experimental study in which calcite was precipitated from solutions of varying Mg/Ca ratios with a constant saturation index (SI) under controlled conditions. The aim of this project was to determine the effect of Mg on calcite growth as reflected in the Ca isotope fractionation between solid and solution. Mg is thought to primarily inhibit calcite precipitation via a kink blocking mechanism. However, the change in Ca isotope fractionation with increasing Mg/Ca and decreasing precipitation rate at constant SI demonstrates that in addition to kink blocking, Mg also impedes growth via incorporation inhibition. This additional mechanism alters the relative Ca attachment and detachment fluxes at the crystal surface, resulting in the observed fractionation. This finding has important implications for the interpretation of Ca isotope fractionation in both marine inorganic and biomineralized calcite precipitating from Mg-bearing seawater and sedimentary pore fluids.",ucb,,https://escholarship.org/uc/item/1gv8m3pv,,,eng,REGULAR,0,0
415,1851,Essays on Externalities and Agriculture in the United States and Brazil,"Bowman, Maria Susannah","Zilberman, David;",2013,"In these three essays collectively entitled ""Essays on Externalities and Agriculture in the United States and Brazil"", I discuss three topics.  In the first essay, I review the economic literature on diversification in farming systems and comment on the economic incentives and disincentives for diversification in 21st century agriculture.  In the second essay, I focus on deforestation in Brazil, which is an externality associated with the expansion of agricultural production at forest frontiers.  Using a natural experiment (changes in international Foot-and-Mouth Disease certification), I identify the portion of annual deforestation that can be attributed to changes in disease status, and suggest that the mechanism for new deforestation may be due to increased prices when beef is considered to be safe for export.  In my third essay, I discuss the production economics behind the use of sub-therapeutic antibiotics in U.S. pork and poultry production, and comment in detail on the potential for heterogeneity in the returns to antibiotic use (and costs of regulation).  A more detailed summary of each essay follows.Chapter 1: Economic Factors Affecting Diversified Farming SystemsIn response to a shift toward specialization and mechanization during the 20th century, there has been momentum on the part of a vocal contingent of consumers, producers, researchers, and policy makers who call for a transition toward a new model of agriculture. This model employs fewer synthetic inputs, incorporates practices which enhance biodiversity and environmental services at local, regional, and global scales, and takes into account the social implications of production practices, market dynamics, and product mixes. Within this vision, diversified farming systems (DFS) have emerged as a model that incorporates functional biodiversity at multiple temporal and spatial scales to maintain ecosystem services critical to agricultural production. This essay's aim is to provide an economists' perspective on the factors which make diversified farming systems (DFS) economically attractive, or not-so-attractive, to farmers, and to discuss the potential for and roadblocks to widespread adoption. The essay focuses on how a range of existing and emerging factors drive profitability and adoption of DFS, and suggests that, in order for DFS to thrive, a number of structural changes are needed. These include: 1) public and private investment in the development of low-cost, practical technologies that reduce the costs of production in DFS, 2) support for and coordination of evolving markets for ecosystem services and products from DFS and 3) the elimination of subsidies and crop insurance programs that perpetuate the unsustainable production of staple crops. This work suggests that subsidies and funding be directed, instead, toward points 1) and 2), as well as toward incentives for consumption of nutritious food. Chapter 2: Foot-and-Mouth Disease and Deforestation in the Brazilian AmazonDeforestation in the Brazilian Amazon released approximately 5.7 billion tons of CO2 to the atmosphere between 2000 and 2010, and 50-80% of this deforestation was for pasture.  Most assume that increasing demand for cattle products produced in Brazil caused this deforestation, but the empirical work to-date on cattle documents only correlations between cattle herd size, pasture expansion, cattle prices, and deforestation.  This essay uses panel data on deforestation and Foot-and-Mouth Disease (FMD) status--an exogenous demand shifter--to estimate whether changes in FMD status caused new deforestation in municipalities in the Brazilian Amazon and cerrado biomes during the 2000-2010 period.  Becoming certified as FMD-free caused annual deforestation to be 42% to 85% higher than deforestation rates in infected municipalities, on average, during the 2000-2010 period.  Chapter 3: Potential for heterogeneity in the returns to sub-therapeutic antibiotics in U.S. pork and poultry operationsEach year, more than 50,000 people in the U.S. die from hospital-acquired bacterial infections, millions experience episodes of foodborne illness, and reported cases of ""superbugs"" such as Methicillin-resistant Staphylococcus aureus (MRSA) and vancomycin-resistant enterococci (VRE) are on the rise.  For those who acquire a resistant infection in their food, in their community, or in a hospital, resistance is associated with a longer duration of treatment, the use of more potent antibiotics, and longer hospital stays. This, in turn, means increased health care costs and costs to society due to antibiotic-resistant infections.  Antibiotic resistance is contributing to the scope and severity of this health care crisis, and at least some of the responsibility for antibiotic resistance sits on the shoulders of industrial livestock production.  In livestock operations, low or sub-therapeutic doses of antibiotics (STAs) are used to promote growth, in addition to their use to prevent and control disease.  Today, more antibiotics are used in livestock production and the production of milk and eggs than in humans.  While the use of sub-therapeutic doses of antibiotics is regulated less stringently in the United States than in the European Union, there is movement toward and potential for such regulation.  Beginning in the 1970s, economic researchers began to study the potential impacts of bans on the use of sub-therapeutic antibiotics on the pork, poultry, and beef sectors and on U.S. consumers, but there has been little study of how heterogeneity impacts antibiotic use, and in turn, how it impacts returns to using antibiotics in U.S. livestock operations.  I concentrate on U.S. pork and poultry operations since they are the largest users of sub-therapeutic antibiotics by volume in the U.S., and explore the existing literature on the economics of sub-therapeutic antibiotic use for glimpses of heterogeneity in the returns to antibiotic use.  Perhaps the most interesting source of heterogeneity in returns to antibiotic use may be heterogeneity in management and/or the use of potential substitutes for antibiotics, such as improved sanitation practices and more modern facilities.  Productivity and use of technologies that substitute for STA use vary amongst producers, and likely by region and farm size.  Thus, the marginal abatement costs of reducing STA use vary across industries, producers, production systems, and regions.",ucb,,https://escholarship.org/uc/item/0nt9d3bs,,,eng,REGULAR,0,0
416,1852,"Dialectic, Desire & Discipline: The Formation of the Philosopher on the Scene of the Platonic Dialogue","Tafolla, Vincent Michael","Boyarin, Daniel;Bates, David W;",2013,"The argument explores the relationship between epistemology and ethics in the Platonic dialogues. Focusing on the drama that takes place on the scene of the dialogues, I trace Socrates' struggle to impose rules on the conversation.  What I show is that though Socrates' question invites a response from his interlocutor (and so, is commonly celebrated as open-ended), the questions he asks and the rules he imposes, restricts contribution that his interlocutor can make to the dialogue. I argue that this discursive practice, the dialectic, not only imports a specific epistemology--a specific image of knowledge--but this episteme is used to interpret what Socrates' interlocutor values and desires; so the discourse brings with it a certain understanding of the form that desire should take.  The epistemological assumptions, then, also assume (and endeavor to produce) a particular type of subject.  A careful examination of the discursive practice by which Plato distinguishes Socrates and philosophy from other figures and practices reveals that Plato's epistemological and ethical ends are more circumscribed than is usually acknowledged.  By developing a sensitivity to the struggle that takes place over method, I argue, one not only becomes sensitive to the exclusions on which philosophy is founded (e.g., the considerations deemed irrelevant to the conversation) but one also develops a sensitivity to the power dynamic that structures the relationship between interlocutors.  More broadly, this raises the problem of how certain discursive practices support particular distributions of power and different ethical possibilities while suppressing others.",ucb,,https://escholarship.org/uc/item/0p19t46d,,,eng,REGULAR,0,0
417,1853,On Stability and Doctor-optimality of Cumulative Offer Process,"Chen, Kun","Shannon, Chris;",2018,"I study the stability and doctor-optimality of doctors' proposing cumulative offer process in the many-to-one matching with contracts. First, I explore some conventional hospital-by-hospital conditions on each hospital's choice function, and show that unilateral substitutability is equivalent to observable substitutability across doctors combined with cumulative offer achievability, each of which is a necessary condition for cumulative offer process to be doctor-optimally stable in a sense that if a hospital does not satisfy the condition, then we could construct some choice functions for other hospitals such that cumulative offer process is not doctor-optimally stable for some doctors' preference profile. Then, I focus on the joint properties of the choice functions for the entire group of hospitals and introduce two joint conditions---independence of proposing order and group cumulative offer achievability---and show that when these conditions are satisfied, cumulative offer process is always doctor-optimally stable. And it is by far the weakest sufficient condition. Moreover, these two conditions are necessary in a sense that if not, then there exists a doctors' preference profile and a proposing order such that cumulative offer process is not doctor-optimally stable. At last, I also introduce doctor's preference monotonicity and show that when cumulative offer process is doctor-optimally stable, this condition guarantees its strategy-proofness.",ucb,,https://escholarship.org/uc/item/0q82k51w,,,eng,REGULAR,0,0
418,1854,Solid-Liquid Interdiffusion Bonding of Silicon Carbide to Steel for High Temperature MEMS Sensor Packaging and Bonding,"Chan, Matthew","Pisano, Albert P;",2013,"Complex engineering systems ranging from automobile engines to geothermal wells require specialized sensors to monitor conditions such as pressure, acceleration and temperature in order to improve efficiency and monitor component lifetime in what may be high temperature, corrosive, harsh environments. Microelectromechanical systems (MEMS) have demonstrated their ability to precisely and accurately take measurements under such conditions. The systems being monitored are typically made from metals, such as steel, while the MEMS sensors used for monitoring are commonly fabricated from silicon, silicon carbide and aluminum nitride, and so there is a sizable thermal expansion mismatch between the two. For these engineering applications the direct bonding of MEMS sensors to the components being monitored is often required. This introduces several challenges, namely the development of a bond that is capable of surviving high temperature harsh environments while mitigating the thermally induced strains produced during bonding.This project investigates the development of a robust packaging and bonding process, using the gold-tin metal system and the solid-liquid interdiffusion (SLID) bonding process, to join silicon carbide substrates directly to type-316 stainless steel. The SLID process enables bonding at lower temperatures while producing a bond capable of surviving higher temperatures. Finite element analysis was performed to model the thermally induced strains generated in the bond and to understand the optimal way to design the bond. The cross-sectional composition of the bonds has been analyzed and the bond strength has been investigated using die shear testing. The effects of high temperature aging on the bond's strength and the metallurgy of the bond were studied. Additionally, loading of the bond was performed at temperatures over 415 Â°C,  more than 100 Â°C, above the temperature used for bonding, with full survival of the bond, thus demonstrating the benefit of SLID bonding for high temperature applications.Lastly,  this dissertation provides recommendations for improving the strength and durability of the bond at temperatures of 400 Â°C and provides the framework for future work in the area of high temperature harsh environment MEMS packaging that would take directly bonded MEMS to temperatures of 600 Â°C and beyond.",ucb,,https://escholarship.org/uc/item/0r09f7ns,,,eng,REGULAR,0,0
419,1855,Structural Response and Cost Characterization of Bridge Construction using Seismic Performance Enhancement Strategies,"Aviram Traubita, Ady","Stojadinovic, Bozidar;",2009,"The improved seismic performance and cost-effectiveness of two innovative performance-enhancement technologies in typical reinforced concrete bridge construction in California were assessed in an analytical and experimental study. The technologies considered were lead rubber bearing isolators located underneath the superstructure and fiber-reinforced concrete for the construction of bridge piers. A typical five-span, single column-bent reinforced concrete overpass bridge was redesigned using the two strategies and modeled in OpenSees finite element program. Two alternative designs of the isolated bridge were considered; one with columns designed to remain elastic and the other such that minor yielding occurs in the columns (maximum displacement ductility demand of 2). The analytical model of the fiber-reinforced concrete bridge columns was calibrated using the results from two bidirectional cyclic tests on approximately Â¼-scale circular cantilever column specimens constructed using concrete with a 1.5% volume fraction of high-strength hooked steel fibers, relaxed transverse reinforcement, and two different longitudinal reinforcement details for the plastic hinge zone. Pushover and nonlinear time history analyses using 140 ground motions were carried out for the different bridge systems. The PEER performance-based earthquake engineering methodology was used to compute the post-earthquake repair cost and repair time of the bridges. Fragility curves displaying the probability of exceeding a specific repair cost and repair time thresholds were developed. The total cost of the bridges included the cost of new construction and post-earthquake repair cost required for a 75 year design life of the structures. The intensity-dependent repair time model for the different bridges was computed in terms of crew working days representing repair efforts. A financial analysis was performed that accounted for a wide range of discount rates and confidence intervals in the estimation of the mean annual post-earthquake repair cost. Despite slightly higher initial construction costs, considerable economic benefits and structural improvements were obtained from the use of the two performance-enhancement techniques considered, in comparison to the fixed-base conventionally reinforced concrete bridge, especially seismic isolation. The isolation of the bridge superstructure resulted in a significant reduction in both column and abutment displacement and force demands. The repair time of the isolated bridges was also significantly reduced, leading to continuous operation of the highway systems and reduced indirect economic losses. The experimental and analytical results also demonstrated that the use of fiber-reinforced concrete to build bridge columns leads to improved damage-tolerance, shear strength, and energy dissipation under cyclic loading compared to conventional reinforced concrete columns. These improvements result in better seismic performance and lower total 75-year cost of the fiber-reinforced column bridges.",ucb,,https://escholarship.org/uc/item/0tc4h7d8,,,eng,REGULAR,0,0
420,1856,MECHANICAL AND TRIBOLOGICAL PROPERTIES OF SKIN STUDIED BY MICROSCALE INDENTATION AND SCRATCHING TECHNIQUES,"Jee, Taekwon","Komvopoulos, Kyriakos;",2013,"Knowledge of the mechanical response and deformation behavior of individual skin layers during microprobe penetration is of high clinical and societal importance. In this thesis, the elastic behavior of stratum corneum, viable epidermis, dermis, and whole multilayer skin were investigated by combining micro/nanoindentation and microscratching techniques. Statistical analysis shows insignificant differences in reduced elastic modulus of skin samples obtained from three different porcine breeds. The reduced elastic modulus of stratum corneum is shown to be about three orders of magnitude higher than that of dermis. For relatively shallow and deep indentation depths, skin elasticity is controlled by that of stratum corneum and dermis, respectively. Skin indentation mechanics are interpreted in the context of a layered structure model consisting of a stiff and hard layer supported by a compliant and soft substrate, derived on the basis of microscopy observations and indentation measurements. Time-dependent deformation of porcine skin was also studied in vitro. The deformation behavior of stratum corneum, dermis, and whole skin tissue are examined in the context of measurements of creep strain, elastic stiffness, and viscoelastic constants obtained for different values of hold time, loading/unloading rate, and maximum indentation depth (load). It is shown that dermis viscoelasticity significantly affects the time-dependent deformation of skin up to a critical indentation depth (load) beyond which, the viscoelastic behavior of skin is controlled by the outermost hard epidermis, particularly stratum corneum. A conceptual deformation model that explains skin viscoelastic behavior under constant load (creep) and zero load (stress relaxation) conditions is developed on the basis of the phenomenological observations and experimental trends of this study. Representative friction and wear results of skin subjected to unidirectional and reciprocal (cyclic) scratching are interpreted in terms of sliding speed, normal load, and scratch cycles to illustrate the effects of stratum corneum, cellular epidermis, and dermis on the skin friction and wear characteristics. Depending on the applied normal load and scratch time (cycles) various friction mechanisms (adhesion, plowing, and squeeze film lubrication) and wear processes (surface plasticity/plowing, bulk shearing, cohesive failure, tearing, and delamination) control shear-induced skin damage. The obtained results provide insight into microscale friction and wear processes influencing the mechanical response of skin to normal and shear surface tractions.",ucb,,https://escholarship.org/uc/item/0vp2t1g5,,,eng,REGULAR,0,0
421,1857,Essays in Urban Transportation and Factor Misallocation,"Zarate Vasquez, Roman David","Rodriguez-Clare, Andres;",2020,"This dissertation studies the effect of different policy interventions on allocative efficiency in developing countries. It evaluates how urban and antitrust policies may generate a better allocation of resources across firms, increasing aggregate welfare, and productivity. To provide causal evidence, it collects rich microdata at a very high granular level and exploits plausibly exogenous variation of the main mechanisms. It complements the main empirical findings with quantitative general equilibrium models that allow simulating the effect of different policy interventions.One of the main factors that explain factor misallocation in developing countries is the informal sector. The fact that firms face different tax schedules generate differences in marginal products of labor. The first chapter of my dissertation proposes a new mechanism to account for the significant presence of the informal economy in developing countries: the high commuting costs to transit within cities. I provide evidence of three different empirical facts that align with this hypothesis. First, due to the high commuting costs, most workers in Mexico City have poor access to formal employment. Second, workers that operate in the informal economy are more sensitive to commuting costs, which implies that it is easier to substitute informal jobs. Third, I exploit the construction of line B of the subway in Mexico City to provide causal evidence of the negative relationship between informality and transit improvements. I estimate a series of difference-in-differences specifications, finding that transit improvements lead to a reduction in informality rates by four percentage points in nearby areas to the new stations relative to other places in Mexico City. This result indicates that workers reallocate to firms with higher total factor revenue productivity, increasing the welfare effects of transit improvements relative to the estimates from previous literature.A fundamental question for economists and policymakers is to measure the welfare effects of transit infrastructure that reduce transit times within a city. While the literature has estimated the `direct'' effects of these projects, there are substantial ``indirect'' effects that previous work has ignored. The second chapter develops a spatial general equilibrium model to account for the ``direct'' effects and also for the ""indirect"" effects driven by the reallocation of workers from the informal into the formal economy. I extend recent theoretical work in the urban literature by adding distortions that generate resource misallocation. From a first-order approximation, I provide a formula that decomposes the welfare effects of trade and commuting shocks into a ``pure'' effect term and an allocative efficiency margin. I estimate the main elasticities of the model exploiting variation from transit shocks. With these parameters at hand, I simulate different counterfactual interventions. The main findings suggest that line B of the subway increased welfare between 1.3\% and 1.6\%, that the indirect effects explain approximately 15-25\% of the total gains, and that the average real income per every dollar spent on infrastructure increases by 15\% relative to a perfectly efficient economy. In addition to the informal sector, there can be other sources of resource misallocation in developing countries. The third chapter, co-authored with Dario Tortarolo, focuses on the role of market power in explaining differences in marginal products of labor across plants. We disentangle the extent of imperfect competition in product and labor markets, assuming cost-minimizing firms that face upward-sloping labor supply and downward-sloping product demand curves. In the first part, we derive a formula for the ratio between markups and markdowns. We compute this ratio by estimating the output elasticity with respect to labor and the wage bill share. We disentangle the measure of market power specific to each firm by estimating labor supply elasticities using as an instrument intermediate inputs. Our results suggest that both markets exhibit imperfect competition, but markups mainly drive the variation. We assume a general equilibrium model to measure the relative gains of removing market power dispersion on allocative efficiency. The findings suggest that markups are more critical in explaining TFP than markdowns.",ucb,,https://escholarship.org/uc/item/0hq7j64g,,,eng,REGULAR,0,0
422,1858,Behavior of Machine Learning Algorithms in Adversarial Environments,"Nelson, Blaine Alan","Joseph, Anthony D;",2010,"Machine learning has become a prevalent tool in many computing applications and modern enterprise systems stand to greatly benefit from learning algorithms.  However, one concern with learning algorithms is that they may introduce a security fault into the system.  The key strengths of learning approaches are their adaptability and ability to infer patterns that can be used for predictions or decision making.  However, these assets of learning can potentially be subverted by adversarial manipulation of the learner's environment, which exposes applications that use machine learning techniques to a new class of security vulnerabilities.I analyze the behavior of learning systems in adversarial environments. My thesis is that learning algorithms are vulnerable to attacks that can transform the learner into a liability for the system they are intended to aid, but by critically analyzing potential security threats, the extent of these threat can be assessed, proper learning techniques can be selected to minimize the adversary's impact, and failures of system can be averted.I present a systematic approach for identifying and analyzing threats against a machine learning system.  I examine real-world learning systems, assess their vulnerabilities, demonstrate real-world attacks against their learning mechanism, and propose defenses that can successful mitigate the effectiveness of such attacks.  In doing so, I provide machine learning practitioners with a systematic methodology for assessing a learner's vulnerability and developing defenses to strengthen their system against such threats.  Additionally, I also examine and answer theoretical questions about the limits of adversarial contamination and classifier evasion.",ucb,,https://escholarship.org/uc/item/0j9233tm,,,eng,REGULAR,0,0
423,1859,"Dexterity in Robotic Grasping, Manipulation and Assembly","Fan, Yongxiang","Tomizuka, Masayoshi;",2019,"Industrial manipulators are programmed and integrated into different systems to deliver various functions. Traditional industrial manipulators are highly efficient and precise in mass production but deficient in flexibility and dexterity in mass customization due to the heavy reprogramming efforts in limited product life cycle, variations of environments and uncertainties during robot-environment interactions. This dissertation aims to address the aforementioned deficiencies by improving the dexterity of industrial manipulators. The manipulators with proposed algorithms are required to 1) reduce the hand-engineering in end-effector design, parameter tuning and system integration, and 2) exhibit robustness to uncertainties during the interaction with environments.  The fulfillment of the requirements is decomposed into three aspects in this dissertation. The first aspect is to realize kinematic dexterity by developing a unified grasping framework with both customized end-effectors and general hands on objects of different categories. The second aspect is to achieve dynamic dexterity by constructing an in-hand manipulation and finger gaiting architecture to manipulate the grasped objects robustly and precisely. The third aspect is to attain skill dexterity by designing an intelligent assembly algorithm to learn assembly skills from uncertain environments. The developed grasping framework actively avoids collision and is able to plan grasps and trajectories efficiently with different hands. The grasp planning with industrial customized grippers by surface fitting is introduced in Chapter 2, and the planning efficiency is improved by a learning-based grasp explorer in Chapter 3. The transferring of grasps from parallel grippers to multi-fingered hands by finger splitting is discussed in Chapter 4. Chapter 5 further presents an optimization model to directly plan precision grasps with multi-fingered hands. The final grasping framework is proposed in Chapter 6 by combining the optimization model with a multi-dimensional iterative surface fitting, to improve the grasp versatility and robustness of the optimization model. The constructed manipulation architecture achieves robust grasping and dexterous manipulation under uncertainties. A comprehensive architecture is introduced and verified with different physical multi-fingered hands in Chapter 7. Chapter 8 proposes a robust manipulation controller within the architecture to further increase the robustness under various uncertainties. To relocate fingers for long-range object motion, the manipulation architecture is augmented with a high-level finger gaits planner in Chapter 9.  The designed assembly scheme learns automatic assembly skills with the proposed guided-deep deterministic policy gradient (guided-DDPG) in Chapter 10. By combining supervised learning and reinforcement learning, the proposed guided-DDPG is more efficient than reinforcement learning and achieves better stability and robustness compared with supervised learning. The proposed grasping and manipulation strategies with customized/general-purposed grippers are able to reduce hand-engineering in mass customization and extend dexterities of automation systems in both kinematic and dynamic levels. The proposed learning assembly scheme increases the efficiency and stability of the assembly in contact-rich scenarios and extends the dexterity of automation systems in skill level. The effectiveness of the grasping, manipulation and assembly algorithms are verified by a series of simulations and experiments on different manipulators and hands.",ucb,,https://escholarship.org/uc/item/0jg7230n,,,eng,REGULAR,0,0
424,1860,Dynamics and Stability of Thermal Flying-height Control Sliders in Hard Disk Drives,"Zheng, Jinglin","Bogy, David B;",2012,"As a recent development to further reduce the flying height of a magnetic head in hard disk drives (HDDs) to nanometers, thermal flying-height (TFC) control technology is now widely applied in the HDD industry because it enables consistent read/write spacing, increased storage density and improved HDD reliability. The fast development of TFC technology presents new challenges to head designers because of the complicated structure of a TFC head, the thermo-mechanical-coupling effects and tribology issues arising at nanometer read/write spacing.A steady-state TFC solver dedicated to obtaining the steady-state flying attitude of a TFC slider is developed in this thesis. This solver uses a finite volume based solver (CML static solver) to solve the generalized Reynolds equation and obtain the pressure and spacing fields in the air bearing and a commercial coupled-field solver (ANSYS) to obtain the stress and strain fields due to internal heating. An iterative procedure is adopted to consider the cooling effect of the air bearing on the heater-induced protrusion. Accuracy of the solver is verified by drive-level magnetic tests on several combinations of air bearing and heater designs. TFC sliders' performances under different ambient conditions are investigated based on the TFC solver. It is found that the thermal actuation efficiency of a TFC slider increases with altitude because of the weakened cooling and reduced air bearing stiffness at the transducer area at a higher altitude. In addition, a TFC slider maintains a more consistent read/write spacing at different humidity levels, compared with a non-TFC slider, because the thermal actuation is able to compensate part of the pressure loss caused by water condensation. A TFC slider's flying height in air-helium mixtures is shown to be a highly nonlinear function of the fraction of helium in the gas mixture due to the combined effects of the gas mean free path, viscosity and heat conductivity. These results provide general guidelines for heater and ABS designers to reduce a TFC slider's sensitivity to ambient conditions and improve HDD reliability.A touchdown numerical model for predicting TFC sliders' dynamics at touchdown and over-pushed conditions is developed and implemented based on the CML dynamic simulator. It extends the solution of the time-varying generalized Reynolds equation to near-contact and contact conditions using a statistical multi-asperity approach. Various interfacial forces are considered by use and further development of a sub-boundary lubrication model to capture important tribological effects occurring at touchdown. This model is able to predict a TFC slider's unstable dynamics at the beginning of touchdown, which has been discovered in many related experimental studies. The effects of different head-disk interface factors are investigated using this numerical model. It is found that the suspension is actively involved in the TFC slider's bouncing vibrations and has a significant influence on the excited second air bearing pitch mode. It is also shown that adhesion force serves as an essential factor in exciting the second air bearing mode whereas other interfacial forces only affect details of the slider's bouncing behaviors. By changing the interfacial properties, namely, the interface roughness and lubricant thickness, the variation of interfacial forces with spacing reduction differs, which leads to very different touchdown patterns. With a rougher interface profile the slider smoothly transfers from a flying stage to a sliding stage. With a smoother interface profile the slider experiences a flying-bouncing-sliding transition. With the smoothest interface the slider goes through a flying-bouncing-surfing-sliding transition. The touchdown behaviors predicted by the numerical simulator are correlated with experiments conducted on industry-provided head parts with the same ABS and suspension design. Similar touchdown stages and excited modes are also discovered in the experiments. Though experiments showed a slider spectrum with richer frequency components, the modes missed from the numerical simulations are recovered by conducting a harmonic analysis on a full HGA model with air bearing included.The different touchdown dynamic patterns predicted here result in significant differences in the successful touchdown detection, which is very important for realizing reliable read/write operations, and therefore this work provides guidelines for head disk interface (HDI) optimization. The general approach proposed here is also applicable to studies on the effects of other important HDI factors, such as air bearing geometric features, heater-induced protrusion profiles, and suspension design parameters, and on the slider's touchdown dynamics behaviors, which will assist in obtaining solutions to performance and reliability issues in current hard disk drives.",ucb,,https://escholarship.org/uc/item/0s99v5xz,,,eng,REGULAR,0,0
425,1861,Urban tree mortality,"Roman, Lara Angelica","McBride, Joe R;",2013,"Urban forests have aesthetic, environmental, human health, and economic benefits that motivate tree planting programs. Realizing these benefits depends on tree survival. Cost-benefit analyses for urban forest ecosystem services are sensitive to mortality rate assumptions and associated population projections. However, long-term mortality data is needed to assess the accuracy of these assumptions. Analytical tools from demography, such as life tables, mortality curves, and survival analysis, can improve our understanding of urban tree mortality. Demographic approaches have been widely used in forest ecology to quantify population dynamics and project future changes in wildland systems. However, to apply demographic techniques to urban forests, longitudinal data is needed, with repeated mortality observations on individual trees. In this dissertation, I analyzed five years of longitudinal data from two Northern California studies: street trees in Oakland and yard trees in Sacramento. These field projects are complemented by a conceptual overview of demographic approaches to urban tree mortality (Chapter 1), and an investigation of practitioner-based tree monitoring programs.For the Oakland study (Chapter 2), I documented tree mortality and planting rates, net population growth, and assessed selected risk factors for survival. I monitored the entire street tree population in a small plot for five years after an initial inventory (2006). I adapted the classic demographic balancing equation to quantify annual inputs and outputs to the system, tracking pools of live and standing dead trees. There was a 17.2% net increase in live tree counts during the study period, with 3.7% overall annual mortality. However, population growth was constrained by high mortality of small/young trees. Size-based mortality rates followed a Type III curve, with highest mortality for small trees, and lower for mid-size and large trees. I used multivariate logistic regression to evaluate the relationship between 2011 survival outcomes and inventory data from 2006. Significant associations were found for size class, foliage condition, planting location, and a multiplicative interaction term for size and foliage condition.For the Sacramento study (Chapter 3), I assessed tree losses during the establishment phase for a residential tree give-away program. A cohort of young trees distributed in 2007 was monitored for five years. I used Random Forests to identify the most important risk factors at different life history stages, and survival analysis to evaluate post-planting survivorship. Analysis included socioeconomic, biophysical, and maintenance characteristics. In addition to field observations of tree planting status, survival, and maintenance, I also collected property ownership information (renter vs. owner-occupancy, homeowner change, and foreclosure) through the Multiple Listing Service and neighborhood socioeconomic characteristics from the U.S. Census. I found that 84.9% of trees were planted, with 70.9% survivorship at five years post-planting. Planting rates were higher in neighborhoods with higher educational attainment, and on owner-occupied properties with stable residential ownership. Five-year survival was also higher for properties with stable homeownership, as well as for tree species with low water use demand. When I incorporated maintenance characteristics from the first year of field observations, factors related to tree care were important to survival. Many residents did not adhere to recommended maintenance practices. These results illustrate the critical role of stewardship and consistent homeownership to young tree mortality on residential properties, and suggest that survival assumptions in urban forest cost-benefit models may be overly optimistic.To learn more about practitioner-driven monitoring efforts, I surveyed 32 local urban forestry organizations across the United States about the goals, challenges, methods, and uses of their monitoring programs (Chapter 4). Non-profit organizations, municipal agencies, state agencies, and utilities participated. Common goals for monitoring included evaluating the success of tree planting and management, taking a proactive approach towards tree care, and engaging communities. Challenges included limited staff and funding, difficulties with data management and technology, and field crew training. Programs used monitoring results to inform tree planting and maintenance practices, provide feedback to individuals responsible for tree care, and manage hazard trees. Participants emphasized the importance of planning ahead: carefully considering what data to collect, setting clear goals, developing an appropriate database, and planning for funding and staff time. Urban tree monitoring partnerships between researchers and local organizations should be developed, with standardized protocols and clear research questions. Such partnerships would provide urban forestry professionals with improved mortality information to evaluate the success of planting programs, while expanding the data sets available to researchers. The Oakland and Sacramento studies (Chapters 2 and 3) offer examples of demographic approaches to urban tree mortality that can be replicated and expanded as more longitudinal data becomes available from both researchers and practitioners.",ucb,,https://escholarship.org/uc/item/0sh9g9gk,,,eng,REGULAR,0,0
426,1862,Reliability Studies of Micro-Relays for Logic Applications,"Chen, Yenhao","King Liu, Tsu-Jae;",2015,"The semiconductor industry is now struggling with an integrated-circuit â€œchipâ€ power density crisis due to the non-scalability of the thermal voltage (kBT/q), which sets the minimum subthreshold swing (SS) of a metal-oxide-semiconductor transistor and hence limits reductions in transistor threshold voltage and hence chip operating voltage.  In contrast to electronic switches, mechanical switches (â€œrelaysâ€) operate by making/breaking physical contact and therefore offer the ideal characteristics of zero off-state leakage current and abrupt transition between on/off states, which provide for zero static power dissipation and (in principle) lower operating voltage, so that they potentially can provide a means for overcoming this crisis.  In order to fully realize their promise, however, miniaturized relays must operate with sufficient reliability to be viable for ultra-low-power digital logic applications.In this work, the reliability of micro-relays designed for digital logic applications is systematically investigated.  Contact resistance (Ron) instability is identified as the limiting factor for micro-relay endurance.  Due to surface oxidation, Ron of prototype relays with tungsten (W) contacting electrodes increases with the number of operating cycles.  This phenomenon is affected by relay operating conditions, including switching frequency and contact force.  Larger contact force is found to be beneficial for stable operation, possibly due to breakdown of the insulating oxide layers.  An alternative contact electrode material, ruthenium (Ru), is demonstrated to ameliorate the problem of metal surface oxidation; however, friction polymer formation and material transfer become dominant contact reliability issues, which eventually degrade Ron.  An inkjet-printed micro-shell encapsulation process is used to provide an isolated ambient environment for the logic relays, and demonstrated to result in improved Ron stability by 100Ã— as compared to devices tested in atmospheric conditions.  In complementary logic circuits, slower switch turn-off than turn-on results in undesirable ""crowbarâ€ current, causing transient power dissipation during signal transitions and potential reliability issues.  The effects of the contact electrode material mechanical properties, relay design parameters, and relay operating conditions on contact detachment delay (Ï„CD) are experimentally studied and theoretically explained.  Specifically, Ï„CD is compared for logic relays with tungsten, ruthenium or nickel contacting electrode materials, and tungsten is found to provide for the smallest Ï„CD.",ucb,,https://escholarship.org/uc/item/0sz126px,,,eng,REGULAR,0,0
427,1863,Chemical glycoproteomics for identification and discovery of glycoprotein alterations in human cancer,"Spiciarich, David","Bertozzi, Carolyn R;Wemmer, David E;",2017,"Changes in glycosylation have long been appreciated to be part of the cancer phenotype; sialylated glycans are found at elevated levels on many types of cancer and have been implicated in disease progression. However, the specific glycoproteins that contribute to cell surface sialylation are not well characterized, specifically in bona fide human cancer. Metabolic and bioorthogonal labeling methods have previously enabled enrichment and identification of sialoglycoproteins from cultured cells and model organisms. The goal of this work was to develop technologies that can be used for detecting  changes  in  glycoproteins in clinical models of human cancer. In Chapter 1 of this dissertation, I present an overview of the structures and functions of glycans and their relationship to cancer progression. I also discuss applications of in vivo bioorthogonal labeling in model organisms and how in humans, the significant regulatory and ethical barriers associated with introducing chemically altered sugars into people have hindered it. Finally, I review mass spectrometry-based proteomics and how it can be applied to clinical glycoproteomics.In Chapter 2, I demonstrate the first application of this bioorthogonal labeling in a glycoproteomics platform applied to human tissues cultured ex vivo. Both normal and cancerous prostate tissues were sliced and cultured in the presence of functionalized derivatives of N-acetyl mannosamine, the sialic acid biosynthetic precursor. Chemical biotinylation followed by enrichment and mass spectrometry led to the identification of glycoproteins that were found at elevated levels or uniquely in cancerous prostate tissue. This work therefore extends the use of bioorthogonal labeling strategies to problems of human clinical relevance.Secretome proteins play important roles in regulation of many physiological processes and show utility as potential biomarkers and for noninvasive diagnostics and treatment monitoring. In Chapter 3, I discuss a platform for identifying sialoglycoproteins that were secreted in the conditioned media from bioorthogonally labeled human prostate tissue slice cultures. This platform could be used to identify disease biomarkers in a faithful clinical model of human disease.Mutations in granulocyte colony-stimulating factor 3 receptor (CSF3R), also known as G-CSFR, occur in the majority of patients with chronic neutrophilic leukemia (CNL) and are more rarely present in other kinds of leukemia. In Chapter 4, I discuss novel variants in CSF3R at asparagine residue N610, one of which was germline. Interestingly, these N610 substitutions are potently oncogenic and result in ligand-independent receptor activation. They confer activation of the JAK-STAT signaling pathway and concurrent sensitivity to JAK kinase inhibitors. The N610 residue is part of a consensus N-linked glycosylation motif in the receptor. Detailed mass spectrometry analysis demonstrates that this site is occupied by both complex and complex bisecting glycans. Further analysis demonstrates that N610 is the primary site of sialylation of the receptor. This study demonstrates that membrane-proximal N-linked glycosylation is critical for maintaining the ligand dependence of the receptor. Furthermore, it expands the repertoire of potently oncogenic mutations in CSF3R that are therapeutically targetable",ucb,,https://escholarship.org/uc/item/0t47b9ws,,,eng,REGULAR,0,0
428,1864,Interactive Prediction and Planning for Autonomous Driving: from Algorithms to Fundamental Aspects,"Zhan, Wei","Tomizuka, Masayoshi;",2019,"Inevitably, autonomous vehicles need to interact with other road participants in a variety of highly complex or critical driving scenarios. It is still an extremely challenging task even for the forefront companies or institutes to enable autonomous vehicles to interactively predict the behavior of others, and plan safe and high-quality motions accordingly. The major obstacles are not just originated from prediction and planning algorithms with insufficient performances. Several fundamental problems in the fields of interactive prediction and planning still remain open, such as formulation, representation and evaluation of interactive prediction methods, motion dataset with densely interactive driving behavior, as well as interface of interactive prediction and planning algorithms. The aforementioned fundamental aspects of interactive prediction and planning are addressed in this dissertation along with various kinds of algorithms. First, generic environmental representation for various scenarios with topological decomposition is constructed, and a corresponding planning algorithm is designed by combining graph search and optimization. Hard constraints in optimization-based planners are also incorporated into the training loss of imitation learning so that the policy net can generate safe and feasible motions in highly constrained scenarios. Unified problem formulation and motion representation are designed for different paradigms of interactive predictors such as planning-based prediction (inverse reinforcement learning), as well as probabilistic graphical models (hidden Markov model) and deep neural networks (mixture density network), which are utilized for the prediction/planning interface design and prediction benchmark. A framework combing decision network and graph-search/optimization/sample-based planner is proposed to achieve a driving strategy which is defensive to potential violations of others, but not overly conservatively to threats of low probabilities. Such driving strategy is achieved via experiments based on the aforementioned interactive prediction and planning algorithms with proper interface designed. These predictors are also evaluated from closed loop perspective considering planning fatality when using the prediction results instead of pure data approximation metrics. Finally, INTERACTION (INTERnational, Adversarial and Cooperative moTION)  dataset with highly interactive driving scenarios and behavior from international locations is constructed with interaction density metric defined to compare different datasets. The dataset has been utilized for various behavior-related research areas such as prediction, planning, imitation learning and behavior modeling, and is inspiring new research fields such as representation learning, interaction extraction and scenario generation.",ucb,,https://escholarship.org/uc/item/0vf4q2x1,,,eng,REGULAR,0,0
429,1865,Illnesses as Interests: The Rise of Disease Advocacy and the Politics of Medical Research,"Best, Rachel Kahn","Hout, Michael;",2012,"In the past 30 years, people with serious diseases have organized politically to an unprecedented degree. They founded hundreds of nonprofits, launched fundraising drives, publicized ribbons and walks, and lobbied Congress for funding for research into their conditions. In the first longitudinal study of the field of disease social movements, this dissertation asks why disease advocacy expanded so quickly and how it changed the politics of medical research funding.I combine quantitative and qualitative data to track the emergence of disease advocacy and document its effects. For 53 diseases from 1989 to 2005, I collected data on the advocacy targeting each disease, the number and characteristics of the people each disease killed, and the amount of federal medical research funding for each disease. I combine statistical analyses with qualitative analyses of congressional testimony, reports, and secondary sources.  Studying the emergence of a field of interest groups allows me to test competing theories about the causes of group emergence and the political effects of advocacy. First, I ask how diseases became an established category for interest group politics. I find that changes in science, medicine, and the experience of illness laid the groundwork for the emergence of disease advocacy. But disease advocacy organizations did not proliferate until after the AIDS and breast cancer movements institutionalized a model that diffused rapidly across diseases. These findings suggest that to understand how forms of organizing emerge, we need to look at processes of social movement spillover and the diffusion of organizational forms.Second, I ask how the emergence of disease advocacy changed the politics of medical research funding. Previous research on the political outcomes of advocacy has focused almost exclusively on whether movements achieve benefits for their constituents. I find that the effects of disease advocacy went far beyond simple increases in research funding for organized diseases. Disease advocacy reshaped the funding distribution, shifting money away from diseases that primarily affect women and racial minorities. Disease advocacy also changed the perceived beneficiaries of policies, introduced metrics for commensuration, and made cultural categories of worth newly relevant to policymaking. These findings highlight movements' cultural effects on politics. Third, I ask whether disease movements influenced each other's effectiveness. Researchers generally examine social movements in isolation. But since movements may fight for space on the government agenda or create political opportunities for each other, their outcomes are unlikely to be independent. As disease advocacy expanded, some critics worried that organized diseases would siphon funds from less-organized diseases in a zero-sum game. I find that on the contrary, disease advocacy was synergistic, with gains spilling over across diseases. An analysis of congressional debates suggests that particularistic politics led to increasing budgets by creating new constituencies and by expanding the boundaries of the competition for funds. These results demonstrate that to understand social movement outcomes, researchers must consider their interactions.",ucb,,https://escholarship.org/uc/item/0vp5b83r,,,eng,REGULAR,0,0
430,1866,"I Think I Can: The Relations Among Parenting Self-Efficacy, Parenting Context, Parenting Practices, and Preschoolers' Socio-Emotional Development Among Low Income Immigrant Families","Anicama, Catherine","Zhou, Qing;",2018,"Parenting self-efficacy has been shown to influence parenting practices and childrenâ€™s developmental outcomes. However, little is known about how cultural orientation and parenting stress shape parenting self-efficacy and parenting practices, and how parenting self-efficacy and parenting practices uniquely shape childrenâ€™s socio-emotional development in low-income, immigrant families. In a sample of 88 Mexican American (MA) and Chinese American (CA) low-income, immigrant mothers and their preschoolers, the present study examined the concurrent associations among mothersâ€™ cultural orientation, parenting stress, parenting self-efficacy, parenting practices, and childrenâ€™s socio-emotional adjustment. All constructs were measured by mothersâ€™ self-reports. First, I examined how family demographic characteristics (family income, mothersâ€™ education, mothersâ€™ years in the US), mothersâ€™ cultural orientation to heritage and American culture, and parenting stress (parental distress and parent-child dysfunctional interactions) were associated with mothersâ€™ parenting self-efficacy. Results indicated that mothersâ€™ heritage and American cultural orientations were both associated with greater parenting self-efficacy. In contrast, parenting stressâ€”specifically, dysfunctional parent-child interactionâ€”was associated with less parenting self-efficacy. Second, I examined the associations between parenting self-efficacy and parenting practices (authoritative and authoritarian parenting). Mothersâ€™ parenting self-efficacy was uniquely associated with greater authoritative parenting. In addition, parenting self-efficacy mediated the associations between a) motherâ€™s heritage cultural orientation, American cultural orientation, and parent-child dysfunctional interaction, and b) mothersâ€™ authoritative parenting. Third, I examined how parenting self-efficacy was uniquely associated with childrenâ€™s socio-emotional adjustment. Parenting self-efficacy was associated with more prosocial behaviors and less externalizing problems. Furthermore, authoritative parenting mediated the association between parenting self-efficacy and childrenâ€™s prosocial behaviors. Last, I explored potential differences between cultural groups. Results indicated that MA mothers reported greater parenting self-efficacy compared to CA mothers. I also found cultural group differences in correlations among variables. Overall, the findings highlight the benefits of parenting self-efficacy for childrenâ€™s socio-emotional adjustment and the complex contextual factors that shape parenting self-efficacy in immigrant families. Implications of findings for the development of parenting interventions for low-income immigrant families are discussed.",ucb,,https://escholarship.org/uc/item/0xp3m9jf,,,eng,REGULAR,0,0
431,1867,The Interplay between Sampling and Optimization,"Cheng, Xiang","Bartlett, Peter L;Jordan, Michael I;",2020,"We study the connections between optimization and sampling. In one direction, we study sampling algorithms from an optimization perspective. We will see how the Langevin MCMC algorithm can be viewed as a deterministic gradient descent in probability space, which enables us to do convergence analysis in KL divergence. We will also see how adding a momentum term improves the convergence rate of Langevin MCMC, much like acceleration in gradient descent. Finally, we will study the problem of sampling from non-logconcave distributions, which is roughly analogous to non-convex optimization.Conversely, we will also study optimization algorithms from a sampling perspective. We will approximate stochastic gradient descent by a Langevin-like stochastic differential equation, and use this to explain some of its remarkable generalization properties.",ucb,,https://escholarship.org/uc/item/0zj7t0hb,,,eng,REGULAR,0,0
432,1868,Short-Sequence Approach to Uncovering Regulatory Mechanisms in the Human Immune System,"Afik, Shaked David","Yosef, Nir;",2020,"Short DNA sequences play an important role in the immune response to pathogens. As part of the non-coding regions of the genome, short DNA sequence motifs regulate cell activation and maturation by binding chromatin modifiers and transcription factors. They also determine the ability of each cell in the adaptive immune system to respond to a specific pathogen by forming the antigen-recognizing region of their receptors. This dissertation outlines computational tools I developed for utilizing and integrating high-throughput sequencing data to study the functions of short DNA sequences in the human immune system. I focus on two main aspects of short DNA sequences: (1) As components of the regulatory landscape that control the activation of dendritic cells (DCs) in response to lipopolysaccharide (LPS), and (2) as the determinants of the specificity of T cells and B cells.The first part of my dissertation investigates the regulatory landscape of DC activation following LPS stimulation. In chapter two I present a model which predicts gene induction based on sequence motif occurrences in the regulatory regions of each gene and show that this regulatory logic is conserved between human and mouse. Chapter three describes a supervised learning pipeline I devised to study the contribution of short sequence motifs to temporal epigenetic changes in human DCs. The second part of my dissertation describes my work on determining the specificity of T and B cells from single-cell RNA-sequencing data. Chapter four presents software I developed to reconstruct the full sequence of T cell receptors from short read single-cell RNA-sequencing. An application of the software links the length of the antigen-recognizing region of the receptor to the state of the cell, demonstrating the importance of such combined analysis in studying the immune response to viral infections. Chapter five describes an extension of the software to reconstruct B cells receptor sequences.",ucb,,https://escholarship.org/uc/item/10k1k8dc,,,eng,REGULAR,0,0
433,1869,Phytochemical Regulation of Tumor Suppressive MicroRNA in Human Breast Cancer Cells,"Hargraves, Kristina G.H.","Firestone, Gary L.;",2013,"MicroRNA post-transcriptionally regulate more than half of the transcribed human genome and could be potential targets of anti-cancer therapeutics. The microRNA family, miR-34, is a component of the p53 tumor suppressor pathway and has been shown to mediate induction of cell cycle arrest, senescence, and apoptosis in cancer cells. Indole-3-carbinol (I3C) derived from cruciferous vegetables, artemisinin isolated from the sweet wormwood plant and artesunate derived from the carbonyl reduction of artemisinin effect components of the p53 pathway to growth arrest human cancer cells, implicating a potential role for miR-34 in their anti-proliferative effects. Flow cytometry and Taqman semi-quantitative PCR analysis of I3C, artemisinin and artesunate treated human breast cancer cells indicate all three phytochemicals upregulate miR-34a in a dose and time-dependant manner that correlates with a pronounced G1 cell cycle arrest. Western blot analysis revealed miR-34a upregulation correlates with induction of functional p53 by I3C as well as artesunate and artemisinin mediated decreases in estrogen-receptor alpha  and the cyclin-dependant kinase CDK4, a known target of miR-34a inhibition. Luciferase assays in which cells were transfected with the miR-34a binding site of CDK4 mRNA attached to the firefly luciferase reporter gene confirmed miR-34a directly inhibits CDK4 expression in cells growth arrested by artemisinin or artesunate. Functional miR-34a appears critical for the anti-proliferative effects of I3C and artemisinin as transfection of non-translatable miR-34a inhibitors prevented I3C mediated growth inhibition and reversed artemisinin mediated down-regulation of CDK4. Artemisinin mediated down-regulation of ERÎ± was also reversed in cells transfected with miR-34a inhibitors, implicating a novel role for miR-34a in the regulation of hormonal signaling. Transfection of dominant negative p53 prevented I3C upregulation of miR-34a in growth arrested cells containing wild-type p53 yet had no effect on artemisinin regulation of miR-34a, indicating a p53-indepedent mechanism of miR-34a regulation. Artemisinin and artesunate also upregulate miR-34a expression levels in breast cancer cell lines containing non-functional p53. All of these data suggest that miR-34a plays a critical role in the anti-proliferative effects of artemisinin, artesunate and indole-3-carbinol in human breast cancer cells. Such evidence elucidates the therapeutic potential of each phytochemical to ectopically express tumor suppressive microRNA while implicating the use of miR-34a expression levels to determine the efficacy of phytochemical treatment. Artemisinin, artesunate and I3C could represent an efficacious means of increasing tumor suppressive microRNA in vivo.",ucb,,https://escholarship.org/uc/item/10v5t7wc,,,eng,REGULAR,0,0
434,1870,"Microbial Health Risks to Sanitation Workers in Low-Resource Settings: Incorporation of Field, Molecular, and Modeling Approaches","Sklar, Rachel Sarah","Hammond, Sally K;",2020,"The purpose of this dissertation research is to develop methods and methodologies for estimating microbial exposures and health risks in low-resource settings. Specifically, we focus on evaluating occupational exposures and pathogenic health risks to sanitation workers, a globally understudied and historically marginalized population. Sanitation workers are vital to the function of sanitation systems which separate society from their hazardous waste. However, little is understood about the health risks sanitation workers encounter during the collection and processing of human waste streams. As a result, sanitation workers remain largely invisible in sanitation infrastructure planning and process design. Quantitative risk assessments are necessary to develop meaningful standards and bring visibility to the health risks of invisible workers. However, there is a current gap in the methods used to estimate microbial risks in low-resource settings.In this dissertation, we explore different strategies for measuring pathogen exposures and estimating health risks in low-resource contexts. Using the methods developed herein, microbial exposures and risks to workers are evaluated at each stage in a waste-to-fuel process in Kigali, Rwanda. Specifically, worker exposure and risk to inhaled endotoxin, ingested adenovirus, and ingested Cryptosporidium are estimated. Throughout the dissertation, a combination of environmental sampling methods, behavioral observations, laboratory analyses, and mathematical modeling techniques are used. A specific emphasis is placed on a stochastic modeling approach in order to overcome the variability and uncertainty of conducting risk assessment in low-resource settings.Chapter 1 is an introduction to onsite sanitation systems and the existing gaps in measuring microbial risks in low-resource settings. In chapter 2, a model is constructed using a combination of empirical measurements and literature reported values. Concentrations of indicator organisms are measured in surface waters while parameters such as the frequency of exposure, exposure volume, and the ratio between indicators and pathogens are derived from literature reported values of other exposure scenarios and country contexts. Although this approach is common, the use of assumptions from the literature introduces a high degree of model uncertainty. Thus, in chapter 3 and chapter 4, an attempt to reduce the uncertainty of using ratios is made by collecting site-specific data on pathogen and bioaerosol concentrations in environmental and personal samples. Site-specific observations of individual worker behavior (including exposure activities, exposure frequency, and duration) were also completed incorporated into models estimating worker risk along the ingestion (chapter 3) and inhalation (chapter 4) routes of exposure. Chapter 5 discusses the gaps in sanitation global goal frameworks which perpetuate sanitation worker marginalization and exclusion from sanitation intervention benefits, and highlights areas of research and action that may bring visibility and voice to sanitation workers worldwide.",ucb,,https://escholarship.org/uc/item/11t4t1qs,,,eng,REGULAR,0,0
435,1871,Spatial Attention Improves Retinotopic Mapping,"Bressler, David William","Silver, Michael;",2012,"The visual world is represented in the brain in numerous retinotopic maps that extend along the cortical surface. Directing spatial attention to a given location in the visual scene enhances perception of that location, and strengthens the neural response to visual stimulation at the attended location. However, the effects of attention on neural responses differ across visual cortical regions.        This thesis is concerned with the study of the variety of effects of visual spatial attention on cortical responses. In the following chapters, we describe results from three major research projects. In the first project, we discovered that spatial attention improves response reliability (a signal-to-noise measure) in every cortical region we measured. The magnitude of these effects were greatest in regions with weak retinotopic signals, suggesting the importance of attention for discovering new areas of cortex with topographic organization. In the second project, we demonstrate that the attention-induced enhancement of response reliability results not only from a strengthening of the cortical response, but also a suppression of slow fluctuations in brain activity that are unrelated to the visual input. Moreover, the suppression of these slow fluctuations predicted performance on a visual detection task, whereas the enhancement of response magnitude did not. Finally, in the third project, we demonstrate attention's effect on response magnitude differs as a function of eccentricity; in early visual areas attention enhanced response strength the most for representations of central vision, whereas in dorsal regions attention enhanced response strength most for representations of peripheral vision. Moreover, we discovered that attention expanded the extent of visual space that generated a significant cortical response, and that this effect also differed across eccentricities and visual regions.",ucb,,https://escholarship.org/uc/item/12k197nm,,,eng,REGULAR,0,0
436,1872,Contradictions and Vile Utterances: The Zoroastrian Critique of Judaism in the Shkand Gumanig Wizar,"Thrope, Samuel Frank","Schwartz, Martin;",2012,"My dissertation examines the critique of Judaism in Chapters Thirteen and Fourteen of the Shkand Gumanig Wizar.  The Shkand Gumanig Wizar is a ninth century CE Zoroastrian theological work that contains polemics against Islam, Christianity, and Manichaeism, as well as Judaism.  The chapters on Judasim include citations of a Jewish sacred text referred to as the ""First Scripture"" and critiques of these citations for their contradictory and illogical portrayals of the divine.  This dissertation comprises two parts.  The first part consists of an introductory chapter, four interpretative essays, and a conclusion.  The second part consists of a text and new English translation of Shkand Gumanig Wizar Chapters Thirteen and Fourteen.My first essay presents a new approach to the relation between the citations from the First Scripture in the Shkand Gumanig Wizar and Jewish literature.  Previous scholars have tried to identify a single parallel text in the Hebrew Bible or rabbinic literature as the origin for each of citation.  Borrowing approaches developed by scholars of the Qur'an and early Islamic literature, I argue that the Shkand Gumanig Wizar's critique draws on a more diverse and, likely, oral network of traditions about the biblical patriarchs and prophets.  My second essay contains a close reading of three linked passages concerning angels in Shkand Gumanig Wizar Chapter Fourteen.  I argue that the depiction of angels in these passages responds to a widespread Jewish belief in Metatron, an angelic co-regent whose power equals God's,.   This essay analyzes the these angelic passages in light of the traces of this belief that can be found in the Babylonian Talmud, Jewish mystical literature, and other texts. My third essay concerns one of the longest citations in the critique of Judaism, a version of the story of the Garden of Eden from the first three chapters of the Book of Genesis.  This essay demonstrates that this citation is one of a motif of connected and mutually illuminating garden passages found throughout the apologetic and polemical chapters of the Shkand Gumanig Wizar.  I argue that gardens' prominence in the critique of Judaism, and the Shkand Gumanig Wizar as a whole, derives from gardens' symbolic role in Iranian culture.My final essay compares the critique of Judaism in the Shkand Gumanig Wizar to a Zoroastrian anti-Jewish text from another Middle Persian work, the Denkard.  Whereas the earlier Denkard depicts Judaism mythically, relating the story of Judaism's creation by an evil demon, the Shkand Gumanig Wizar depicts Judaism textually, as citations from the First Scripture.  I argue that the <Shkand Gumanig Wizar's presentation of Judaism as a text is an interpretative key for understanding the Zoroastrian work as a whole.",ucb,,https://escholarship.org/uc/item/14b43599,,,eng,REGULAR,0,0
437,1873,Danger and Data Collection in American Policing,"Arsiniega, Brittany","Morrill, Calvin;",2019,"Empirical evidence is critical for democratic policing. Data are paramount for the effective governance of the police. This has become especially clear as police officer-involved homicides have gained national attention in the last half-decade. In the wake of the killing of Michael Brown in 2014, scholars and activists quickly identified and decried the lack of reliable national data on police use of force. Media outlets like The Washington Post intervened, establishing a dataset on all civilians shot and killed by police that has created, for the first time ever, close-to-accurate data on the lethal dangers to civilians from encounters with the police. Yet danger in policing is not just present for civilians that the police encounter. Officers themselves are in danger while carrying out their duties. While many other jobs are statistically more dangerous than policing, including commercial fishing, logging, and roofing, policing is unique in the potential for intentional assault by civilians that officers sometimes face while performing their jobs. Because of this, in the public imagination and in the views of officers themselves, being a police officer is one of the most dangerous jobs in the United States. Many researchers have shown that a preoccupation with danger is a central â€“ if not the central â€“ element of police occupational culture.As officers carry out their various duties as law enforcers, maintainers of order, and social service providers, however, they experience critical incidents from a much wider variety of sources than just intentional violence from civilians. Officers respond to gruesome traffic accidents and crime scenes; they investigate child abuse; they encounter individuals in the worst moments of their lives or who are suffering from severe mental illnesses. All of these encounters create stress that, accumulated over time, can and does place officers at an elevated risk of mental illness (such as PTSD and depression) and can lead to maladaptive behaviors including substance abuse and even suicide.This projects interrogates what is known empirically about the dangers of being an American law enforcement officer and what is knowable, given extant data collection. I explore the ways in which data collection about the dangers of policing is co-constitutive with police culture itself. I explore the questions: What do we know about the dangers of policing, what do we not know, and why? I use a variety of sources in this project to demonstrate the ways in which the data that are available on policing are themselves a cultural product, the result of a social process in which police themselves historically played, and continue to play, a critical role. I explore the history of the International Association of Chiefs of Police (IACP) using archival records of that organizationâ€™s meetings between 1893 and 1905. I demonstrate the presence, even in the IACPâ€™s earliest days, of certain dominant normative orders of policing, including danger and white male hegemony. I explore the IACPâ€™s cooperation with and encouragement of the FBI, which started the nationâ€™s first data collection on police officer fatalities in 1937. I draw a through line between the IACP, the FBI, and modern data collection which focuses disproportionately on civilian assault to the exclusion of other (and statistically more likely) harms to officers, including mental illness and suicide. I present the databases that track dangers to officers and discuss who runs these datasets, what information they collect, and what knowledge is thus created about the hazards of policing. I suggest, in the spirit of critical data studies, that we cannot take data at face value. Instead, we must continue to understand the ways in which policing data and police culture are co-constitutive.I then present data on police officer injuries from two urban police departments in majority-Black cities in the United States as case studies in what data are available on the dangers of policing. For one department, which I call CPD, I present officer injury data from all causes between 2010 and 2018. In line with earlier research, I show that civilian assaults account for only 11% of all officer injuries. For another department, which I call MPD, I present data from 2015 to 2018 on assaults on officers. I show that, even in a dataset already constrained to civilian assaults, injuries to officers are generally minor; not a single officer from MPD was shot or stabbed during my four-year sample. I use MPD and CPD data to reinforce the understanding that police data are designed to create knowledge about the physical dangers of policing, especially from civilian assault, but are currently incapable of creating reliable knowledge about suicide and mental illness. Data collection practices help to perpetuate the myth that the most dangerous part of policing is the civilians that officers encounter.",ucb,,https://escholarship.org/uc/item/14s7x8hj,,,eng,REGULAR,0,0
438,1874,The extension of bound state electronic structure methods to molecular resonances,"White, Alec Frederick","Head-Gordon, Martin P.;",2017,"In this thesis, we present our work in pursuit of black-box, \textit{ab initio} methods for computing positions and widths of molecular resonances. The method of complex basis functions is efficiently implemented and applied in the context of various electronic structure approximations. Within the static exchange approximation, basis set effects are investigated and the method is applied to a series of N-containing hetercycles. The extension to Hartree-Fock theory allows for more accurate calculations. These methods have been applied to several small molecules, and the computation of properties within this framework is discussed. The application of complex basis functions to shape and Feshbach resonances at correlated levels of theory including M\o ller-Plesset perturbation theory at second order and equation of motion coupled cluster singles and doubles is also investigated from a practical perspective, and the prospect of using these methods for computing accurate potential energy surfaces is explored. Finally, we describe some theoretical and practical aspects of computing positions and widths of low-energy shape resonances by analytic continuation in the coupling constant. We find that the properties of attenuated Coulomb potentials make them ideal for such calculations.",ucb,,https://escholarship.org/uc/item/15s4j57t,,,eng,REGULAR,0,0
439,1875,Consuming the Native Other: Mestiza/o Melancholia and the Performance of Indigeneity in Michoacan,"Spears-Rico, Gabriela","Biolsi, Thomas;",2015,"This projects examines the contested terrain of cultural appropriation within mestizo/indigenous relations in MÃ©xico. Inspired by Phillip Deloriaâ€™s theory of â€˜playing Indian,â€™ I sought to understand the conceptâ€™s applicability to the Mexican context. I utilize performance theory, specifically the performance of racialized identities and the performance of embodied memory, as a lens to examine touristic consumption and as a means to understand the relationship between indigenous identities and mestizaje. Employing ethnographic field methods, I consider how both mestizos and natives act as performers in touristic transactions during the Days of the Dead in Michoacan as well as how the commodification of the Pâ€™urhepecha dead racializes Pâ€™urhepecha Indians as inferior others and impacts Pâ€™urhepecha communities. I pose that mestiza/o visits to MichoacÃ¡n are motivated by their desire to alleviate mixed identity anxiety; mestizos seek indigenous people to resolve their feelings about the Spanish Conquest and to understand the violent moment of rape which birthed mestizaje as well as to encounter their romanticized notion of indigenous primitivity in its purest form. I propose that mestiza/o tourists view Pâ€™urhepechas as surrogate stand-ins for their pre-Columbian ancestors while Pâ€™urhepechas struggle to represent themselves as contemporary beings invested in the globalized political economy.  The mestiza/o longing to tour Pâ€™urhepecha communities functions alongside an articulated Pâ€™urhepecha fear of being viewed as accessories to an imaginary â€˜pre-Columbianâ€™ landscape which relegates indigenous people to the past. I argue that mestizosâ€™ majoritarian position in Mexican society and their distance from contemporary indigenous realities facilitates their consumption and appropriation of the indigenous dead as well as the commodification of living Indians. Mestiza/o tourists engage in mestiza/o melancholia by mourning what they view as the decline of â€˜traditional Pâ€™urhepecha cultureâ€™ while not acknowledging their own participation in the ongoing destruction of indigenous communities. Touring indigenous communities and appropriating indigenous culture does not resolve the violence but further propagates it. Pâ€™urhepechas, however, consider themselves partners in the touristic relationship with an investment in preserving their communitiesâ€™ intimacy and in controlling how they are perceived, consumed, and toured. Pâ€™urhepechas view mestizos as spiritually disoriented, culturally astray people who should be catered to for the sake of profit. The Pâ€™urhepecha preoccupation with preserving community intimacy motivates Pâ€™urhepechas to engage in acts of resistance such as constructing cheap/imitation cultural goods to sell to tourists, barring tourists from particular festivities, and holding specific ceremonies away from the public eye.",ucb,,https://escholarship.org/uc/item/16p2s3tn,,,eng,REGULAR,0,0
440,1876,Essays on the Health Effects of Pollution in China,"He, Guojun","Perloff, Jeffrey M.;",2013,"This dissertation consists of three chapters that analyze the health effects of pollution in China. The first chapter investigates the effect of air pollution on cardiovascular mortality in the urban areas of China. The second chapter estimates the effect of water pollution on infant mortality. The third chapter studies the relationship between water pollution and cancer among the elderly. The first chapter entitled ""The Effect of Air Pollution on Cardiovascular Mortality: Evidences from the Beijing Olympic Games"". I explore the exogenous air pollution variations induced by the 2008 Olympic Games to estimate the effects of air pollution on cardiovascular mortality in China. I use the regulation status during the Olympic Games as an instrument for air pollution. In the fixed-effects instrumental variable model, I find that air pollution has a robust and significant effect on cardiovascular mortality. In contrast, estimates from the conventional associational models are not robust. I estimate that decreasing current PM_10 concentration by 10% will save more than 67,000 lives (from cardiovascular diseases) in the urban areas in China each year.China's surface water system has been severely polluted in the process of rapid industrialization. The second chapter investigates how this water pollution affects infant mortality. I find that surface water pollution has a significant, nonlinear effect on infant mortality. As surface water quality deteriorates, infant mortality first increases and then decreases. Moderate levels of pollution are the most dangerous. People's avoidance behavior may explain the results: as water becomes more polluted people reduce the consumption of surface water. The ordered-probit selection model is applied to estimate the effects, and precipitation and wastewater dumping are used as the instruments for surface water quality. China also witnessed a dramatic increase in cancer rate in the past thirty years. In the third chapter, I investigate whether this high cancer rate is caused by water pollution. The difficulty in estimating the long-run health effects of pollution is that the lifetime exposure to pollution is hard to measure. However, China provides an ideal setting to estimate the long-run health effects of pollution because the Household Registration System (Hukou) effectively stopped people from migrating for many years. I focus on the elderly people (Age>60) because their mobility is extremely restricted by the System, so their life-time exposure to water pollution is more likely captured by the water quality data in recent years. I find that water pollution has large, significant, positive effects on all cancer mortality rate, digestive cancer mortality rate, urinary cancer mortality rate, liver and stomach cancer mortality rate. I also find that water pollution has no impact on cancer mortality rates for the younger adults (Age from 20-50), which may partially justify our argument that pollution exposure for the younger people cannot be accurately measured because they migrate.",ucb,,https://escholarship.org/uc/item/1747h809,,,eng,REGULAR,0,0
441,1877,Modeling of GHG Mitigation Strategies in the Trucking Sector,"Guerrero, Sebastian E.","Madanat, Samer M.;Leachman, Robert C;",2013,"In response to the growing climate change problem, many governments around the world are seeking ways to reduce the greenhouse gas (GHG) emissions of various sectors of the economy. The trucking sector is important in meeting this challenge in the US because it is responsible for a share of emissions that is significant and rapidly growing. For governments to intervene in this sector smartly, they need models that capture its key incentives, constraints and dynamics, while making the most out of the limited data available. However, existing models fall short of this ideal. This dissertation first introduces the Trucking Sector Optimization Model (TSO) as a tool for studying the decisions that carriers and shippers make within a short-run time horizon--modeling the dynamics of truck fleets, penetration rate of Fuel Saving Technologies (FSTs) such as aerodynamic improvements and low rolling resistance tires, and changes in the demand of trucking. In addition to estimating tailpipe GHG emissions, the model also estimates emissions from upstream fuel production sources, vehicle manufacturing, and pavement rehabilitation activities. This model is then used to evaluate the effectiveness of various incentives-based and regulation-based strategies that California's government could implement in the trucking sector to help achieve the objectives of the Global Warming Solutions Act of 2006 (AB 32). The strategies analyzed are: fuel taxation, mileage taxation, truck purchase taxation, FST subsidies, FST regulations, increases in the allowed weight of trucks, and the Low Carbon Fuel Standard recently introduced in California. Results indicate that there presently exist significant economic incentives for carriers to invest in FSTs beyond what is currently commonplace. The correction of market mechanisms that are responsible for this apparently suboptimal behavior, would lead to significant reductions in emissions, and would also allow for incentive-based strategies to have their first-best outcomes. Without making these corrections, the regulation approach currently adopted in California, of mandating certain investments in FSTs, serves as a reasonable first-step in meeting AB 32's medium-term emissions target. However, moving forward, the correction of these market mechanisms and subsequent implementation of incentives-based strategies, particularly those that are complementary with each other, should be a priority. Based on their estimated effectiveness, these and other recommendations are articulated in a seven-step plan for reducing trucking related emissions in the state.The remaining chapters of this work study some long-run factors that affect how carriers manage their fleets and invest in FSTs, in particular considering that they often discount heavily the future because of the existence of various market failures, hidden costs and uncertainties in the industry. The nature of these issues is not investigated deeply in this research, but their effect on carriers is captured by parameterizing the level of discounting in an improved model called the Trucking Sector Trip Segmentation Model (TSTS). This model represents the long-term decisions made in this sector better than the TSO model by: (i) modeling endogenously how trucks are utilized throughout their service-lives, and (ii) capturing some heterogeneity in truck retirements. The first of these improvements is made possible by incorporating information on the performance of trucking (the ability of carriers to complete shipments) and on the spatial distribution of shipment demand. The second of these improvements is made possible by assuming that truck retirements follow a log-logistic function. Combining both of these methodological improvements with a parameterized discount rate provides analysts a more flexible model for studying the long-term decisions made in the trucking sector, especially regarding FST investments, which impact greatly emissions and costs.The TSTS model is then used to evaluate the effectiveness of three additional governmental interventions that reduce GHG emissions, which could not have been studied with the TSO model. Improvements in trucking performance--by reducing congestion or shipment waiting times for example--were found to significantly incentivize investments in FSTs and reduce GHG emissions. However, 40 - 50% of these reductions were offset in the aggregate by increases in the demand for shipments precipitated by the lower market prices of trucking. Mode-shifts were also found to incentivize investments in FSTs because they distort the spatial distribution of shipments in ways that favor making greater capital investments because trucks are used more intensely and retired quicker. And finally, implementing FST regulations that only apply to a subset of the truck fleet (as in California currently) also reduces emissions, but incentivizes other changes in how the industry operates. The TSO model is best suited for studying the dynamics and transitions of truck fleets in response to governmental interventions, while the TSTS model is best suited for studying long-run responses. Together, they allow policy makers and researchers to study a wide range of issues in the trucking sector, considering many interactions and responses that had not been adequately explored previously. They also share a rich theoretical framework that can be used in future research to develop better models of this sector, especially to help design interventions that have environmental objectives.",ucb,,https://escholarship.org/uc/item/17f3r5jg,,,eng,REGULAR,0,0
442,1878,Metal Photocathodes for Free Electron Laser Applications,"Greaves, Corin Michael Ricardo","Falcone, Roger;",2012,"Synchrotron x-ray radiation sources have revolutionized many areas of science from elucidating the atomic structure of proteins to understanding the electronic structure of complex materials such as the cuprate superconductors.  These advances have been possible because of the high brightness of synchrotron radiation.  This high brightness comes from the very small size and divergence of the electron beam, and the interference of light produced by  transverse acceleration of the beam in the periodic magnetic structures in the straight sections of a storage ring.  The radiation produced is however a sum over the emission from individual electrons as there is no phase relationship between the positions of each electron; the emission therefore scales as the number of electrons.  In a Free Electron Laser (FEL), the main difference to the synchrotron radiation mechanism is that the light field acts on the electron beam, over a long distance in an undulator, and causes electron bunching at the optical wavelength.  Electrons in different parts of the electron bunch are therefore correlated, and so emit coherently, with a brightness that scales as the square of the number of electrons.  This coherent emission process results in the FEL having a brightness typically 10 orders of magnitude higher than a synchrotron radiation source.  Although the FEL concept has been around since the mid 1970's, only within the last few months has the world's first x-ray FEL lased.  This FEL, the Linac Coherent Light Source (LCLS) at the Stanford Linear Accelerator Center (SLAC), produces x-rays up to an energy of around 8.5 keV.   A much lower energy FEL (FLASH) capable of operation to a few 100 eV started to operate at the DESY laboratory several years ago. In order to lase, the electron beam in a FEL must have a transverse geometric emittance less than the wavelength of the light to be produced.  For the generation of x-ray wavelengths, this is one of the most difficult challenges in the design and construction of a FEL.  The geometric emittance can be ""compressed"" by acceleration to very high energy, but with the penalty of very large physical size and very large cost.  The motivation for this work was provided by the desire to investigate the fundamental origin of the emittance of an electron beam as it is born at a photocathode.  If this initial, or ""thermal"" emittance can be reduced, the energy, scale and cost of accelerators potentially would be reduced.  As the LCLS used copper as its photocathode, this material was the one studied in this work.  Copper was used in the LCLS as it represented a ""robust"" material that could stand the very high accelerating gradients used in the photoinjector of the FEL.   Metals are also prompt photoemitters, and so can be used to produce very short electron bunches.  This can be a useful property for creation of extremely short FEL pulses, and also for creation of beams that are allowed to expand under space charge forces, but in a way that results in linear fields, allowing subsequent recompression.  An ideal photocathode for FEL photoinjector should have high quantum efficiency (QE), small emittance, fast temporal response, long lifetime, and minimal complexity. High QE of cathodes require less power for driving laser and also reduce the risk of damaging the cathode materials. Small emittance reduce the scale of the accelerator, therefore, the cost. Metal photocathodes such as copper exhibit long lifetime and fast response, but have quite low quantum efficiency \((<10^{-4})\).  The aim in this work was to understand the quantum yield of the metal, and the transverse momentum spectrum, as the product of the latter and the cathode beam spot size gives the transverse emittance.  Initial x-ray diffraction work provided evidence that the LCLS photocathode consisted of large low index single crystal grains, and so work focused on the study of single crystals that could be produced with atomically ordered surfaces, rather than a polycrystalline material.  Present theories of quantum yield and transverse emittance assume the basic premise that the metal is entirely disordered, and work here shows that this is fundamentally incorrect, and that the order of the surface plays a critical role in determining the characteristics of emission.  In order to investigate these surfaces, I constructed a laser - based ultra-low energy angle resolved photoemission system, capable of measuring the momentum spectrum of the emission and wavelength and angle dependent electron yield.  This system has been commissioned, and data taken on low index surfaces of copper.   Results from this work on single crystal copper demonstrates that emitted electrons from the band structure of a material can exhibit small emittance and high quantum efficiency.  We show that the emission from the Cu(111) surface state is highly correlated between angle of incidence and excitation energy. This manifests itself in the form of a truncated emission cone, rather than the isotropic emission predicted from the normal model.  This clearly then reduces the emittance from the normal values.   It also results in extremely strong polarization dependence, with p-s asymmetry of up to 16 at low photon energy.  It also directly suggests ways through changing materials, or by material design to significantly reduce emittance, at the same time increasing electron yield.  These results show the benefits that could be gained from electronic engineering of cathodes and should have direct impact in the design of future FEL photoinjectors.",ucb,,https://escholarship.org/uc/item/0kv9x582,,,eng,REGULAR,0,0
443,1879,The Use of Egyptian and Egyptianizing Material Culture in Nubian Burials of the Classic Kerma Period,"Minor, Elizabeth Joanna","Redmount, Carol;",2012,"The ancient Nubian Classic Kerma culture remains understudied despite the excavation of the burials of the main community at the Kingdom's capital at Kerma almost one-hundred years ago. The finds and associated archive from this historical excavation remain as the primary resource for reconstructing the political and social changes of the Classic Kerma Period (1700-1550 BCE). The Kerman king is implicated in military conflicts of the Second Intermediate Period (1700-1550 BCE), as recorded in several ancient Egyptian texts. As the Egyptian pharaoh lost control of northern territory to the Hyksos of Dynasty 15, southern territory appears to have fallen into Kerman control. Both the royal and private mortuary complexes of the Classic Kerma cemetery contain Egyptian imports in increasing concentrations, demonstrating that increased interregional interaction had repercussions for the Nubian community. This dissertation argues that the nature, scope, and larger implications of the interregional interaction between Kermans and Egypt during the Classic Kerma / Second Intermediate Periods can be reconstructed by analyzing the use of Egyptian and Egyptianizing material culture contained in Classic Kerma burials.Chapter 2 argues that previous studies of the Classic Kerma culture have included misguided or incomplete discussions of the evidence for Egyptian ""influence"" on this Nubian culture. The first publications on the site of Kerma by George Reisner were heavily skewed by his Egypto-centric and colonialist perspectives. The result of his interpretation of the site as an Egyptian colonial outpost was a legacy of reliance on the process of diffusion of Egyptian cultural advances to explain changes in Nubian cultures. Recent scholarship on ancient Nubia instead focuses on continuities over the long history of indigenous cultural developments. This dissertation argues for a nuanced and balanced discussion of Kerman interaction with Egypt, in which it is the relationship between them that creates social changes in the Kerman community.Chapters 3 and 4 on royal Classic Kerma contexts argue for the use of Egyptian and Egyptianizing material culture in programs of kingship ideology. Egyptian sculpture was interred in royal tumuli burials in the same loci as sacrificed Kermans, demonstrating the Kerman king's control over symbolic resources and his subjects. These Egyptian imports can be used to reconstruct the geographic scope and chronological progression of successive Nubian raids into southern Egyptian territory. The motivation for obtaining these imports may have been to stand as material evidence of Kerman military achievements. As conflict with Egypt increased over time, Kerman kings also integrated Egyptian visual elements into their programs of decoration in their monumental mortuary complexes. Wall paintings from the early part of the Classic Kerma Period may have included such Egyptianizing elements as part of a visual presentation of narratives of north - south conflict, speaking to the political events of the time. Faience tile decorations from the close of the Classic Kerma Period demonstrate how Kerman workshops developed previously Egyptian technologies for the design and creation of royal iconography. The use of the Egyptian winged sun disc motif in the form of Egyptian and Egyptianizing material culture concentrated in the last Classic Kerma king's mortuary complex illustrates how material and visual references to Egypt worked in combination to construct a royal Kerman persona. At the same time, a singular use of the same Egyptian winged sun disc motif in a private Classic Kerman burial argues for the connection of royal and private expressions of status and identity.Chapters 5 and 6 on private Classic Kerman contexts argue that the political events of the Second Intermediate Period, and the resulting changes in Kerman kingship also affected social relationships within the rest of the Kerman community. The use of Egyptian imports increases over the four generations of private Classic Kerma subsidiary burials, which are constructed directly into the four main Classic Kerma royal tumuli. Control of Egyptian imports is concentrated in private graves with the most complex burial equipment, suggesting there was a link between the acquisition of exotic material culture and the construction of social status. Additionally, closely Egyptianizing object types were produced at Kerma to provide more accessible alternatives to `authentic' Egyptian imports. The continued use of Nubian burial goods within the same system of social negotiation argues against the use of Egyptian material culture as a process of acculturation. In fact, most Egyptian object types are placed in Kerman burials in ways that diverge significantly from their use in Egyptian funerary practices. The use of Egyptianizing animal motifs in combination with traditional Nubian and fantastical forms in the personalized funerary equipment of the highest-status private Classic Kerman burials also argues against acculturation. Instead, exotic and fantastical motifs were sought out for use in individual distinction in the increasingly restrictive highest-status social faction of the Classic Kerma community. The adaptation of the Egyptian Taweret hippopotamus goddess to represent high-status women demonstrates that they were active participants in the religious - economic exchange of material resources at Kerma.The use of Egyptian and Egyptianizing material culture in Classic Kerma burials demonstrates that this ancient Nubian culture was affected by its changing relationship with Egypt in the Second Intermediate Period. The Kerman king grew in his command of local and foreign material resources, as conflict with Egypt increased over time. As the nature of royal power changed, the internal relationships of the Classic Kerma community increased in social stratification, and Egyptian objects and visual references were used in strategies of status negotiation. Overall, the cultural practices and strategies of interaction of the Classic Kermans remained essentially Nubian, as part of a long history of development of this ancient African culture.",ucb,,https://escholarship.org/uc/item/0nn0m0fv,,,eng,REGULAR,0,0
444,1880,Lenga nÃ²stra?: Local Discourses on Occitan in Southwestern France,"Ritchey, Elyse","McLaughlin, Mairi;",2020,"Use of the Occitan language in southern France has steadily declined over the past eight centuries, as part of a societal shift toward French. This shift has culminated in the current endangerment of Occitan (UNESCO, Ethnologue). Native speakers are aging rapidly, and according to some estimates, the language will disappear by the end of the 21st century (Bernissan 2012, Kranzer 2015). Contemporary efforts to revitalize Occitan have been well received. Whereas the language was once an obstacle to the acquisition of French and denigrated as a patois, it now carries widely recognized cultural cachet (Martel 2013). Previous studies on Occitan have centered on language attitudes (Paulston 1994, Priest 2008, Joubert 2010), on ideological clashes between groups of different speakers (Blanchet 1992, Sumien 2006, Costa 2016, EscudÃ© 2009), and on the presence of Occitan within particular spheres like education (Boyer 2009, Costa 2015) and the media (AlÃ©n-Garabato 2011, HagÃ¨ge 2015). At present, there is a lack of research into the social, historical, and political factors that affect Occitan revitalization efforts on the local level. This study aims to address these factors by analyzing public discourses on Occitan circulating in two small communities in southwestern France, Carmaux and Villefranche-de-Rouergue. These towns are both located in rural areas, where contemporary society is troubled by economic transformation, an aging population, and the pressures of globalization.	The study is an in-depth analysis of contemporary texts drawn from three sources of public discourse: the press, government documents, and documents circulated by associations promoting the Occitan language. Such public discourses both reflect and shape social attitudes and practices. Thus, the representations of Occitan that appear in the corpus allow me to analyze the role that it plays in each community.	The three research questions that guide the study are as follows:Â Â Â Â Â Â Â Â Â Â Â Â 	1. How is Occitan portrayed in public discourse in Carmaux and Villefranche-de-					Rouergue? 	2. What is Occitanâ€™s role in local society, as evidenced in in public discourse in Carmaux 				and Villefranche-de-Rouergue? 	3. In what domains of language use is Occitan portrayed as being present, according to				public discourse in Carmaux and Villefranche-de-Rouergue?In order to address the questions, I use a purpose-built corpus of public discourses, gathered during a fieldwork period at the two research sites. In order to analyze these texts, I employ a methodology adapted from Reisigl and Wodakâ€™s Discourse-historical approach (2009).	Analysis related to the first research question indicates that explicit portrayals of Occitan are largely positive. I find that three Discourses predominate such depictions. The first insists on the aesthetic, emotional, and social value of Occitan. The second casts Occitan as a valuable part of the community. The third insists on Occitanâ€™s role as a link with history, culture, and tradition. All three of them combine to create the impression that Occitan is vital to the community. This finding contrasts interestingly with analysis related to the second question, which finds that Occitan is highly restricted to a set of creative cultural practices that serve to reinforce a larger Occitan regional identity, not necessarily the local character of the town. Therefore, it appears that the limited presence of Occitan in the community is at odds with assertions of its vitality and relevance that appear in the corpus. Finally, analysis of the third research question shows that Occitan is represented as being marginally present or completely absent from all major domains of language use, save that of secular society. Most manifestations of Occitan are facilitated by language promotion and other civil society associations, and are related to cultural events. Thus, I conclude that the function of Occitan in Villefranche-de-Rouergue and Carmaux, as represented in the study corpus, is to facilitate community engagement and local identity, with emphasis on expansion of language use as a lower priority.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  This study suggests that Occitan is undergoing a process of language revalorization and being refashioned as a marker of community belonging in Villefranche-de-Rouergue and Carmaux (Beier and Michael 2018, Dauenhauer and Dauenhauer 1998). Previous studies have also remarked on the lack of dynamism and language acquisition in Occitan as a consequence of a lack of nationalism (Paulston 1994), being limited to ideology (Costa 2016), and failing to break free of state language ideology (Escude 2009). However, I propose that Occitan revitalization is best viewed as a community revitalization project that privileges traditional cultural and linguistic practices that emphasize conviviality and creativity as a response to modern social pressures.",ucb,,https://escholarship.org/uc/item/1813k8nn,,,eng,REGULAR,0,0
445,1881,The Consonant System of Middle-Old Tibetan and the Tonogenesis of Tibetan,"Zhang, Lian",,1987,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/18g449sj,,,eng,REGULAR,0,0
446,1882,Exposure to Manganese from Agricultural Pesticide Use and Neurodevelopment in Young Children,"Gunier, Robert Bruce","Eskenazi, Brenda;Jerrett, Michael;",2013,"Using data from the Center for the Health Assessment of Mothers and Children of Salinas (CHAMACOS) study, this dissertation shows that agricultural use of fungicides that contain manganese (Mn) results in higher levels of Mn in children's homes and teeth, and that higher Mn levels in children's teeth are associated with a modest deficit in neurodevelopment at 6-months of age. In Chapter 2, predictors of Mn concentrations and loadings in house dust samples are evaluated.  The fungicides maneb and mancozeb are approximately 21% Mn by weight and more than 150,000 kg are applied each year to crops in the Salinas Valley, California. It is not clear whether agricultural use of these fungicides increases Mn levels in homes. In this study, predictors of Mn levels in house dust samples are evaluated. House dust samples were collected from 378 residences enrolled in the CHAMACOS study with a second sample collected nine months later from 90 residences.  House dust samples were analyzed for Mn using inductively coupled plasma optical emission spectroscopy. Information from interviews, home inspections, and pesticide use reporting data was used to identify potential predictors of Mn dust concentrations and loadings. Linear mixed-effects models were used to identify significant predictors. Mn was detectable in dust samples from all homes. The median Mn concentration was 171 Âµg/g and median Mn loading was 1,908 Âµg/m2 at first visit. In multivariable models, Mn dust concentrations and loadings increased with the number of farmworkers in the home and the amount of agricultural Mn fungicides applied within three kilometers of the residence. Dust concentrations and loadings were higher in residences located on Antioch Loam soil than other soil types, in homes with poor or average compared to excellent housekeeping practices, and residences located in the southern Salinas Valley compared those located in the town of Salinas or the northern part of the Salinas Valley. In summary, agricultural use of Mn containing fungicides were found to contribute to Mn dust concentrations and loadings in nearby residences and farmworker homes.Chapter 3 presents an analysis that identifies determinants of Mn in prenatal dentin from children's shed teeth. Mn is an essential nutrient, but over-exposure can be neurotoxic. Over 800,000 kilograms of Mn-containing fungicides are applied each year in California. Manganese levels in teeth are a promising biomarker of perinatal exposure. Participants in this analysis included 207 children enrolled in the CHAMACOS study, a longitudinal birth cohort study in an agricultural area of California.  Mn was measured in teeth using laser-ablation-inductively coupled plasma-mass spectrometry. The purpose of this analysis was to determine environmental and lifestyle factors related to prenatal Mn levels in shed teeth. Storage of farmworkers' shoes in the home, maternal farm work, agricultural use of Mn-containing fungicides within 3 km of the residence, residence built on Antioch Loam soil and  Mn dust loading (Âµg/m2 of floor area) during pregnancy were associated with higher Mn levels in prenatal dentin (p<0.05). Maternal smoking during pregnancy was inversely related to Mn levels in prenatal dentin (p<0.01). Multivariable regression models explained 22 - 29% of the variability of Mn in prenatal dentin. These results suggest that Mn measured in prenatal dentin provides retrospective and time specific levels of exposure to the fetus resulting from environmental and occupational sources.Chapter 4 evaluates the association between Mn in prenatal and postnatal dentin of children's shed teeth and early neurodevelopment. Previous studies have observed associations between Mn exposure and children's neurodevelopment, primarily using concurrent exposure measurements in blood or hair. Prenatal and postnatal Mn exposures have not been evaluated together in a prospective study of neurodevelopment in young children. Mn levels in prenatal and postnatal dentin were measured from children's shed teeth. The relationship between prenatal and postnatal exposure and children's performance at 6, 12 and 24-months of age on the Bayley Scales of Infant Development mental and psychomotor development indices was examined. The possibility of an inverted U-shaped association with neurodevelopment was explored since Mn is an essential nutrient. Potential interactions between Mn exposure and blood lead concentrations were also evaluated as well as effect modification by maternal iron status during pregnancy. An inverse association between postnatal Mn levels in dentin and psychomotor development at 6-months of age was observed with a modest decrease in psychomotor development scores, which followed an inverse U-shape, with the strongest effect observed when comparing the highest tertile of Mn levels in teeth to the middle tertile of Mn levels in teeth (-4.6 points; 95% Confidence Interval: -8.0, -1.3). Among children whose mothers' were iron deficient during pregnancy, prenatal Mn levels in dentin were associated with both mental and psychomotor development at 6-months. No interactions with prenatal or postnatal blood lead concentrations were observed in this cohort. In conclusion, a modest decrease in psychomotor development at 6-months of age was associated with postnatal Mn levels in dentin from a mean score of 96 for the middle tertile compared to 94 at the highest tertile. Iron status during pregnancy appeared to be a potentially important effect modifier of prenatal Mn exposure and neurodevelopment at 6-months of age.",ucb,,https://escholarship.org/uc/item/1908k2m0,,,eng,REGULAR,0,0
447,1883,"Silence and Alterity in Russia after Stalin, 1955-1975","Kayiatos, Anastasia Ioanna","Naiman, Eric;Chen, Mel Y.;",2012,"Taking as its theme the unsayable and the unsaid in post-Stalin Russia, this interdisciplinary dissertation pushes scholars to see more of the Soviet experience than the usual `totalitarianism' lens would allow. Its six chapters apply pressure to the cold war repressive hypothesis that casts the whispering citizens of Stalin's Russia as restored to speech during Khrushchev's cultural thaw only to be muted once more in the late sixties by political stagnation. The prevalence of this view in Russian cultural studies and national collective memory has rendered it rather difficult to write about late socialism until recently, when scholars started to take a multisensory approach to the Soviet past--not only listening to the verbal narratives of the era (whether official or dissenting), but also looking at the dynamic tensions between socialist speech and the socialist body. To counter the commonplace of Soviet history that makes quiet consonant with submission or complicity, this study attends instead to the manners in which Soviet subjects opted for silence to speak truth to power, as with the Aesopian gestural language of avant-garde pantomimists. It also pursues the wily ways that subjects presumed or produced as unspeaking or unspeakable--including the deaf-mute, the racial primitive, the sexual deviant, and the illiterate criminal--performed the silences imputed to them to say something else and, so doing, improvised interesting and unexpected scripts for late socialism.",ucb,,https://escholarship.org/uc/item/1989594t,,,eng,REGULAR,0,0
448,1884,"Ecco la radio: Music, Media, and Politics in Fascist Italy","Simon, Danielle Andrea","Smart, Mary Ann;",2020,"Italian radio was politicized from the first broadcast of a speech by Mussolini in 1925. The dictatorâ€™s thunderous voice may have sought domination over the airwaves, but in the only Western nation where radio broadcasting evolved under a totalitarian government, the vast majority of broadcasting hours was devoted to entertainment. Far from being apolitical, radio transmissions of music and theater were central players in fascismâ€™s culture industry, shaped both by the profit-driven strategies of media enterprises and by the emotional investment and listening habits of consumers. In this dissertation, through analysis of musical scores, recordings, films and unpublished archival materials, I demonstrate that radio during fascism sought to immerse listeners within a statist fantasy, and that cultural broadcasts were not only political, but themselves generated fascist ideas and discourse. I argue that we cannot understand either fascismâ€™s historical role or its lingering presence in global politics and culture without understanding the ways in which it was mediated by and through radio.",ucb,,https://escholarship.org/uc/item/19f5v00q,,,eng,REGULAR,0,0
449,1885,Essays in Public Economics,"Wingender, Philippe","Saez, Emmanuel;",2011,"This dissertation explores the impact of government interventions on economic outcomes. In the first chapter, my colleague Juan Carlos SuÃ¡rez Serrato and I propose a new identification strategy to measure the causal impact of government spending on the economy. Our methodology isolates exogenous cross-sectional variation in government spending using a novel instrument. We use the fact that a large number of federal spending programs depend on local population levels. Every ten years, the Census provides a count of local populations. A different method is used to estimate non-Census year populations and this discontinuous change in methodology leads to variation in the allocation of billions of dollars in federal spending. We use this variation to analyze the effect of exogenous changes in federal spending across counties on local economic outcomes. Our IV estimates imply that government spending has a local income multiplier of 1.88 and an estimated cost per job of $30,000 per year. These estimates are robust to the inclusion of potential confounders, such as local demand shocks. We also show that the local effects of government spending are not larger than aggregate effects at the MSA and state levels. Finally, we characterize the cross-sectional heterogeneity of the impacts of government spending. These results confirm that government spending has a higher impact in low growth areas and leads to reduction of inequality in economic outcomes.The second chapter uses timing of childbirth to measure the income effect of taxes on parents' labor supply. The IRS Residency Test states that families can claim a dependent for the entire fiscal year if the child was born at any time during the year. This rule provides an exogenous source of variation in tax liabilities for births that occur late in the year versus those that occur early the following year. By measuring the difference in earnings in the subsequent year for parents of December and January births, I can identify the impact of a one-time non-labor income shock on parents' labor supply since both groups face on average the same future stream of tax schedules after birth. Using data from two large scale household surveys in the United States, I find that a temporary increase in after-tax income leads to a significant decrease in mothers' earnings with an estimated income effect of -0.9. This result demonstrates that the income effect of taxes on labor supply can potentially be very large. It also highlights the crucial role of liquidity constraints in parents' labor supply decisions around the time of birth.The third chapter also uses timing of childbirth to measure the income effect of taxes on mothers' labor supply. The analysis is done using Canadian data. Until 1992, various provisions in the Canadian tax code gave important tax reductions to low and middle-income parents of eligible children. Families could claim a child as a dependent for the entire fiscal year if the child was born at any time during the year. In 1992, the last year the Canadian tax code featured these fiscal benefits, a two parent family claiming a dependent could save nearly a thousand dollars in taxes due to the Child Tax Credit, the dependent amount and the GST credit. Using this variation in tax liabilities, it is possible to identify the impact of a one-time non-labor income shock on mothers' labor supply. This important parameter has not been systematically measured in the literature on the effect of taxes on labor supply. Using natural experiments provided by tax reforms in various countries, the literature has mostly focused on changes in earnings due to the price effect of marginal tax rate changes. However, if the income effect of a tax change is large, observed elasticities of income with respect to net-of-tax rates understate the distortions associated with these changes.",ucb,,https://escholarship.org/uc/item/1bq8c9nt,,,eng,REGULAR,0,0
450,1886,Energy systems transformation and the political economy of climate change,"Huberty, Mark E.","Zysman, John;",2013,"Climate change mitigation requires immediate and enduring cuts to greenhouse gas emissions, achievable only through the transformation of today's fossil fuel energy systems. Those systems today provide high-quality, inexpensive, and dependable energy to industrial societies. The low-emissions renewable energy systems that would replace them are, as of 2013, still more expensive, more complex, and unproven.This combination of factors makes the political economy of climate change mitigation immensely difficult. Achieving real emissions reduction will impose very large material costs powerful interests, in pursuit of distant--if potentially massive--environmental benefits. These conditions are not auspicious for adopting, much less sustaining, effective climate policy.Yet an increasingly large number of countries have taken explicit or implicit action to reduce greenhouse gas emissions. These actions include emissions pricing schemes, renewable energy research and development, energy efficiency mandates, technological research and development, and a host of other policies. The list of states pursuing such policy is as diverse as the policies themselves: the European Union, South Korea, India, and even China have adopted some or all of the provisions outlined above.This dissertation addresses how states have overcome the apparently sizable barriers to climate change mitigation. It argues that successful states have made progress by choosing policies that target environmental ends with economic gains. Those gains comes through benefits derived from the transformation of national energy systems--transformations that improve energy security, increase economic competitiveness, improve technological leadership, or target other opportunities and challenges in the legacy energy infrastructure. By targeting such areas explicitly, these policies create new constituents with a material stake in long-term policy stability. Those constituents act as valuable political allies in the political fight over the scale and distribution of costs for emissions reduction.But while such policy strategies have proven successful to date, they do not resolve the underlying problem of cost that has plagued climate change mitigation to date. Massive emissions reduction still poses net economic costs, even if it yields huge positive environmental benefits. The benefits created by a low-emissions energy systems transformation can offset those costs, and if targeted can create supportive economic constituents. But the low-emissions energy systems of the future still do not, as of this writing, offer any novel economic or technological improvements over the reliably and ubiquitous energy we enjoy today. Hence these policies remain at risk of disruption from outside forces. The recent policy stagnation in the United States and Europe point to the risks posed by this inherent policy vulnerability.",ucb,,https://escholarship.org/uc/item/1bz65969,,,eng,REGULAR,0,0
451,1887,"Sonic Negations: Sound, Affect, and Unbelonging Between Mexico and the United States","Ramos, Ivan Alejandro","Catanese, Brandi W;RodrÃ­guez, Juana M;",2015,"This dissertation uses the concept of auditory cultures to trace how Mexican and U.S. Latina/o subjects use sound and music to articulate political dissent. â€œSonic Negations: Sound, Affect, and Unbelonging Between Mexico and the United Statesâ€ brings together the fields of performance and sound studies to show how the sonic presents a contested political arena through which transnational Latina/o artists, musicians and listening publics construct alternative sensory realms detached from national forms of belonging. I contend that non-culturally dependent forms of soundâ€”noise, metal, punk, and 80s British musicâ€”allow individuals to approach negative affects, such as melancholia, despair, and idleness. Drawing from recent work in performance studies and queer theory on the political potentialities of negative affects, the study argues for the importance of attending to the political critiques inherent in auditory cultures and practices within both Latina/o and Latin American contexts. The first chapter analyzes Mexico City-based artist IvÃ¡n Abreuâ€™s series M(R.P.M), in which he creates playable ice-records of nationalist Mexican songs. I investigate how the pieces invite listeners into the complex history of ice in the Latin American imaginary to question contemporary calls for Mexican nationalism. The second chapter contemplates the early performances of Mexican artist collective SEMEFO and the work of underground Tijuana-based musician MarÃ­a y JosÃ©. I argue for the ways these artists use aggressive sound to confront the unending violence that has plagued Mexico over the last few decades. My third section concentrates on performance artist Nao Bustamante and her â€œmariachi-punkâ€ band Las Cucas to consider the lesbian punk scream as an ethical rejection of normative Latina and queer identity. The final chapter contemplates the phenomenon of the Latina/o fan culture around British rock star Morrissey. I show how Latina/o listening publics turn to melancholia and depression as affects that force us to reconsider relationality by questioning the nation as the site of inclusion.",ucb,,https://escholarship.org/uc/item/1d17830z,,,eng,REGULAR,0,0
452,1888,HipHop Scholastics: Effective Teacher Professional Learning,"Garcia, Itoco","Pearson, David;",2016,"AbstractHipHop Scholastics: Effective Teacher Professional LearningbyItoco GarcÃ­aDoctor of EducationUniversity of California, BerkeleyProfessor P. David Pearson, ChairMeeting the needs of culturally and linguistically diverse students in order to improve their academic engagement and achievement is a central challenge to American education. A Critical Race analysis and synthesis of academic research on teacher professional learning, Culturally Relevant Pedagogy (CRP), and the mismatch between schools and students revealed: problems of practice and obstacles to changing them, limitations, a conceptual framework for effective professional learning and a vehicle for changing from current to more promising practices. Problematic beliefs including: teacherâ€™s frames of reference; deficit thinking; and low expectations; combine with problematic practices such as rejection of student language and lack of: reflection; culturally relevant materials; and critical comprehension strategies to create a cultural mismatch, low student engagement, and low academic achievement in Urban Bay Unified Schools where this study took place and I worked as a principal. Teacher professional learning on CRP surfaced as a method for change. The theory of action suggested sequential professional development on systematic reflection, student academic engagement, and critical literacy would: improve teacherâ€™s awareness of student academic engagement and linguistic affirmation; improve teacherâ€™s perceptions of students; increase the use of critical literacy, culturally relevant materials and systematic reflection and more closely align schools with students thereby impacting student academic engagement and achievement. This participant action design study focused on an intervention that took place over ten sessions spanning four months consisting of four elementary teachers from the same school that yielded the process data. Participants were given surveys, interviewed, and observed by researchers both before and after the intervention to determine any impact on their beliefs and practices. Outlining each participantâ€™s learning process supported a cross comparison of teachers to better understand how the impacts might have occurred. The intervention impacted systematic reflection, awareness of student academic engagement, perceptions of students, and the use of culturally and linguistically responsive practices like HipHop and linguistic affirmation. Research design challenges lessened the impact on critical literacy and may have contributed to confirmation bias. Future applications of the conceptual framework and iterations of the intervention design might (a) inform teacher professional learning on transitioning underlying beliefs from deficit to positive; (b) shift practice towards more culturally relevant materials and the use of HipHop; and (c) inform research on student academic engagement. The implications of this work might contribute to research on teacher learning by adding systematic reflection, reflective journaling and guided meditation to existing ideas about optimal form. Mechanisms that surface teacher trauma and help heal compassion fatigue in teacher professional learning are critical areas for future study and may be key in making it effective.  Investigating sequential professional learning in systematic reflection, student academic engagement, critical literacy and the use of HipHop to examine implicit bias, improve empathy and shift practice to improve student engagement and achievement is a compelling area for future research as well.",ucb,,https://escholarship.org/uc/item/1d71h84k,,,eng,REGULAR,0,0
453,1889,Essays on Consumption and Labor Supply,"Koustas, Dmitri Konstantine","Card, David;Romer, David;",2018,"This thesis investigates topics in consumption and labor supply using ""big"" data from bank accounts and credit cards from personal financial records. Chapter 1 discusses how these data allow researchers to examine economic activity in gig economy jobs, a sector has been difficult to measure due to lack of data.  Chapter 2 focuses on one popular gig economy industry, ridesharing, to explore whether flexible work can help workers better smooth their consumption.  Chapter 3 examines how these data can be used to measure the marginal propensity to consume out of permanent shocks by exploiting changes in gasoline prices.",ucb,,https://escholarship.org/uc/item/1dg6g3z0,,,eng,REGULAR,0,0
454,1890,Regulation of cellular signaling pathways by endocytosis and protein degradation,"Windler, Sarah","Bilder, David;",2010,"The defined shapes and sizes of the diverse tissues that comprise multicellular organisms are intimately linked to their specialized functions.  The plethora of known cellular behaviors and characteristics that contribute to tissue function and development comes in large part from any given cell's ability to receive and respond to signals from its environment.  These signals are often received via transmembrane protein receptors that interpret extracellular signals and transmit them inside the cell to effect cellular changes.  Cellular signaling pathways influence cellular shape, proliferative capacity, cell fate, cell movement and many other traits that are crucial for proper cellular and tissue function both during development and over the lifetime of an animal.  Therefore, understanding the mechanisms by which cells regulate these signals is a central question in biology.	Drosophila is an ideal model organism in which to study the regulation of cellular signaling pathways.  In addition to the ease of genetic manipulation, Drosophila contains epithelial tissues comparable to those in mammals, which have defined apico-basal polarity that is essential for tissue function.  Cell polarity signaling pathways in epithelial cells can be studied by the isolation of neoplastic tumor suppressor genes (TSGs) from genetic screens.  Drosophila neoplastic TSGs are genes whose functions are absolutely required for proper apico-basal polarity and the regulation of epithelial cell proliferation, though the underlying mechanisms of this regulation and the links between polarity and proliferation remain unknown.  In addition to studying how these genes regulate cellular polarity, the identification of certain classes of neoplastic TSGs, such as the endocytic regulators, has given us tools to investigate the regulation of other developmentally important signaling pathways.     A number of cellular signaling pathways are regulated by endocytosis, which is commonly thought to play a role in the attenuation of signaling, but recently appreciated to provide a much more complex method of regulation that may in some contexts promote signaling as well.  In general, the roles of both cargo internalization routes and subsequent trafficking down a degradative or recycling pathway in the regulation of cellular signaling pathways are not understood.  In Chapter 2 I investigate the role of endocytosis on both trafficking and signaling regulation of the Notch/Delta pathway in Drosophila.  I address the roles of specific components of the endocytic internalization machinery in the endocytosis of Notch and Delta, and define whether distinct paths of entry into the cell impact Notch signaling.  In Chapter 3, I explore how endocytic trafficking regulates epithelial cell polarity and identify links between the endocytic and junctional scaffold classes of tumor suppressors.  In particular, I investigate the possibilities that a common underlying mechanism of tumor suppression exists for these different classes.     Another mechanism through which cells regulate signaling pathways is the ubiquitin-proteasome system.  In Drosophila, one well-known example of this is the negative regulation of Hedgehog and Wingless signaling pathways via the ubiquitylation of downstream effectors by the E3 ubiquitin ligase SCFSlmb.  In a screen for new neoplastic TSGs, a component of this E3 ubiquitin ligase was recovered along with many endocytic regulators.  In Chapter 4, I detail my unexpected discovery that slmb regulates the epithelial cell polarity signaling pathway.  I further describe my progress toward finding a presumed polarity-regulating Slmb substrate via a genetic interaction screen.     In sum, my graduate work sheds new light on the ways in which basic cell biological mechanisms, including endocytosis and regulated proteolysis, regulate specific signaling pathways to control cell fate, proliferation, and polarity.",ucb,,https://escholarship.org/uc/item/1gj2j5v7,,,eng,REGULAR,0,0
455,1891,The Development and Validation of a Tactile Processing Speed Measure,"McKerracher, Amanda Lee","Worrell, Frank C;",2014,"Processing speed refers to the cognitive ability that is involved in fluently performing cognitive tasks with simple stimuli.  Individual differences in processing speed can predict performance on tests of complex cognitive functions such as memory and fluid reasoning, as well as performance on academic tests in reading, writing, and mathematics.  For this reason, measures of processing speed are included in most cognitive assessment batteries.  However, extant measures of processing speed rely on visual stimuli, making them inaccessible to individuals with visual impairments.  The current study describes the adaptation of one of the most commonly used processing speed measures, WISC-IV Coding, into a tactile task.  Using a sample of 19 high school students (Mage = 15.74) with visual impairments who use braille as their primary literacy medium, have no additional disabilities, and are on track to receive their high school diploma by age 22, preliminary validation analyses were conducted.  Split-half reliability calculations showed that scores on the instrument were reliable (a = .92).  Additional instruments were administered - including the KeyMath-R braille adaptation, DIBELS braille reading fluency, and the Blind Learning Aptitude Test - to examine convergent and discriminant validity, and results provided evidence of convergent validity.  Implications for practice and future directions for research are discussed.",ucb,,https://escholarship.org/uc/item/0q13296s,,,eng,REGULAR,0,0
456,1892,Myriad Mirids: The spectacular radiation of Pseudoloxops (Hemiptera: Miridae) plant bugs in French Polynesia (and the kids that love them!),"Balukjian, Bradley James","Gillespie, Rosemary G;",2013,"Studies of natural history and biodiversity may not top most funding agencies' priority lists, but they should. It is an exciting time for the field of biology--we are sequencing whole genomes, devising sophisticated models to cope with accelerating climate change, and even tinkering with the possibility of bringing extinct species back to life. But in the meantime, we continue to ignore the documentation and discovery of the vast majority of extant life on our planet. Millions of species, each with their own unique evolutionary history and trajectory, remain unknown, waiting to tell us their story and teach us their strategies for success. Here, my collaborators and I demonstrate the importance of documenting the diversity contained within a single lineage of insects, from examining the best methods for accurately determining numbers of species to showing the downstream benefits of incorporating that knowledge into local education for the benefit of all. In the first chapter, we revise the taxonomy of a lineage of plant bugs (Hemiptera: Miridae) that has radiated in the islands of French Polynesia. Six species of endemic Pseudoloxops/italic> plant bugs were previously known from two islands in French Polynesia, indicating a small radiation. We collected ecological, morphological, molecular, and geographical data for hundreds of fresh and historical  Pseudoloxops/italic> specimens, expanding the genus' range to nine islands in two archipelagoes (the Austral and Society Islands). We combined all of the above data sources in an iterative integrative taxonomy framework to test the six existing species hypotheses and to search for new diversity. We confirmed 3 of the 6 original species designations and synonymized the remaining 3 species, and delimited and described an additional 23 species, for a total of 26. Our analysis demonstrates the value of an integrative approach, as we discovered cryptic species and color polymorphism that may have been missed or misinterpreted using a species concept that relied on a single line of evidence. We also found evidence for population-level diversification and discuss the potential for future research on the role of color in this radiation.In the second chapter, we explore the relative importance of ecology and geographic isolation in this lineage to provide a first approximation of whether the radiation was adaptive or non-adaptive. We collected  Pseudoloxops/italic> from a wide range of plants, with 27 species in 25 different plant families and 13 orders. We then inferred a combined Bayesian molecular phylogeny from three genes, including 25 of the 26 known  Pseudoloxops/italic> species, to examine the roles of plant affiliation and geography (island distribution) in speciation. We reconstructed the ancestral states using parsimony for these two characters, and found 12 speciation events that were well-supported in the phylogeny. Both plant-switching and island-hopping were correlated with speciation. For the 7 speciation events for which we could unequivocally determine plant affiliation before and after speciation, 4 were associated with a plant shift. For the 8 speciation events where island distribution could be reconstructed, two involved shifts to a new island. There were 5 cases for which we could determine both character states before and after speciation. In three of them, speciation occurred within the same locality with a switch in plant taxonomic order, suggesting that the lineage has great dietary versatility. However, much more research into feeding needs to be conducted, as anecdotal evidence from  Pseudoloxops/italic> outside of French Polynesia suggests they may be facultative predators. In the other two speciation events, there was neither a geographic shift nor a change in plant affiliation, suggesting some other mechanism for speciation. Based on our results, both plant-switching and geography have played a role in the diversification of this radiation. Finally, plant switching from flowering plants (angiosperms) to ferns was observed in two different parts of the radiation. This finding was surprising for two reasons--first, plant bugs are rarely associated with ferns, likely because of their highly toxic secondary compounds, and second because the expectation on islands is that organisms colonize ferns first and then switch on to other plants, since ferns are often among the first plants to arrive on newly formed oceanic islands. While a better-resolved phylogeny is needed to reconstruct the timing of speciation events, the character polarity in our phylogeny indicates that angiosperm use is basal to fern use. In the third chapter, we address the larger societal impact of taxonomic and biodiversity research by examining the effect of a natural history-driven curriculum on elementary schoolchildren's scientific knowledge. While studies have demonstrated the potential for natural history education to improve children's attitudes towards and knowledge of science and nature, few studies have been done in areas where indigenous culture heavily influences children's worldview. The lead author taught a nine-month natural history/biodiversity class focused on insects and plants to fifth-graders at the Pao Pao elementary school on the French Polynesian island of Moorea and tested their scientific knowledge before and after receiving the program. We compared their results to a control that did not receive the program, and while both cohorts improved, the experimental group's improvement was significantly greater (mean of 82.2% vs. 30.5%). We performed a delayed post-test evaluation three years after the conclusion of the program with a subset of the experimental cohort to test their retention and interest in science. A one-way ANOVA revealed significant differences between their pre-, post-, and delayed post-test scores, with the post- and follow-up scores significantly higher than the pre-scores. While the raw delayed post-test scores were lower than the post-scores, suggesting some regression, this finding was not statistically significant. The follow-up students also reported a strong interest in science, with 66.7% answering the question ""Do you like science?""  with ""yes"" and 20% with ""sort of."" They also indicated a strong affinity for insects and plants, with 50% of them volunteering insects as their favorite subject in science and 26.7% volunteering plants. Finally, the qualitative coding of the experimental group's test and survey responses revealed both the influence of indigenous culture on their scientific understanding and the appeal of taxonomy and field trips to children. When prompted for an example of a native plant, 24% of the experimental group named a plant introduced by the Polynesians, suggesting the misconception that plants with a prevalent role in indigenous culture have always been there. In the follow-up survey, 36.7% mentioned the field trips among their memories of the course, and 20% gave full scientific names for species they recalled from the class. The latter contrasts with the commonly held belief that taxonomy is too arcane to connect with the general public.Overall, our research demonstrates the scientific and societal benefits of thorough natural history and biodiversity studies. The use of integrative methods allowed for the discovery of a staggering number of plant bug species in a very small area of land, and the documentation of ecological attributes allowed us to show how this radiation of bugs has been both adaptive and non-adaptive. The integration of this biodiversity information and a focus on traditionally ""uncharismatic"" groups of organisms (insects and plants) in local education provided substantial gains in schoolchildren's scientific knowledge, and perhaps more importantly, helped to popularize science and nature. Our hope is that this work inspires future graduate students to pursue research on the unknown and the undiscovered, and to link their findings directly to local communities.",ucb,,https://escholarship.org/uc/item/0q8547b2,,,eng,REGULAR,0,0
457,1893,The Decoupling of Linear Dynamical Systems,"Kawano, Daniel Takashi","Ma, Fai;",2011,"Decoupling a second-order linear dynamical system requires that one develop a transformation that simultaneously diagonalizes the coefficient matrices that define the system in terms of its distribution of inertia and viscoelasticity.  A traditional approach to decoupling a viscously damped system uses the eigenvectors of the corresponding undamped system to diagonalize the mass, damping, and stiffness matrices through a real congruence transformation in the configuration space, a process known as classical modal analysis.  However, it is well known that classical modal analysis fails to decouple a linear dynamical system if its damping matrix does not satisfy a commutativity relationship involving the system matrices.  Such a system is said to be non-classically damped.  We demonstrate that it is possible to decouple any non-classically damped system in the configuration and state spaces through generally time-dependent transformations constructed using spectral data obtained from the solution of a quadratic eigenvalue problem.When a non-classically damped system has complex but non-defective eigenvalues, the effect of non-classical damping is that it introduces constant phase shifts among the components of the system's free response.  Decoupling of free vibration in the configuration space is achieved through a real, linear, time-shifting transformation that eliminates these phase differences, yielding classical modes of vibration.  This decoupling transformation, referred to as phase synchronization, preserves both the eigenvalues and their multiplicities.  When cast in a state space form, the transformation between the coupled and decoupled systems is real, linear, but time-invariant.  Through the concept of real quadratic conjugation, we illustrate that there is no fundamental difference in the representation of the free response of a system with complex eigenvalues and one with real eigenvalues, and thus systems with non-defective real eigenvalues can also be decoupled by phase synchronization.  When phase synchronization is extended to forced systems, the decoupling transformation in both the configuration and state spaces is nonlinear and depends continuously on the applied excitation. If a non-classically damped system is defective, it may only be partially decoupled if one insists on preserving the geometric multiplicities of the defective eigenvalues.  We present the first systematic effort to decouple defective systems in free or forced vibration by not demanding invariance of the geometric multiplicities.  In the course of this development, the notion of critical damping in multi-degree-of-freedom systems is clarified and expanded.  It is shown that the decoupling of defective systems is a rather delicate procedure that depends on the multiplicities of the system eigenvalues.  A generalized state space-based decoupling transformation is developed that relates the response of any non-classically damped system to that of its decoupled form.  In principle, one could extract from the state space a decoupling transformation in the configuration space, but it generally does not have an explicit form.  The decoupling transformation in both the configuration and state spaces is real and time-dependent.  Several numerical examples are provided to illustrate the theoretical developments.",ucb,,https://escholarship.org/uc/item/0j17g8hw,,,eng,REGULAR,0,0
458,1894,Cooperative Interference Management in Wireless Networks,"Wang, I-Hsiang","Tse, David;",2011,"With the growing number of users along with ever-increasing demand for higher data rates and better quality of service in modern wireless networks, interference has become the major barrier against efficient utilization of limited resources. On the other hand, opportunities for cooperation among radios also increase with the growing number of users, which potentially lead to better interference management. In traditional wireless system design, however, such opportunities are usually neglected and only basic interference management schemes are employed, mainly due to lack of fundamental understanding of interference and cooperation. In this dissertation, we study the fundamental aspects of cooperative interference management through the lens of network information theory. In the first and the second part, we characterize both qualitatively and quantitatively how limited cooperation between transmitting or receiving terminals helps mitigate interference in a canonical two-transmitter-two-receiver wireless system. We identify two regions regarding the gain from limited cooperation: linear and saturation regions. In the linear region cooperation is efficient and provides a degrees-of-freedom gain, which is either one cooperation bit buys one bit over the air or two cooperation bits buy one bit over the air until saturation. In the saturation region cooperation is inefficient and only provides a bounded power gain. The conclusions are drawn from the approximate characterization of the capacity regions.In the third part, we investigate how intermediate relay nodes help resolve interference in delivering information from two sources to their respective destinations in multi-hop wireless networks. We focus on a linear deterministic approximate model for wireless networks, and when the minimum cut value between each source-destination pair is constrained to be 1, we completely characterize the capacity region. One of the interesting findings is that, at most four nodes need to take special coding operations so that interference can be canceled over-the-air or within-a-node, while other nodes can take oblivious operations. We also develop a systematic approach to identify these special nodes.",ucb,,https://escholarship.org/uc/item/0jg7401m,,,eng,REGULAR,0,0
459,1895,A Spin TQFT Related to the Ising Categories,"Donoghue, Kevin","Agol, Ian;",2019,"Most interesting 3d Topological Quantum Field Theories (TQFTs) are constructed by starting with algebraic data, usually in the form of some kind of category. This category typically comes from an area of mathematics different from 3-manifold topology, and its topological nature can be hard to understand. This dissertation reverses the process, at least in one simple example, by constructing a Spin TQFT from pure topology and then uncovering some interesting categories.The topology used to construct the Spin TQFT is entirely classical. If $(M, s)$ is a closed Spin 3-manifold, then an embedded surface $\Sigma\subset M$ inherits a $\Pin^-$ structure, $s|_{\Sigma}$, from $(M, s)$. If $\Sigma$ and $\Sigma'$ represent the same class in $H_2(M;\bbZ/2)$, then $s|_{\Sigma}$ and $s|_{\Sigma'}$ are isomorphic. If $t$ is a $\Pin^-$ structure on $\Sigma$, there is a classical invariant of the isomorphism type of $(\Sigma, t)$, denoted $\beta(\Sigma, t)$, that is an eight root of unity. One can therefore form the Spin 3-manifold invariant\[    Z_3(M, s) := \frac{1}{2^{b_0(M)}} \sum_{[\Sigma]\in H_2(M;\bbZ/2)} \beta(\Sigma, s|_{\Sigma}).\]It turns out that this invariant fits into a Spin TQFT. The detailed construction of this TQFT is the subject of this dissertation.By a theorem of Kirby and Melvin, $Z_3$ is very much related to the Ising categories. In extending the TQFT for $Z_3$, one encounters a category (associated with the bounding Spin circle) which has most of the same properties as the Ising categories. One also encounters a category (associated with the interval) from which $Z_3$ can be reconstructed in the style of Turaev-Viro and Barrett-Westbury. Because $Z_3$ is a Spin TQFT, these categories are linear over super vector spaces. In fact, they are realized as the module categories of certain explicit super algebras.",ucb,,https://escholarship.org/uc/item/0z4447sb,,,eng,REGULAR,0,0
460,1896,Lateral Mobility of Amphiphiles Adsorbed at the Air/Water Interface,"Carlson, Eric David","Majda, Marcin M;",2009,"Dynamic properties of the air/water interface were explored using Langmuir monolayer methods, ring and drop shape tensiometry, Brewster angle microscopy, and above all, cyclic voltammetry with 2D line and barrier microelectrodes.  The goal is to gain insight into the rate of lateral self-diffusion of water molecules in the air/water interfacial region, defined as the space in which water transitions from 90 to 10 percent of its bulk density.  Because measuring the rate of water self-diffusion directly is experimentally impossible, 2,2,6,6-tetramethylpiperidnyl-1-oxy (TEMPO) was employed as a surfactant probe molecule.  The dynamics and kinetics of TEMPO partitioning to the interface were thoroughly investigated under a variety of conditions.  TEMPO was found to have a partition constant of 380 Â± 30 M-1 in solutions of 1 mM HClO4 and 2 mM LiClO4.  The partition constant of TEMPO can be adjusted by synthesizing derivatives of varying hydrophobicity at the C-4 position in the carbon ring.  Placing a hydoxy group results in a compound that does not partition to the air/water interface.  Adding an ethyl group results in approximately an order of magnitude increase of the partition constant.The first attempts to determine the lateral diffusion coefficient of TEMPO (Dsurf) employed 2D line microelectrodes.  However, these measurements were found to be insufficient for determining Dsurf of TEMPO for several reasons.  The kinetics of TEMPO partitioning to the interface are fast relative to the experiment.  This allows the surface population of TEMPO, as it is oxidized to TEMPO+, to be replenished by the solution population of TEMPO, thereby enhancing the surface current.   It is therefore impossible to independently determine Dsurf without knowing the desorption rate constant, kdes.  Numerical simulation with COMSOL Multiphysics allowed us to obtain a number of Dsurf, kdes pairs, and a calibration plot was created showing possible values of Dsurf as a function of kdes for 2D line microelectrodes.  The limitation of 2D line microelectrodes is that they exhibit degradation of the voltammetric signal over the time scale of our experiment (15-45 s), burdening the experiment with a systematic negative error. The characteristics of the signal decay indicate that it stems from a loss of the gold/air/solution triple phase line, thereby preventing electrooxidation of TEMPO from occurring precisely at the microline.  A variety of mechanisms were hypothesized and tested to determine the exact cause of signal decay, with the aim of either eliminating it or finding a correction term to account for it.  No hypothesis was successfully confirmed as the cause of decay.   Because decay was a function of time the line electrode was in contact with solution, faster scan rates were preferred, typically 50 mV/s.  The calibration curve obtained  from 2D line microelectrodes was taken as a lower bound value for Dsurf.  For values of kdes > 103 s-1, the dependence of the value of Dsurf on kdes becomes small, and the calibration curve obtained for 2D line microelectrodes gave a value of 8 Â± 4 cm2/s for Dsurf in this region.Further experiments were designed to more accurately determine the value of Dsurf. This was done by modifying the electrode geometry.  Thin barrier films were placed over the gold surface that prevented direct electrooxidation at the microline.  Surface adsorbed TEMPO is still able to influence the voltammetric signal by desorbing to replace TEMPO that has been oxidized in the bulk solution.  This allowed the creation of a set of independent calibration curves and the identification of a point of intersection, thereby determining both Dsurf and kdes.  Two types of barrier films were employed: spin coated SU-8 photoresist and vapor deposited silicon monoxide.  The first type of film employed was the SU-8 photoresist.  SU-8 barrier films provided high reproducibility, but there was uncertainty as to the rigidity of the polymer film after breaking.  Vapor deposited silicon monoxide was chosen as an alternative barrier film due to its ease of fabrication and greater confidence in its rigidity.  Silicon monoxide barrier films were thoroughly characterized and found to break unevenly at the point of contact with solution.  Due to their unusual breaking characteristics, SiO barrier electrodes had to be calibrated with a non-partitioning electroactive analyte.  Both types of films generated calibration plots that intersected to yield Dsurf values of 7 Â± 3 Ã— 10-5 cm2/s.  However, calibration curves obtained from simulating experiments performed with SU-8 barrier electrodes intersected at a kdes value of 104 s-1, while the curves obtained using results from SiO barrier films intersected at a kdes value of 103 s-1.  Resolving the discrepancy between these two values is a possible direction for a future project.  It is encouraging to note the remarkably good agreement between the values of Dsurf obtained from the 2D microline experiments and the barrier electrodes.  This value for Dsurf is approximately a factor of 4 greater than the value obtained by Pohorille and Wilson in MD simulations.  Applying the 2D diffusive model described by Hughes and coworkers, the viscosity of the air/water interfacial region is estimated to be one third of the viscosity of bulk water.",ucb,,https://escholarship.org/uc/item/0zr4m6s7,,,eng,REGULAR,0,0
461,1897,On the Local Sensitivity of M-Estimation: Bayesian and Frequentist Applications,"Giordano, Ryan James","Jordan, Michael I;McAuliffe, Jon;",2019,"This thesis uses the local sensitivity of M-estimators to address a number ofextant problems in Bayesian and frequentist statistics.  First, by exploiting aduality from the Bayesian robustness literature between sensitivity andcovariances, I provide significantly improved covariance estimates for meanfield variational Bayes (MFVB) procedures at little extra computational cost.Prior to this work, applications of MFVB have arguably been limited toprediction problems rather than inference problems for lack of reliableuncertainty measures. Second, I provide practical finite-sample accuracy boundsfor the ``infinitesimal jackknife'' (IJ), a classical measure of localsensitivity to an empirical process.  In doing so, I bridge a gap betweenclassical IJ theory and recent machine learning practice, showing that stringentclassical conditions for the consistency of the IJ can be relaxed for restrictedbut useful classes of weight vectors, such as those of leave-K-out crossvalidation. Finally, I provide techniques to quantify the sensitivity of theinferred number of clusters in Bayesian nonparametric (BNP) unsupervisedclustering problems to the form of the Dirichlet process prior.  By consideringlocal sensitivity to be an approximation to global sensitivity rather than ameasure of robustness per se, I provide tools with considerablyimproved ability to extrapolate to different priors. Because each of thesediverse applications are based on the same formal technique---the Taylor seriesexpansion of an M-estimator---this work captures in a unified way thecomputational difficulties associated with each, and I provide open-source toolsin Python and R to assist in their computation.",ucb,,https://escholarship.org/uc/item/10b275v3,,,eng,REGULAR,0,0
462,1898,Developmental Genetic Basis of Tooth Number Evolution in Stickleback Fish,"Cleves, Phillip Alfonso","Miller, Craig T;",2015,"Teeth are a classic model for studying vertebrate organogenesis and evolution. Despite the incredible phenotypic diversification of dentition in vertebrates, our understanding of the molecular and developmental basis behind this variation is limited. A derived benthic freshwater stickleback population has evolved a nearly two-fold increase in ventral pharyngeal tooth number compared to their ancestral marine counterparts. This evolved tooth gain provides an excellent system to study the developmental and molecular genetic basis of evolved dental variation. To ask when during development evolved tooth gain appears, we generated lab-reared developmental time courses of a low-toothed marine population and this high-toothed freshwater population. Early in development, no differences in dental patterning are observed. However, at late larval stages, an increase in tooth number, an increase in tooth plate area, and a decrease in tooth spacing arise. We identified genomic regions controlling these evolved patterning changes by mapping quantitative trait loci (QTL) controlling tooth number, area, and spacing in a marine by freshwater F2 cross. One large effect QTL controlling tooth number fine-maps to a genomic region containing an excellent candidate gene, Bone morphogenetic protein 6 (Bmp6). Stickleback Bmp6 is expressed in developing teeth, but no coding changes are found between the two populations. However, by quantitatively comparing allele specific expression of Bmp6, we find cis-regulatory changes have down-regulated the relative expression level of the freshwater Bmp6 allele at late, but not early, stages of development. To functionally test the role of Bmp6 in controlling tooth patterning, we generated predicted loss-of-function alleles of Bmp6 in freshwater sticklebacks. We found that Bmp6 is required for tooth formation and tooth plate area mirroring aspects of the evolved changes. Next, to discover enhancers that contain marine/freshwater sequence differences, we compared the chromosome 21 genomic sequences from fish with the tooth QTL to fish without the QTL. We identified a partially conserved region of the fourth intron of Bmp6 containing QTL-associated variants. This region is a tooth and fin enhancer that drives partially distinct expression patterns during tooth development compared to a 5â€™ Bmp6 tooth and fin enhancer we previously discovered that lacks consistent sequence differences associated with the tooth QTL. Future genetic and transgenic approaches will functionally test this intron 4 enhancer of Bmp6 as a candidate for underlying evolved tooth gain in sticklebacks.",ucb,,https://escholarship.org/uc/item/1206k86c,,,eng,REGULAR,0,0
463,1899,"The Gascon Ã‰nonciatif System: Past, Present, and Future. A study of language contact, change, endangerment, and maintenance","Marcus, Nicole","Holland, Gary;",2010,"The Ã©nonciatif system is a defining linguistic feature of Gascon, an endangered Romance language spoken primarily in southwestern France, separating it not only from its neighboring Occitan languages, but from the entire Romance language family.  This study examines this preverbal particle system from a diachronic and synchronic perspective to shed light on issues of language contact, change, endangerment, and maintenance.  	The diachronic source of this system has important implications regarding its current and future status.  My research indicates that this system is an ancient feature of the language, deriving from contact between the original inhabitants of Gascony, who spoke Basque or an ancestral form of the language, and the Romans who conquered the region in 56 B.C.  Since this system initially arose via language contact and Gascon is a minority language threatened by French, can language contact also be the same mechanism to cause its demise?  To answer this question, I conducted fieldwork in the Gascon region during 2008-2009 to examine how this system is currently used and taught.	My findings reflect both the damaging effects of language marginalization and the significant effects of language maintenance.  While I found that the overall syntactic behavior of the Ã©nonciatif system is not endangered and that these preverbal particles are in fact spreading to Gascon regions that historically never used them, the system's semantic foundation and regional variations found mainly among native speakers are in danger of disappearing.  The significant variation encountered in the Ã©nonciatif usage not only challenges some of the prior semantic theories that have been proposed to account for the Ã©nonciatif behavior, but more importantly indicates that there is a pressing need to record older native speakers throughout Gascony before this information, of particular import to future speakers who wish to speak the Gascon-specific dialect of their relatives, becomes lost.  	Since this system is simply one aspect of the language, the final chapter examines the overall future of Gascon.  While I am optimistic provided there is much more political and economic support, the study of Gascon provides yet another example of how quickly a language can disappear and how important it is that action be taken to prevent its loss.  Just as the Ã©nonciatif system identifies Gascon and is a link to the region's ancestry, the Gascon language is integral to the rich culture, history, and identity of the Gascon region and people.",ucb,,https://escholarship.org/uc/item/12v9d1gx,,,eng,REGULAR,0,0
464,1900,Synthesis and Characterization of Multivalent Conjugates,"Svedlund, Felicia Lynn","Healy, Kevin;",2016,"The work described in this dissertation presents the synthesis and characterization of a novel multivalent conjugate of mechano-growth factor. Mechano-growth factor is a peptide derived from a splice variant of insulin-like growth factor-1 protein, which has shown promising cardioprotective effects. Multivalent conjugate technology provides a platform in which growth factors of interest are multivalently conjugated to a long, soluble polymer chain. This multivalent conjugation can result in improved pharmacokinetics and decreased degradation of the growth factor, as well as potentially increasing the bioactivity of the growth factor compared to its unconjugated form. Since the benefits of multivalent conjugate technology depend heavily upon the final valency of the conjugated growth factor, as well as the size and distribution of the multivalent conjugate molecules, it was necessary to have a characterization technique that could provide this information. This work focused on using multi-angle light scattering to thoroughly characterize the mechano-growth factor conjugates, as well as other conjugate molecules and macromolecules of interest in the field of tissue engineering. Additionally, this work focused on the development of in vitro cell-based assays for use in studying the bioactivity of the mechano-growth factor conjugates.Chapter 3 presents the development of a reaction method to allow for the multivalent conjugation of MGF peptide to a HyA backbone chain. The conjugation reaction required first the synthesis of a HyA intermediate by the addition of acrylate groups, which were characterized through gelation and NMR spectroscopy. Then the conjugation was achieved by a Michael addition reaction between the acrylate groups on the HyA and the c-terminal cysteine on the MGF peptide. The success of this conjugation reaction was verified through a BCA assay, which also provided an estimate of the final conjugation ratios. The characterization using NMR spectroscopy, acrylated HyA gelation, and BCA was able to confirm the success of conjugation and provide estimates of the peptide concentration and final conjugation ratio.Chapter 4 provides a more thorough characterization of the conjugate molecules through the application of multi-angle light scattering. This analysis utilized a SEC-MALS-UV-RI method, where the inline use of two concentration detectors allowed for the determination of the relative compositions of the MVCs. Using the measured specific refractive index increment and UV extinction coefficient values measured for the two MVC components, it was possible to determine the weight fractions of the MGF and HyA in the total MVC molecule. This analysis confirmed that the bioconjugate chemistry technique utilized in this work was successful, and that it was possible to determine the total molecular weight, polydispersity, conjugation efficiency, and valency of the MVCs.Chapter 5 presents an alternative SEC-MALS methodology for characterizing MVC molecules using branching analysis. It was first demonstrated that the MVC molecules behave as branched molecules, and then it was shown that branching analysis methods could be successfully applied to the MVCs. Although a linear hyaluronic acid was used as the linear counterpart for the calculations instead of a chemically identical linear counterpart, it was still possible to achieve good agreement between the values calculated by branching analysis and multivalent conjugate analysis.Chapter 6 further demonstrates the power of SEC-MALS as a characterization technique for macromolecules and MVCs in dilute solutions. SEC-MALS was applied to a variety of different projects in order to provide detailed information about the molecular weight, radius of gyration, polydispersity, and valency of these different macromolecular systems. Chapter 7 focuses on the development of two different assays for assessing the bioactivity of mvMGF. The first assay used hypoxia to mimic the ischemic environment of the cardiac tissue during an MI, while the second assay stressed CMs through cryopreservation and subsequent thawing. Both assays resulted in functional impairment of the CMs, which will allow for the in vitro assessment of the cardioprotective ability of the mvMGF.",ucb,,https://escholarship.org/uc/item/14s8r632,,,eng,REGULAR,0,0
465,1901,"Essays on Foreign Investment, Agglomeration Economies, and Industrial Policy","Du, Luosha","Harrison, Ann E.;Magruder, Jeremy;",2012,"Since opening its economy to the outside world in late 1978, China has experienced a massive, protracted, and unexpected economic upsurge, which has attracted the attention of a large and diverse group of researchers. China's three-decade economic reforms have reshaped the economic structure from plan to market, through a variety of policy actions, such as openness to foreign investment and efforts to build economic zones. Economic growth and potential technology transfer are indeed the main rationale behind the Chinese government's aggressive efforts over the past three decades to enhance openness and to increase domestic competition. This dissertation consists of three chapters. All chapters study firm behavior and their policy implications. However, the focus of each chapter is different. The first chapter (coauthored with Ann Harrison and Gary Jefferson) studies how institutions affect productivity spillovers from foreign direct investment (FDI) to China's domestic industrial enterprises. The second chapter separates the effect of agglomeration economies on firm performance (measured by total factor productivity) from the impact of competition and better transport infrastructure. The third chapter (coauthored with Philippe Aghion, Mathias Dewatripont, Ann Harrison, Patrick Legros) tests for the complementarity between competition and industrial policy.The first Chapter (co-authored with Ann Harrison and Gary Jefferson) investigates how institutions affect productivity spillovers from foreign direct investment (FDI) to China's domestic industrial enterprises during 1998-2007. We examine three institutional features that comprise aspects of China's ""special characteristics"":  (1) the different sources of FDI, where FDI is nearly evenly divided between mostly Organization for Economic Co-operation and Development (OECD) countries and Hong Kong (SAR of China), Taiwan (China), and Macau (SAR of China); (2) China's heterogeneous ownership structure, involving state- (SOEs) and non-state owned (non-SOEs) enterprises, firms with foreign equity participation, and non-SOE, domestic firms; and (3) industrial promotion via tariffs or through tax holidays to foreign direct investment. We also explore how productivity spillovers from FDI changed with China's entry into the WTO in late 2001.  We find robust positive and significant spillovers to domestic firms via backward linkages (the contacts between foreign buyers and local suppliers).  Our results suggest varied success with industrial promotion policies.  Final goods tariffs as well as input tariffs are negatively associated with firm-level productivity.  However, we find that productivity spillovers were higher from foreign firms that paid less than the statutory corporate tax rate. The second chapter separates the effect of agglomeration economies on firm performance (measured by total factor productivity) from the impact of competition and better transport infrastructure. Consequently, this paper primarily addresses the problem of omitted variable bias in estimating the impact of agglomeration economies on firm performance. The results suggest that firm productivity is improved only by the presence of other firms in the same sector (localization economies). The inclusion of information on road construction does not affect the importance of pure localization economies. However, including a measure of competition in the estimation significantly reduces the importance of localization externalities. The results also suggest that both road-building and competition are positively associated with productivity growth. The results for sub-samples indicate that exporting firms and firms financed by foreign investment benefit more from localization externalities than do their non-exporting and domestically-financed counterparts. The third chapter (co-authored with Philippe Aghion, Ann Harrison, Mathias Dewatripont, and Patrick Legros) argues that sectoral state  aid tends to foster productivity, productivity growth,  and product  innovation  to a larger extent when it targets more  competitive  sectors  and  when  it  is not  concentrated on one or a small number  of firms in the  sector.   A main implication from our analysis is that the debate on industrial policy should no longer be for or against having such a policy.  As it turns out, sectoral policies are being implemented in one form or another by a large number of countries worldwide, starting with China.  Rather, the  issue should  be on how to  design  and  govern  sectoral  policies in order  to make  them  more  competition-friendly and  therefore  more  growth-enhancing. Our  analysis  suggests  that proper  selection  criteria  together  with  good guidelines  for governing  sectoral  support can  make a significant difference in terms  of growth  and  innovation  performance.  Yet the issue remains of how to minimize the scope for influence activities by sectoral interests when a sectoral state aid policy is to be implemented. One answer is that the less concentrated and  more  competition-compatible the  allocation  of state  aid  to  a sector,  the less firms in that sector  will lobby  for that aid  as  they  will anticipate  lower profits from it.  In other words, political economy considerations should reinforce the interaction between competition and the efficiency of sectoral state aid.  A comprehensive analysis of the optimal governance of sectoral policies still awaits further research.",ucb,,https://escholarship.org/uc/item/15c290qg,,,eng,REGULAR,0,0
466,1902,Antagonistic interactions between microRNAs underlie robust tumor suppressor responses.,"Sabio, Erich Yanson","He, Lin;",2015,"The oncogenic mir-17-92 cluster encodes six coordinately expressed miRNAs with a unique capacity for gene regulation. In this dissertation, we have extended our studies of the mir-17-92 cluster in order to explore two fundamental concepts: functional consequences of aberrant miRNA expression, and distinct regulation within a miRNA polycistron. In the EÎ¼-myc Burkittâ€™s lymphoma model, mir-17-92 exhibits potent oncogenic activity by repressing c-Myc-induced apoptosis, primarily through its miR-19 components. Here we report that, surprisingly, mir-17-92 also encodes the miR-92 component that negatively regulates its oncogenic cooperation with c-Myc. The effect of miR-92 is mediated, in part, through direct repression of Fbw7 which normally promotes the proteosomal degradation of c-Myc. Thus, overexpressing miR-92 leads to c-Myc-induced proliferation strongly coupled to p53-dependent apoptosis. Ultimately, we reveal an antagonistic interaction between miR-19 and miR-92 that is disrupted in B-lymphoma cells, which favor a greater increase for miR-19 than miR-92. Furthermore, we have identified an RBP, Hnrnpu, as a factor that is bound to miR-19b, transcriptionally regulated by Myc, overexpressed in Burkittâ€™s Lymphoma, required for maintining miR-19b abundance, and therefore a plausible candidates for trans-acting factors that alter the ratio of miR-19:miR-92. These studies add to our understanding of how individual miRNAs within a single cluster may act as antagonists, as well as how these antagonistic interactions are regulated.",ucb,,https://escholarship.org/uc/item/16p635xb,,,eng,REGULAR,0,0
467,1903,State and the Paradox of Gender Segregation in Iran,"Shahrokni, Nazanin","Ray, Raka;",2013,"This dissertation is about state formation processes and gender segregation practices in postrevolutionary Iran. It uses gender segregation as a policy and spatial indicator to trace the shifts in state power between the establishment of the Islamic Republic (in 1979) and 2009. It explores the politics around the production of gender-segregated spaces, the imperatives of the state that produces them, and the implications for women's public presence.Building on 182 interviews, more than 16 months of fieldwork between 2008 and 2011, this dissertation offers a thorough account of the (trans)formation of three major sites of gender segregation in Tehran, Iran, namely women-only parks, segregated buses, and men-only sports stadiums. Throughout this work, I argue that current models that attribute the endurance of the Islamic Republic of Iran to its application of coercion and prohibitive measures, fail to account for the productivity of the Iranian state power. Thus, instead of dismissing the Iranian government mode of rule as that of religious totalitarianism using repression, I illustrate that it is more productive to look closely at the intricacies of power and the multitudes of their logics, in order to understand spaces, gender, Islamic rule, and subject formation.This dissertation contributes to theories of the state, feminist theories, and theories of urban governmentality: First, a close examination of the state's official rationales for gender segregation policies and the tensions and problems these policies address illuminates how the state continuously reconfigures its power in order to maintain its legitimacy in an increasingly globalized world with its shifting geopolitical alignments. By delineating several historically contingent shifts in gender segregation policies, I illustrate the broader shift in the Iranian state power from authoritarian sovereignty in the 1980s to disciplinary governmentality.",ucb,,https://escholarship.org/uc/item/16x707q2,,,eng,REGULAR,0,0
468,1904,"Quasiparticle Coherence, Collective Modes, and Competing Order in Cuprate Superconductors","Hinton, James Patrick","Orenstein, Joseph W;",2014,"In recent years, the study of cuprate superconductors has been dominated by the investigation of normal state properties. Of particular interest is the nature of interactions between superconductivity and other incipient orders which emerge above the superconducting transition temperature, Tc. The discovery of charge density wave (CDW) correlations in YBa2Cu3O6+x (YBCO) and HgBa2CuO4+d (Hg-1201) has established that some form of charge order is ubiquitous in the cuprates. In this work, we explore the non-equilibrium dynamics of systems which sit near the boundary between superconductivity and competing orders.Ultrafast pump-probe spectroscopy is ideally suited to the study of competing order. Exciting the sample with an optical pulse perturbs the system from equilibrium, altering the balance between the co-existing orders. The return to equilibrium is then monitored by a time-delayed probe pulse, revealing multiple decay processes as well as collective excitations. We first apply this technique to Hg-1201, conducting a detailed study of the phase diagram. At temperatures near  Tc, the pump pulse induces a non-equilibrium quasiparticle population. At  Tc we observe a doping-dependent peak in the relaxation time of these quasiparticles which we associate with a divergence in the coherence time of the fluctuating CDW. Using heterodyne probing in the transient grating geometry, we are able to disentangle the transient reflectivity components associated with superconductivity and the pseudogap, domonstrating competition across the phase diagram. We also discuss the observation of a sharp transition in the nature of the pseudogap signal at âˆ¼ 11% doping.In YBCO, we explore the temperature and doping dependence of coherent oscillations excited by the pump pulse. We associate these oscillations with the excitation of the CDW amplitude mode, and model their temperature dependence within the framework of a Landau model of competing orders.We conclude with an investigation of pseudogap dynamics in the electron doped compound Nd2-xCexCuO4+d as a function of temperature and doping. Near optimal doping, we observe the impulsive excitation of a critically damped mode, with time-temperature scaling consistent with quantum-critical fluctuations. This mode competes with superconductivity in a dynamical fashion, such that the suppression of this mode below  Tc can be lifted via photo-evaporation of the superconducting condensate.",ucb,,https://escholarship.org/uc/item/17f6q55b,,,eng,REGULAR,0,0
469,1905,Serial Dependence in Visual Working Memory: Time Course and Neural Mechanisms,"Bliss, Daniel","D'Esposito, Mark;",2017,"Visual cognition applies temporal smoothing to its inputs, which creates a serial dependence between successive representations at the focus of attention. This is thought to promote perceptual stability. While the benefits of serial dependence have been assumed, evidence that perception itself is altered has been limited. In the first chapter of this dissertation, I vary the delay between stimulus and response in a spatial delayed response task to investigate whether serial dependence occurs at the time of perception or later in working memory. I find that behavioral responses made immediately after viewing a stimulus are on average veridical. Only as memory demands increase is a blending of past and present information apparent in behavior, reaching its maximum with a memory delay of six seconds.In the second chapter, I explore potential neural-circuit mechanisms of serial dependence. I consider two possible substrates of the effect: stable persistent activity during the memory delay and dynamic â€œactivity-silentâ€ synaptic plasticity. I find that networks endowed with both strong reverberation to support persistent activity and dynamic synapses can closely reproduce behavioral serial dependence. Specifically, elevated activity drives synaptic augmentation, which biases activity on the upcoming trial, giving rise to a spatiotemporally tuned shift in the population response. My hybrid neural model is a theoretical advance beyond abstract mathematical characterizations of working memory and demonstrates the power of biological insights to provide a quantitative explanation of human behavior.The model developed in Chapter 2 proposes that serial dependence is due in part to synaptic augmentation, which is especially prominent in prefrontal cortex. In the third chapter, I investigate whether the bias in behavior depends on activity in three separate nodes of prefrontal cortex (PFC) â€“ the frontal eye fields, the dorsolateral PFC, and the anterior PFC near the frontal pole. I find that transcranial magnetic stimulation (TMS) to these nodes causes reductions in serial dependence consistent with the modelâ€™s predictions. In contrast, TMS to posterior sites â€“ either primary somatosensory cortex or posterior parietal cortex â€“ fails to alter the magnitude of the behavioral effect. This general result holds across TMS protocols (online vs. offline) and tasks with different stimulus and response types.",ucb,,https://escholarship.org/uc/item/1bf6q544,,,eng,REGULAR,0,0
470,1906,"Cosmopolitan Suburbs: Race, Immigration, and the Politics of Development in the Silicon Valley","Lung Amam, Willow","Hester, Randolph T.;",2012,"Within the last half century, the geography of race and immigration in the U.S. has shifted. While many white middle class residents are moving into revitalized central cities, the suburbs have become home to the majority of immigrants and ethnic minorities in the U.S.  Fremont, California, which only 30 years ago was a prototypical white, middle class suburb, is now home to an Asian American majority, including many of Silicon Valley's highly educated and high-income engineers from China, Taiwan, and India.In a case study of Fremont, my dissertation looks at the changing material forms and uses of the built environment, and politics of space in suburbia amidst its rapid demographic changes.  Using GIS mapping, archival analysis, participant observations, and in-depth interviews with 74 residents, city officials, planners, designers, and developers, my analysis centers on three spaces common to many high tech suburbs--McMansions, high-performing schools, and Asian malls.  I look at the meaning of community and home as expressed by Asian immigrants in debates over residential teardowns and McMansions and the cultural politics of design guidelines and development standards used to regulate them.  In a case study of Mission San Jose High, I then look at the value of high performing school districts to Asian immigrant families and how their educational priorities are reshaping neighborhood geographies of race and battles over school boundaries.  And finally, I explore Asian malls' form, geography, and uses, and the politics of their regulation in Fremont.Together these investigations show that Asian immigrants have introduced new spatial imaginaries and practices, values, meanings, and sources of economic capital that are reshaping suburban form and use in the Silicon Valley.  But I also show that suburbia's increasing diversity has upset its presumed social and spatial order, leading to a politics of backlash that is producing new spaces and modes of marginality, even among immigrants of means.  Both city officials and established residents have consistently portrayed landscapes built by or for Asian immigrants as non-normative and subjected them to critique and new forms of regulation, while simultaneously reinforcing white middle class norms, meanings, and values through planning, design, and public policy.  These spaces, however, have also served as sites of cultural contest and collective resistance that threaten to undermine the dominance of suburbia's assumed spatial norms.  I argue that Asian immigrants' assertions for more inclusive, open, and diverse suburban spaces represents an emergent suburban spatial politics of difference aimed at bringing about new forms and norms of belonging, as well as new platforms for social and spatial justice.The dissertation contributes to the existing scholarship in suburban studies, urban planning, design, and cosmopolitan theory.  It extends the suburban studies literature on the contributions of minorities and immigrants to making a diverse suburban landscape by looking at understudied place and groups--Asian Americans in high tech suburbs--and at the spatial landscape of suburbs as an important object of study.  In a new American century defined by suburbanization and diversity, this case study also speaks to the ways that cities manage vast demographic changes, and the role of design, planning, development, and public policy in supporting social differences and justice, as well as reinforcing existing social hierarchies and inequalities.  And finally, this study grounds discourses on emergent forms of cosmopolitanism citizenship within the everyday struggles of immigrants to make home in the Silicon Valley suburbs.",ucb,,https://escholarship.org/uc/item/1d1860q7,,,eng,REGULAR,0,0
471,1907,Essays in Macroeconomics and Financial Economics,"Jauregui, Christian T","Gorodnichenko, Yuriy;",2019,"This dissertation is comprised of two chapters on separate topics at the intersection of Macroeconomics and Financial Economics. The first chapter analyzes the relationship between non-financial U.S. corporationsâ€™ debt structure and their behavior in the product market. The second chapter, which is co-authored with Ganesh Viswanath Natraj, examines the international real effects of monetary policy through financial markets.In the first chapter, I answer the following crucial question: how does a non-financial firmâ€™s product market behavior interact with its capital structure choice and cash-flow process? A significant portion of the corporate finance literature considers debt borrowing the primary source of financing through which firms smooth cash-flow shocks, with bank loans and market debt the two primary sources of debt financing. However, firms can also smooth cash-flow shocks through adjustments in their variable markup of products. This behavior is consistently unaccounted for, yet provides financial flexibility, more so for firms with a loyal customer base. I study how firms smooth cash-flows via traditional financing in the form of a bank loan or market debt instrument, as well as through non-traditional internal financing generated from variable markup adjustments. First, I hypothesize the empirical relationship between a firmâ€™s markup strategy and debt financing choice, measured as the share of market debt in total debt, is conditionally non-linear. I find a robust, conditional hump-shaped relationship between the variable markup and market debt share. On average, markups rise with market debt shares, peaking at a share of 61-67% before declining. Second, I demonstrate this novel finding with a quantitative model of firm dynamics in a monopolistically competitive economy. In my model, firms set variable markups in a customer market while trading off restructurable bank loans for marginally cheaper, non-restructurable market debt. Market debt contracts reduce flexibility in cash flows, increasing a firmâ€™s incentive to raise todayâ€™s profits by setting a higher markup. However, the trade-off between current and future profits implies the benefits of a high markup are maximized at a given market debt share. Beyond this share, markup reductions are required to attract new customers, thus generating the hump shape. My model replicates the empirical hump shape while matching several key cross-sectional and aggregate features of the data. Third, I use my model as a laboratory to study the response of firms to a bank credit crunch, akin to that of the 2008-09 U.S. financial crisis. I show how my model explains 75% of the decline in total sales by public U.S. corporations following the crisis.In the second chapter, I document the international real spillovers of major central banks policiesâ€™ through their indirect effect on a set of base asset prices, by using high-frequency identification of monetary policy announcements. I implement a gross domestic product (GDP)-tracking approach to identify real spillovers of monetary policy, by mimicking real GDP news based on my set of asset returns around monetary announcements. This procedure enables me to estimate news regarding real GDP growth due to monetary policy. Most importantly, this provides me with a direction of causation from monetary announcements to real variables through their indirect effects on asset prices. In response to positive, domestic monetary shocks, I find real GDP-tracking news becomes negative for the U.S., Australia, and Canada. My methodology indicates significant spillovers of U.S. monetary policy to asset prices in periphery countries, such as Australia and Canada, with a U.S. monetary contraction leading to a significant effect in both of these countriesâ€™ real GDP-tracking news measures, albeit the effects differ between both countries: contractionary U.S. monetary policy is contractionary in Australia after a year, but expansionary in Canada within two quarters.Summarizing, my dissertationâ€™s first chapter yields crucial information for better predicting a non-financial firmâ€™s default choice. Moreover, it provides insight into how a firmâ€™s customer base â€“ a source of market power â€“ directly impacts capital structure decisions and vice versa. My second chapter shows the U.S. Federal Reserve is a fundamental driver of global asset prices and real output abroad, which is a topic at the core of recent policy discussions in international macroeconomics and finance.",ucb,,https://escholarship.org/uc/item/1d74z4xf,,,eng,REGULAR,0,0
472,1908,Essays in Public Economics and Development,"Gerard, Francois","Saez, Emmanuel;Miguel, Edward;",2013,"The present thesis studies public economics questions in the context of developing countries. In particular, I investigate the impact and design of specific government policies in Brazil. Government interventions may be desirable when unregulated market economies deliver socially inefficient outcomes. Goods and services tend to be under-provided in the presence of imperfect or asymmetric information. Such market failures may be pervasive in the insurance market and prompt governments to provide certain types of insurance directly. Chapters 1 and 2 study social insurance programs, and more specifically unemployment insurance (UI). In contrast, goods and services tend to be over-provided if they generate negative externalities. In recent years, there has been a lot of interest in the negative externalities associated with energy consumption. Chapter 3 studies energy conservation policies, and more specifically residential electricity conservation. In each of the three essays, I develop a simple theoretical framework to guide my empirical analysis. I then estimate the relevant impacts and combine theory and empirics to inform the design of government programs.There is vast literature in public economics (and related fields) on social insurance programs and energy conservation policies. Yet, as for most research in public economics, existing work focuses almost entirely on the context of developed countries. Arguably, social insurance and energy conservation are not first-order priorities in least developed countries. However, these topics are becoming increasingly relevant for developing countries. Most of the growth in energy demand is forecast to come from the developing world, especially for residential consumers. Social insurance programs have been adopted in a growing number of developing countries. Currently some form of UI exists in Algeria, Argentina, Barbados, Brazil, Chile, China, Ecuador, Egypt, Iran, Turkey, Uruguay, Venezuela and Vietnam; Mexico, the Philippines, Sri Lanka, and Thailand have been considering its introduction. Moreover, the severe data constraints that limited empirical work at the intersection of public and development economics are being removed. Today, large administrative datasets and high-quality surveys are available in many developing countries. Importantly, results from more advanced countries are unlikely to translate easily to a developing country context. For instance, the enforcement of social program eligibility is a major challenge in developing countries where the informal sector accounts for a large share of the economy. In Brazil, about half of the employed population works in jobs that escape oversight and monitoring from the government. The presence of a large informal sector is widely believed to increase the efficiency costs of social programs. The main concern is that informal job opportunities exacerbate programs' disincentives to work in the formal sector. The essay in the first chapter (joint work with Gustavo Gonzaga) evaluates such a claim. We begin by developing a simple theoretical model of optimal UI that specifies the efficiency-insurance tradeoff in the presence of informal job opportunities. We then combine the model with evidence drawn from 15 years of uniquely comprehensive administrative data to quantify the social costs of the UI program in Brazil. We first show that exogenous extensions of UI benefits led to falls in formal-sector reemployment rates due to offsetting rises in informal employment. However, because reemployment rates in the formal sector are low, most of the extra benefits were actually received by claimants who did not change their employment behavior. Consequently, only a fraction of the cost of UI extensions was due to perverse incentive effects and the efficiency costs were thus relatively small (only 20% as large as in the US, for example). Using variation in the relative size of the formal sector across different regions and over time in Brazil, we then show that the efficiency costs of UI extensions are actually larger in regions with a larger formal sector. Finally, we show that UI exhaustees have relatively low levels of disposable income, suggesting that the insurance value of longer benefits in Brazil may be sizeable. In sum, the results overturn the conventional wisdom, and indicate that efficiency considerations may in fact become more relevant as the formal sector expands.The findings of this essay have broader implications for our understanding of social policies in developing countries. Many social programs and taxes generate incentives for people to carry out their economic activities informally. For the same reasons as for UI, they are viewed as imposing large efficiency costs in a context of high informality. By going against the conventional wisdom, our results cast doubt on whether efficiency considerations actually limit the expansion of social policies in these cases too.The essay in the second chapter (joint work with Gustavo Gonzaga) follows directly from the above results. Governments face two main informational constraints when implementing any program or regulation (e.g., welfare program). First, there is a screening issue. Government may fail to identify the ex-ante population of interest (e.g., poorest households). Second, there is a monitoring issue. Agents may adopt unobserved behaviors to join or escape the population of interest (e.g., reducing work efforts). The lack of strict monitoring policies for government programs is often considered to be a major issue in developing countries where non-compliance is widespread. Yet, we know surprisingly little about the magnitude of the behavioral responses that we wish to mitigate, relative to the cost of efficient monitoring policies. The Brazilian UI program offers a stark example of a weak monitoring environment. Until recently and for over 20 years, there was absolutely no monitoring of formal job search for UI beneficiaries in Brazil, even though many beneficiaries work informally when drawing UI benefits. In the second chapter, we argue that the results presented in the first chapter may rationalize the complete lack of monitoring in Brazil until 2011. We begin by deriving a theoretical upper bound for the maximum price that a government should be willing to pay per beneficiary to perfectly monitor the formal job search of UI beneficiaries. We show that the bound corresponds to the share of program costs due to behavioral responses. Intuitively, there is little incentive to introduce monitoring if most beneficiaries draw UI benefits without changing their formal reemployment behavior. The overall scope of the monitoring issue is thus limited in Brazil because most beneficiaries would collect UI benefits absent any behavioral response, as shown in the first chapter. Yet, monitoring policies may still be cost-effective if the government is able to target them towards workers with relatively larger behavioral responses. In the empirical analysis, we investigate to what extent the government could use information readily available ex ante (a signal) to identify worker categories with relatively larger behavioral responses. We find that most of the heterogeneity is not easily captured by observable characteristics. Therefore, monitoring policies would be relatively costly even if the government used available signals to target them efficiently. These results motivate future work on the cost-effectiveness of job-search requirements for UI beneficiaries, which have been recently introduced in Brazil.If there is little evidence on the impact of social insurance programs in developing countries, there is almost no evidence on the impact of energy conservation policies. Moreover, results from more advanced countries are also unlikely to translate easily to the context of developing countries. Households in the developing world own fewer appliances and consume much less energy on average. Average monthly residential electricity consumption in Brazil was below 200 kilowatt hours in 2000. Enforcement is also a major challenge. Electricity theft amounts to 15% of the total load for some utilities in Brazil. In the third chapter, I investigate the short- and long-term impacts on residential consumption of the largest electricity conservation program to date. This was an innovative program of economic (fines) and social (conservation appeals) incentives implemented by the Brazilian government in 2001-2002 in response to supply shortages of over 20%.Achieving ambitious energy conservation targets through economic incentives is often considered infeasible. Yet, there is little evidence from ambitious conservation policies. I find that the Brazilian conservation program reduced average electricity consumption per customer by .25 log point during the nine months of the crisis. Importantly, the program induced sizable lumpy adjustments; it reduced consumption by .12 log point until at least 2011. Using individual billing data from three million customers, I show that average effects came from dramatic reductions by most customers. I also provide suggestive evidence that lumpy adjustments came from new habits rather than physical investments. Finally, I structurally estimate a simple model to quantify the role of social incentives and lumpy adjustments. Social incentives amounted to a 1.2 log point increase in electricity tariffs, and may thus be particularly powerful in times of crisis. Importantly, a .6 log point permanent increase in tariffs would have been necessary to achieve the observed consumption levels during and after the crisis absent any lumpy adjustment. The possibility of triggering lumpy adjustments may thus substantially reduce the incentives necessary to achieve ambitious energy conservation targets. Beyond the specific issues it addresses, I hope that this dissertation will help convince senior and junior scholars alike of the relevance and feasibility of academic research at the intersection of public and development economics. More work is deeply needed.",ucb,,https://escholarship.org/uc/item/1f55s25v,,,eng,REGULAR,0,0
473,1909,Understanding Risk Aversion in Older Americans: New Approaches Using Genetic Data,"Harrati, Amal Cherifa","Wachter, Kenneth;",2014,"In this dissertation, I explore the nature and role of risk aversion among older Americans from a variety of perspectives. Risk preferences are important to demographers for several reasons. First, risk preferences are fundamental to most individual-level demographic events, including to financial decision-making, health behaviors, labor market decisions, migration, and marriage and family-formation. Second, there is substantial evidence that risk aversion increases with age. With age also comes increased responsibility in terms of making specific financial and health decisions. In the age of decreasing pensions, older persons must make significant decisions about their financial portfolios and finances in light of pending retirement decisions. In fact, the decision to retire is itself one in which risk plays a role. Third, health behaviors, which are a function of one's riskiness, often display their effects at older ages.  I explore the genetic nature of risk aversion through a number of approaches. Taking advantage of a newly-released database with over two million pieces of genetic variants, I examine the specific genetic nature of risk aversion through two genomic techniques: a Genome-Wide Association study (GWAS) and a Genome-Wide Complex Trait Analysis (GCTA). I provide evidence that risk aversion is a highly complex trait that is a function of a large number of possibly interactive genetic variants. Through the GWAS, I show that the number of genetic variants influencing individual-level differences in risk aversion is numerous and that these variants are likely to be scattered across the genome. The GCTA, while using a separate methodological approach, confirms this finding.  I argue that the intricate nature of the genetic underpinnings to risk aversion should be better understood in order to more precisely model economic decisions involving risk preferences.I also characterize risk aversion from a non-genetic perspective. Using panel data of risk aversion collected over nearly two decades, I use longitudinal methods to explore the extent to which the relationship between hypothetical risk and measurable risky behaviors remain consistent across both time and among individuals.  As a follow-up, I examine the specific time period following the 2008 recession to examine any change in the relationship in portfolio allocations relative to stated risk tolerance for individuals after the global financial crisis. I conclude that the relationship between measured risk and risky behaviors remains relatively constant across the 15 years prior to the global financial crisis.  The analysis also shows that the relationship between risk and financial assets does in fact change slightly after the global financial crisis, though the statistical evidence is not very strong.  This dissertation provides a contribution to the understanding of the complex nature of risk aversion and is on of the first to characterise it's genetic influences. This research helps to answer questions on the economic, social and biological drivers and consequences of risk aversion among older Americans.",ucb,,https://escholarship.org/uc/item/1fp2k1kp,,,eng,REGULAR,0,0
474,1910,Mapping a Monastic Network: Peter Damian and Fonte Avellana in the Eleventh Century,"Jasper, Kathryn Lee","MIller, Maureen C.;",2012,"Mapping a Monastic Network examines a ""grassroots"" reform movement in the Italian Marches, the monastic congregation of Fonte Avellana, how it functioned as a communications network, and how interactions between individuals and with the landscape produced a compelling and politically potent vision of personal and institutional change. In a new take on ""history from the bottom up,"" I have used two strategies to rewrite the prehistory of the papal revolution of the late eleventh century usually called the Gregorian Reform and the Investiture Conflict. One is, literally, to look at the ground: by mapping the spread of this monastic network with Geographic Information System (GIS) tools and site surveys, I have reconstructed relations among communities within the congregation and considered the impact of topography on religious ideals and political relations. Second, having discovered and utilized documentation from the daughter houses of Fonte Avellana, I have reconsidered the center from the periphery, recovering the contributions of those who collaborated with the congregation's charismatic prior, the theologian, cardinal, and papal polemicist, (Saint) Peter Damian (1007-1072). The result is a more dynamic and inclusive portrait of how and why ecclesiastical reform convulsed European society at the end of the eleventh century.",ucb,,https://escholarship.org/uc/item/1g91m95m,,,eng,REGULAR,0,0
475,1911,"Planning for water efficient cities: Landscape, microclimate, and heterogeneity in residential water demand","Lassiter, Allison","Radke, John;",2015,"California is confronting its largest drought in recorded history, which may signal the onset of a megadrought. An executive order mandates reduction in residential water consumption. At the same time, the state's population continues to grow. Reducing residential outdoor water use is a critical management objective to adapt to scarce water resources. This dissertation asks, to what degree can land planning contribute to outdoor water demand management? Can principles of compact development and strategic growth mitigate use?  First, the dissertation reviews the literature of water demand models that incorporate landscape variables, thematically characterizing the variables and examining model spatiotemporal resolution. Next, the dissertation examines methods of quantitatively characterizing the urban environment using land cover, weather, and landform variables. It develops variables by parcel, and then defines microclimate zones of similar parcels by clustering on each parcel's microclimate signature. In the fourth chapter, it examines the contributions of landscape and microclimate to household water consumption in the East Bay Municipal Utility District of California.  It evaluates 26 million observations of monthly water use data from 2005-2011 recorded at over 300,000 single family residences. It analyzes the data as a whole, and then subsets the full population by quantile of water user and by microclimate zone. Results reveal heterogeneous water demand profiles, but all models indicate that landscape type and the presence of a pool are important predictors of consumption. Less important are lot size and microclimate variables. With both high and low water users spatially distributed throughout the study area and across microclimate zones, there is evidence that many different development styles in many different locations can be water efficient.",ucb,,https://escholarship.org/uc/item/0m93s0jr,,,eng,REGULAR,0,0
476,1912,The Kinetics of Dislocation Loop Formation in Ferritic Alloys Through the Aggregation of Irradiation Induced Defects,"Kohnert, Aaron","Norman, Eric B;Wirth, Brian D;",2014,"The mechanical properties of materials are often degraded over time by exposure to irradiation environments, a phenomenon that has hindered the development of multiple nuclear reactor design concepts.  Such property changes are the result of microstructural changes induced by the collision of high energy particles with the atoms in a material.  The lattice defects generated in these recoil events migrate and interact to form extended damage structures. This study has used theoretical models based on the mean field chemical reaction rate theory to analyze the aggregation of isolated lattice defects into larger microstructural features that are responsible for long term property changes, focusing on the development of black dot damage in ferritic iron based alloys.  The purpose of such endeavors is two-fold.  Primarily, such models explain and quantify the processes through which these microstructures form.  Additionally, models provide insight into the behavior and properties of the point defects and defect clusters which drive general microstructural evolution processes.The modeling effort presented in this work has focused on physical fidelity, drawing from a variety of sources of information to characterize the unobservable defect generation and agglomeration processes that give rise to the observable features reported in experimental data.   As such, the models are based not solely on isolated point defect creation, as is the case with many older rate theory approaches, but instead on realistic estimates of the defect cluster population produced in high energy cascade damage events.  Experimental assessments of the microstructural changes evident in transmission electron microscopy studies provide a means to measure the efficacy of the kinetic models.  Using common assumptions of the mobility of defect clusters generated in cascade damage conditions, an unphysically high density of damage features develops at the temperatures of interest with a temperature dependence that is much too strong.  The so-called nucleation catastrophe motivates a re-examination of the properties of interstitial defect clusters in iron.  The behavior of interstitial clusters in iron is a complex puzzle, with high mobility predicted by computational techniques, much lower thermal mobility observed in electron microscopes, and a series of discrete discontinuous motions seen during in situ ion irradiation performed in a transmission electron microscope.  This work has combined these observations and presented a trap mediated concept of interstitial cluster motion that has been incorporated into a larger scale kinetic model.  This superior description of interstitial mobility is crucial to realizing many aspects of black dot damage structures, from saturation behavior to temperature dependence.Another focus of this work was to analyze the assumptions widely employed in rate theory models.  Cluster dynamics, the rate theory method employed in this work, is usually invoked with a number of potentially dubious assumptions regarding the mobility and interaction characteristics of defect clusters.  The effects of anisotropic reaction volumes and one dimensional diffusion have both been analyzed to determine the effect they have on the development of black dot microstructures.  In the trap mediated system, one dimensional diffusion proved far more significant, and the cross section for interaction between one dimensionally diffusing interstitial clusters strongly influenced the size and density of visible damage structures.  The validity of the reaction rate approach to determining cluster evolution in the trap mediated environment has been established by comparison with Monte Carlo methods.In total, this work has demonstrated the ability of mean field models to capture the key characteristics of low temperature damage microstructures in irradiated ferritic alloys when incorporating the full knowledge of interstitial cluster properties in iron, and the legitimacy of the mean field assumptions by comparison to other methods.",ucb,,https://escholarship.org/uc/item/0mr896rp,,,eng,REGULAR,0,0
477,1913,"Expansion and Exclusion: Race, Gender and Immigration in American Politics","Phillips, Christian Dyogi","Hero, Rodney;Lee, Taeku;",2017,"The United Statesâ€™ population is rapidly changing, but the ways in which political scientists measure and understand representation have not kept apace. Marginal shifts in descriptive representation over the past two decades have run counter to widely espoused ideals regarding political accessibility and democratic competition. A central assumption often made by academics, and the public, has been that groups which are otherwise disadvantaged in politics may leverage their communitiesâ€™ numerical size as a political resource to gain influence. To this end, many studies of racial descriptive representation find that a larger minority population is associated with a higher likelihood of a racial minority running for and/or winning. However, these positive relationships between population growth and descriptive representation are tempered by an extensive literature documenting limits on racial minority groupsâ€™ political incorporation. Moreover, current frameworks for understanding group competition or patterns of descriptive representation are silent about whether shifts in racial demographics may also have an effect on the balance of representation between women and men.  	These contradictions in debates over representation, and how groups gain influence, undermine the notion that eventually, marginalized groups will be fully incorporated into politics. White women have had de jure access to the voting franchise in the United States since 1920. In the intervening period, women have made up approximately half the population, and outnumbered male voters in every presidential election since 1964. Yet, women have held a quarter or less of all state legislative seats across the country for well over two decades, and only reached 100 members of Congress in 2014. 	The case for eventual incorporation is similarly dubious when we consider the racial composition of elected bodies. The racial balance of American communities is in flux largely due to Asian and Latina/o immigration, which will continue to be the case into the forecastable future. Presently, Asian Americans and Latina/os make up 23 percent of the U.S. population and are the two fastest growing racial groups in the country. Members of these immigrant communities hold less than ten percent of all state legislative seats, and a similar fraction of seats in the 115th Congress. Taken together, these yawning gaps between presence in the population and representation in elected office strongly suggest that â€œtimeâ€ alone may be an insufficient remedy for underrepresentation. 	Moreover, for those who are living in the United States now, the current demographic makeup of state legislaturesâ€”which includes over 7500 elected seats nationallyâ€”raises doubts about their representative legitimacy. Asian American and Latina/o women and men typically have socioeconomic experiences, political perspectives and policy priorities that are distinct from that of their most likely descriptive representativeâ€”a White man. At the same time, state legislatures have been veritable policy engines for bills and resolutions related to immigration and immigrants in recent years. The National Council of State Legislatures reports that in 2015 state legislatures enacted 216 laws and passed 274 resolutions related to immigrants and immigration. Even as these bodies write, debate, and pass legislation targeting immigrant communities, Asian Americans and Latina/os are rarely in the room. Researchers increasingly point to the scarcity of female or racial minority candidates as a key explanatory factor, but seldom examine issues related to race and gender at the same time. As a result, the extant scholarship obscures the outsized effects that White menâ€™s candidacies have in defining American elections, and overlooks the distinct challenges that women of color face in getting on the ballot.	This dissertation examines the intersecting roles of race and gender in elections, with particular attention to how they may be changing as immigrant communities become a larger proportion of the American population. I analyze the Gender Race and Communities in Elections dataset, which encompasses all state legislative general election winners and candidates from 1996-2015, and includes demographic information for candidates and their district populations. This original dataset provides the first opportunity to simultaneously analyze descriptive representation in state legislatures, for women and men in the four largest racial groups, at the national level. I also present the results of a national survey of state legislators, and in-depth interviews with political elites, in order to reveal race-gendered, informal, processes of candidate development and deterrence.	I also show that practical opportunities to compete in elections are sharply, and simultaneously, constrained by candidatesâ€™ race and gender. These constraints are most evident in the lopsided distribution of racial populations across districts, the uneven candidate development efforts of civic and political organizations, and the dominance of men in elite political networks, across racial groups.  	Based on my examination of Asian American and Latina/o candidates in elections, I advance a Race-Gendered Model for understanding the persistence of underrepresentation in state legislatures. I conceptualize elections as competitions for descriptive representation, and account for the disparate social and political experiences of women and men from different racial groups. Within this framework, race and gender simultaneously constrain potential candidatesâ€™ access to elections, producing a frequent absence of competition for descriptive representation. This model uses an intersectional approach to explain why Asian American and Latina/o women and men do not run more often, and why the majority of ballots are made up exclusively of White male candidates. 	I demonstrate that the increasing â€œstrength in numbersâ€ of Asian American and Latina/o communities has primarily served as a resource for increasing the racial diversity of men in statehousesâ€”to the limited extent that racial diversity has increased at all.  I also show that the most advantaged descriptive group, White men, benefits from an absence of competition in most electoral contests. At the same time, the fastest growing groups of womenâ€”Asian Americans and Latinasâ€”are also the groups most frequently excluded from competing.	The Race-Gendered Model expands the intellectual terrain available to answer longstanding questions in the study of women and racial minoritiesâ€™ underrepresentation. Along the way, I argue that it is necessary to simultaneously consider why White menâ€™s overrepresentation is similarly persistent. More broadly, the theory of competition presented in this dissertation shifts away from a central focus on the advantages and disadvantages groups face during election campaigns. Instead, I argue that the choices voters face in electing a descriptive representative are limited long before election day.",ucb,,https://escholarship.org/uc/item/0n754699,,,eng,REGULAR,0,0
478,1914,DESIGN AND OPERATION OF MINIMALLY ACTUATED MEDICAL EXOSKELETONS FOR INDIVIDUALS WITH PARALYSIS,"Tung, Wayne Yi-Wei","Kazerooni, Homayoon;",2013,"Powered lower-extremity exoskeletons have traditionally used four to ten powered degrees of freedom to provide ambulation assistance for individuals with spinal cord injury. Systems with numerous high-impedance powered degrees of freedom commonly suffer from cumbersome walking dynamics and decreased utility due to added weight and increased control complexity. This work proposes a new approach to powered exoskeleton design that minimizes actuation and control complexity through embedding intelligence into the hardware. Two novel, minimally actuated exoskeleton systems (the Austin and the Ryan) are presented in this dissertation. Unlike conventional powered exoskeletons, the presented devices use a single motor for each exoskeleton leg in conjunction with a unique hip-knee coupling system to enable their users to walk, sit, and stand. The two types of joint coupling systems used are as follows.The Austin Exoskeleton employs a bio-inspired mechanical joint coupling system designed to mimic the biarticular coupling of human leg muscles. This system allows a single actuator to power both hip and knee motions simultaneously. More specifically, when the mechanical hamstring and rectus femoris of the exoskeleton are activated, power from the hip actuator is transferred to the knee, generating synchronized hip-knee flexion and extension. The coupling mechanism is switched on and off at specific phases of the gait (and the sit-stand cycle) to generate the desired joint trajectories. The device has been proven to be successful in assisting a complete T12 paraplegic subject to walk, sit, and stand.The Ryan Exoskeleton (also called the Passive Knee Exoskeleton) uses dynamic joint coupling. Dynamic joint coupling refers to a method of generating knee rotation through deliberate swinging of the hip joint. This minimalistic system is the first powered exoskeleton that weighs less than 20 pounds and has a compact form factor that more closely resembles a reciprocating gait orthosis than a conventional exoskeleton. The Passive Knee Exoskeleton has been validated by several SCI test pilots with injury levels ranging from T5 to T12. The lightweight, ambulation-centric assistive device have been tested to be able to comfortably reach an average ambulation speed of 0.27 m/s and have demonstrated high levels of maneuverability. The dynamic joint coupling paradigm has been proven to be effective especially for newly injured individuals who have not yet developed significant amounts of joint contracture or sustain high levels of spasticity. Overall, this dissertation focuses on the design and operation of the Austin and Ryan Exoskeletons.",ucb,,https://escholarship.org/uc/item/0p24g6v0,,,eng,REGULAR,0,0
479,1915,Innate immune responses to soluble factors from Pseudomonas aeruginosa,"Grabiner, Mark Aaron","Machen, Terry;",2013,"Pseudomonas aeruginosa are gram-negative bacteria that colonize the human airway.  They are of great clinical importance, especially for patients with the genetic disorder Cystic Fibrosis, a disease characterized by persistent infection and hyper-inflammation in the airways (Hoiby et al., 1977).  The innate immune response to P. aeruginosa in airway cells consists of fluid secretion driven by the Cystic Fibrosis Transmembrane conductance Regulator (CFTR) and the release of inflammatory cytokines for the recruitment of phagocytes.  The present work addresses the role of two secreted products from P. aeruginosa and their effects on these processes.  Flagellin, the protein monomer of the P. aeruginosa flagellum, has been previously shown to induce cytokine secretion through Toll-Like Receptor 5 (TLR5) (Zhang et al., 2005) and to activate CFTR-mediated secretion through an unknown mechanism (Illek et al., 2008).  In the present study I attempted to discover the signaling pathway mediating CFTR secretion from flagellin.  Though this work did not yield a definitive pathway, many possibilities were explored and the response to flagellin was better characterized than in previous work.  In addition to my study of flagellin, I also studied the effects of a P. aeruginosa quorum-sensing signaling molecule, N-(3-Oxododecanoyl)-L-homoserine lactone (HSL-C12), on inflammatory signaling in mammalian cells.  Previous work has characterized the response to HSL-C12 as either pro- or anti-inflammatory depending on the system used and the measurements taken  (Telford et al., 1998, Smith et al., 2001, Smith et al., 2002, Kravchenko et al., 2006, Jahoor et al., 2008, Kravchenko et al., 2008).  In my study I utilized both gene expression and cytokine secretion measurements to determine that HSL-C12 has anti-inflammatory characteristics in short treatments but pro-inflammatory characteristics in longer treatments and that both of these phenotypes stem from an inhibition of host protein synthesis.  Together with what is already known about P. aeruginosa infection, my data helps to paint a picture of how secreted factors affect the course of infection and inflammatory response to P. aeruginosa in the human airway.",ucb,,https://escholarship.org/uc/item/0s34q7bv,,,eng,REGULAR,0,0
480,1916,Selling School Reform: Neoliberal Crisis-Making and the Reconstruction of Public Education,"Jani, Nirali","Perlstein, Daniel;",2017,"AbstractThis study asks how neoliberal reform became the hegemonic framework for racial justice and educational equity.  Using an interdisciplinary methodology, I examine three reform projects that operate on different terrains â€“ or scales â€“ of â€˜governmentalityâ€™: that of broad public sense-making, that of district policymaking, and that of individual and community-based subjectivities.  The first project (Chapter Two) was a national publicity campaign funded by the Broad and Gates foundations.  In this chapter, I use Critical Discourse Analysis (CDA) to understand how reformers used language to shape public consciousness, pointing to the continuity of an educational â€œcrisis discourseâ€ first manufactured in the Reagan era.  Chapter Three examines the state takeover and neoliberal reconstruction of an urban school district.  Using the theoretical framework of â€œdisaster capitalismâ€ (Klein, 2007), I trace how the neoliberal reform network penetrated the district, fundamentally reshaping its structures and processes.  In the fourth chapter, I use ethnographic methods to study the effects of â€˜punitive privatizationâ€™ on a school site steeped in historical traditions of anti-racist and anti-capitalist critique.  I argue that neoliberal accountability is â€œdevitalizingâ€ (McDermott & Hall, 2007) to the political vision and practices of the school, and that it works to co-opt dissent and redirect parent participation.   Taken together, these projects demonstrate both coercive and consensual processes: the corporate reform network penetrates public institutions and democratic processes, redirecting them to do the work of marketization and capital accumulation.  At the same time, it employs sophisticated and well-funded marketing to articulate these projects across a breadth of terrains and at different scales.  Each project demonstrates how market advocates, driven by venture-philanthropic funding, position their work as the only possible means for racial justice and educational equity.  The findings point to two powerful aspects of neoliberalism: its role in creating and manipulating educational crises and its ability to absorb and reframe challenges to capitalism.",ucb,,https://escholarship.org/uc/item/0sq6w3rk,,,eng,REGULAR,0,0
481,1917,"The Place, Space, and Practice of Andrew Wyeth's Hay Ledge","Harvey, Edwin Rein","Lovell, Margaretta M;",2014,"The works of the American painter Andrew Wyeth (1917-2009) have for many decades been subjects of contentious debate among historians and critics of art. The great majority of these scholars have tacitly agreed, however, that Wyeth's works are simple matters--that be they good or bad, ""artistic"" or ""illustrative,"" innovative or apish, ""modern"" or ""traditional,"" they are obviously or self-evidently so. Such beliefs are implied, at least, by the practice of withholding from publication the concrete observations about individual works upon which broad, totalizing claims about Wyeth and his practice have been reached.Intending to correct this mistaken belief about the simplicity and uniformity of Wyeth's work, this dissertation enacts a sustained encounter with a single paintingâ€”Hay Ledge (1957)â€”working at length over the course of three close-knit chapters to demonstrate 1) the formal, conceptual, and sentimental depth of this particular work, 2) the fact that Wyeth's art practice changed over time, and 3) the complexity of the cultural contexts to which that practice responded and in which Wyeth's works in general, and Hay Ledge specifically, have been received and appraised. It thereby begins to bring Wyeth's practice into a more stable, balanced light, thus enabling scholars of art and culture more broadly to reconsider an historically significant phenomenon that they might previously have found too opaque or too polarizing to engage.",ucb,,https://escholarship.org/uc/item/0q1403wj,,,eng,REGULAR,0,0
482,1918,A Phenomenological Study of High-Field Optically Pumped 13C NMR in Diamond,"Scott, Eric","Reimer, Jeffrey A;Pines, Alexander;",2015,"Nuclear magnetic resonance (NMR) is a powerful spectroscopic technique capable of probing the local electronic environment in a wide array of materials.  The sensitivity of an NMR experiment is proportional to a net nuclear spin polarization that is often generated by placing the sample of interest in a static external magnetic field and allowing the spin state populations to come to thermal equilibrium.  Unfortunately these thermal polarizations are exceedingly small.  Even with a 23.5 Tesla magnet, the strongest currently available, the room temperature 1H polarization is only 0.0081%.  Significant sensitivity enhancements can be achieved by hyperpolarizing the nuclear spin system.  Transferring polarization from electrons to nuclei, also known as dynamic nuclear polarization (DNP), is one method by which this hyperpolarization is achieved.  Optically pumped NMR (OPNMR) is a form of DNP, which uses laser light to athermally polarize a reservoir of electron spins which in turn athermally polarize coupled nuclei.	For much of its history solid state OPNMR involved transferring polarization from photo-excited conduction electrons to hyperfine coupled nuclei in zincblende semiconductors at temperatures â‰¤ ~80 K.  Within the last decade it was found that OPNMR of 15N, 14N, and 13C in single crystal diamond is possible via optically polarized negatively charged nitrogen vacancy (NV-) defects.  The best characterized form of OPNMR in diamond is a hyperfine-mediated phenomenon that takes advantage of the NV- excited state level anti-crossing that occurs when the external field is set to ~50 mT.  A high-field form of 13C optical pumping has been observed at 7.05 and 9.4 T, well beyond the level anti-crossing.  Polarization rates, lifetimes, and magnitudes are influenced by the concentrations of NV- and P1 defects in the diamond.  Two of the samples characterized in this study have defect concentrations that allow for the generation of room temperature 13C polarizations up to 200 times that of thermal equilibrium.   Both positive and negative polarizations are observed.  The sign and magnitude of the polarization exhibit an extraordinary sensitivity to the orientation of the crystal with respect to the polarization of the electric field vector of the optical illumination incident on the sample.  For example, the sign of the polarization can flip with as little as a 0.5Â° change in the orientation of the crystal.	The mechanism responsible for this high-field pumping process remains unknown.  Progress in developing a theoretical model is hindered by the simultaneous presence of four defect orientations for every orientation of the crystal.",ucb,,https://escholarship.org/uc/item/0k41b221,,,eng,REGULAR,0,0
483,1919,Surface Response of Tungsten to Helium and Hydrogen Plasma Flux as a Function of Temperature and Incident Kinetic Energy,"Sefta, Faiza","Peterson, Per;",2013,"Tungsten is a leading candidate material for the diverter in future nuclear fusion reactors. Previous experiments have demonstrated that surface defects and bubbles form in tungsten when ex- posed to helium and hydrogen plasmas, even at modest ion energies. In some regimes, between 1000K and 2000K, and for He energies below 100eV, ""fuzz"" like features form. The mechanisms leading to these surfaces comprised of nanometer sized tungsten tendrils which include visible helium bubbles are not currently known. The role of helium bubble formation in tendril morphology could very likely be the starting point of these mechanisms. Using Molecular dynamics (MD) simulations, the role of helium and hydrogen exposure in the initial formation mechanisms of tungsten ""fuzz"" are investigated. Molecular dynamics simulations are well suited to describe the time and length scales associated with initial formation of helium clusters that eventually grow to nano-meter sized helium bubbles. MD simulations also easily enable the modeling of a variety of surfaces such as single crystals, grain boundaries or ""tendrils"".While the sputtering yield of tungsten is generally low, previous observations of surface modification due to plasma exposure raise questions about the effects of surface morphology and sub-surface helium bubble populations on the sputtering behavior. Results of computational molecular dynamics are reported that investigate the influence of sub-surface helium bubble distributions on the sputtering yield of tungsten (100) and (110) surfaces induced by helium ion exposure in the range of 300 eV to 1 keV. The calculated sputtering yields are in reasonable agreement with a wide range of experimental data; but do not show any significant variation as a result of the pre-existing helium bubbles.Molecular dynamics simulations reveal a number of sub-surface mechanisms leading to nanometer- sized ""fuzz"" in tungsten exposed to low-energy helium plasmas. We find that during the bubble formation process, helium clusters create self-interstitial defect clusters in tungsten by a trap mutation process, followed by the migration of these defects to the surface that leads to the formation of layers of adatom islands on the tungsten surface. As the helium clusters grow into nanometer sized bubbles, their proximity to the surface and extremely high gas pressures can cause them to rupture the surface thus enabling helium release. Helium bubble bursting induces additional surface damage and tungsten mass loss which varies depending on the nature of the surface. We then show tendril-like geometries have surfaces that are more resilient to helium clustering and bubble formation and rupture. Finally, the study includes hydrogen to reveal the effect of a mixed 90%H-10%He plasma mix on the tungsten surface. We find that hydrogen greatly affects the tungsten surface, with a near surface hydrogen saturation layer, and that helium clusters still form and are attractive trapping sites for hydrogen.Molecular dynamics simulations have also investigated the effect of sub-surface helium bubble evolution on tungsten surface morphology. The helium bubble/tungsten surface interaction has been systematically studied to determine how parameters such as bubble shape and size, temperature, tungsten surface orientation and ligament thickness above the bubble impact bubble stability and surface evolution. The tungsten surface is roughened by a combination of adatom islands, craters and pinholes. The study provides insight into the mechanisms and conditions leading to various tungsten topology changes, most notably the formation of nanoscale fuzz.An atomistic study of the mechanisms behind initial phases of tungsten nano-fuzz growth has determined that tungsten surfaces are affected by sub-displacement energy helium and hydrogen fluxes through a series of mechanisms. Sub-surface helium atom clustering, bubble nucleation, growth and rupture lead to tungsten surface deformation. Helium clustering processes vary near grain boundaries or in tendril-like surface geometries. In the presence of hydrogen, these mechanisms are coupled with hydrogen surface saturation. Finally, further investigation to connect these atomistic mechanisms to nano-size tungsten fuzz growth is needed to get a comprehensive under- standing of the effects of low energy helium and hydrogen on tungsten.",ucb,,https://escholarship.org/uc/item/0kw3r768,,,eng,REGULAR,0,0
484,1920,Spatiotemporal Dynamics of Working Memory in Humans,"Johnson, Elizabeth","Knight, Robert;",2016,"Working memory (WM) is the ability to hold information for online processing. As the basis of long-term memory formation and a fundamental construct of thinking, it is paramount that we understand how WM works. Distributed network models posit that the prefrontal cortex (PFC) supports WM by coordinating top-down control over other regions involved in sensory representation and long-term memory. We utilized an episodic memory paradigm that probes WM for identity, spatial, and temporal information to examine the PFC dependent model of WM. In two studies, multimodal electrophysiology data reveal that PFC control over WM is fundamentally dynamic in nature, and that WM is dependent on activity distributed across anterior and posterior cortical regions. Results challenge the simple PFC model of WM.Ch. 1 presents evidence from intracranial recordings that frontal and medial temporal lobe (MTL) theta rhythms carry WM-related activity, and uncovers two WM systems. The PFC-MTL system exhibits bidirectional interaction that shifts with msec precision in response to task demands. In contrast, MTL rhythms direct activity in the orbitofrontal cortex via theta rhythms that do not vary with task demands. These findings support a bidirectional PFC-MTL system in humans â€“ in which theta rhythms subserve executive control during episodic memory formation. Ch. 2 presents evidence from patients with unilateral PFC damage, which shows that a posteriorly-sourced alpha-beta network provides adequate resources for well above-chance WM accuracy. However, when the PFC is intact, PFC low theta activity increases commensurate with executive demand, and PFC-sourced slow rhythms and posteriorly-sourced alpha-beta rhythms travel in opposite directions to support optimal WM.Ch. 3 reviews 15 years of intracranial research on human memory, and considers the potential of intracranial electrophysiology as a technique to address unresolved questions in the neuroscience of human memory. Ch. 4 presents key themes from this work for younger readers; specifically, it introduces the concepts of cross-frequency coupling between theta rhythms and fast activities in the MTL, and inter-regional PFC-MTL synchrony for memory formation. In a second public outreach piece, appendix 1 introduces the logic of neuropsychology to younger readers to understand why memories of music are resilient to the deleterious effects of amnesia and dementia. Appendix 2 shows that WM develops in children along with increases in sustained attention, and appendix 3 reviews evidence that executive control develops commensurate with PFC connectivity across distributed neural networks. Finally, appendices 4-5 present applications of research on WM and control, together delineating behavioral and neural underpinnings of optimal relational reasoning in neurologically healthy adults.",ucb,,https://escholarship.org/uc/item/0m56r7ct,,,eng,REGULAR,0,0
485,1921,"Silicon Nanowires for Chemical Sensing, pH Measurement and Ion Species Identificaiton in Solution","Pace, Maria E.","Zohdi, Tarek I;Pisano, Albert P;",2015,"AbstractSilicon Nanowires for Chemical Sensing, pH Measurement and Ion Species Identification in SolutionbyMaria E. PaceDoctor of Philosophy in Applied Science and TechnologyUniversity of California, BerkeleyProfessor Tarek I. Zohdi, Co-ChairDean Albert P. Pisano, Co-Chair 	In situ measurement of true pH would be useful in many applications. True pH is the negative log of the Hydrogen ion concentration, however, measurement of this is elusive in many practical applications due to the presence of interfering ions, such as sodium and potassium. Monitoring true pH in surgical procedures, for instance, would be very useful, however, interfering ions necessitate preprocessing of the blood and laboratory analysis making it not in situ, and not in real time. This work presents a measurement system capable of measuring true pH in the presence of interfering ions using nanowire sensors and electrospectroscopy.  This system is also able to operate as a chemical sensor by discriminating between different ionic species in solution and can separately measure concentrations of other ionic species for leak detection and chemical identification. Nanowire sensors offer many advantages such as small size, low power, and inexpensive fabrication. These advantages allow real time, in situ monitoring in many applications.       In addition, silicon nanowires are integrated as a semiconductor pH sensor and species identification chip. Using electrospectroscopy, ions drift in the fluid at different times allowing the nanowire to make measurements of different species present in the fluid. To accomplish this, various modes of operation including â€œthe time of flightâ€ have been developed to maximize ion identification and species concentration measurement. The advantages of these sensors include high sensitivity at low concentrations, 80% sensitivity at 1e-6 M with ion species identification and measurement of true pH. Depending on the species of interest, a particular mode of operation can be employed to achieve desirable results.  Advantages of these modes of operation are isolation of hydrogen ions from other species including sodium to measure true pH in real time and a method for deconvolving the species both in temporal and spatial maps. Additionally, a method for electrically cleaning the nanowires sensors and a method to re-zero the nanowires has been explored allowing more accurate measurement of the species and true pH.       A novel top down fabrication process has been developed which reduces the line edge roughness of the nanowire for more reproducible sensors, reduces dielectric pin hole density for minimal sensor drift over time and reduces parasitic resistance for higher ion sensitivity.  This novel fabrication process is truly CMOS compatible allowing more compatibility with other electronics. The SiNW is covered by thin film which protects SiNW from liquid penetration and can also work as ion sensitive film or functionalized surface. As a fabrication simplicity, the entire structure above can be built on a standard SOI (Silicon on Insulator) wafer. Experimental results have shown a linear relation between resistance change in the nanowire and pH in the fluid.",ucb,,https://escholarship.org/uc/item/0m94k81x,,,eng,REGULAR,0,0
486,1922,"Writing the Postcolonial City: Phnom Penh and Modernity during Sangkum Reastr Niyum, 1955-1970","Keo, Siti Galang","Zinoman, Peter B.;",2019,"This dissertation examines novels, essays, films and songs of the Sangkum Reastr Niyum period, 1955-1970, to explore the layers of meanings Cambodians held of Phnom Penh.  After the Geneva Accords in 1954, Phnom Penh emerged as the capital city of a newly independent nation-state, the Kingdom of Cambodia.  The city under French colonial rule was secondary to Hanoi and Saigon, but once Indochina dissolved, its population exponentially increased.  Phnom Penh was at the center of Cambodiaâ€™s road networks, its banking system, and was home to the best universities and schools.  The many jobs and opportunities attracted rural migrants to the city.  The population boom was one of the many ways Phnom Penh transformed.  Norodom Sihanouk, then the head of state, made Phnom Penh the epicenter of government modernization projects.  Under his watch, the capital transformed from being a marshy, provincial hub into an exciting scene of cosmopolitan innovation.  Urban Cambodians combined ideas from Le Corbusier with traditional Khmer architectural details to design their â€œmodernâ€ buildings.  Their songs were influenced by the French singer Johnny Halliday and the American Wilson Pickett.  They wrote novels that built upon the ideas found in Buddhism and French Existentialism.  Through their works, urban intellectuals sought to define a Cambodian identity independent of French colonialism.  Phnom Penh, with its new roads, many schools, bars and publishing houses, was a space where Cambodians became modern and developed new identities, such as the neary samey tmey and the pannavoan.  These changes to the landscape and social composition of Phnom Penh engendered a new consciousness amongst Cambodian intellectuals.  Their writings expressed a concern over changes in heterosexual relationships and the behavior of Cambodians in public spaces.  To some, not all the changes were good.  They mourned the marshes and wooden shacks that asphalt and concrete had replaced.   They were aware of ruptures, loss, and fragmentation.  This consciousness of the new amongst urban Cambodian intellectuals is what I term postcolonial modernity.  This study contributes to the history of Sangkum Reastr Niyum by taking seriously the historical value of Cambodian writings and focuses the lives of everyday urban Cambodians.  It describes the Sangkum period as a time of unprecedent change that witnessed the emergence of a new urban middle class.",ucb,,https://escholarship.org/uc/item/0mh943bz,,,eng,REGULAR,0,0
487,1923,"Theoretical Investigation of the Oxidative Carbonylation of Toluene to Toluic Acid over Rh(III) and Pd(II), and Theoretical Method Development for the Rapid Identification of ab initio Transition States","Behn, Andrew Paul","Bell, Alexis T;Head-Gordon, Martin;",2011,"The use of theoretical chemistry techniques in the investigation of catalytic reactions has been able to provide strong insights into the inner workings of various chemical mechanisms.  In tandem with experimental results, such studies often provide information by computing reaction rates with transition state theory, predicting and/or confirming various spectroscopic experiments, and elucidating the identities and structures of key stable intermediates and short-lived transition states.  The present work is concerned with the application of these techniques to the study of the oxidative carbonization of toluene to toluic acid over Rh(III) and Pd(II), as well as the development of theoretical techniques to efficiently find ab initio transition states for use in such studies.	Previous work has shown that the oxidative carbonylation of toluene to form toluic acid is possible with Rh(III) and Pd(II) with acetic acid.  These reactions are believed to operate via a rate-limiting electrophilic mechanism in which toluene binds to the metal complex and has a C-H bond activated.  Previous works have suggested that the active catalyst for the Rh(III) system is Rh(CF3COO)3(CO)2, and Pd(CF3COO)2 for the Pd(II) system, though these were not rigorously confirmed.  In this work, we properly identify the Rh(III) species as the active catalyst through a series of ab initio spectroscopic calculations with comparison to experiments.  Additionally, an unprecedented interaction between an acetate and carbonyl ligand on the Rh(CF3COO)2(CO)2 catalyst is investigated and found to be the result of an unusual charge balance within the structure.  Prior work has shown that using trifluoroacetic acid instead of acetic acid significantly increases the rate of reaction, without investigating further.  This work demonstrates that the reaction rate passes through a maximum for intermediate strength acids, which is due to competition between the two sub-steps of the rate-limiting step.  Weakly basic anionic ligands increase the positive charge on the metal center and increase the rate of toluene binding while decreasing the ability of the same ligands to accept the activated proton.  A similar trend and explanation were found with a model catalyst for the Pd(II) system as the ligands were varied.          The second part of this work concerns the development of efficient transition state searching algorithms.  The calculation of theoretical rate constants often employs transition state theory, but requires the user to possess the transition state structure.  The local search for such a structure requires an extremely good guess, and is most practically obtained with the help of an automated guess generator.  The most commonly used algorithms operate by optimizing a chain of molecular images connecting known reactant and product structures into the reaction pathway.  One such routine, the Growing String Method (GSM), grows a chain of states inward from the known endpoints while optimizing these points.  The original GSM algorithm relies upon cartesian coordinates with cubic splines for adding new structures to the chain, however this often leads to unrealistic images which require many steps to relax into the reaction pathway.  By replacing the cartesian coordinate interpolation with Linear Synchronous Transit interpolation, the computational cost of optimizing complex reaction pathways may be cut approximately in half.  Additionally, by simplifying the algorithm to focus computational effort on the location of just the transition state rather than the entire reaction pathway, the overall cost may be reduced even further.  In this new method, the Freezing String Method, nodes are iteratively added to a growing chain, optimized for several steps, and then frozen in place for the remainder of the execution.",ucb,,https://escholarship.org/uc/item/0mr8w2r5,,,eng,REGULAR,0,0
488,1924,In the shadow of the secular: Theories of reconciliation and the South African TRC,"VanAntwerpen, Jonathan Dirk","Burawoy, Michael;",2011,"In the aftermath of the South African Truth and Reconciliation Commission (TRC), ""reconciliation"" has come to the fore as a keyword in global political culture. Although the possibilities associated with the transitional politics of truth and reconciliation have been widely touted, however, and the TRC much celebrated, the South African commission and its master narrative of truth and reconciliation have also been vigorously and repeatedly criticized. Reconciliation--closely associated with both amnesty and forgiveness, and explicitly theological in many of its articulations--has been tremendously controversial. Rising to global prominence as a result of experiments with innovative truth commissions in the global South, the promise of reconciliation was received warily at best by many North American scholars and human rights activists. Yet in the midst of both spirited critique and uncertain embrace, discourses of truth and reconciliation have come to represent one of the most prolific traveling theories of our time. Examining the intellectual struggles over reconciliation that accompanied the invention of truth commissions and the rise of transitional justice, analyzing the field-transforming efforts and effects of the South African TRC, and attending in particular to the critical uptake of reconciliation within North America, this dissertation pursues a critical sociological study of the secularity of contemporary intellectual culture.",ucb,,https://escholarship.org/uc/item/0sz396p4,,,eng,REGULAR,0,0
489,1925,Asymptotically Conical Metrics and Expanding Ricci Solitons,"Wilson, Patrick Farrell","Lott, John;",2018,"In this thesis we first show, at the level of formal expansions, thatany compact manifold can be the sphere at infinity of an asymptot-ically conical gradient expanding Ricci soliton. We then prove theexistence of a smooth blowdown limit for any Ricci-DeTurck flow onR n , starting from possibly non-smooth data which is asymptoticallyconical and sufficiently L âˆž -close to an expanding soliton on R n . Fur-thermore, this blowdown flow is an expanding Ricci-DeTurck solitoncoming out of the asymptotic cone of the initial data.",ucb,,https://escholarship.org/uc/item/0vf7h67j,,,eng,REGULAR,0,0
490,1926,Evaluation and Adaptations of a Community-Based Participatory Research Partnership in San Francisco's Chinatown,"Chang, Charlotte","Minkler, Meredith;",2010,"Interest in community-based participatory research (CBPR) continues to grow in public health across diverse populations and settings, and over the past two decades, the field has gained a great deal of experience in understanding what makes for successful CBPR. In spite of its increasing application, however, there is still much to be learned in terms of systematic evaluation in CBPR, how it is that CBPR partnerships adapt principles and practices to local context, and the nature of the specific adaptations they make. This dissertation looks at the state of the field in terms of recommended principles and practices of CBPR and then centers on the experience of the San Francisco Chinatown Restaurant Worker Health and Safety Project, a CBPR partnership focused on studying and addressing working conditions for Chinese immigrant restaurant workers. First, an examination of the major CBPR review literature finds that the existing guidance on recommended CBPR principles and practices is large in volume and generally in agreement. But it also finds inconsistent use of terminology and typology with regard to CBPR characteristics and an overall lack of specificity associated with how the concepts should be applied in evaluation, particularly for partnership goal-setting and prioritization. Second, using a recently developed CBPR process-to-outcomes model as a reporting framework, the dissertation details the salient contextual, group dynamics, intervention and research, and outcome factors emerging from the Chinatown partnership evaluation. Contexts of interest include the broader social and immigration environment of the community, historical trust and mistrust, and university and community capacity. In terms of group dynamics factors, partnership diversity and complexity, resource availability, and roles of individuals were important in shaping partnership dynamics, with formal partnership agreements playing less of a role. â€œProcess outcomesâ€ of dialogue, mutual learning, and communication; power dynamics; decision-making; leadership; trust; and perceptions of CBPR authenticity were in turn all affected and structured by the contexts. Research dynamics and capacity change outcomes were generally perceived positively, particularly with regard to the leadership development of restaurant worker partners.Finally, the dissertation draws on evaluation data to focus on the CBPR principle of â€œequitable participation, â€ particularly for Chinese immigrant worker partners on the project. The research finds that the social context and political or participatory â€œstarting pointsâ€ of the immigrant community, social justice values and drivers of the community-based organization partner, linguistic and cultural diversity within the partnership, and constrained resources led to specific adaptations in the structure and processes of the collaboration. Partner reflections on the outcomes of the adaptations are discussed. Implications for this research suggest that further elucidation of the concepts and functions of CBPR principles and practices will advance the field's ability to effectively evaluate CBPR efforts and further understanding of CBPR â€œauthenticity.â€ Future evaluation efforts may find use of a model of CBPR process to outcomes helpful in systematically designing and reporting on evaluations. Attention to contextual variables of particular communities and partnerships can contribute to understanding how adaptations unfold in CBPR efforts, what the adaptations actually entail, and to what extent they are consistent with CBPR principles and practices.",ucb,,https://escholarship.org/uc/item/0wk9x7nq,,,eng,REGULAR,0,0
491,1927,Tough Construction in English: A Construction Grammar Approach,"Chung, Yoon-Suk",,2001,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0z44h6zf,,,eng,REGULAR,0,0
492,1928,To Fix the Image in Memory: Adaptive Analog Coding in Emerging Memory Systems,"Zarcone, Ryan","DeWeese, Michael R;Olshausen, Bruno;",2020,"Motivated by (i) natureâ€™s ability to perform reliable, efficient computation with stochastic components, (ii) the end of Mooreâ€™s Law (and other associated scaling laws) for our current computational paradigm, and (iii) the exponentially increasing amount of data (especially of the image variety) generated over the last decade, we examine herein the ability of analog valued emerging memory devices to directly store analog valued data.  Specifically, we start by recasting the problem of data storage as a communication problem, and then use tools from the field of analog communications and show, using Phase Change Memory (PCM) as a prototypical multi-level storage technology, that analog-valued emerging memory devices can achieve higher capacities when paired with analog codes.  Further, we show that storing analog signals directly through joint coding can achieve low distortion with reduced coding complexity.   We then scale the problem up to store natural images on a  simulated array of PCM devices.  Here, we construct an autoencoder framework, with encoder and decoder implemented as neural networks with parameters that are trained end-to-end to minimize distortion for a  fixed number of devices.   We show that the autoencoder achieves a  rate-distortion performance above that achieved by a separate JPEG source coding and binary channel coding scheme.  Next, we demonstrate, this time by experiment, an image storage and compression task by directly storing analog image data onto an analog-valued ResistiveRAM (RRAM) array.  A joint source-channel coding algorithm is developed with a neural network to encode and retrieve natural images.  This adaptive joint source-channel coding method is resilient to RRAM array non-idealities such as cycle-to-cycle and device-to-device variations,  time-dependent variability,  and non-functional  storage  cells,  while achieving a reasonable reconstruction performance of ~20 dB using only 0.1 devices/pixel for the analog mage.  Finally, in an attempt to explicitly tackle device-device variation and drift, we use data from a commercial fabrication facility at TSMC and demonstrate preliminary results showing the ability to create an effective drift model capable of inferring values stored at previous times.",ucb,,https://escholarship.org/uc/item/1330b89z,,,eng,REGULAR,0,0
493,1929,Essays on the Economics of Urban Crime,"Dominguez Rivera, Patricio","Raphael, Steven;",2018,"This dissertation focuses on what determines crime and victimization in the urban space. The theoretical basis of this work strongly rests on the contributions of Gary Becker and Ronald V. Clarke to the crime literature. Chapter one offers a brief and selected overview of many important contributions in the economics of crime literature. I focus on some particular puzzles and the kind of solutions that have been offered in the last fifty years where the understanding regarding the role of prisons as a crime-control policy tool is critically assessed. I emphasize the complexity and multi-causal nature of crime as a way to motivate the theoretical and empirical contributions of chapters two and three. In the following chapters I pay special attention on how criminal activity reacts to changes in incentives or situational factors under different theoretical and empirical settings. Chapter two is devoted to understand criminal activity in the public transportation sector which represents a salient place where criminal activity takes place. I incorporate a simple model of offenders and victim's interactions to describe how victim's behavior reacts and determines the set of criminal opportunities available for potential offenders. Empirically, I exploit a set of subsequent reforms in the public transportation system where variations to driver's incentives as well as the availability of cash substantially altered the amount and the nature of criminal activity observed. In chapter three, jointly written with Kenzo Asahi, we study the effect of a particular situational factor such as ambient light on crime. In both cases, the evidence I find supports the relevance of incentives. I identify specific policy changes or situational deviations whose effects in terms of criminal victimization can be equivalent to enormous public policy interventions in terms of prosecution. All findings presented in this dissertation attempts to enlarge the growing body of scientific understanding regarding criminal activity in urban areas.",ucb,,https://escholarship.org/uc/item/14t1b55c,,,eng,REGULAR,0,0
494,1930,"Preaching Sex: Gender and Official Church Discourses in Mexico City, 1720-1875","Witschorik II, Charles Arthur","Chowning, Margaret;",2011,"This dissertation project analyzes the different ways that, over time, gendered images, metaphors and hagiographical examples were used in sermons and other documents that the Church approved for publication (that is, what I am calling official Church discourses) to help it negotiate challenges to its cultural and ideological hegemony. I argue that beginning in the seventeenth century and continuing into the first half of the eighteenth preachers utilized the discursive openness of the Baroque in New Spain to articulate surprisingly flexible visions of gender. For example, some preachers portrayed exceptional, saintly women as ""manly,"" while others did not hesitate to describe how some men took on motherly, nurturing roles. As part of a larger Baroque aesthetic in which opposites and excesses were celebrated, unusual gendered language and associations provided a way for prelates to engage their audiences even while upholding, albeit paradoxically, received conventions.As ""enlightened"" ideas began to penetrate New Spain in the later part of the century, however, preachers' visions of gender evolved according to the prevailing reformist spirit. As with church architecture, paintings and other media, so also in sermons those stylistic elements which had formerly displayed the Baroque spirit were increasingly jettisoned in favor of new, more austere features, resulting, in the case of sermons, in less flexibility in how men and women were portrayed. While women could to an extent still aspire to imitate virtues associated with men, preachers' language about men grew significantly more masculinized and even militarized, often at the service of promoting the needs and interests of the Crown. Then, in the years preceding independence (1821), preaching once again changed course as the implications of some of the earlier changes grew clearer. Where reformers had promoted obedience to secular authorities and a close church-state alliance, later prelates became alarmed as the Crown began to chip away at traditional Church privileges and rational, sometimes even secular ideas and trends took hold in New Spain with a tenacity beyond the Church's ability to control. As a result, the high clergy responded by promoting moral renewal and reinvigorated obedience to ""legitimate"" authorities, by whom they often meant God, not the now-unreliable Crown. Women were the particular targets of clerical admonishments, not because they had strayed more than men, but because women, in a patriarchal society that emphasized female obedience, symbolized the dangers of excessive freedom. In the wake of independence and the struggles that followed it, however, it became clearer that not every church-state battle could be won, and preachers turned increasingly to women as idealized members of ""the devout sex,"" charged with fomenting devotion and moral virtue among their contemporaries, both female and male. Put another way, liberals after independence were not the only ones to assert a kind of ""republican motherhood"": preachers countered with a vision of ""Catholic motherhood"" that had great resonance in Mexico even into the twentieth century.This dissertation offers contributions to the history of the Catholic Church in Mexico, suggesting some of the ways that the institution sought to strengthen and perpetuate its traditionally prominent role in Mexican society. Though historians of the Baroque and its various manifestations in New Spain sometimes use the floweriness or elaborateness of sermons to exemplify Baroque style, few studies have sought to examine in depth how the Baroque aesthetic of excess and opposites functioned discursively in sermons. Likewise, a handful of other historians have explored issues of the Enlightenment and the Church, but primarily from the point of view of internal reform efforts, and they have not used sermons as a source for understanding the ways that official ecclesiastical discourses were shaped and articulated. Sermons have been used by a small number of historians to build a picture of a Church that is neither monolithic nor fully reactionary, but their aim has been primarily to understand the Church's political role after independence, not the earlier struggles it faced in the late colonial period or the adaptations it made later in the nineteenth century. Finally, none of these students of the Church have been especially interested in gender as a key aspect of how ecclesiastical discourses were constructed. My dissertation addresses these lacunae.",ucb,,https://escholarship.org/uc/item/1632p0q8,,,eng,REGULAR,0,0
495,1931,Molecular Insights into the Structure and Function of the Telomerase Holoenzyme in Tetrahymena thermophila,"Upton, Heather","Collins, Kathleen;",2016,"Telomeres are specialized, G-rich simple-sequence repeats that cap the ends of linear chromosomes to prevent genome instability. These tandem DNA repeats are bound by sequence-specific proteins to create a protective structure that marks the chromosome end thereby preventing aberrant chromosomal recombination, resection, degradation, and fusion. Due to inherent limitations of genome replication and chromosome end processing, telomeres shorten over time leading to potential loss of genetic information if not restored or maintained. The ribonucleoprotein (RNP) telomerase functions in this regard by using an integral RNA template (TER) to synthesize single stranded telomeric repeats at the chromosome end. In vitro minimal catalytic activity can be reconstituted from the telomerase protein component TERT and TER; however, in vivo biologically active holoenzyme requires further protein components for repeat addition synthesis, enzyme recruitment, and regulation in the cell. The ciliate Tetrahymena thermophila serves as an experimentally favorable model system for the study of telomerase due high levels of constitutively active enzyme and robust molecular and genetic techniques. Furthermore, our understanding of the holoenzyme is arguably best characterized from the Tetrahymena enzyme, which consists of nine protein components and the RNA (TERT, TER, p65, p50, Teb1, Teb2, Teb3, p75, p45, and p19). Despite knowledge of the overall architecture, relationships between multiple proteins within the holoenzyme and their specific physiological roles had remained unresolved.Using a variety of in vitro and in vivo biochemical techniques, I show that the holoenzyme component p50 functions as a central hub for enzyme assembly, connecting the RNP catalytic core to the RPA-like Teb1-Teb2-Teb3 (TEB) and p75-p45- p19 (CST) subcomplexes. To answer existing questions concerning telomerase recruitment, I employ endogenously tagged holoenzyme proteins to show that all telomerase holoenzyme subunits are subject to coordinate telomere recruitment and release dependent on the cell cycle. Using domain tagging and truncation strategies, I demonstrate that the high-affinity single-stranded telomeric DNA binding component Teb1 is necessary and sufficient for interaction between telomerase and the telomere. This work supports a model for Tetrahymena telomerase-telomere recruitment that breaks the precedent established by studies in yeast and vertebrate cells: Teb1- containing holoenzyme is recruited directly to the telomeric DNA rather than telomerase recruitment by interaction with a telomere-bound protein. Together, along with ongoing studies of the Tetrahymena TEB and CST subcomplexes, these results suggest commonalities of telomerase interaction, action, and regulation at telomeres across species.",ucb,,https://escholarship.org/uc/item/1697z5rk,,,eng,REGULAR,0,0
496,1932,Chemical Methods to Modify Proteins and Glycans,"Agarwal, Paresh","Bertozzi, Carolyn R;",2015,"The ability to chemically modify biomolecules has facilitated our ability to detect, manipulate, and study them both in vitro and in vivo, as well as to prepare and tailor the properties of biologics and other pharmaceuticals based on natural products. This thesis describes several projects united around the theme of new methods to chemically modify biomolecules. The bulk of the thesis describes a new method to prepare protein bioconjugates relevant to diagnosis and treatment of human disease, while the last chapter describes the development of a new research tool to image protein glycosylation in vivo.	Chapter 1 describes the state of the art in protein modification methods by examining them through the lens of site-specific antibodyâ€“drug conjugates. Many methods for site-specific protein modification are now known in the literature, but only the subset that have met stringent requirements with respect to reagent and conjugate stability, minimal side reactivity, fast reaction kinetics, and amenability to structure-activity relationship studies have been seriously considered for use on a commercial scale. Thus, rather than cataloging every known method for site-specific protein modification, this chapter harnesses the collective wisdom of the field in detailing the origins and practical uses of the most popular and well-validated methods for site-specific protein modification.	Chapters 2 and 3 describe my contributions to the field resulting in a reaction for protein modification known as the Pictetâ€“Spengler ligation. Aldehyde- and ketone-functionalized proteins are appealing substrates for the development of chemically modified biotherapeutics and protein-based materials. Their reactive carbonyl groups are typically conjugated with Î±-effect nucleophiles, such as substituted hydrazines and alkoxyamines, to generate hydrazones and oximes, respectively. However, the resulting C=N linkages are susceptible to hydrolysis under physiologically relevant conditions, which limits the utility of such conjugates in biological systems. The Pictetâ€“Spengler ligation addresses this problem by providing a means to generate a stable linkage to protein aldehydes and ketones.	Aside from their hydrolytic instability, another drawback of oxime linkages is that the optimal conditions for their formation are acidic (pH 4.5), preventing their use with acid-sensitive proteins and post-translational modifications. The work in Chapter 3 describes a variant of the Pictetâ€“Spengler ligation, the hydrazino-Pictetâ€“Spengler ligation, that proceeds quickly near neutral pH. This work was carried out at Redwood Bioscience (now part of Catalent Pharma Solutions), a biotechnology company based in Emeryville, CA that uses aldehyde-functionalized proteins to prepare site-specifically modified antibody-drug conjugates.	Chapter 4 transitions from protein modification to glycan modification, describing a new method that combines the Bertozzi labâ€™s longstanding interest in metabolic glycoengineering with recent advances in fluorogenic bioorthogonal reactions to image internal cell-surface glycans in live zebrafish. Vertebrate glycans constitute a large, important, and dynamic set of post-translational modifications that are notoriously difficult to manipulate and image. We have previously used the chemical reporter strategy in conjunction with bioorthogonal chemistry to image glycans on the enveloping layer of live zebrafish embryos; however, the ability to image glycans systemically inside a live organism has remained elusive. This chapter describes a method that combines metabolic incorporation of a cyclooctyne-functionalized sialic acid derivative with a fluorogenic tetrazine ligation reaction, allowing us to image sialylated glycoconjugates within in live zebrafish embryos.",ucb,,https://escholarship.org/uc/item/16x9d1ss,,,eng,REGULAR,0,0
497,1933,Interactions and Assemblies of Polymeric Materials and Colloidal Nanocrystals,"Williams, Teresa Elaine","Helms, Brett A;Xu, Ting;",2017,"Our need to reduce global energy use is well known and without question, not just froman economic standpoint but also to decrease human impact on climate change. Emergingadvances in this area result from the ability to tailor-make materials and energy-savingdevices using solutionâ€“phase chemistry and deposition techniques. Colloidallysynthesized nanocrystals, with their tunable size, shape, and composition, and unusualoptical and electronic properties, are leading candidates in these efforts. Because ofrecent advances in colloidal chemistries, the inventory of monodisperse nanocrystals hasexpanded to now include metals, semiconductors, magnetic materials, and dielectricmaterials. For a variety of applications, an active layer composed of a thin film ofrandomly close-packed nanocrystals is not ideal for optimized device performance; here,the ability to arrange these nano building units into mesoporous (2 nm < d < 50 nm)architectures is highly desirable. Given this, the goal of the work in this dissertation is todetermine and understand the design rules that govern the interactions between ligand-strippednanocrystals and polymeric materials, leading to their hierarchical assembly intocolloidal nanocrystal frameworks. I also include the development of quantitative, andnovel, characterization techniques, and the application of such frameworks in energyefficiency devices such as electrochromic windows.Understanding the local environment of nanocrystal surfaces and their interactionwith surrounding media is vital to their controlled assembly into higher-order structures.Though work has continued in this field for over a decade, researchers have yet toprovide a simple and straightforward procedure to scale across nanoscale materialsystems and applications allowing for synthetic and structural tunability and quantitativecharacterization. In this dissertation, I have synthesized a new class of amphiphilic blockcopolymer architecture-directing agents based upon poly(dimethylacrylamide)-b-poly(styrene) (PDMA-b-PS), which are strategically designed to enhance the interactionbetween the hydrophilic PDMA block and ligand-stripped nanocrystals. As a result,stable assemblies are produced which, following solution deposition and removal of theblock copolymer template, renders a mesoporous framework. Leveraging the use of thissacrificial block copolymer allows for the formation of highly tunable structures, wherecontrol over multiple length scales (e.g., pore size, film thickness) is achieved through thejudicious selection of the two building blocks. I also combine X-ray scattering, electronimaging, and image analysis as novel quantitative analysis techniques for the physicalcharacterization of the frameworks.Last, I demonstrate the applicability of these porous frameworks as platforms forchemical transformation and energy efficiency devices. Examining the active layer in anelectrochromic window, I show a direct comparison between, and improved performancefor, devices built from both randomly close-packed nanocrystals and those arranged inmesoporous framework architectures. I show that the framework also serves as a scaffoldfor in-filling with a second active material, rendering a dualâ€“mode electrochromic device.These results imply that there may exist a broad application space for these techniques inthe development of ordered composite architectures.",ucb,,https://escholarship.org/uc/item/1910x9pm,,,eng,REGULAR,0,0
498,1934,The Impact of Adverse Weather on Freeway Bottleneck Performance,"Seeherman, Joshua Lawrence","Skabardonis, Alexander;",2014,"Congestion on freeways occurs when demand exceeds the available capacity.  Common causes of recurring congestion, also known as freeway bottlenecks, include lane drops, on-ramp merges, and weaving sections. Adverse weather reduces traffic speeds and the maximum queue discharge flow at freeway bottlenecks. However, the impact of weather characteristics on bottleneck discharge flows has not been systematically investigated. This research investigated the relationship between bottleneck queue discharge flow and weather characteristics including rainfall intensity, wind speed, and visibility.Queue discharge rates at four isolated merge bottlenecks within Orange County, California were measured utilizing an established methodology of cumulative count and occupancy curves. An analysis of how queue discharge varied by rainfall intensity revealed reduced discharge ranging from 5% in drizzle (rainfall <0.02 inches/hour) up to 27% in heavy rainfall (rainfall >0.1 inches/hour). However, variation in this single weather characteristic only accounted for a small percentage of the variability in discharge flow, particularly in light rain. Several hypotheses were proposed and tested utilizing the two additional variables of wind speed and visibility and dividing the periods of discharge flow into three groupings. Analyses based on these hypotheses better described the variation in queue discharge flow than the analysis with rainfall intensity alone. A model was developed to predict bottleneck discharge flow by combining data points from all sites. This model predicted that an increase in rainfall intensity of 0.1 inches per hour reduced queue discharge by approximately 1.8% at all sites after the onset of congestion.This research shows that weather characteristics are an important predictor of bottleneck queue discharge rates.  Forecasted weather patterns could be used to predict reductions in bottleneck capacity. Complementary research building on this work by examining changes in trip start time during adverse weather would allow an improved prediction of vehicle delay and travel time reliability. This information would allow traveler information services to incorporate weather characteristics in order to provide more accurate predicted route times for commuters.",ucb,,https://escholarship.org/uc/item/19r4x797,,,eng,REGULAR,0,0
499,1935,Global well-posedness and parametrices for critical Maxwell-Dirac and massive Maxwell-Klein-Gordon equations with small Sobolev data,"Gavrus, Cristian Dan","Tataru, Daniel;",2017,"In this thesis we prove global well-posedness and modified scattering for the massive Maxwell-Klein-Gordon (MKG) and for the massless Maxwell-Dirac (MD) equations, in the Coulomb gauge on $\bbR^{1+d}$ $(d \geq 4)$ for data with small critical Sobolev norm. For MKG, this work extends to the general case $ m^2 > 0 $ the results of Krieger-Sterbenz-Tataru ($d=4,5 $) and Rodnianski-Tao ($ d \geq 6 $), who considered the case $ m=0$. We proceed by generalizing the global parametrix construction for the covariant wave operator and the functional framework from the massless case to the Klein-Gordon setting.  The equation exhibits a trilinear cancelation structure identified by Machedon-Sterbenz. To treat it one needs sharp $ L^2 $ null form bounds, which we prove by estimating renormalized solutions in null frames spaces similar to the ones considered by Bejenaru-Herr.   To overcome logarithmic divergences we rely on an embedding property of $ \Box^{-1} $ in conjunction with endpoint Strichartz estimates in Lorentz spaces. For MD, the main components of the proof consist of A) uncovering of the null structure of Maxwell-Dirac in the Coulomb gauge, and B) proving solvability of the underlying covariant Dirac equation. A key step for achieving both is to exploit and justify a deep analogy between MD and MKG, which says that the most difficult part of MD takes essentially the same form as parts of the Maxwell-Klein-Gordon structure. As a result, the aforementioned functional framework and parametrix construction become applicable.",ucb,,https://escholarship.org/uc/item/1c46218z,,,eng,REGULAR,0,0
500,1936,"Political Subjectivity in Contemporary Arab Thought: The Political Theory of Abdullah Laroui, Hassan Hanafi, and Mohamed Abed al-Jabiri","Daifallah, Yasmeen","Brown, Wendy;",2012,"This dissertation is an examination of the work of three twentieth century Arab thinkers and the significance of their thought to questions of political subjectivity and consciousness in political theory. The project analyzes the oeuvres of the Moroccan historian Abdullah Laroui, the Egyptian philosopher Hassan Hanafi, and the Moroccan philosopher Mohamed Abed al-Jabiri for the purpose of understanding how contemporary critiques of Arab-Islamic cultural heritage and ideology constitute a political theoretical tradition aimed at reforming the Arab political subject. Each thinker locates the consciousness of ""the Arab self"" at the heart of the troubled ""Arab condition;"" each conceives social and political progress as dependent upon the transformation of that consciousness. Thus, I argue that much of what usually passes as ""cultural critique"" in contemporary Arab thought should rather be considered as a critical examination of the formation of the Arab self carried out in the registers of cultural history, revisionist theology and ideology critique.By examining three quite different intellectual figures, I am able to show that the trend to identify the Arab self as the locus of Arab political problems, and to critique that self through an examination of the Arab-Islamic cultural tradition, is not limited to any single ideological current, but is practiced across contemporary Arab political thought. What varies among these thinkers is how they diagnose, characterize and attempt to redress this tradition. Whereas Laroui's critique culminates in a call for rupture with the tradition, Hanafi attempts its reconstruction and Jabiri offers a deconstruction aimed at sifting out and making use of its potentially progressive elements. Common to these various mobilizations of historical tradition is a modernist conception of history as necessarily progressive and as driven by a subject capable of shaping the future. Thus I argue that these thinkers and contemporary Arab thought more generally, inhabit an understanding that is counter-colonial but not yet postcolonial, one that is aware of the historicity of the Arab self and the profound influence of colonialism on its formation, yet is absent any critique of universalist and other conceits of Western modernity and democracy.",ucb,,https://escholarship.org/uc/item/1cc0g870,,,eng,REGULAR,0,0
501,1937,"The SHU:SH Project Slurs Hurt Us: Safety and Health - Lesbian, Gay, Bisexual, and Transgender Students at School","Soles, Brooke Lynn","Mintrop, Heinrich;",2013,"Teachers can be one of the most powerful factors in creating a safer school culture through intervening when they hear lesbian, gay, bisexual, and transgender (LGBT) slurs (Bockenek and Brown, 2001; Kosciw et al., 2009). Teachers are the primary adult contact students have throughout their school day, and many teachers hear gay slurs in the classroom and do not intervene. My design study focuses on creating a school culture where teachers intervene when they hear students using LGBT slurs in the classroom or on campus. This study does not focus on attempting to shift the entire school culture within the duration of the design study process but rather begin to acknowledge the critical LGBT issues on campus by addressing gay slurs. This design study is the beginning of a larger school culture change process.	The SHU:SH Project Slurs Hurt Us: Safety and Health - Lesbian, Gay, Bisexual, and Transgender Students at School, a mixed-method study combining qualitative and quantitative methods, begins with identifying the problem of practice: students and teachers hear LGBT slurs on a daily basis in the classroom and hallways. What is problematic about this behavior is that teachers ignore these slurs, tolerate them, and do not intervene when they hear slurs. For this study, I developed a theory of action to guide the design. Drawing from the literature, I identified five key design elements in creating a professional development process by which a school culture is created to enable teachers and staff to intervene when they hear LGBT slurs on campus: create cognitive dissonance and awareness, develop a safe space for conversation and reduce fear and defensiveness while creating responsibility and personalization, acknowledge depth of problem and deepen insight, engage in inquiry cycle while creating action space, and efficacy. Overall, I found the unpredictability of difficult, volatile, and complex human interactions around social status requires enormously capable leaders (Theoharis, 2007). The local context of silence pervasive in this social justice high school embodied the complexity of addressing slurs. My hope is the next design iteration will focus on self-critical inquiry for social justice leaders, examine the local context of silence, and analyze the effective implementation of theory to practice within social justice initiatives.",ucb,,https://escholarship.org/uc/item/1dh2w98z,,,eng,REGULAR,0,0
502,1938,"Connected Worlds: Communication Networks in the Colonial Southeast, 1513-1740","Dubcovsky, Alejandra","Peterson, Mark;",2011," <&ldquo>""Connected Worlds: Communication Networks in the Colonial Southeast, 1513-1740"" <&rdquo> is a study of the struggle to acquire and control information in a pre-postal, pre-printing press colonial world.  This dissertation focuses on the period between 1513 and 1740 in the American Southeast. It argues that the acquisition and transmission of news was crucial to the creation, development and growth of colonial spaces. Secondly, this study examines the different groups and individuals who traversed and traded in the region, the routes that Spanish, English, French, Indian and African individuals followed and constructed, and the changing interpretations and values assigned to news. The dissertation addresses a simple, yet often overlooked concern with how people in the colonial world came to know what they knew. The principal questions therefore explore both the practical as well as conceptual aspects of information. How was news acquired and transmitted in the colonial Southeast? What do these networks of communication reveal about the relations within and between the different groups that inhabited this geopolitical region? To answer these questions, the dissertation draws upon a wide range of sources, such as official dispatches, newspaper articles, personal reports, and other governmental records from Spanish, British, and North American archives. Part I of the dissertation analyzes early definitions and understandings of news in the exploration and settlement of Florida. Part II turns to the practical aspects of information spread, providing an examination of Spanish networks of communication. Part III shifts the focus to the English, detailing how South Carolina used information networks to establish and define its authority in the region. And Part IV examines how changes to the economy, demography, and political structure of the Southeast in the 1730s altered the value and emphasis placed on news.",ucb,,https://escholarship.org/uc/item/1fp4h1gn,,,eng,REGULAR,0,0
503,1939,Imperial Institutions: The Soft Power of German Economic and Cultural Diplomacy in Southeastern Europe 1920-1940,"Gross, Stephen Gerard","Connelly, John;",2010,"The two decades following World War I witnessed the collapse of the international trade, capital flows, and migration that had united much of world in the late 19th century. Germany lay at the center of this global economic crisis, which in many ways led to National Socialism and the Second World War. As Adam Tooze has illustrated, rather than meekly accepting its place in a global order dominated by Great Britain and America, the â€œoriginalityâ€ of Nazi Germany was to mount an epic challenge through the conquest of territory in Eastern Europe and Eurasia.Formal empire in the east, however, was only one solution to the de-globalizing world that German elites pursued during the 1920s and 1930s. My dissertation shows how a diverse group of German businessmen and academics used the economic crisis to shift their nationâ€™s commercial ties away from America and the West and toward Central and Southeastern Europe. They created a continental economic bloc dominated by Germany, one that in many ways had more in common with the liberal imperialism of Great Britain and France than with the highly racist agenda of National Socialism.My research helps us re-conceptualize Germanyâ€™s place in Europe in two ways. First, it demonstrates how German businessmen used soft power to make their nationâ€™s hard, economic preponderance legitimate to the commercial elites of Southeastern Europe. Scholars conventionally use this term to describe the foreign policy of liberal states like America, but I show how authoritarian regimes like Nazi Germany have also deployed soft power. German area studies institutes, trade fairs, and business associations operated through a network of agents in Southeastern Europe to cultivate personal contacts with local elites, train local merchants, lobby local governments, advertise for German products, and ease the flow of information between the commercial centers of Southeastern Europe and Germany. By centering my study of German imperialism on private institutions instead of the state, I argue that imperialism rests as much on webs of co-opted sociability as raw military or political power.Second, I show how a German-led European economic bloc remained a policy pursued by German leaders until late into the 1930s. Historians usually frame Nazi Germanyâ€™s foreign policy as a tense combination of Pan-German Nationalism and the drive for Lebensraum in Eastern Europe. Yet in the 1920s German business elites designed a third pathâ€”Grossraumwirtschaft, or large area economyâ€”that would bring stability to their industries during this period of crisis. This strategy represents a clear line of continuity between the Weimar Republic and Third Reich, since many business elites in both periods believed a continental bloc offered a better long-term strategy for Germany than either free trade or autarchy and war. And in contrast to Eastern Europeâ€”the heart of Nazi Germany's radical plans for re-population and genocideâ€”these businessmen planned to develop the economies of Southeastern Europe by fashioning them into a complementary economic space that would serve German industry.I conclude my dissertation by recounting how this alternative imperial vision succumbed to the allure of the Naziâ€™s more radical re-ordering of Europe after 1938. By then Germany's private organizations progressively lost their freedom to maneuver, and tacked with the wind by adopting certain aspects of National Socialist ideology. They helped remove Jewish merchants from German-Balkan commerce, and they eventually used their expertise to use Southeastern Europe for the Nazi war machine.",ucb,,https://escholarship.org/uc/item/0q17z3t9,,,eng,REGULAR,0,0
504,1940,Imagining Turkish Literature: Between the French Republic of Letters and the Ottoman Empire,"Haddad, Jonathan","Tlatli, Soraya;",2016,"This study traces the emergence of the category â€œTurkish literatureâ€ within the French-speaking scholarly community in eighteenth-century Europe. By uncovering forgotten debates in the eighteenth century among French scholars, courtiers, and diplomats about the existence of Turkish literature, I show how the articulation of the notion of literature drew boundaries between France and the Islamicate world. These debates offer insight into how competing definitions of â€œTurkâ€ and â€œliteratureâ€ conditioned whether the French Republic of Letters integrated or excluded Ottoman â€œmen of letters.â€ My analysis of French definitions of Turkish literature highlights two core themes: politeness as literature and the borderlines between French and Ottoman. In the chapter â€œWorthy of Crossing the Sea,â€ I show the fluidity of both categories in the words of Jean de Laroque. A journalist writing for the widely read Mercure de France, Laroque used his native Marseille as a template for his beliefs about the Muslim Ottomans, leading him to define literature as an active commerce among persons. In a series of letters published in the periodical Mercure de France between 1732 and 1738, Laroque emphasizes the role of the court in policing the society of men of letters. The following chapter, â€œPeople before Printâ€ builds on this court-centered and interpersonal definition of Turkish literature, arguing that the reactions of the Parisian academic milieu to the establishment in 1727 of the first Arabic movable-type press at the Ottoman court contrasts with ongoing cultural exchanges between French and Ottoman diplomats. Rather than representing a threshold, I reveal, print was actually ancillary to the activities of the Republic of Letters. Rather, French men of letters perceived Turkish literature as the product of Ottoman elite formation and the circulation of manuscripts. These manuscripts, in turn, provided the source material for a number of translations of Oriental tales. The chapter â€œThe Snake in the Libraryâ€ examines the collections published by French Orientalists PÃ©tis de La Croix, Caylus, and Cardonne from 1707 to 1770. Close readings of these three authorsâ€™ adaptations bring to light the representation of a â€œTurkish style.â€ Over the course of the century, this style comes to replace references to Ottoman poetics with a generic and self-referential Orientalist literary corpus. Together, the analyses conducted in these three chapters demonstrates the importance of elite and court-centered practices to the integration of Islamicate culture within the Republic of Letters. Ultimately, the findings of this dissertation contribute to two fields of study that have witnessed a resurgence of interest in recent years. First, by exhuming long buried debates about Turkish literature, I provide a more comprehensive account of the movement of Muslims and the circulation of Islamicate culture in Europe in the eighteenth century. In addition, I add my work to an emerging critique of center-periphery models of â€œworld literatureâ€ by retracing the historical processes by which Orientalism comes to absorb Turkish literature into the French Republic of Letters.",ucb,,https://escholarship.org/uc/item/0rf8768m,,,eng,REGULAR,0,0
505,1941,Estimating Causal Effects of Occupational Exposures,"Izano, Monika A","Eisen, Ellen A;",2017,"Estimates of the risk of occupational exposures are typically based on observational workplace studies that are subject to bias due to the healthy worker survivor effect (HWSE), a ubiquitous process  that results in the healthiest workers accruing the most exposure.  This body of work is concerned with the estimation of causal effects of occupational exposures from observational workplace studies, in the context of the HWSE.  We estimate the effect of cumulative exposure to straight, soluble, and synthetic metalworking fluids (MWFs) on the incidence of colon cancer in the United Autoworkers-General Motors (UAW-GM) cohort. We use longitudinal targeted minimum loss-based estimation (TMLE) to compute the 25-year risk difference if always exposed above compared to if always exposed below an exposure cutoff while at work.  Exposure cutoffs were selected a priori at the median of exposed person-years among colon cancer cases. Risk differences are 0.038  (95% CI = 0.022 to 0.054), 0.002 (95% CI = -0.016 to 0.019), and 0.008 (95% CI = 0.002 to 0.014) for straight, soluble, and synthetic MWFs, respectively. By control of the time-varying confounding on the casual pathway that characterizes the HWSE, TMLE estimated effects that were undetectable in earlier reports.   Most workers in UAW-GM were hired decades before the reporting of incident cancers began.  Incident cancers that occurred before the start of reporting were left filtered.  We show that if ignored, left filtering can lead to downward bias in exposure effect estimates. Further, we propose a novel delayed-entry adjusted Kaplan-Meier estimator that controls for time-varying confounding, and permits delayed risk-set entry. The estimator results in little bias in simulated datasets when the outcome is sufficiently rare.   In addition to dynamic (realistic) interventions that assign exposure according to workers' employment status, causal contrasts can be defined under static (etiologic) interventions that additionally prevent leaving work. Causal effect estimates of the two classes of interventions can differ substantially. While ideally the choice of intervention would be driven by the research question, in practice it may be dictated by the available data.  Furthermore, when estimates of the long-term etiologic effects of occupational exposures are not available, guidelines for exposure limits may be based on studies that estimated effects of realistic interventions.  In a simulation study we investigate the conditions under which the two effect measures are comparable, and identify factors that drive the differences between the two.",ucb,,https://escholarship.org/uc/item/0s34s16p,,,eng,REGULAR,0,0
506,1942,Modified scattering for small data solutions to the cubic SchrÃ¶dinger equation on product space,"Liu, Grace","Tataru, Daniel Ioan;",2018,"In this paper we consider the long time behavior of solutions to the cubic nonlinear Schr\""{o}dinger equation posed on the spatial domain $\mathbb{R}\times\mathbb{T}^{d}$, $1\leq d\leq4$. We first prove the local well-posedness in $C(I;L_x^2H_y^s)\cap C(I;L_{x,y}^4)$ for solutions with initial data $u_0\in H^{0,1}_xL_y^2\cap L_x^2H_y^s$. Then, for sufficiently small, smooth, decaying data, we prove global existence and derive modified asymptotic dynamics by using the wave packet method and normal form corrections. The modified scattering behavior on $\mathbb{R}\times\mathbb{T}^d$ combines the modified scattering of the cubic NLS on real line $\mathbb{R}$ with cubic NLS dynamics on torus.  We also consider the corresponding asymptotic completeness problem.",ucb,,https://escholarship.org/uc/item/0r72m877,,,eng,REGULAR,0,0
507,1943,Development and Applications of (Hetero)cycloisomerization Methodologies to Access Natural Product Scaffolds,"Wilkerson-Hill, SIdney Malik","Sarpong, Richmond;",2015,"The development of new heterocycloisomerization reactions as a tactic to access natural product scaffolds is an active area of research. Chapter 1 describes the development of a new heterocycloisomerization reaction of alkynyl-[4.1.0]-bicycloheptanones using W(CO)5â€¢THF complex to access 4,5-dihydrobenzo[b]-furans and â€“indoles. Specifically, the methodology developed provides a unique entry into dihydro-benzofurans and â€“indoles that contain carbon substitution at the C4-position, which is a common motif in many biologically active indole alkaloid natural products (e.g. the ergot alkaloids). The unique reactivity of dihydro-benzofurans and â€“indoles as it pertains to accessing natural product scaffolds is also described.Chapter 2 describes a mechanistic investigation of the trace-metal catalyzed cycloisomerization of alkynyl-[4.1.0]-bicycloheptanones to access annulated aminopyrroles by heating the ketone substrates with p-toluenesulfonylhydrazide in methanol. From our mechanistic studies, we demonstrate that the cycloisomerization reaction, which was previously thought to have been metal free, is actually catalyzed by trace copper salts at parts-per-million loading. Furthermore, we demonstrate the presence of E- and Z-hydrazone intermediates and conclusively demonstrate that, the more sterically encumbered Z-hydrazone is initially formed in the reaction and is thermodynamically lower in energy than its corresponding E- isomer. These studies were carried out in collaboration with the Hein group at the University of California, Merced and the Tantillo Group at the University of California, Davis and are a testament to the importance and power of collaborative research.Chapter 3 describes our efforts to leverage a Pt(II)-catalyzed carbocycloisomerization reaction as a means for accessing functionalized tetrahydrofluorenes through the use of 2-substituted indene compounds. We were able to synthesize a variety of functionalized tetrahydrofluorenes using a Dielsâ€”Alder cycloaddition reaction of 2-vinylindenes and various dienophiles. We also describe our attempts to effect a double Dielsâ€”Alder cycloaddition reaction using bisketenes or bisketene equivalents with 2-vinylindenes to access the dimeric lomaiviticin natural products. Though we were unable to realize the desired double Dielsâ€”Alder cycloaddition reactivity, we discovered a new method for generating 3-oxidopyrylium ions from bis(1-cyanovinyl acetate). Furthermore, we were able to access a variety of 2-alkynyl indenes and utilize these substrates to access the carbocyclic core of the diterpenoid euphorbactin using Rh(II)-catalyzed cycloaddition chemistry.",ucb,,https://escholarship.org/uc/item/0sq7h2qd,,,eng,REGULAR,0,0
508,1944,Negatively Stereotyping Historically Black Colleges and Universities as an Intergroup Process,"Boykin, Curtis Malik Starks","Mendoza-Denton, Rodolfo;",2018,"Educating enslaved Africans was illegal in antebellum America. However, in the mid-1800s philanthropists and the U.S. Congress established higher education institutions for newly freed people. From then until present, inequitable economic policy and the lower status of Historically Black Colleges and Universities (HBCUs), vs. Historically White Institutions (HWIs), have reified a hierarchical ordering of Black and White schools â€” consistent with Social Dominance Theory (Sidanius & Pratto, 1999). In four studies (n=1059), endorsement of negative HBCU stereotypes via the HBCU Stereotype Scale (HBCU-SS) captured individuals' mental representations of this hierarchical ordering in ways that illuminate connections between racialized educational spaces and intergroup attitudes more broadly. Studies 1 & 2 demonstrate a unifactor structure across all four samples and establish construct validity for both Black and White participants. For all participants, the HBCU-SS was predicted by preference for group inequality â€” via the Social Dominance Orientation scale. For Blacks, the HBCU-SS was uniquely related to measures of racial identity. For White individuals the HBCU-SS was related to political attitudes, measures of generalized prejudice, as well as motivations to control prejudice. Study three shows that when White participants read about governmental in HBCUs (vs. HWIs), they were more likely to negatively stereotype HBCUs, suggesting hierarchy-maintaining motivations may drive these perceptions. Lastly, study four shows that when White participants are primed with threat to their majority status in society, they increase their stereotyping of HBCUs and increase their endorsement of inequitable resource allocations toward HBCUs.",ucb,,https://escholarship.org/uc/item/0t54h8qp,,,eng,REGULAR,0,0
509,1945,Investigations into nanometer scale surface opto-electro-mechanical coupling,"Levy, Niv Binyamin","Crommie, Michael F;",2010,"We have used scanning tunneling microscopy and spectroscopy to investigate electro-mechanical coupling in two different nanoscale systems coupled to a condensed matter environment - single molecules, where we observe light induced conformational changes (photoisomerization), and graphene, where we observe conformationally controlled pseudo gauge fields.     We have observed the effects of molecule-surface and molecule-molecule coupling in photoisomerization of TTB-Azobenzene molecules on the Au(111) surface. In addition, through measurement of the photoisomerization cross section of surface bound molecules and comparing the chirality of the initial and final products of the reaction, we have gained knowledge of the likely pathways. Our studies of graphene films catalytically grown on the Pt(111) surface have found strain-induced pseudo Landau levels, which are a unique consequence of how graphene's electronic structure interacts with the local environment. These studies have allowed us to gain insight into the quantitative and qualitative ways in which the environment affects electro-mechanical coupling of nanoscale structures.      Single molecule photoisomerization and strain-induced effects serve as complementary examples of electro-mechanical coupling, since in the former case the position of the molecule's constituent atoms is affected via electric fields, while in the latter the position of the constituent atoms modifies the electrical properties of the film.    The combination of these two effects, e.g. by depositing photoswitching molecules on a gateable and strainable graphene membrane, may open the door to new applications and enable better control of matter at the nanoscale.",ucb,,https://escholarship.org/uc/item/0td78950,,,eng,REGULAR,0,0
510,1946,"Tourism Development from Its Beginnings to Current Environmental Impacts and Contemporary Governance: Application to the Southern Red Sea, Egypt","Gohar, Amir","Kondolf, Mathias;",2017,"Through the ages, traveling through the world through tourism has familiarized the foreign. Unknown frontiers become urbanized; travel pathways coalesce around human evolution in spaces; and governance structures harness the power that such exploratory opportunities present. This research navigates the spatial dimension of travel evolution alongside the attendant expansion of urbanization. It defines the nexus between tourism as a global demand and the physical infrastructure that accommodated such a force. The built environment, manifested in both its urban forms and its systems of mobility, has shaped and been shaped by many factors, including tourism. This work explores the historical co-evolution of urbanization and tourism. Egypt, considered one of the world's oldest tourist destinations, is used here to demonstrate the interlocking relationship of tourism and urbanization; it is difficult to separate these two phenomena because the evolution of tourism through time is not only attributable to demand, but also to the shape and form of the destination and the transport systems available in each era and locale. This research focuses on the contemporary Egyptian era because it is dynamic and replete with diverse forms of tourism. Both professional and academic literature has widely discussed the concept of ecotourism as an important, and growing, subset of the tourism industry. Nevertheless, no accurate definition of ecotourism has been agreed upon. This research compares ecotourism to conventional or mass tourism along the Red Sea coast of Egypt. It systematically examines the tourist establishments in the study area based on identifiable environmental parameters, including swimming pool surface area, distance from mangrove patches, conflict with flood plains, extent of lawn area, and means of access to deep water. The investigation finds that ecotourism establishments are not significantly different from typical tourism resorts and that they create comparable stress on ecological resources. The research concludes that ecotourism is a self-proclaimed designation in this rapidly developing international tourism zone. The study recommends that future ecotourism operations be modified in two key ways. First, on the planning level, the regional master plan created by the central government tourism authorities must be modified to recognize the unique environmental characteristics of specific sites, and these plans must guide development with specific requirements designed to protect the regionâ€™s unique environmental resources. Second, on the site design level, significant improvements to design approval processes must be introduced in the build-out process to ensure compliance with environmental requirements and minimize stress on local environmental resources. After examining the Egyptian case study, the research explores the governing rubric for tourism development and land use in that area.Despite being one of the most important revenue sources for Egypt, tourism development remains a byproduct of a very complex governing system. Although current tourism development causes much environmental degradation along the Red Sea coast (scholarly work has delineated its footprint), little has been written on the governance of this tourism development and its implications for the enduring environmental footprint of tourism along the Red Sea. This piece defines the various institutions responsible for tourism development and explores the relationship between institutions and development modes on their specific land jurisdictions. It concludes that tourism development will likely continue to create more adverse impacts if the governing agencies responsible for shaping its development do not overhaul their operating paradigms to take into account the attendant holistic and discrete ramifications of their appropriation choices.",ucb,,https://escholarship.org/uc/item/0tq1c31r,,,eng,REGULAR,0,0
511,1947,Planning Under Uncertainty for Human-Compatible Robots,"Liu, Chang","Borrelli, Francesco;",2017,"Recent progress in robotic systems has significantly advanced robot functional capabilities, including perception, planning, and control. As robots are gaining wider applications in our society, they have started entering our workplace and interacting with us. This leads to new challenges for robots: they are expected to not only be more functionally capable automatic machines, but also become human-compatible, which requires robots to make themselves competent agents to work for people and collaborative partners to work with people on diverse tasks. The capability to planning under uncertainty lies at the core to achieving this goal. The aim of this dissertation is to develop new approaches that improve the autonomy and intelligence of robots to enable them to reliably work for and with people. Especially, this dissertation investigates uncertainty reduction and the planning under various types of uncertainty with the focus on three related topics, including distributed filtering, informative path planning, and planning for human-robot interaction.In the first topic, the dissertation studies uncertainty reduction via distributed filtering using networked robots. We consider the distributed version of the generic Bayesian filter. Two new methods of measurement exchange among networked robots are proposed, which enable the dissemination of robotsâ€™ sensor measurements in time-invariant and time-variant communication networks. By using such methods, the communication burden of the robot network can be significantly reduced compared to traditionally used methods. Based on these measurement exchange methods, we develop two distributed Bayesian filters for time-invariant and time-variant networks. It has been proved that the proposed distributed Bayesian filter can achieve consistent estimation. The application in target localization and tracking is presented.In the second part, the dissertation focuses on planning under the uncertainty of target position and motion model. This part investigates the path planning of a mobile robot to autonomously search and localize/track a static/moving target. We first study the case of linear Gaussian sensing and mobility models. A path planning approach based on model predictive control (MPC) is proposed, which uses a modified Kalman filter for uncertainty prediction and a sequential planning strategy for path generation. We then investigate the path planning in a dynamic environment, with the sensor using a binary model. A closed-form objective function for the MPC-based path planner is proposed, which significantly reduces the computational complexity. The safety of robot is enforced by using a barrier function in the objective function of MPC.The first two topics concentrate on making robots autonomously work for people. In the third topic, the dissertation addresses the demands to make robots work with people and achieve coordination. We first consider the planning of robots under the uncertainty of humansâ€™ trajectory in a human-following application, where the robot needs to generate a path to follow a person in a safe and comfortable way. We propose a model-based human motion prediction approach using the principle of interacting multiple model estimation. A path planner based on nonlinear MPC is then developed for the robot to generate human-following paths, which takes into account the safety and comfort of the accompanied person.We then investigate the planning of robots under the uncertainty of humansâ€™ internal states, including their intention and belief. Especially, the task planning in the human-robot collaboration is considered. We develop an adaptive task planning scheme that allows a robot to use motion-level inference to understand a human partnerâ€™s plan and then adjust its task-level plan to coordinate with the person. In addition, we model a personâ€™s inference process and develop a task planning approach for a robot to generate human-predictable plans, which aims to reduce the misalignment between peopleâ€™s belief and robotsâ€™ plan.",ucb,,https://escholarship.org/uc/item/0wm075n9,,,eng,REGULAR,0,0
512,1948,Essays on Information and Beliefs in Credit Markets,"Botsch, Matthew","Malmendier, Ulrike;",2014,"This dissertation is a collection of three essays in financial economics, specifically focused on the role of information and beliefs in credit markets. The first chapter establishes that private bank information about customers in primary lending markets exists. The second chapter shows that private  information hinders banks' capacities to sell loans on secondary markets, unless the purchaser believes that the bank has committed to remain uninformed. The third chapter explores the welfare consequences of incorrect borrower beliefs about the economic environment on financial product choice.In the first chapter, my co-author and I hypothesize that while lending to a firm, a bank receives signals that allow it to learn and better understand the firm's fundamentals; and that this learning is private; that is, it is information that is not fully reflected in publicly-observable variables. We test this hypothesis using data from the syndicated loan market between 1987 and 2003. We construct a variable that proxies for firm quality and is unobservable by the bank, so it cannot be priced when the firm enters our sample. We show that the loading on this factor in the pricing equation increases with relationship time, hinting that banks are able to learn about firm quality when they are in an established relationship with the firm.In the second chapter, I present new evidence that lemon problems hinder trade on secondary mortgage markets. Using the geographic distance from lenders to borrowers as a proxy for the absence of private bank information, I document a systematic positive link between distance and the mortgage sale rate. Mortgage sale rates are higher when the originating lender is less likely to be informed about the borrower. I further show that the private mortgage sale rate locally depends on lender-borrower distance only above the conforming loan limit, in the illiquid jumbo market where the GSEs are barred from purchasing mortgages. This is consistent with the familiar tradeoff between market liquidity and seller incentives to acquire information.In the third chapter, I investigate how borrowers' incorrect beliefs about future inflation might bias their choice between fixed-rate and adjustable-rate mortgages. Borrowers who have experienced recent periods of greater inflation pay more for fixed-rate mortgage contracts and pay more in interest, at least over the first six yeras of the mortgage's life. That is, incorrect beliefs about future inflation are welfare-reducing both ex ante and ex post.",ucb,,https://escholarship.org/uc/item/0zk1t9m3,,,eng,REGULAR,0,0
513,1949,Performance Analysis of Nonlinear Systems Combining Integral Quadratic Constraints and Sum-of-Squares Techniques,"Summers, Melissa Erin","Packard, Andrew;Arcak, Murat;",2012,"This thesis investigates performance analysis for nonlinear systems, which consist of both known and unknown dynamics and may only be defined locally. We apply combinations of integral quadratic constraints (IQCs), developed by Megretski and Rantzer, and sum-of-squares (SOS) techniques for the analysis.In this context, analysis of stability and input-output properties is performed in three ways.If the known portion of the dynamics is linear, the stability test from Megretski and Rantzer, which generalize early frequency-domain based theorems of robust control (Zames, Safonov, Doyle, and others), are well suited. If the known portion of the dynamics is nonlinear, frequency domain methods are not directly applicable. SOS methods using polynomial storage functions to satisfy dissipation inequalities are used to certify the stability and performance characteristics.  However, if the known dynamics are high dimensional, then this approach to the analysis is (currently) intractable. An alternate approach is proposed here to address this dimensionality issue.  The known portion is decomposed into a linear interconnection of smaller, nonlinear systems. We derive IQCs satisfied by the nonlinear subsystems. This is computationally feasible. With this library of IQCs coarsely describing the subsystems' behaviors, we apply the techniques from Megretski and Rantzer to the interconnection description involving the known linear part and all of the individual subsystems. Traditionally, IQCs have been used to cover unknown portions of the dynamics.  Our approach is novel in that we cover known nonlinear dynamics with IQCs, by employing SOS methods including novel techniques for estimating the input-output gain of a system. This perspective is a step towards reducing the dimensionality of the analysis of large, interconnected nonlinear systems. The IQC stability analysis by Megretski and Rantzer is only applicable for systems that are well-posed in the large.  This thesis makes contributions towards extending this analysis for with more limited notions of well-posedness. We define the notion of a local or ""conditional"" IQC, and develop a new test to verify stability and performance criteria.We also study a specific class of interconnected, passive subsystems.  If the subsystems also exhibit gain roll-off at high frequencies, one would expect improved analysis results.  In fact, we characterized the gain roll-off property as an integral quadratic constraint, and achieved an improved bound on the performance with respect to the allowable time delay in order for the interconnected system to remain stable. In the case where the interconnection is cyclic, we derive an analytical condition for stability.",ucb,,https://escholarship.org/uc/item/1034719m,,,eng,REGULAR,0,0
514,1950,System Design for Large Scale Machine Learning,"Venkataraman, Shivaram","Franklin, Michael J;Stoica, Ion;",2017,"The last decade has seen two main trends in the large scale computing: on the one hand we have seen the growth of cloud computing where a number of big data applications are deployed on shared cluster of machines. On the other hand there is a deluge of machine learning algorithms used for applications ranging from image classification, machine translation to graph processing, and scientific analysis on large datasets. In light of these trends, a number of challenges arise in terms of how we program, deploy and achieve high performance for large scale machine learning applications.In this dissertation we study the execution properties of machine learning applications and based on these properties we present the design and implementation of systems that can address the above challenges. We first identify how choosing the appropriate hardware can affect the performance of applications and describe Ernest, an efficient performance prediction scheme that uses experiment design to minimize the cost and time taken for building performance models. We then design scheduling mechanisms that can improve performance using two approaches: first by improving data access time by accounting for locality using data-aware scheduling and then by using scalable scheduling techniques that can reduce coordination overheads.",ucb,,https://escholarship.org/uc/item/1140s4st,,,eng,REGULAR,0,0
515,1951,Asian and Latina Migrants in the United States and the Invisible / Visible Paradigm of Human Trafficking,"Fukushima, Annie Isabel","Glenn, Evelyn N;",2012,"Asian and Latina Migrants in the United States and the Invisible / Visible Paradigm of Human Trafficking addresses a critical question: who is seen as trafficked? And who is rendered invisible? How the trafficked person has come to matter in the 21st century is a function of the diversity of discourses that extends beyond the legal definition of human trafficking. In order to make sense of ""who"" is visible as a trafficked person necessitates a method that is interdisciplinary. Narratives produce who is categorized as the trafficked, the trafficker, and the anti-trafficker. Structural and cultural factors solidify categories of human trafficking, further perpetuating what I refer to as an invisible / visible paradigm of human trafficking. I develop an understanding of human trafficking as an invisible / visible paradigm of human trafficking in order to enable a nuanced critique of the erasures that currently exist in narratives of exploitation and labor migration. To address who is visible and invisible when trafficked I examine comparatively Asian and Latinas trafficked into the United States.Asian and Latina Migrants offers an interdisciplinary approach. The methodology is informed by sociological methods of participation and participant observation as a scholar activist (2005 - present). Between 2009 and 2011 I worked with over 70 organizations and advocated for and / or assessed over one hundred human trafficking cases as a caseworker, programs coordinator and technical assistant provider to anti-trafficking organizations. The fieldwork enabled me to examine how victims of human trafficking are constituted. The legal and social imagining of human trafficking manifests in legal systems, in the representation of the policy and legal cases in the media and in campaigns. And human trafficking is continually redefined by discourses of freedom, labor migration, and sexual economies. Therefore, I also employ a cultural studies lens to unpack the discourse of human trafficking. Who this person is inextricably linked to gendered and raced perceptions of illegality and victimhood.Asian and Latina Migrants examines transnational labor that bridges Asia-Pacific to the Americas. Chapter two maps the scholarly discourse about human trafficking as intertwined with discourses about freedom, labor and migration, and sexual economies. Chapter three describes the method of Asian and Latina Migrants as drawing upon sociology, legal analysis, and cultural studies. Chapters four through six offers a qualitative analysis of Asian and Latinas trafficked through homo-social relations (women trafficking women). In particular, I study Koreans, Filipinas, and a Peruvian trafficked into domestic work, servitude, sexual slavery, and massage parlors. Chapters four through six focus on feminized labor (domestic worker and sexual economies) and exploitation. Chapter seven concludes with situating resistance and human trafficking; in spite of the violence as systemic and naturalized, survivors are always resisting.",ucb,,https://escholarship.org/uc/item/11n5x298,,,eng,REGULAR,0,0
516,1952,Exploring Motivations and Expectations of Churches in Public Health Partnerships,"Allen, Monica Delores","Bloom, Joan R.;",2014,"Partnerships between public health and faith communities have been identified as effective avenues to promote health behaviors, owing to the role of churches in the community. As the focus on resolving public health issues using the sociological model increases, the role of the church in communities, particularly communities of color, grows in importance. With the increasing attention and resources devoted to faith/public health partnerships, it is important to obtain evidence on these collaborations, particularly from the perspective of the churches, and to plan for the next generation of programs. The purpose of this study is to collect information on the involvement of churches in health promotion activities, to look specifically at why some churches choose to collaborate. The church is an acknowledged means to implement health programs in the African-American community, and Black pastors have traditionally been leaders of their congregations and in the community at large. This study looks at faith/public health collaborations from the view of the Black pastor, to gain more insight into the faith community's partnerships with public health organizations. Pastors of Black churches in the San Francisco Bay area were recruited for the study, which consisted of individual semi-structured interviews, conducted using an interview guide. Churches involved in health promotion activities. Analysis identified the range of themes and domains that characterize each interview, and responses from each pastor were compared, identifying similar and dissimilar themes. Observations of churches' worship services were also conducted.Pastors from 20 churches participated in the study. The churches represented seven denominations, ranged in size from 50 members to 5,000, and were located in four Bay Area counties. Analysis indicates that pastors believe the church is important in reducing health disparities in the African-American community, and value public health organizations as resources and partners. Pastors offered insight into the reasons for churches to partner with public health organizations, prohibiting factors to such partnerships, and their views on elements of a successful partnership.Additional findings indicated that pastors feel strongly that the church plays a primary role in their communities, and that this role extends to health. Pastors also communicated two main reasons for entering partnerships to promote heath; a responsibility to work to increase the health of their congregations and communities, and the need for resources to do so. They see obtaining these resources as a satisfactory partnership outcome.Examining collaborations from the perspective of pastors of African-American churches offers an understanding of partnership formation, and elucidates barriers and facilitators. This study can make a contribution to the dialogue about the role of churches in health interventions and help set the foundation for developing a framework within which to think about and develop faith-based health programs in the Black community. Pastors need to be included in discussions on health disparities, what is needed to impact health in their communities and how to expand the role of churches as partners.",ucb,,https://escholarship.org/uc/item/0jw8s9jx,,,eng,REGULAR,0,0
517,1953,Measurement of the B-mode power spectrum with POLARBEAR,"Steinbach, Bryan","Lee, Adrian T;",2014,"We present the \pb experiment and its measurement of the B-mode power spectrum.  \pb is a millimeter wave telescope that is measuring the Cosmic Microwave Background (CMB) polarization, using large format arrays of photon noise limited Transition-Edge Sensor (TES) bolometers.  The instrument observes from the Atacama Desert at 5.2km in elevation with a 2.5m primary aperture telescope.  This telescope has sufficient angular resolution (3.5' FWHM) to resolve the gravitational lensing features of the CMB, a new channel for obtaining information on fundamental physics such as the sum of the mass of neutrinos.This dissertation describes the design, integration and results of the first season of \pb observations.  The receiver observed for 1 year with a noise-equivalent temperature (NET) of $22.8\mu K \sqrt{s}$ and mapped a 30 square degree area of the CMB, obtaining evidence for gravitational lensing in the BB power spectrum at a significance of $2\sigma$.",ucb,,https://escholarship.org/uc/item/13s9s6n6,,,eng,REGULAR,0,0
518,1954,Why Do Educational Administrators and Other Stakeholders Fail to Recognize and Learn From Failure? What Lessons Can We Learn From Theories of Irrational Behavior?,"Mukerjee, Diane Elizabeth-Wirt","Gifford, Bernard R.;",2011,"This study investigates the persistence of the academic achievement gap through national and state testing data.  It considers this gap by discussing studies of compensatory education programs and mandates.  The inconclusive findings regarding identifying effective programs led to an exploration of decision making theories and persistence to maintain ineffective programs. This information guided the purpose of this study: to explore why decision makers continue an ""irrational"" course of action. Through application of the decision making theories of Escalation, Sunk-cost and Prospect, this study explored the rationale of decision makers at two California schools, with ineffective programs, via interviews and scenarios. This study concludes with discussion of the applicability of the decision making theories in education, potential application, and future research.",ucb,,https://escholarship.org/uc/item/15j193r1,,,eng,REGULAR,0,0
519,1955,"PolÃ­tica y estÃ©tica del fraude en el modernismo latinoamericano: escritura, canibalismo y pedrerÃ­a en Azul...de RubÃ©n DarÃ­o","Salvatierra, Leon","del Valle, Ivonne;Rosa, Richard;",2014,"This dissertation draws on colonial texts in order to expand the critical corpus of Azul, beyond its conventional binary readings, either exclusively aesthetic or primarily Marxist, to render a vision of RubÃ©n DarÃ­o that includes an account of Latin America as a space of criticism and transgression that challenges fixed notions of national identity, historical contexts, and authenticity in art. Expanding on definitions of fraud, this project develops the concept of fraude as a theoretical framework to show how DarÃ­o aesthetically constructs a writing which is deceptively apolitical. By fraude, I illustrate the form in which DarÃ­o appropriates and inhabits European identities and their spaces in order to invert the colonial violence historically inflicted in Latin America. Far from l&rsquoart pour l&rsquoart figure that is still so prevalent in the criticism ofAzul, the fraude, once decoded, reveals DarÃ­o as a poet whose writing opposes the empires of Spain, England, and the United States.  By reexamining the colonial discourse-- in relation to sculpture, cannibalism and precious stones--, my research exposes the colonial texts, designed to appropriate and exploit the &ldquoother,&rdquo as an act of fraud in and of itself. Thus, DarÃ­o&rsquos writing, in reversing the colonial fraud, acquires a ""hidden"" political significance. To engage the colonial discourse within a modern nineteenth-century perspective also establishes new ground for reinterpreting the politics behind the works of Modernismo authors, such as JosÃ© MartÃ­ and JosÃ© Enrique RodÃ³ with whom this dissertation is in conversation. From this viewpoint, the literary texts are to be understood as socially symbolic acts that aesthetically question and undermine dominant narratives. Even in the twenty-first century, when the global markets more than ever mediate and displace historical occurrences through the use of technology in visual mass media, blurring the boundaries between falseness and authenticity, an evaluation of fraude as a strategy for analyzing and questioning our social ""reality"" is necessary. This project places Modernismo in conversation with Transnational American and Postcolonial Studies by investigating how people, through shared colonial histories (still relevant in our present time), aesthetically negotiate their legacy.",ucb,,https://escholarship.org/uc/item/16b070t4,,,eng,REGULAR,0,0
520,1956,Engaging Bioanthropology College Students: The Role of Active and Cooperative Pedagogies,"Soluri, Kathaeryne Elizabeth","Agarwal, Sabrina C;",2010,"This dissertation examines the design and implementation of an active, cooperative pedagogy in an undergraduate biological anthropology course.  The research draws upon a theoretical framework constructed from anthropology, education, and psychology research.  The pedagogy studied was developed for and used in the laboratory component of a large, introductory course at the University of California at Berkeley.  As such, the results of this research are relevant to not only other biological anthropology courses but also other large, introductory courses or laboratory-based courses in related disciplines.         The dissertation evaluates the efficacy of the stated pedagogy by answering the following questions: Does the pedagogy effectively engage students of all learning styles in higher-level thinking?  Does the pedagogy effectively promote high quality student learning, particularly learning of high-level course material?  Does the pedagogy effectively foster long-term retention of key course concepts and higher-level thinking across students of various learning styles?  Three lines of evidence are employed to answer these research questions.  To address issues of student engagement, classroom observations of students are qualitatively evaluated.  Student performance on the final exam in the course is statistically analyzed to answer questions related to initial student learning.  In considering issues of student retention from a longitudinal perspective, student performance on a follow-up survey (administered between three months and two years after course completion) is statistically analyzed.  	Research presented here suggests the pedagogy does effectively promote student engagement with course material and foster students' long-term retention of learned course material.  This efficacy is similar for students of different learning styles and generally applies to both low-level and high-level course material.  Preliminary findings also suggest the pedagogy effectively promotes high quality student learning, including learning of high-level material.	It is argued that the stated pedagogy is effective and that this and similar pedagogies could be usefully employed in similar courses in anthropology and biology.  Within higher education, generally, and the discipline of anthropology, specifically, there is growing concern over how to effectively teach undergraduate students.  The research discussed here provides evidence that active, cooperative approaches to undergraduate anthropology instruction are successful in promoting student engagement, learning, and retention.  This research also provides a template for the evaluation of pedagogical approaches that can be applied to future investigations of various pedagogies used in anthropology instruction.",ucb,,https://escholarship.org/uc/item/199573s1,,,eng,REGULAR,0,0
521,1957,Miami Language Reclamation in the Home: A Case Study,"Leonard, Wesley",,2007,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/1c4779gb,,,eng,REGULAR,0,0
522,1958,Family Support: How Does Perceived Emotional and Instrumental Support for Latino Families with Children with Disabilities Relate to Caregiver and Family Well-being?,"Cohen, Shana Raquel","Holloway, Susan D.;",2011,"Research shows that mothers who raise a child with a severe intellectual disability may experience more stress than mothers who care for typically developing children, or other caregivers who care for children with less severe intellectual disabilities (Olsson & Hwang, 2001, 2002; Stores, Stores, Fellows, & Buckley, 1998). These mothers may benefit from support. Latina mothers in particular who care for a child with an intellectual disability experience more stress and depression than non-Latina mothers (Blacher, Shapiro, Lopez, Diaz, & Fusco, 1997). The research examining the specific types and sources of support that are relevant to Latino families with children with disabilities is limited. In this study I examined the types and sources of support that are available to 146 mothers (84 Latina mothers, and 62 non-Latina mothers), with children with disabilities, and how that support relates to family and caregiver well-being. Specifically, I described the types and sources of familial support available to Latina and non-Latina mothers. Then, I examined the relation between perceived familial support and three dimensions of family and caregiver well-being: caregiver satisfaction with life, parenting self efficacy/empowerment, and family quality of life. Finally, I examined how culturally situated beliefs about family obligation moderate the relation between perceived support and family and caregiver well-being. Results indicate that Latina mothers had significantly less partner emotional support than non-Latina mothers. Latina and non-Latina mothers did not differ on other dimensions of support (i.e., instrumental). Also, partner emotional support and some instrumental support significantly predicted caregiver and family well-being. Finally, familism moderated the relationship between familial support and family well-being but not caregiver well-being.  Implications for research and practice are discussed.",ucb,,https://escholarship.org/uc/item/0mr8x3vb,,,eng,REGULAR,0,0
523,1959,Algebraic methods for biochemical reaction network theory,"Shiu, Anne","Pachter, Lior;Sturmfels, Bernd;",2010,"This dissertation develops the algebraic study of chemical reaction networks endowed with mass-action kinetics. These form a class of dynamical systems that have a wide range of applications in the physical and biological sciences. Early results in chemical reaction network theory relied on techniques from linear algebra, dynamical systems, and graph theory. More recently, motivated by problems in systems biology, other areas of mathematics have contributed to this subject. These branches of mathematics include control theory, homotopy theory, and matroid theory. As a complement to these varied perspectives, the approach of this dissertation is algebraic. Chapter 2 develops the basic theory of toric dynamical systems, which are those chemical reaction systems that have the property that for any chemical complex (a product or reactant), the amount produced of that complex at steady state is equal to the amount consumed by reactions. Toric dynamical systems are known as complex-balancing mass-action systems in the mathematical chemistry literature, where many of their properties have been established. Special cases of toric dynamical systems include all zero deficiency systems and all detailed-balancing systems. One feature is that the steady state locus of a toric dynamical system is a toric variety. Furthermore, this variety intersects the interior of each invariant polyhedron (a polyhedron in which a trajectory of the dynamical system is confined) in a unique point. For any chemical reaction network, there is an associated moduli space that consists of those vectors of reaction rate constants for which the resulting dynamical system is a toric dynamical system. The main result states that this moduli space is a toric variety whose combinatorial structure we can characterize. To determine the steady states on the boundaries of invariant polyhedra, the concept of a siphon is important. Siphons in a chemical reaction system are subsets of the chemical species that have the potential of being absent in a steady state. The main result of Chapter 3 characterizes minimal siphons in terms of primary decomposition of binomial ideals. Further, we explore the underlying geometry, and we demonstrate the effective computation of siphons using computer algebra software. This leads to a new method for determining whether given initial concentrations allow for various boundary steady states; this classification arises from a chamber decomposition. Siphons determine which faces of an invariant polyhedron contain steady states, and a relevant question is whether any trajectories of a chemical reaction system approach such a boundary state.  This dissertation develops the algebraic study of chemical reaction networks endowed with mass-action kinetics. These form a class of dynamical systems that have a wide range of applications in the physical and biological sciences. Early results in chemical reaction network theory relied on techniques from linear algebra, dynamical systems, and graph theory. More recently, motivated by problems in systems biology, other areas of mathematics have contributed to this subject. These branches of mathematics include control theory, homotopy theory, and matroid theory. As a complement to these varied perspectives, the approach of this dissertation is algebraic. Chapter 2 develops the basic theory of toric dynamical systems, which are those chemical reaction systems that have the property that for any chemical complex (a product or reactant), the amount produced of that complex at steady state is equal to the amount consumed by reactions. Toric dynamical systems are known as complex-balancing mass-action systems in the mathematical chemistry literature, where many of their properties have been established. Special cases of toric dynamical systems include all zero deficiency systems and all detailed-balancing systems. One feature is that the steady state locus of a toric dynamical system is a toric variety. Furthermore, this variety intersects the interior of each invariant polyhedron (a polyhedron in which a trajectory of the dynamical system is confined) in a unique point. For any chemical reaction network, there is an associated moduli space that consists of those vectors of reaction rate constants for which the resulting dynamical system is a toric dynamical system. The main result states that this moduli space is a toric variety whose combinatorial structure we can characterize. To determine the steady states on the boundaries of invariant polyhedra, the concept of a siphon is important. Siphons in a chemical reaction system are subsets of the chemical species that have the potential of being absent in a steady state. The main result of Chapter 3 characterizes minimal siphons in terms of primary decomposition of binomial ideals. Further, we explore the underlying geometry, and we demonstrate the effective computation of siphons using computer algebra software. This leads to a new method for determining whether given initial concentrations allow for various boundary steady states; this classification arises from a chamber decomposition. Siphons determine which faces of an invariant polyhedron contain steady states, and a relevant question is whether any trajectories of a chemical reaction system approach such a boundary steady state. The global attractor conjecture, which is the subject of Chapter 4, implies that no interior trajectories approach boundary steady states in the case of toric dynamical systems. Our main result states that this conjecture holds when all of the siphons correspond to facets (codimension-one faces), vertices (zero-dimensional faces), or empty faces of the invariant polyhedron. As a corollary, the conjecture holds when the associated invariant polyhedra are two-dimensional. Chapter 5 pertains to the topic of multistationarity, which refers to the capacity of a biochemical reaction system to exhibit multiple steady states in one invariant polyhedron. Known results from chemical reaction network theory provide sufficient conditions for the existence of bistability, and on the other hand can rule out the possibility of multiple steady states. Understanding small networks is important because the existence of multiple steady states in a subnetwork of a biochemical model sometimes can be lifted to establish multistationarity in the larger network. The main result establishes the smallest reversible, mass-preserving network that admits bistability and determines the semi-algebraic set of parameters for which more than one steady state exists. Chapter 6 focuses on mathematical methods for predicting gene expression from regulatory sequence. The chemical reactions that underlie transcriptional regulation include the bindings of transcription factors to cis-regulatory sequences of genes. For each such sequence, many possible combinations of transcription factors can bind to the sequence. Accordingly, thermodynamic models give quantitative predictions of gene expression levels that are weighted averages over the set of all possible binding configurations. This chapter describes the implementation of such a model in the context of early embryonic development in Drosophila melanogaster.",ucb,,https://escholarship.org/uc/item/0n79k458,,,eng,REGULAR,0,0
524,1960,Human Exposure to Urban Vehicle Emissions,"Apte, Joshua Schulz","Nazaroff, William W;",2013,"This dissertation investigates human exposure to vehicular air pollutant emissions in urban areas. Since resources for protecting human health from the adverse consequences of inadvertent environmental releases are constrained, it is often desirable to identify sources and settings in which emissions controls could lead to especially high human health benefits per unit effort. The three measurement and modeling studies that comprise this dissertation aim to contribute towards this goal by advancing a mechanistic understanding of the relationship between urban vehicle emissions and subsequent human exposures. Two key themes that permeate these investigations include the exposure consequences of vehicle emissions in low-income settings such as developing world cities, and the role of dynamic processes in influencing the emissions-to-exposure relationship for urban air pollutant sources. Chapter 1 introduces each of the dissertation chapters and provides context and background related to the broader themes motivating the investigation.In Chapter 2, I report on exposures to particulate matter (PM) in the megacity of New Delhi, India. Previous work has identified New Delhi as a hotspot for ambient PM pollution. To investigate the degree to which in-vehicle exposures can be represented by ambient fixed-site measurements in New Delhi, I undertook a multi-month field campaign in 2010. In-vehicle measurements focused on concentrations of fine particulate matter (PM2.5), black carbon (BC) and ultrafine particles (UFP, measured by particle number count, PN) inside the cabins of auto-rickshaws, a common type of unenclosed vehicle in South Asia. Supplemental measurements considered PM levels inside conventional (enclosed) automobiles. Contemporaneously with the in-vehicle measurements, I conducted routine ambient monitoring of PM2.5, BC and PN at a rooftop fixed site. In-vehicle particle concentrations measured during this field campaign were substantially elevated relative to the levels recorded at the ambient monitoring site. Geometric mean concentrations inside the auto-rickshaw, averaged over ~160 h of 1 Hz data, were 190 Âµg m-3 PM2.5, 42 Âµg m-3 BC, and 280 Ã— 103 particles cm-3. These concentrations rank among the highest levels ever reported for routine transportation microenvironments. Short-duration peak concentrations (averaging time: 10 s), attributable to exhaust plumes of nearby vehicles, were greater than 300 Âµg m-3 for PM2.5, 85 Âµg m-3 for BC, and 650 Ã— 103 particles cm-3 for PN. In-vehicle PM2.5 levels were 1.5Ã— higher than the high ambient PM2.5 concentrations (geometric mean: 120 Âµg m-3) in Delhi. In-vehicle BC and PN levels were more substantially elevated above background levels (respectively 3.6Ã— and 8.4Ã—). The especially high degree of elevation for PN suggests that in-vehicle exposures might account for a large fraction of daily PN exposure for auto-rickshaw users. The in-vehicle amplification for PN is likely attributable to proximity to a major PN source (traffic emissions) as well as dynamic loss processes such as coagulation that may remove UFP from ambient air.A small subset of measurements collected inside conventional cars with open windows resulted in similar mean concentrations to contemporaneous measurements inside auto-rickshaws. In contrast, concentrations were somewhat lower inside automobiles with air conditioning, likely owing to dynamic in-vehicle particle removal mechanisms. Overall, this investigation concludes that in-vehicle exposures in New Delhi substantially exceed the high ambient background concentrations recorded at fixed sites. Chapter 3 presents a global analysis of the population exposure implications of urban vehicle emissions using the intake fraction (iF) metric. Intake fraction is a dimensionless parameter that represents the fraction of a source's emissions that are ultimately inhaled by all exposed individuals. In this chapter, I develop and apply a model to estimate iF for spatially distributed, ground-level emissions (e.g., from vehicles) in 3646 worldwide urban areas, each with year-2000 population > 100,000. This large dataset of cities accounts for ~ 2.0 billion people, roughly ~70% of the year-2000 urban population. The investigation develops the first-ever iF estimates for urban emissions in many regions outside of North America and Europe, including for numerous megacities for which iF data did not previously exist.In particular, Chapter 3 considers the intraurban iF for each of the cities in this dataset, which accounts for the inhalation exposure to an urban area's emissions that occurs within that same city. Base-case model runs consider an archetypal primary, conserved, non-reactive pollutant. Sensitivity scenarios consider primary pollutants with first-order decay. These broad classifications are representative of many health-relevant pollutants emitted by vehicles and other urban sources. Moreover, they provide a point of comparison for understanding the exposure implications of non-conserved and secondary pollutants, which are explored in more detail in Chapter 4. For conserved primary pollutants, population-weighted median, mean, and interquartile range iF values are 26, 39, and 14 - 52 ppm, respectively, where 1 ppm signifies 1 g inhaled per tonne emitted. The global mean urban iF determined here is roughly twice as large as previous estimates for cities in the United States and Europe, owing primarily to the inclusion of cities with higher iF located outside of these two regions. Intake fractions vary among cities owing to differences in population size, population density, and meteorology. Sorting by size, population-weighted mean iF values are 65, 35, and 15 ppm, respectively, for cities with populations larger than 3 million, 0.6 - 3 million, and 0.1 - 0.6 million. For the 20 worldwide megacities in the dataset, the population-weighted mean intraurban iF is 83 ppm. Overall, mean iF values are greatest in Asia and lowest in land-rich high-income regions, owing primarily to differing patterns in urban form between these two regions. Among the 10 countries with the largest urban populations, population-weighted mean intraurban iF varies by a factor of 3. Intake fraction results for individual cities are predicted well by a parsimonious regression model that incorporates metrics of urban land area, population density, and meteorology. Chapter 4 extends the concept of intake fraction to incorporate semivolatile organic emissions. The analysis emphasizes the consequences of these emissions for population exposure to organic particulate matter, which is a major constituent in both vehicle exhaust and ambient urban air. Organic aerosols (OA) blur traditional notions of primary and secondary pollutants owing to dynamic exchange of material between the vapor and particle phases. Dilution of fresh organic PM emissions (primary organic aerosol, POA) with ambient air typically causes a profound shift of material from particle to vapor phase. Relatively more volatile vapor-phase material is then ""aged"" into lower-volatility products over its residence time in a regional airshed via oxidation reactions initiated by photochemically produced radicals (e.g., the hydroxyl radical OH). In turn, these lower volatility products oxidized from evaporated emissions then condense to form quasi-secondary particles that make up the so-called oxidized primary organic aerosol (OPOA). In this analysis, I update the definition of intake fraction to accommodate the cumulative contributions of population exposure to primary and quasi-secondary organic particles (i.e., POA and OPOA) as well as vapor-phase material to the overall intake fraction for semivolatile organic emissions. As in Chapter 3, the primary emphasis of the analysis is on vehicles and other urban, ground level emissions sources. Because photochemical aging at the regional scale is the major mechanism for converting evaporated POA material into OPOA, I develop and employ a nested multi-compartment mechanistic model to consider exposures at the urban, periurban and regional scales with a 400-km domain. Base-case model simulations consider an archetypal medium-sized US city (population 1.5 M); alternative cases include a model of iF for a global megacity (population 12 M). Key transformation processes for semivolatile emissions (e.g., dilution, partitioning, aging) are represented using the Volatility Basis Set (VBS) framework. A major goal of the modeling exercise is to contrast the magnitude and spatial distribution of iF for semivolatile organic emissions with patterns in iF for nonreactive pollutants.For urban emissions of non-reactive particles, ~75% of domain-wide population intake occurs in the same urban compartment as emissions. In contrast, for semivolatile emissions, spatial patterns and gas-particle partitioning of intake depend substantially on emissions volatility. Low volatility organic emissions in urban areas produce predominantly intraurban, particle-phase exposures (similar to inert pollutants). As volatility of material emitted in urban areas increases, three key trends emerge that reduce particle-phase iF: (1) the overall proportion of population exposure that takes place in the particle phase decreases and the proportion of exposure in the gas phase increases, (2) photochemically aged material (OPOA) accounts for a larger fraction of particle-phase population intake, and (3) regional-scale exposures account for the predominant fraction of organic aerosol exposure attributable to urban precursor emissions. Since higher volatility compounds account for a large fraction of motor vehicle emissions, the overall iF for organic particles attributable to urban semivolatile organic emissions is lower than for inert pollutants. For example, for the default emissions volatility distribution and considering the base-case model, the particle- and vapor-phase intake fractions for semivolatile emissions are respectively 3.1 ppm and 12.5 ppm. Thus, exposure to organic particle-phase material accounts for only ~ 20% of the population intake of all semivolatile organic emissions. For comparison, the iF for primary, non-reactive, conserved pollutants is substantially higher (iF ~ 16.9 ppm) and is only modestly attenuated for inert particles that are subject to loss via dry deposition (iF ~ 12.9 ppm). The dissertation concludes with a summary and synthesis chapter (Chapter 5), which explores policy implications and provides recommendations for future research.",ucb,,https://escholarship.org/uc/item/0nf849mt,,,eng,REGULAR,0,0
525,1961,Modeling of natural stimulus representation in the human brain using canonical correlation analysis,"Bilenko, Natalia Yurievna","Gallant, Jack L;",2016,"Understanding the representation of natural stimuli in the human brain is one of the fundamental goals of neuroscience. In this dissertation, I describe an unsupervised learning approach for investigating BOLD (blood oxygen level dependent) responses throughout the cortex to natural movies and speech. This approach for data-driven learning of representational features is particularly useful for regions of the brain where the features aren't well understood.First, I describe Python software for canonical correlation analysis called Pyrcca, developed for implementing this approach. Then, I introduce a method called FISCA (functional inter-subject component analysis). FISCA uses canonical correlation analysis to estimate components of BOLD responses based on cross-subject similarity. FISCA can be used for combining data across subjects according to functional and not anatomical similarity. FISCA predictively models BOLD responses to novel stimuli across subjects and allows accurate identification of stimulus timepoints based on these predictions.FISCA components are interpretable. For both natural movie and natural story experiments, I visualize the components on the cortical surface to characterize the role of each component in explaining representation in different brain areas. I also compute the tuning of each component for low-level visual and semantic features. FISCA components are stimulus-agnostic. I demonstrate how FISCA can be used to identify amodal regions by combining data between natural movie and speech experiments.FISCA is an accurate, flexible, and extensible unsupervised method for modeling brain responses to natural stimuli. Using this approach, I uncover representation of natural visual and speech stimuli throughout the cortex. I hope that in the future, this approach will be used for creating more accurate models and improving our understanding of cortical processing.",ucb,,https://escholarship.org/uc/item/0p29930k,,,eng,REGULAR,0,0
526,1962,Uncertainty Propagation in Transistor-level Statistical Circuit Analysis,"Tang, Qian Ying","Spanos, Costas;",2011,"In today's semiconductor technology, the size of a transistor is made smaller and smaller. One of the key challenges presently faced by the designers is the increasing impact of process variations to circuit performances. As a result, circuits designed using the traditional methods can deviate from the desired specifications after being manufactured. Therefore, new circuit design and characterization methodologies are required to handle these process variations.The problem of estimating the circuit performance at a transistor-level due to parameter uncertainties is examined. Uncertainty in circuit process and device parameters arises as a result of manufacturing variability. Since electrical circuits are, in general, complex and nonlinear systems, estimating their performances efficiently and accurately is very challenging. Existing methods on propagating uncertainties in circuit parameters to circuit performance include worst case corner analysis, Monte-Carlo simulations, response surface modeling, sensitivity analysis and unscented transformation.  In this work, a novel interval based circuit simulation algorithm is proposed. An interval is a quantity consists of noise variables following Gaussian. The algorithm is developed for both Gaussian and non-Gaussian process variations. When the uncertainty in the circuit process and device parameters can be captured by correlated Gaussian distributions, the process/device parameters are first represented by the appropriate interval representations. An interval-valued SPICE simulator, in which all real number operations are replaced by interval operations, is used to simulate the circuit. The simulation results are therefore interval-values that can be used to extract performance statistics. In this approach, only one circuit simulation is required to obtain the best Gaussian distribution approximation for any circuit performance. The algorithm is tested on RC circuits and transistor circuits with excellent simulation accuracy (<2% error) as compared to Monte Carlo simulation results. It is shown analytically that the runtime of the interval valued circuit simulation is on the order of O(n+m)O(c3) where n is the average number of noise variables per interval operation, m is the average number of noise variables shared between any two interval quantities and c is the number of nodes in the circuit. In the case when the process/device parameters cannot be modeled with Gaussian distributions, A Mixture of Gaussian (MOG) distribution is used to approximate all non-Gaussian distributions and a novel extension to the interval representation is proposed. The algorithm is tested on circuit paths of 100 stages containing inverters, NAND gates and NOR gates. The simulation result of the proposed algorithm agrees very well with Monte-Carlo simulation. In addition, the runtime of the proposed algorithm shows a 54X speed up compared to Monte-Carlo simulation. The proposed interval-value based simulation engine for both Gaussian and non-Gaussian process variations can be directly applied to fast and accurate standard cell library characterization, where the distribution of the cell delay and power are calculated. In addition, the distribution information provided by the proposed simulation method can be feed into statistical timing analysis engine for full-chip level timing closure. The proposed simulation engine can also be incorporated into statistical circuit optimizations.",ucb,,https://escholarship.org/uc/item/0pr8x534,,,eng,REGULAR,0,0
527,1963,Population and Community Dynamics of Alpine Plants in a Changing Climate Across Topographically Heterogeneous Landscapes,"Oldfather, Meagan F","Ackerly, David D;",2018,"Understanding how species and communities shift locally and regionally poses a great challenge as we manage for resilience in the face of a changing climate. Shifts in species distributions are expected to be one of the largest biological effects of climate change and alpine plants are considered early indicators of these biographic responses. However, in montane systems, highly heterogeneous terrain results in a decoupling of climatic gradients complicating straightforward expectations of polar or upslope distributional shifts of plants in response to warmer, drier conditions. Species range shifts will be driven by how these interlaced climate gradients shape current and future population performance across species ranges. This work examines how the differential responses of life history transitions that shape population performance (demographic rates) may mediate range shifts in a changing climate across topographically heterogeneous landscapes.The focal species for this work is Ivesia lycopodioides A. Gray var. scandularis (Rydb.) Ertter & Reveal (Rosaceae), an iteroparous alpine plant with an approximate 20-year lifespan. I first explored the importance of multiple microclimatic gradients in shaping individual demographic rates and population growth rate in sixteen populations across the elevational distribution of this species in the xeric White Mountains, CA USA. I found that multiple microclimate gradients drove variation in demographic rates across this species range, and that complementary and compensatory relationships between demographic rates lead to stable range-wide population growth through multiple demographic pathways. This work motivated a range-wide multi-year field experiment manipulating summertime temperature and precipitation in nine of the study populations to investigate the degree to which climate change may perturb this population stability. Building integral projection modeling based on experimental demographic data, I found a negative effect of experimentally increased summertime temperature on population growth rate in all populations across this species range. This universal reduction is population growth in both trailing and leading range edge populations was due to size-dependent and variable relationships between the climate manipulation and demographic rates, and lead to predictions of population contractions at mid elevations of the species range. These results highlight that differential and size-dependent responses of life history transitions to changing climate influence the rate and magnitude of species range shifts and can lead to unexpected shifts.In order to place the experimental responses of the focal species in a community context, I quantified shifts in abundances for the entire alpine plant community under manipulated climatic conditions. Under experimentally warmer conditions, I observed an increase of hot, dry adapted species relative to their surrounding community members and this effect was not ameliorated by experimental additions of summertime precipitation. Concordantly, I found that overall plant abundance increased and species richness decreased with experimental heating. Together, these results indicate that, with warmer conditions, the White Mountain alpine zone will comprise less diverse plant communities dominated by species associated with hotter, drier conditions.",ucb,,https://escholarship.org/uc/item/0vq6v6vs,,,eng,REGULAR,0,0
528,1964,Eruptive Rates and Volatile Emissions of the Deccan Large Igneous Province and Environmental Consequences,"Fendley, Isabel","Renne, Paul R;",2020,"The end-Cretaceous mass extinction resulted in the demise of the dinosaurs and many other plant and animal species, irrevocably changing Earthâ€™s ecosystems. The environmental changes which influenced the mass extinction and recovery were most likely caused by a combination of the Chicxulub impact and Deccan Traps Large Igneous Province volcanism. The climate and ecological effects of the Deccan Traps remain ambiguous, due in part to a paucity of constraints on eruption rates and frequency. I have addressed this by using mercury concentration as an indicator of eruptions and combining these chemostratigraphic records with mercury box models to estimate eruptive rate and emissions rate of climate-altering gases. Additionally, the timing of the largest known Deccan eruptions, the Rajahmundry Traps, was unclear with respect to the extinction event. As the largest eruptions have been hypothesized to have the most significant climate impact, knowing the timing of these eruptions allows for direct comparison with climate records. Using a combination of geochronological techniques, I found that all three subaerially exposed Rajahmundry Traps lava flows erupted after the Cretaceous-Paleogene boundary. I then used scalings from the LOSCAR carbon cycle model along with paleoclimate records and found that these eruptions most likely did not cause multi-degree warming. However, they could have contributed to ongoing carbon cycle perturbations during the recovery interval. I then investigated the nature of these carbon cycle perturbations by creating a new high-resolution carbon isotope record from terrestrial organic material. I specifically addressed the discrepancy between terrestrial carbon isotope records, which have been interpreted as containing short (<10 ka) excursions, and marine records, which contain a much longer (âˆ¼ 1.5 Ma) excursion linked to decreased export productivity in the surface ocean. My new record shows that, contrary to previously thought, terrestrial records also show a long duration excursion during this time interval. However, while the duration of the excursions is now known to be similar between terrestrial and marine realms, the shape of the excursion is different. The different trends indicate that the open ocean DIC pool and marine CO2 were possibly in disequilibrium during parts of the recovery interval.",ucb,,https://escholarship.org/uc/item/0xw9405v,,,eng,REGULAR,0,0
529,1965,Looking Back with Interest (Rates): Merger Retrospectives in the U.S. Banking Industry,"Palmer, Joshua Lee","Farrell, Joseph;",2011,"In the following three essays I use quantitative evidence to address the effectiveness of horizontal merger policy in the United States in protecting the competitive balance of markets.  Since the enactment of the Sherman Antitrust Act in 1890, the Federal Government has attempted to protect the public from increased prices, reduced innovation, and other adverse effects that may occur when firms are able to exercise market power.  Horizontal mergers, in which firms that were previously direct competitors combine to operate as a single entity, are of particular concern to the Antitrust Authorities (mainly the Department of Justice and the Federal Trade Commission) as these mergers necessarily remove a competitor from the market.  Surprisingly, there are relatively few empirical studies that investigate whether the Antitrust Authorities are able to accurately identify the potentially anticompetitive mergers, or whether once identified, the proposed remedies are appropriate to alleviate concerns over gains in market power.  I provide empirical work on these two issues.In the first essay, I consider the question of how the antitrust agencies identify potentially troublesome mergers based on pre-merger information.  I use pre-merger data from the commercial banking industry to implement a model proposed in the literature to be an improvement over the current merger application screening tool.  I find that relative to the current screen the proposed model reduced the number of mergers flagged as being competitively troublesome.  This result is important given the time and resource constraints that antitrust agencies face when evaluating merger applications.  To test whether the proposed model was not to lax, I combine pre- and post-merger data and implement a difference-in-differences method to estimate the actual merger affects.  I then check to see whether mergers estimated to have resulted in price effects were identified by the proposed model and find that all of the mergers estimated to have resulted in statistically significant price increases were flagged by the proposed model.In my second essay, I turn to the question of whether branch divestitures are sufficient market power remedies in bank mergers.  In particular, I combine data on pre- and post-merger market conditions with data on the Federal Reserve Board's (FRB) stated post-merger expectations to investigate how well divestiture remedies alleviate market power concerns.  I identify mergers that involved the divestitures of bank branches in at least one of the banking markets in which the merging banks competed prior to the merger.  I then divide the sample of merger markets into two subsamples, one in which divestitures occurred and one in which no divestitures occurred and compare the interest rates banks paid depositors in divestiture markets to the rates paid to depositors in non-divestiture markets.  I find that in the second year after the merger the interest rate spread between divestiture and non-divestiture markets was over 135 percent larger on deposit accounts than the pre-merger spread and remained almost 40% higher in the third year following the merger.  As a result, depositors in divestiture markets were relatively worse off than depositors in non-divestiture markets following the merger.  This evidence suggests that the divestitures may not have been sufficient market power remedies. In the final chapter I address a troubling aspect of antitrust policy.  Namely, despite its long history and recognized importance, a deficiency of the empirical evidence required to accurately assess the effectiveness of antitrust policy has persisted.  I identify publicly available data sets on pre- and post-merger market data, as well as publicly available data on specific post-merger market predictions made by the Federal Reserve Board at the time of the merger application review that can be collected to address this deficiency.  I then check to see how the FRB's pro forma estimates of the post-merger market shares underlying their review process compare to the post-merger market shares observed in the data.  I find that pro forma market shares consistently overestimate the post-merger market share of the surviving bank, a finding that is troublesome to the extent that it reflects a restriction in output by the merged bank.  I conclude that the findings reaffirm the warnings from a number of economists that assumptions underlying merger policy need to be analyzed and argue that the availability of data useful for such analyses should be viewed by the profession and policy makers as rendering the deficiency of empirical evidence on antitrust policy as irresponsible and unacceptable.",ucb,,https://escholarship.org/uc/item/0zs5n4df,,,eng,REGULAR,0,0
530,1966,Mechanisms of translocation-coupled protein unfolding using anthrax toxin as a model,"Thoren, Katie Lynn","Krantz, Bryan;",2011,"Molecular machines face a number of challenges in transporting a protein either across a membrane or into a proteolytic complex. In many cases, a substrate protein must first be unfolded before being transported through a narrow channel. Despite its importance and relevance to a variety of different processes in the cell, translocation-coupled protein unfolding is still not well understood. In effort to determine the general biophysical mechanisms of this process, anthrax toxin is used as a model system. In order to understand how a protein unfolds on a translocase channel, planar lipid bilayer electrophysiology, site-directed mutagenesis and thermodynamic stability studies were used to first identify the barriers in the translocation pathway and determine which barrier corresponds to substrate unfolding. Working under conditions where substrate unfolding is rate-limiting, we were then able to map how LFN actually unfolds on the surface of the PA channel. Next, the role of the channel in substrate unfolding and translocation is discussed. In particular, a novel substrate binding site on the surface of PA was identified from the crystal structure of a PA octamer bound to four LFN substrates. This structure, which was solved by my colleague, Geoffrey Feld, reveals that the first Î± helix and Î² strand of each LFN unfold and dock into a deep amphipathic cleft, termed the Î± clamp. Through extensive mutatgenesis studies on both PA and LFN, Geoff and I determined that this site can bind a broad array of polypeptide substrates. The role of the Î± clamp in substrate unfolding, channel oligomerization and translocation is investigated and discussed.Finally, in effort to further probe the Î± clamp's role in translocation, binding to the site is disrupted and the effects on translocation are investigated. Preliminary hypotheses and future directions are discussed.",ucb,,https://escholarship.org/uc/item/10k914xr,,,eng,REGULAR,0,0
531,1967,The electrophysiology of language perception and production,"Flinker, Adeen","Knight, Robert T;",2012,"For over a century, an abundance of research has tried to elucidate the neurobiological basis of language processing in the human cortex. Neuroimaging and lesion studies have provided great insight into what functions different brain structures subserve. While these techniques provide a high spatial resolution they are limited in the temporal domain. Conversely, contributions from non-invasive electrophysiology provided a high temporal resolution with a limited ability to localize cortical sources. The combined spatial and temporal dynamics of cortical processing during language perception and production remains largely unknown. This dissertation addresses this issue by employing unique neuronal population recordings from neurosurgical patients performing linguistic tasks. The studies described here elucidate the timing, magnitude and spatial extent of cortical processing during perception and production of language. The results provide evidence on the level of single-trial that: 1) A rich network of independent and spatially distinct functional sub-regions of cortex subserve perception and production of language. 2) Neighboring sub-regions 4 mm apart can exhibit inverse functional specific responses to linguistic stimuli and self produced speech. 3) Broca's area is not involved in the actual act of articulation but rather in speech preparation and interfacing perception and production. Taken together, these results defy century old dogmas and suggest that language is supported by a complex network of independent sub-regions, with Broca's area acting as a mediator between perception and production rather than as the seat of articulation.",ucb,,https://escholarship.org/uc/item/10x174t2,,,eng,REGULAR,0,0
532,1968,Studies on Complex and Connected Vehicle Traffic Networks,"Wright, Matthew Abbott","Horowitz, Roberto;",2019,"Transportation networks such as road networks are well-known for their complexity. Its users make choices of route, which mode to take, etc.; these users then interact with each other, producing emergent dynamics such as traffic jams on roads. These localized multi-user emergent physical phenomena then interact with similar group movements occurring in other locations, creating more complex network-scale dynamics. These patterns of hierarchical levels of organization and emergent phenomena at each level are typical of so-called ""complex systems."" In addition, the increasing adoption of information-technology systems like connected and autonomous vehicles is creating new challenges in modeling transportation networks, as new emergent behaviors become possible, but also provide new sources of information and possibilities for traffic operations management.The complexity of transportation networks precludes the use of a single all-encompassing theory for all situations at all scales. This dissertation describes several analyses into understanding and controlling emergent dynamics on road traffic networks. It is broken into three parts. The first part proposes models for several new phenomena at the ""macroscopic,"" group-of-vehicles to group-of-vehicles, level. In particular, we solve a problem of modeling arbitrary road junctions with populations of behaviorally-heterogenous vehicles, where the vehicle flows are modelled by a continuum-approximation, partial-differential-equation-based model. We also present several new modeling constructions for a particular complex road network topology: freeways with managed lanes. It has been noted that these managed lane-freeway networks induce new emergent behaviors that are not present in traditional freeways; we propose modeling techniques for several of them, and fit them into traditional modeling paradigms.The second part presents several contributions for estimating the state of the macro-scale traffic dynamics on the road network, based on the micro-scale data of global navigational satellite system readings of the speed and position of individual vehicles. These contributions are extensions of the particle filtering mathematical framework. First, we demonstrate the use of a Rao-Blackwellized particle filter in assimilating vehicle-local speed measurements to better estimate the macroscopic density state of a freeway. Then, we propose new ""hypothesis-testing"" particle filters that can be used to reject outlier or otherwise malign measurements in a principled statistical manner.The third and final part presents two items on applying deep neural networks to transportation system problems at smaller scales. Both items make use of neural attention, which is a neural network design technique that allows for the integration of structural domain knowledge. First, we demonstrate the applicability of this technique towards estimating aggregate traffic states at the lane level, and present evidence that designing the neural network architecture to encode different types of lane-to-lane relationships (e.g., upstream lane vs neighboring lane) greatly benefits statistical learning. Then, we apply similar methods to an autonomous vehicle coordination problem in a deep reinforcement learning framework, and show that an attention-based neural network that allows each vehicle to attend to the other vehicles enables superior learning compared to a naive, non-attention-based architecture, and also allows principled generalization between varying numbers of vehicles.",ucb,,https://escholarship.org/uc/item/114125dg,,,eng,REGULAR,0,0
533,1969,LNA and Mixer Designs for Multi-Band Receiver Front-Ends,"Poobuapheun, Nuntachai","Niknejad, Ali M;",2009,"With the proliferation of wireless standards and frequency bands, the manufacturers of consumer electronics have tried to integrate many features in a single hand-held device. This has given rise to a need for receivers that are compatible with as many standards and frequency bands as possible. Most current integrated multi-band receivers rely on multiple receiver front-ends to process signals at different bands. The major drawback of this approach is that each front-end must be individually optimized, resulting in longer design-time and higher silicon die areas. This is due to the number of circuit blocks and interface complexity. In addition, this type of implementation is highly standard-specific: thus, it is likely that a major redesign would be required if the same topology were used for different standards.The primary objective of this research is to investigate efficient ways of implementing such a receiver front-end with minimal cost, power consumption, and design complexity. CMOS will be the targeted process technology for this design, due to the opportunities for analog-digital system integration and cost-reduction. Despite its attractiveness, designing a front-end for multi-band operations in deep-submicron CMOS technology is non-trivial. The main challenge lies in maintaining moderate gain, noise figure, and linearity at minimum current consumption across a wide frequency spectrum with the abating supply voltage. In this work, we investigate and discuss several receiver front-end building blocks and system designs, with a focus on the issues that arise when designing a multi-band receiver front-end. In addition, we propose several circuit building blocks and systems, and implement design prototypes to validate the possibilities. The results suggest that by exploiting high-speed CMOS transistors and innovative low-voltage design techniques, it is possible to design a low-voltage, low-power, wideband receiver front-end path that is capable of processing signals using the proposed architectures.",ucb,,https://escholarship.org/uc/item/11n5x36x,,,eng,REGULAR,0,0
534,1970,The Astrological Imaginary in Early Twentieth-Century German Culture,"Zahrt, Jennifer Lynn","Largier, Niklaus;Kaes, Anton;",2012,"My dissertation focuses on astrological discourses in early twentieth-century Germany. In four chapters, I examine films, literary texts, and selected academic and intellectual prose that engage astrology and its symbolism as a response to the experience of modernity in Germany. Often this response is couched within the context of a return to early modern German culture, the historical period when astrology last had popular validity. Proceeding from the understanding of astrology as a multiplicity of practices with their own histories, my dissertation analyzes the specific forms of astrological discourse that are taken up in early twentieth-century German culture. In my first chapter I examine the revival of astrology in Germany from the perspective of Oscar A. H. Schmitz (1873-1931), who galvanized a community of astrologers to use the term Erfahrungswissenschaft to promote astrology diagnostically, as an art of discursive subject formation. In my second chapter, I discuss how Paul Wegener's Golem film cycle both responds to and intensifies the astrological and the occult revivals. As the revivals gain momentum, Wegener explicitly depicts seventeenth-century astrological and occult practices with the intention of generating a ""purely filmic"" experience. I provide new insight into Wegener's last Golem film through the film architect Hans Poelzig's personal investment in both baroque architecture and contemporary astrology. My third chapter explores Aby M. Warburg's lifelong preoccupation with the investigation of astrological symbolism in art. This pursuit led him to create an institution that has played a pivotal role in the material conditions of possibility for people to study the history of astrology, even today. In my fourth chapter, I investigate the many references to astrological phenomena in Walter Benjamin's intellectual work. I read his work on mimesis and experience alongside his practice of graphology in order to situate him in the debates brought up in the first chapter on the idea of astrology as an Erfahrungswissenschaft. My dissertation reveals how aspects of astrological discourse--specifically its approaches to issues of legibility and interpretation--constructively informed and shaped the middle brow (Schmitz) and the high brow (Warburg), the cinematic (Wegener) and the literary (Schmitz), the institutional (Warburg) and the philosophical (Benjamin) realms of early twentieth-century German culture.",ucb,,https://escholarship.org/uc/item/1210x15f,,,eng,REGULAR,0,0
535,1971,"Nonprofits in Production: Race, Place, and the Politics of Care","Herrera, Juan Carlos","Grosfoguel, Ramon;",2013,"In the contemporary United States, nonprofits serve as central conduits of urban reform and welfare provision including legal, health and job assistance for racialized neighborhoods. Despite the salience of nonprofit organizations in urban politics, few academic analyses investigate their crucial political work. My work critiques normative academic and popular understandings of nonprofit organizations as ahistorical and nonpolitical service providers fundamentally delinked from the state. In contrast, my dissertation examines how nonprofits operated as a critical technology that intensified the state's relationship to urban racialized communities in the mid 20th century. Based on over two years of ethnographic fieldwork in the Fruitvale district of Oakland, CA and archival research in four different sites, I argue that nonprofit organizations are a powerful vehicle in the remaking of contemporary racial subjectivities and citizenship. As critical community-routed organizations, they negotiate how urban racial subjects relate to the state and social movements.This project probes the material and political consequences of discourses of benevolence in state, nonprofit, and social movement projects. By focusing on projects professing compassion, I unsettle dominant academic frameworks that overwhelmingly focus on two problematics regarding race making: 1) the state as a monolithic entity monopolizing all modes of power; and 2) the attribution of intentional violence to projects of race making. I advance the ""politics of care"" as an analytic for understanding contentious projects of urban improvement normalized as benevolent acts of kindness. Academic debates typically construct welfare as the privileged site of state projects. In contrast, my conceptualization of the ""politics of care"" attends to the role of the state and the work of non-state actors such as nonprofit health clinics, legal-aid centers, and community development corporations. Far more than mere service providers, nonprofits enact diverse techniques of government that target specific racial identities and populations.My findings reveal that nonprofit organizations are a productive site of power in contemporary urban racialized communities like Fruitvale. Nonprofits engaged in multiple sites/acts of production that have spatial, demographic, as well as political effects. First, they build extensive patronage networks that cohere Fruitvale residents as a united Latino ""community"" despite the existence of diverse and often competing factions along class and nationality. By producing this community as a target of projects of improvement and care, nonprofits also link Fruitvale with fiscal patrons outside the geographical confines of the neighborhood. Second, they market the neighborhood as Latino and produce representations of Latinidad that are architecturally and aesthetically visible in the urban form. Third, nonprofit-mediated projects demarcate Latinos from other racial groups and politicize the neighborhood as a haven for immigrant rights and in so doing link residents with constricted citizenship to alternative avenues of belonging. My study fills an important gap in the social movement literature by demonstrating the diversity of 1960s Chicano mobilizations, how they related to African American movements, Asian American experiences, and how this translated into contemporary political formations. Furthermore, my dissertation troubles academic and popular conceptions of Oakland as a Black/White city. This move remaps Latino Studies scholarship into less traditional areas of inquiry outside the metropoles of Los Angeles and Chicago.",ucb,,https://escholarship.org/uc/item/13j7q447,,,eng,REGULAR,0,0
536,1972,"The Collaborative Divide: Crafting Architectural Identity, Authority, and Authorship in the Twentieth Century","Doctors, Steven I.","Crysler, C Greig;",2010,"ABSTRACTThe Collaborative Divide:Crafting Architectural Identity, Authority, and Authorship in the Twentieth CenturybySteven I. DoctorsDoctor of Philosophy in ArchitectureUniversity of California, BerkeleyProfessor C. Greig Crysler	The object of study in this dissertation is a discourse promulgated by architects for much of the twentieth century that assigned transformative attributes to collaboration relative to the purpose and potentiality of the profession.  Underpinning these aspirations was an assertion of the fundamentally collective character of architectural production, yet realization of the purported transformative promise of collaboration recurrently fell short of its idealization.  My intention here is to examine this historical divide by considering: motivations fueling the idealization of collaboration; its engagement in the crafting of architectural identity, authority, and authorship; the mechanisms of professional and state authority employed in its promotion and dissemination; and the socio-economic forces acting upon practice that precluded realization of its transformative promise. 	To enter into this topic, I draw upon primary archival materials to construct an historical narrative contextualized by socio-economic and political forces, with an emphasis on protagonists whose contributions to the American discourse on collaboration are most representative of specific moments in the twentieth-century.  In each instance, the idealization of collaboration operates at the boundaries of the profession, the edges where architects affirm the collective nature of architecture by engaging with non-architect `others' in the conception and production of buildings.  Tensions between the advocacy of collaboration as a transformative means and concurrent quests to articulate the identity, authority, and authorship of the architect tell us much about the efficacy of collaboration as a signifier of collective action, how architects wished to be viewed by non-architect `others,' and more broadly, the implications when theories of practice differ from their realization.  I begin at the close of the nineteenth century with a prevailing historicist paradigm that glorified architecture as art and a concomitant agenda of collaboration intended to resist the temptations of an emerging modernism.  In the second case study, I examine modernist dominance of the Depression-era discourse, and competition between collaboration and cooperation as the ideal basis of collective action for social change.  In the third and final case study, I consider the rise of a process-oriented collaboration stripped of stylistic affiliations in a post-Second World War milieu in which techno-military accomplishments and a burgeoning global American presence inspired seemingly infinite possibilities for architecture as a science-based profession.	The principal contribution of this dissertation is a foregrounding of the historical problematics of collaboration specifically as it pertains to architects in their engagement with non-architect `others.'  By examining tensions between the architectural promotion of collaboration and the crafting of architectural identity, authority, and authorship, I establish a framework for assessing the twenty-first century re-emergence and idealization of collaboration as a transformative practice, in this instance, one characterized by connectivity empowered by information and communication technologies.",ucb,,https://escholarship.org/uc/item/13t043q2,,,eng,REGULAR,0,0
537,1973,Language Support for Loosely Consistent Distributed Programming,"Conway, Neil Robert George","Hellerstein, Joseph M;",2014,"Driven by the widespread adoption of both cloud computing and mobile devices,  distributed computing is increasingly commonplace. As a result, a growing  proportion of developers must tackle the complexity of distributed  programming---that is, they must ensure correct application behavior in the  face of asynchrony, concurrency, and partial failure.  To help address these difficulties, developers have traditionally relied upon  system infrastructure that provides strong consistency guarantees  (e.g., consensus protocols and distributed transactions). These mechanisms  hide much of the complexity of distributed computing---for example, by  allowing programmers to assume that all nodes observe the same set of events  in the same order. Unfortunately, providing such strong guarantees becomes  increasingly expensive as the scale of the system grows, resulting in  availability and latency costs that are unacceptable for many modern  applications.  Hence, many developers have explored building applications that only require loose consistency guarantees---for example, storage systems that only  guarantee that all replicas eventually converge to the same state, meaning  that a replica might exhibit an arbitrary state at any particular  time. Adopting loose consistency involves making a well-known tradeoff:  developers can avoid paying the latency and availability costs incurred by  mechanisms for achieving strong consistency, but in exchange they must deal  with the full complexity of distributed computing. As a result, achieving  correct application behavior in this environment is very difficult.  This thesis explores how to aid developers of loosely consistent applications  by providing programming language support for the difficulties they  face. The language level is a natural place to tackle this problem: because  developers that use loose consistency have fewer system facilities that they  can depend on, consistency concerns are naturally pushed into application  logic. In part, our goal has been to recognize, formalize, and automate  application-level consistency patterns.  We describe three language variants that each tackle a different challenge in  distributed programming. Each variant is a modification of Bloom, a  declarative language for distributed programming we have developed at UC  Berkeley. The first variant of Bloom, BloomL, enables  deterministic distributed programming without the need for distributed  coordination. Second, Edelweiss allows distributed storage reclamation  protocols to be generated in a safe and automatic fashion. Finally, BloomPO adds sophisticated ordering constraints that  we use to develop a declarative, high-level implementation of concurrent  editing, a particularly difficult class of loosely consistent programs.",ucb,,https://escholarship.org/uc/item/1427704s,,,eng,REGULAR,0,0
538,1974,"Golden Tools in Green Design: What drives sustainability, innovation, and value in green design methods?","Faludi, Jeremy","Agogino, Alice;",2017,"What do product design teams value in sustainable design methods? Specifically, what kinds of activities and mindsets comprise different design methods, and which ones do design teams believe drive sustainability, innovation, and other value? How could they be combined to improve sustainable designâ€™s value to companies? This study was the first to deconstruct green product design practices into their constituent activities and mindsets to characterize them and hypothesize their potential synergies. It was also the first to empirically test and compare what practitioners value within three of these sustainable design practicesâ€”The Natural Step, Whole System Mapping, and Biomimicry.Others have identified mindsets in sustainable design practices, or have identified activities in general engineering design practices, but none have done both for sustainable design practices. Such analysis is important, because most designers do not follow design methods like tunnels of process to pass through completely, but like toolboxes to draw from opportunistically. Here, fourteen design methods, guides, and certifications were deconstructed to categorize their component activities and mindsets, and hypothesize what designers, engineers, and managers would consider useful tools to select for different purposes, or could combine to multiply their value. It also hypothesized some green design methods might be preferred by designers, while others might be preferred by engineers or managers.Empirical testing of the activities and mindsets within The Natural Step, Whole System Mapping, and Biomimicry measured their value for general purposes, sustainability, and innovation. It did so by providing 29 workshops on these design methods to 520 participants, with 376 survey respondents: 172 professionals from over 30 different companies and 204 Berkeley students, totaling 1,432 pre- and post-workshop survey responses, due to many people participating in multiple workshops. This testing of multiple design methods was new because most literature on sustainable product design either treats all sustainable design the same, or proposes a specific new design method and studies it. Quantitative and qualitative analysis of survey results validated the earlier deconstruction and found â€œgolden toolsâ€ in each design method: In The Natural Step, Backcasting was most valued, largely for its strategic benefit of focusing thought to accomplish goals, and providing a new lens. In Whole System Mapping,Draw System Map was most valued, largely for broadening scope, visually showing the larger system, and aiding collaboration. In Biomimicry, Nature as Mentor was highly valued as a new lens to approach problems, and for being inspiring; AskNature.org was greatly valued for providing new ideas and for being interesting / engaging. Some of these and other components of the design methods were valued for sustainability, innovation, or both, and some for neither. Results were broken down by demographics (job role, company type, company size, industry sector, and gender) to see if different groups valued different things, as hypothesized above. However, differences were generally too small to be statistically significant at these sample sizes, which implies that sustainable design methods can be taught and used universally between all these groups, even though individuals vary in what they most value and why.In addition to these theoretical analyses and empirical tests, 42 professional designers, engineers, and managers were interviewed at the beginning and end of the study to help establish background context for the research, recommend what green design methods to analyze, validate survey responses, and test for longer-term impact of workshops. They valued a wide range of design practices for several different reasons; some design practices were valued for both sustainability and innovation. Differences in responses from sustainable design experts versus traditional design practitioners showed how specialized skills help sustainable design; this implied design teams should not merely use standard design practices while thinking green thoughts. Multiple respondents mentioned the value of combining green design practices with both each other and traditional design practices. The interviews also investigated how design professionals measure innovation, though they were surprisingly resistant to the idea of quantifying it. Interviews also investigated who can best lead sustainability in design teams, why sustainability might provide business value, and how adoption of sustainability might best be driven in design teams.This studyâ€™s results should help designers, engineers, product managers, and others who create our material world to practice sustainable design more effectively. It can help practitioners mindfully choose and combine golden tools from various green design toolboxes to build a better world while building business value.",ucb,,https://escholarship.org/uc/item/14k3p96v,,,eng,REGULAR,0,0
539,1975,Models of Information Acquisition under Ambiguity,"Li, Jian","Shannon, Chris;",2012,"This dissertation studies models of dynamic choices under uncertainty with endogenous information acquisition. In particular we are interested in exploring the interactions between ambiguity attitudes and the incentive to collect new information.The first chapter explores the link between intrinsic preferences for information and ambiguity attitudes in settings with subjective uncertainty.  We enrich the standard dynamic choice model in two dimensions. First, we introduce a novel choice domain that allowspreferences to be indexed by the intermediate information, modeled as partitions of the underlying state space. Second, conditional on a given information partition, we allow preferences over state-contingent outcomes to depart from expected utility axioms. In particularwe accommodate ambiguity sensitive preferences. We show that aversion to partial information is equivalent to a property of static preferences called Event Complementarity. We show that Event Complementarity and aversion to partial information are closely related toambiguity attitudes. In familiar classes of ambiguity preferences, we identify conditions that characterize aversion to partial information.The second chapter extends the basic model to allow for choices from non-singleton menus after partial information is revealed, and studies the value of information under ambiguity. We show that the value of information is not monotonic under ambiguity. Intrinsic aversionto partial information in the basic model is equivalent to a preference for perfect information in the extended model. Moreover, the value of information is not monotone in the degree of ambiguity aversion.The third chapter studies the impact of ambiguity in a classic information acquisition model-the K-armed bandit problem. We consider a particular family of ambiguity averse preferences, the multiple-priors model [Marinacci, 2002]. A previous paper [Li, 2012] shows that major classic characterizations of optimal strategies in the K-armed bandit problemsextend to incorporate ambiguity in the multiple-priors model. Here we explore new implications of ambiguity on the optimal incentive to experiment. First, increasing ambiguity in the unknown arm reduces the incentive to experiment, while increasing risk in the unknown arm typically increases the incentive to experiment. This suggests that ambiguity can offer an explanation for the widely observed under-experimentation in novel technology and consumer products. Second, optimal experimentation in the multiple-priors bandit problem generally cannot be reduced to that in a classic bandit problem with an equivalentsingle prior. In particular, the lower envelope of the classic single-prior Gittins-Jones index for every prior lying in the multiple-priors set can be strictly higher than the generalized multiple-prior Gittins-Jones index. In one-dimensional parametric family, we identify monotonicity conditions under which this discrepancy disappears so an equivalent single priorexists.",ucb,,https://escholarship.org/uc/item/16j0g7nd,,,eng,REGULAR,0,0
540,1976,"Span Programs, Electrical Flows, and Beyond: New Approaches to Quantum Algorithms","Wang, Guoming","Vazirani, Umesh;",2014,"Over the last decade, a large number of quantum algorithms have been discovered that outperform their classical counterparts. However, depending on the main  techniques used, most of them fall into only three categories. The first category uses quantum Fourier transform to achieve super-polynomial speedup in group-theoretical problems. The second category uses amplitude amplification to achieve polynomial speedup in search problems. The third category simulates quantum many-body  systems. Finding a new class of quantum algorithms has proven a challenging task. This dissertation explores several new approaches to developing quantum  algorithms. These approaches include span programs, electrical flows and nonsparse Hamiltonian simulation. We demonstrate their power by successfully applying them to some useful problems, including tree detection, effective resistance estimation and curve fitting. All of these algorithms are time-efficient, and some of them are proven to be (nearly) optimal.Span program is a linear-algebraic computation model originally proposed to prove circuit lower bounds. Recently, it is found to be closely related to quantum query complexity. We develop a span-program-based quantum algorithm for the following variant of the tree containment problem. Let $T$ be a fixed tree. Given the $n times n$ adjacency matrix of a graph $G$, we need to decide whether $G$ contains $T$ as a subgraph, or $G$ does not contain $T$ as a minor, under the promise that one of these cases holds. We call this problem is the subgraph/not-a-minor problem for $T$. We show that this problem can be solved by a quantum algorithm with $O(n)$ query complexity and $tilde{O}(n)$ time complexity. This query complexity is optimal, and this time complexity is tight up to poly-logarithmic factors. Electrical network theory has many applications to the design and analysis of  classical algorithms. Its connection to quantum computation, however, remains mostly unclear. We give two quantum algorithms for approximating the effective resistance between two given vertices in an electrical network. Both of them have time complexity polynomial in $log{n}$, $d$, $c$, $1/phi$ and $1/epsilon$, where $n$ is the number of vertices, $d$ is the maximum degree of the vertices, $c$ is the ratio of the largest to the smallest edge resistance, $phi$ is the conductance of the network, and $epsilon$ is the relative error. Our algorithms have exponentially better dependence on $n$ than classical algorithms. Furthermore, we prove that the polynomial dependence on the inverse conductance is necessary. As a consequence, our algorithms cannot be greatly improved. Finally, as a by-product, our second algorithm also produces a quantum state encoding the electrical flow between two given vertices in a network, which might be of independent interest. Our algorithms are developed by using quantum tools to analyze the algebraic properties of graph-related matrices. While the first one relies on inverting the Laplacian matrix, the second one relies on projecting onto the kernel of the weighted incidence matrix. It is hopeful that more quantum algorithms could be designed in similar way. Curve fitting is the process of constructing a (simple continuous) curve that has the best fit to a series of data points. It is a common practice in many fields of science and engineering, because it can help us to understand the relationships among two or more variables, and to infer the values of a function where no data are available. We propose efficient quantum algorithms for estimating the best-fit parameters and the quality of least-square curve fitting. The running times of our algorithms are polynomial in $log{n}$, $d$, $kappa$, $nu$, $chi$, $1/Phi$ and $1/epsilon$, where $n$ is the number of data points to be fitted, $d$ is the dimension of the feature vectors, $kappa$ is the condition number of the design matrix, $nu$ and $chi$ are some parameters reflecting the variances of the design matrix and response vector, $Phi$ is the fit quality, and $epsilon$ is the tolerable error. Our algorithms have exponentially better dependence on $n$ than classical algorithms. They are designed by combining the techniques of phase estimation and density matrix exponentiation for nonsparse Hamiltonian simulation.",ucb,,https://escholarship.org/uc/item/18b166mv,,,eng,REGULAR,0,0
541,1977,Essays on Asymmetric Information in Financial Markets,"Breon-Drish, Bradyn Mitchel","Parlour, Christine A;",2011,"This dissertation studies the effects of asymmetric information and learning on asset prices and investor decision-making. Two main themes run through the work. The first is the linkage between investor decisions and the information used to make those decisions; that is, portfolio choices reflect the nature and quality of available information. The second theme is the interaction between investor learning and price informativeness. The information held by individual investors is reflected in market prices through their trading decisions, and prices thus transmit this information to other investors.In the first chapter, Asymmetric Information in Financial Markets: Anything Goes, I study a standard Grossman and Stiglitz (1980) noisy rational expectations economy, but relax the usual assumption of the joint normality of asset payoff and supply. The primary contribution is to characterize how the equilibrium relation between price and fundamentals depends on the way in which investors react to the information contained in price. My solution approach dispenses with the typical â€œconjecture and verifyâ€ method, which allows me to analytically solve an entire class of previously intractable nonlinear models that nests the standard model. This simple generalization provides a purely information-based channel for many common phenomena. In particular, price jumps and crashes may arise endogenously, purely due to learning effects, and observation of the net trading volume may be valuable for investors in the economy as it can provide a refinement of the information conveyed by price. Furthermore, the value of acquiring information may be non-monotonic in the number of informed traders, leading to multiple equilibria in the information market. I show also that the relation between investor disagreement and returns is ambiguous and depends on higher moments of the return distribution. In short, many of the standard results from noisy rational expectations models are not robust. I introduce monotone likelihood ratio conditions that determine the signs of the various comparative statics, which represents the first demonstration of the implicit importance of the MLRP in the noisy rational expectations literature.In the second chapter Do Fund Managers Make Informed Asset Allocation Decisions?, a joint work with Jacob S. Sagi, we derive a dynamic model in which mutual fund managers make asset allocation decisions based on private and public information. The model predicts that the portfolio market weights of better informed managers will mean revert faster and be more variable. Conversely, portfolio weights that mean revert faster and are more variable should have better forecasting power for expected returns. We test the model on a large dataset of US mutual fund domestic equity holdings and find evidence consistent with the hypothesis of timing ability, especially at three- to 12-month forecasting horizons. Nevertheless, whatever timing ability may be reflected in portfolio weights does not appear to translate into higher realized returns on funds' portfolios.",ucb,,https://escholarship.org/uc/item/1995j4hp,,,eng,REGULAR,0,0
542,1978,A Novel Role for TGF-Beta Signaling in Epileptogenesis,"Cacheaux, Luisa","Kaufer, Daniela;",2010,"Epilepsy, one of the most common neurological disorders, affects between 0.5 and 2 percent of the population worldwide. Post-traumatic epilepsy is one of the most difficult forms of epilepsy to treat and the mechanism leading to the characteristic hypersynchronous activity has yet to be elucidated. Previous clinical studies have shown that perturbations in the blood-brain barrier seen after brain injury may be associated with epileptic activity in these patients. We previously demonstrated that albumin is critical in the generation of epilepsy following blood-brain barrier compromise and in this thesis TGF-Beta pathway activation is identified as the underlying mechanism. We demonstrate that direct activation of the TGF-Beta pathway by TGF-Beta1 results in epileptiform activity similar to that following exposure to albumin. Co-immunoprecipitation revealed binding of albumin to TGF-Beta receptor II and Smad2 phosphorylation confirmed downstream activation of this pathway. Transcriptome profiling demonstrated similar expression patterns following BBB breakdown, albumin and TGF-Beta1 exposure, including modulation of genes associated with the TGF-Beta pathway, early astrocytic activation, inflammation, and reduced inhibitory transmission. Importantly, TGF-Beta pathway blockers suppressed most albumin-induced transcriptional changes and prevented the generation of epileptiform activity. Microarray data also revealed changes in many astrocytic genes following BBB disruption and albumin treatment including downregulation of glutamate transporters, glutamine synthetase, the potassium channel Kcnj10, and several connexins. Primary cortical cultures enriched for astrocytes were treated with albumin and confirmed these changes in gene expression, indicating a disruption in astrocytic glutamate and potassium buffering. Finally cell type specific changes in TGF-Beta signaling pathways were evaluated with primary cortical cultures enriched for astrocytes or neurons. In astrocytes, treatment with albumin resulted in preferential activation of the canonical TGF-Beta pathway mediated by the TGF-Beta type I receptor Alk5. Treatment resulted in an increase in Smad2 phosphorylation at 4 hours and an increase in Smad1 phosphorylation as well as Alk5 expression at 24 hours. In neurons, albumin treatment resulted in preferential activation of an alternate TGF-Beta pathway mediated by the TGF-Beta type I receptor Alk1. Treatment resulted in an increase in Smad1 phosphorylation at 4 and 24 hours as well as a small increase in Alk1 expression at 24 hours. In addition, TGF-BetaR2 expression was decreased in both cell types and TGF-Beta pathway blockers prevented astrocytic Smad2 phosphorylation.  Our present data identifies the TGF-Beta pathway as a novel putative epileptogenic signaling cascade and therapeutic target for the prevention of injury-induced epilepsy.",ucb,,https://escholarship.org/uc/item/1c4875d3,,,eng,REGULAR,0,0
543,1979,Resurrected and Reevaluated: The Neo-Assyrian Temple as a Ritualized and Ritualizing Built Environment,"Neumann, Kiersten A.","Feldman, Marian;Rochberg, Francesca;",2014,"This dissertation concerns the ways in which the ritualized materials and ritualized practice of the Neo-Assyrian temple, through their culturally valued and prioritized visual and experiential characteristics, created and marked the special status and divine aspect of the house of a god, differentiating this built environment within the Neo-Assyrian landscape and making it fit for a god. In the study ritual is not approached as a distinct entity, but rather as a characteristic of contrasting practice, that is, as a strategic mode of acting that inflects the practice itself and the associated materials, drawing on Catherine Bell's notions of ritualization. Contributing further to the discussion are such concepts as materiality, agency, phenomenology, visuality, and performance. In order to reconstruct ritualized practice and material interaction in the temples, the physical, aesthetic, and sensory features of the architectural, non-portable, and portable works of art stand at the forefront of the discussion. This study also reinserts active agents into the discussion of material culture and practice in Neo-Assyria, and brings the temple itself, as well as the vast collection of materials housed therein, into this discussion. Complementing the material culture is a study of the Neo-Assyrian royal inscriptions and correspondence, administrative records, ritual instructions, and omen collections. These texts were written in Akkadian, the official language of the Neo-Assyrian Empire, on an assortment of material culture, ranging from clay tablets and prisms to stone statues and wall reliefs. This comprehensive, analytical, and interdisciplinary approach to the Neo-Assyrian temple built environment offers a means of accessing previously unrecognized and under appreciated characteristics of the Neo-Assyrian imperial elite that produced and used these spaces, reevaluating notions of culturally meaningful practice, the role of material and architecture in such acts, culturally valued sensorial experiences, social relations, and the place of ritualized performance within the larger social network. The experiential dimensions of the raw materials and crafted works of art from the temple manifest a prioritization by the Neo-Assyrian elite for what was seen: the brilliance, texture, and polychromatic qualities of this built environment acted as both sign and substance; yet the stimulation of additional senses, such as touch and smell, was also of import. The material and textual evidence from the temple also demonstrates the ways in which this built environment controlled and isolated spheres of practice that served critical functions in the dynamics of the Neo-Assyrian royal court, in particular in the relationships between the king and the scholarly elite. The temple served as a mediating point between the king and the ummÃ¢nusâ€”scholarly experts and skilled craftsmenâ€”and both with the gods. The variability of these relationships materialized in the developments of the temple during the Neo-Assyrian period, the attitudes and preferences of particular kings toward scholarly knowledge and the gods finding expression in their temple work and practice. Moreover, the king's relationship with the temple differentiated this space from the Neo-Assyrian royal palace. Though constructed using the same raw materials and personnel as part of royal building projects, the palace's prioritization of the kingâ€”in both material culture and practiceâ€”illustrates a different inflection of ritualization for a royal dwelling place of Neo-Assyria. The outcomes of this study of the Neo-Assyrian temple make an important contribution to the ongoing dialogues in art historical, material culture, post-colonial and globalization studies regarding the role of material worlds and ritualizing activity in social and political arenas. The textual and material evidence from the Neo-Assyrian temple makes an argument for recognizing degrees of ritualization as an element of ritual theory and practice; for acknowledging meaningful variations in the individual's experience; and for appreciating the variability that results from discrete preferences and attitudes, as characteristics of ritualized practice alongside culturally-grounded traditions and rules. The ritualizing power of the materials and practices explored in this study acted to constitute the divine nature of the temple, a sign of its status as the house of a god in Neo-Assyria. The outcomes of this study therefore also lend themselves to the larger discussion of the house-owner relationshipâ€”in Neo-Assyria and beyondâ€”and the formative role of the latter in conferring and displaying status.",ucb,,https://escholarship.org/uc/item/1cc6r9fj,,,eng,REGULAR,0,0
544,1980,Minor Leagues: The Commercialization of Youth Sports and its Implications for Privatization in Education,"Nicholson, Brandon L","Fuller, Bruce;",2010,"As scholars and policymakers across disciplines argue the merits of market influences in public schooling, few have taken aim at the ongoing privatization of youth sports.  Even as treatment for the poor academic performances of disadvantaged groups, too many point to the perceived cultural shortcomings of the children and their families, and ignore the inadequacies of the underlying opportunity structures.  This phenomenon also manifests itself in the characterization of athletics as a potentially harmful distraction to youth in underserved communities and as an asset to youth in privileged backgrounds.   While such an assertion trivializes prevailing systemic inequalities in access to opportunities, it also ignores the realities of contemporary youth sports institutions, which have become highly commercialized as pathways to college admission. This case study utilizes organizational and institutional theory to illustrate the broad-based participation in the youth athletic enterprise facilitated by the grassroots marketing divisions within a multi-national sports apparel firm.   More specifically, this project identifies the actors within these institutional fields--namely young athletes and their families, youth club and college coaches, event planners and corporate marketing representatives--and the motivations, demands, and associated responses that drive their behaviors.  It demonstrates that these actors span a range of racial and socioeconomic backgrounds, all of whom act simultaneously as ""buyers"" and ""sellers"" of services within the market context.  However, the demands of this privatized field transcend technical efficiency and material benefit, as institutional-normative legitimacy also takes on great significance.  Accordingly, these actors respond to the demands they face, both by adapting their behaviors as well as leveraging their resources to assert their expectations on other groups.  Finally, this investigation of a market-based youth structure informs a discussion of the implications of privatization in public schooling.",ucb,,https://escholarship.org/uc/item/1dp2q18n,,,eng,REGULAR,0,0
545,1981,"Ubiquitin-Dependent Control of Myogenic Development: Mechanistic Insights into Getting Huge, and Staying Huge","Rodriguez Perez, Fernando","Rape, Michael;",2020,"Metazoan development is dependent on the robust spatiotemporal execution of stem cell cell-fate determination programs. Although changes in transcriptional and translational landscapes have been well characterized throughout many differentiation paradigms, their regulatory mechanisms remain poorly understood. Ubiquitin has recently been found to be a key modulator of developmental programs. Ubiquitylation of target proteins occurs through a cascade of enzymatic reactions beginning with a ubiquitin activating enzyme (E1) which transfer the ubiquitin moiety to a ubiquitin conjugating enzyme (E2). The reaction is finalized by the transfer of ubiquitin to its target protein by a ubiquitin ligase (E3). Post-translational modification of proteins can lead to several different outcomes, depending on the context of the modification, known as the ubiquitin code. The precise spatiotemporal execution of ubiquitylation is critical for organismal development and homeostasis. Due to the modular and reversible nature of ubiquitylation, it is an ideal moiety in the control of a plethora of cellular processes.  Cell-cell fusion is a frequent and essential event during development, whose dysregulation causes diseases ranging from infertility to muscle weakness. Critical to this process, cells repeatedly need to remodel their plasma membrane through orchestrated formation and disassembly of cortical actin filaments. In Chapter 2, I describe the identification of a ubiquitin-dependent toggle switch that establishes reversible actin bundling during mammalian cell fusion. My work identified KCTD10 as a modulator of the EPS8-IRSp53 complex, which stabilizes cortical actin bundles at sites of cell contact to push fusing cells towards each other. This work highlights how cytoskeletal rearrangements during development are precisely controlled, raising the possibility of modulating the efficiency of cell fusion for therapeutic benefit.Organismal development must rely on the timely and robust execution of quality control responses. However, how these responses modulate metazoan development is poorly understood. Showcasing the versatility of ubiquitin signaling, Chapters 3 and 4 provide insight into the role of ubiquitin in controlling stress and quality control responses. Chapter 3 describes the reductive stress response, in which FEM1B senses and reacts to persistent depletion of reactive oxygen species. Loss of ROS is detrimental for development, as it inhibits myogenesis. Concomitant to this stress response is the identification of multimerization quality control, regulated by BTBD9. MQC surveys multimeric BTB complex composition, ensuring that multimeric complexes contain the correct stoichiometries and compositions. MQC is critical for development, as loss of MQC als prevents myogenesis. These two chapters showcase the integration of ubiquitin signaling, stress/quality control pathways, and development. These writings provide a more holistic understanding into the robust regulatory underpinnings of organismal formation",ucb,,https://escholarship.org/uc/item/1gx9p3q8,,,eng,REGULAR,0,0
546,1982,"Provincial Urbanity: Intellectuals and Public Life in Patna, 1880-1930","Boyk, David Sol","Bakhle, Janaki;",2015,"Scholarly and popular discussions of cities tend to concentrate on the largest exemplarsâ€”Bombay and Calcutta, in the case of South Asiaâ€”and to neglect the smaller cities and towns where most urban people live. This dissertation examines the history of Patna, a small city in the north Indian region of Bihar, during the late nineteenth and early twentieth centuries. Patna had been a bustling center of trade, culture, and administration for much of the early modern period but, like many cities in the Gangetic plains, it was marginalized by the political and economic changes of the nineteenth century. To many observers, it seemed to have become part of the provincial hinterland or, to use the term that developed under colonial rule, the ""mofussil.""Even a diminished and demeaned Patna, however, remained a major center. Despite the city's apparent decline, it sustained its connections with other mofussil towns and with the rest of the world, and maintained ways of being urban and urbane that distinguished it from larger cities as well as from more rural places. Patna was still Bihar's economic and political hub and a central node in the dynamic public culture that linked Patnaites with readers and writers in nearby towns and distant cities. Questions of the ""backwardness"" of Patna and Bihar entered national politics when activists based in the city began to call for Bihar to be separated from Bengal and established as a new province with Patna as its capital. When they succeeded in 1912, the city itself was reshaped along with its forms of community and authority. The same transformations that seemed to reverse Patna's decline also weakened its links with the networks that had defined its public culture.This dissertation documents Patna's distinctiveness and vitality by combining several approaches. First, it is a cultural history of provinciality and urbanity that shows how these concepts were formed through social practice. Secondly, it is an urbanÂ history that examines the city's politics and social geography together with its relationships with its region. And thirdly, it is a social history of intellectuals that locates their literary and scholarly activities within their urban community. Ultimately, it argues, Patna's urbanity was inextricably linked with its provinciality.",ucb,,https://escholarship.org/uc/item/0jq2997x,,,eng,REGULAR,0,0
547,1983,Progress in Microfluidic Nuclear Magnetic Resonance,"Kennedy, Daniel Joseph","Pines, Alexander;",2014,"Nuclear magnetic resonance (NMR) has matured into an extremely powerful technique, with applications ranging from chemical analysis to molecular biology to medical diagnostic imaging. Unfortunately, the hardware associated with this technique has traditionally been large, immobile, expensive, and required frequent specialized maintenance. Microfluidic technology has allowed for a great reduction in complexity and expense for most common analytical techniques; however, little work has been done to leverage the advantages of microfluidics and nuclear magnetic resonance in a single device. In this thesis, I describe advances toward the goal of creating small, portable, inexpensive, easy-to-use NMR microfluidic instrumentation. First, I describe projects relating to the use of Xe-129 chemical sensors on microfluidic devices. I demonstrate a silicon and glass microdevice which allows the production and optical detection of hyperpolarized Xe-129 at low magnetic fields. The device enhances the NMR sensitivity of Xe-129 experiments by a factor 10^4 on hardware that is a factor of 10^6 smaller in size and power requirements than current polarizers. I describe techniques for patterning arrays of Xe-129 chemical sensors on microfluidic devices and using them to detect chemicals at picomolar sensitivities. These technologies may be integrated to produce highly integrated portable devices for detecting arbitrary chemicals in complex fluids. Due to the use of NMR as the detection modality, they may be used on dirty samples which cannot be interrogated through the use of optical spectroscopy or mass spectrometry, such as biofuels or whole human blood. The second part of this thesis deals with a method of encoding imaging information using arrays of thin magnetic films instead of magnetic field gradients. This technique obviates the need for large instrumentation in imaging experiments and may allow for greater portability, further increasing the practicality of microfluidic NMR experiments. These advances represent a significant step toward microscale NMR technology.",ucb,,https://escholarship.org/uc/item/0mh9q665,,,eng,REGULAR,0,0
548,1984,Instability of Internal Gravity Waves from Wave-wave/topography Interactions,"Liang, Yong","Alam, M.-Reza;",2018,"It is well-known that internal wave breaking is the principle cause of ocean mixing. The latter plays a role in shaping global climate and ocean ecology.  To understand how internal wave loses instability and possibly breaks, in this thesis I present some new findings on the instability mechanisms of internal gravity waves due to wave-wave/topography interactions.Parametric Subharmonic Instability (PSI) is one of the most important mechanisms that transfer energy from tidally-generated long internal waves to short steep waves. Breaking of these short waves results in diapycnal mixing through which oceanic abyssal stratification is maintained. It has long been believed that PSI is strongest between a primary internal wave and perturbative waves of half the frequency of the primary wave. Here, I rigorously show that this is not the case. Specifically, I show that neither the initial growth rate nor the maximum long-term amplification occur at the half frequency, and demonstrate that the dominant subharmonic waves have much longer wavelengths than previously thought.Next I show that there exist internal gravity waves that are inherently unstable, that is, they cannot exist in nature for a long time. The instability mechanism is a one-way (irreversible) harmonic-generation resonance that permanently transfers the energy of an internal wave to its higher harmonics. I show that, in fact, there are a countably infinite number of such unstable waves. For the harmonic-generation resonance to take place, the nonlinear terms in the free surface boundary condition play a pivotal role, and the instability does not occur in a linearly-stratified fluid if a simplified boundary condition such as a rigid lid or a linearized boundary condition is employed. Harmonic-generation resonance presented here provides a mechanism for the transfer of internal wave energy to the higher-frequency part of the spectrum hence affecting, potentially significantly, the evolution of the internal waves spectrum. Following the work on harmonic generations of internal waves,  we show that monochromatic internal gravity waves in a stratified ocean can continuously descend to smaller scales of higher wave numbers after being reflected from no-penetration boundaries, provided their second harmonics satisfy dispersion relation.  The wave with wave number $k$ and frequency $\omega$ first generate its second harmonic due to the non-linearity in the momentum equation and the free surface boundary. Then the resonant interaction of the parent wave with its second harmonic gives rise to a $(3k, \omega)$ wave upon reflection, starting from which a series of triad resonances continuously form in the vicinity of the frequency $\omega$ or $2\omega$. The energy of the internal wave is sent to waves with higher and higher wave numbers. The mechanism potentially explains how part of internal wave energy converts from large scales to that of turbulent mixing near reflecting boundaries, such as ocean ridges and steep continental slopes.Internal waves mainly arise from wind disturbing the upper ocean mixed layer and barotropic tide flowing over topographic features in the ocean. I report a mechanism for conversion of tidal energy to internal gravity waves, through baratropic tidal currents  interacting with ambient internal waves. The newly-generated internal waves have the same wave length as the ambient waves and frequencies higher by the tidal frequency.  I show that they grow exponentially in time and continuously extract energy from the tidal currents. The mechanism exists in any stratified ocean with smooth density profiles and is only possible if a nonlinear free surface is taken into account. I show that for a typical water depth of 4000 m and density stratification 5\% in the Pacific Ocean, the energy flux from baratropic tide to plane internal waves via this mechanism can reach order of 1MWm$^{-1}$. It therefore can be significant enough to account for a fraction of the 1TW total tidal energy loss in the open deep ocean. Ocean mixing has been seen increased where there is rough bottom topography in the ocean. I show in theory that, in a two dimensional domain, with a free surface taken into account, the interaction between an internal wave and corrugated bottom topography can efficiently generate  a series of co-directional internal waves with higher wave numbers. As a result, the internal wave energy can be transferred to smaller scales where internal wave is more prone to breaking and being dissipated.",ucb,,https://escholarship.org/uc/item/0ms2x2f2,,,eng,REGULAR,0,0
549,1985,Many-Body Quantum Dynamics and Non-Equilibrium Phases of Matter,"Potirniche, Ionut-Dragos","Altman, Ehud;",2019,"Isolated, many-body quantum systems, evolving under their intrinsic dynamics, exhibit a multitude of exotic phenomena and raise foundational questions about statistical mechanics. A flurry of theoretical work has been devoted to understanding how these systems reach thermal equilibrium in the absence of coupling to an external bath and, when thermalization does not occur, investigating the emergent non-equilibrium phases of matter. With the advent of synthetic quantum systems, such as ultra-cold atoms in optical lattices or trapped ions, these questions are no longer academic and can be directly studied in the laboratory. This dissertation explores the non-equilibrium phenomena that stem from the interplay between interactions, disorder, symmetry, topology, and external driving. First, we study how strong disorder, leading to many-body localization, can arrest the heating of a Floquet system and stabilize symmetry-protected topological order that does not have a static analogue. We analyze its dynamical and entanglement properties, highlight its duality to a discrete time crystal, and propose an experimental implementation in a cold-atom setting.Quenched disorder and the many-body localized state are crucial ingredients in protecting macroscopic quantum coherence. We explore the stability of many-body localization in two and higher dimensions and analyze its robustness to rare regions of weak disorder.We then study a second example of non-thermal behavior, namely integrability. We show that a class of random spin models, realizable in systems of atoms coupled to an optical cavity, gives rise to a rich dynamical phase diagram, which includes regions of integrability, classical chaos, and of a novel integrable structure whose conservation laws are reminiscent of the integrals of motion found in a many-body localized phase.The third group of disordered, non-ergodic systems we consider, spin glasses, have fascinating connections to complexity theory and the hardness of constraint satisfaction. We define a statistical ensemble that interpolates between the classical and quantum limits of such a problem and show that there exists a sharp boundary separating satisfiable and unsatisfiable phases.",ucb,,https://escholarship.org/uc/item/0mz6z434,,,eng,REGULAR,0,0
550,1986,In Situ Investigations into CaCO3 Nucleation,"Nielsen, Michael Harold","Minor, Andrew M;De Yoreo, James J;",2015,"Classical theories of nucleation were developed over a hundred years ago starting with Gibbs. However, much remains unknown about the process of phase transition in aqueous electrolyte solutions due to the lack of experimental tools able to probe dynamic processes at the time and length scales of the phase transformation. In the calcium carbonate system, recent discovery of an amorphous phase, as well as the suggested existence of potential precursor states such as so-called 'pre-nucleation clusters' or dense liquid droplets, has called into question the utility of the classical framework in making accurate predictions of nucleation.Added to these questions are those regarding the effects that chemical templates have on nucleating calcium carbonate. Many organisms use complex organic matrices to form architecturally complex functional structures out of sea water at ambient temperatures. By contrast, laboratory methods to materials synthesis often require extreme conditions yet maintain at best a low level of control over the development of the resulting material. With the goal of tightly controlling formation of functional materials, scientists have looked to such biomineral systems for inspiration. Self-assembled monolayers (SAMs) of functionalized alkanethiols have been found to act as idealized chemical templates for calcium carbonate nucleation, controlling the nucleating plane of the calcite phase for many surface functionalities. Yet there remain many open questions as to the fundamental mechanisms by which these templates achieve this control. In this dissertation many investigations of calcium carbonate nucleation are discussed, which examine the nucleation pathways of calcium carbonate and mechanisms of control by which alkanethiol surfaces direct the oriented formation of calcite. Traditional in situ microscopy techniques are used to make nucleation rate measurements of templated calcite nucleation on alkanethiol SAMs to test the applicability of the predictions of classical nucleation theory to this system. Low resolution microscopy techniques are further used to provide indirect evidence for the formation pathways of calcite on SAMs exhibiting different surface chemistries. The development of a platform for liquid phase transmission electron microscopy (TEM) utilizing a sealed liquid cell is described, and its utility in making novel observations of materials formation processes is demonstrated. Liquid phase TEM is further employed, using an open cell system which allows for mixing reagents, to directly observe formation pathways in the CaCO3 system.",ucb,,https://escholarship.org/uc/item/0nw504gt,,,eng,REGULAR,0,0
551,1987,Carbon and Greenhouse Gas Dynamics in Annual Grasslands: Effects of Management and Potential for Climate Change Mitigation,"Ryals, Rebecca","Silver, Whendee L;",2012,"Ecosystem management practices that sequester carbon (C) may play an important role in mitigating climate change. Grasslands managed for livestock (e.g., rangelands) constitute the largest land-use area globally. Critical components of the long-term sustainability of rangelands are the maintenance of net primary production (NPP) and soil organic carbon (C) pools. However, overgrazing, plant invasions, and climate change have led to significant C losses from many rangeland ecosystems. Thus, management practices may have considerable potential to restore or increase grassland C storage and help mitigate climate change. Practices that promote C sequestration may have valuable co-benefits, including increased forage production and improved soil water holding capacity. Despite the potential for C sequestration through management interventions, the question remains largely unexplored in grassland ecosystems. 	I used a combination of laboratory experiments, field manipulations, and modeling simulations to examine the effects of rangeland management practices on C sequestration and greenhouse gas emissions. The specific goals of this research were to 1) assess the immediate and carry-over effects of management practices on the net C balance and greenhouse gas emissions in grasslands amended with compost, 2) measure changes to soil C and N stocks following amendment, 3) investigate the long-term fate of compost C and net climate change mitigation potential, and 4) explore the extent of tradeoffs between C sequestration strategies and vegetation characteristics. 	In the first chapter, I conducted a three-year field manipulation replicated within and across valley and coastal grassland sites to determine the effects of a single application of composted organic matter amendment on net ecosystem C balance. Amendments increased C losses through soil respiration, and estimates of net C storage were sensitive to models of respiration partitioning of autotrophic and heterotrophic components. Over the three-year study, amendments increased C inputs by stimulating net primary production by 2.1 Â± 0.8 at the coastal grassland and 4.7 Â± 0.7 Mg C ha-1 at the valley grassland. Carbon gains through above- and belowground NPP significantly outweighed C losses, with the exception of a sandy textured soil at the coastal grasslands. Treatment effects persisted over the course of the study. Net ecosystem C storage increased by 25 to 70 % over three years, not including direct C inputs from the amendment.The purpose of chapter two was to further investigate changes to rangeland soil C and N stocks three years after a one-time application of composted organic material. Increases in bulk soil C, though often difficult to detect over short timeframes, were significant at the valley grassland study site. Physical fractionation of soil revealed greater amounts of C and N in the free and occluded light fractions by 3.31 Â± 1.64 and 3.11 Â± 1.08 Mg C/ha in the valley and coastal grassland, respectively. Analysis of the chemical composition of soil fractions by diffuse reflectance infrared Fourier transform (DRIFT) showed chemical protection and inclusion of compost C into the light fractions. The combination of physical and chemical analyses suggests that the newly incorporated C was physically protected and less available for decomposition.In the third chapter, I employed the ecosystem biogeochemical model, DAYCENT, to investigate the short (10 yr), medium (30 yr), and long-term (100 yr) climate change mitigation potential of compost amendments to grasslands. Climate change mitigation potential was estimated as the balance of total ecosystem C sequestration minus soil greenhouse gas emissions and indirect emissions of N2O via nitrate leaching. The model was parameterized using site-specific characteristics and validated with data from the three-year field manipulation. Model simulations included variations in the applications rate and C:N ratio of the composted material. Above- and belowground NPP and soil C pools increased under all amendment scenarios. The greatest increase of soil C occurred in the slow pool. Ecosystem C sequestration rates were highest under low C:N scenarios, but these scenarios also resulted in greater N2O fluxes. Single or short-term applications of compost resulted in positive climate change mitigation potential over 10 and 30-year time frames, despite slight offsets from increased greenhouse gas emissions. Finally, chapter four examined important tradeoffs between rangeland C sequestration activities and vegetation characteristics. I measured aboveground biomass, plant N content, vegetation communities, and the abundance of noxious weed species for four years following single management events of compost amendment, keyling plowing, and a combination of amendment and plowing. During the first year, plant N content and aboveground biomass was significantly higher in the amended plots and lower in the plowed plots. In the amended plots, forage quantity and quality increases were sustained over the four-year study. During spring grazing events, cows consumed more forage from amended plots without adversely increasing grazing impacts on residual biomass. Plant communities at both grasslands were relatively resistant to management events, however there were short-term declines in the abundance of a noxious annual grass at the valley grassland and increases in a noxious forb at the coastal grassland. 	Grassland management practices, such as the application of composted organic matter, have considerable potential to mitigate climate change while improving plant production, soil fertility, and diverting organic wastes from landfills. This research illustrates the potential for grassland management to sequester while explicitly considering impacts on greenhouse gas emissions, plant production, and vegetation communities over multiple time frames. Overall, my dissertation contributes toward a better understanding of the role of ecosystem management interventions in climate change mitigation.",ucb,,https://escholarship.org/uc/item/0pb550zn,,,eng,REGULAR,0,0
552,1988,In vivo Proximity labeling of translation initiation complexes and repressive RNA granules by APEX-Seq,"Padron, Alejandro J.","Ingolia, Nicholas T;",2019,"RNA localization is a fundamentally important regulatory mechanism for the control of gene expression. I developed an â€‹in vivo â€‹subcellular RNA proximity labeling technique called APEX-Seq to monitor RNA localization and organization in the cell. In my approach, a proximity labeling enzyme is fused to a query protein and rapidly (<1 minute) biotinylates nearby RNAs â€‹in vivo upon addition of a labeling substrate, allowing their subsequent purification and analysis. We show first that APEX-Seq can distinguish the localization of transcripts to different regions of the cell. Further, we can detect the enrichment of specific transcripts in proximity to translation initiation proteins, showing that our system can capture more subtle, RNA-protein localization patterns. Finally, we present a high-resolution time course of protein and RNA condensation into stress induced RNA granules. A powerful aspect of APEX-Seq is the ability to match the spatial transcriptome with quantitative spatial proteomics, allowing for a more complete picture of the spatial landscape of the cell. Additionally, I rationally engineered a split APEX enzyme for conditional spatial proteomics and transcriptomics. I found evidence of unique translation initiation complexes through the use of split APEX on the initiation factors eIF1A/eIF4H, and eIF4A/eIF4B. Overall, these tools open the door for comprehensive and high throughout spatial transcriptomics, and proteomics with subcellular resolution.",ucb,,https://escholarship.org/uc/item/0ph8h52t,,,eng,REGULAR,0,0
553,1989,Chemical Isolation of Am-240 and the Adsorption of Europium and Americium Using Silica Supported CMPO-Calix[4]arenes,"May, Erin Marie Gantz","Arnold, John;",2016,"My dissertation is composed of two projects. The first area of study was the conclusion of a six-year project to determine the neutron-induced fission cross-section of Am-240. This cross-section is difficult to study due to the 50.8 hour half-life of Am-240 but is important for the evaluation of neutron signatures to establish the history of plutonium-containing materials.Such a study is relevant for the fields of nuclear stockpile stewardship and nuclear forensics. Prior to the study outlined here, much of the ground work for this determination had already been provided, including the identification of the Pu-242(p; 3n)Am-240 reaction for the production of approximately 100 nanograms of Am-240 and the development of a preliminary separation procedure to isolate the Am-240 from the unreacted Pu-242 and the undesirable fission and decay products generated from other reactions with the target. However, many variables in the separation procedure needed further investigation. Therefore, a series of column chromatography experiments was performed to determine which of the two previouslydesigned procedures would provide the best purification of 240Am while minimizing the losses. Once the general outline of the procedure was determined, a collaboration with Idaho National Laboratory (INL) made it possible to further refine and test it. Of the four steps in the originally designed procedure, the final step was among the most difficult to accomplish as it involved the separation of the light trivalent lanthanides from americium. Americium/lanthanide separations are dificult due to their identical oxidation states and very similar ionic radii of americium and the lanthanides. The original step utilized a columncomposed of TEVAÂ® extraction chromatography resin and a mobile phase composed of ammonium thiocyanate and formic acid. Due to the complexity of this procedure, several alternative procedures were tested at INL with the TRUÂ® resin and a mixture of fission products more representative of those that would be found in the actual target. A gas pressurized extraction chromatography (GPEC) system was used to automate the separation andachieve greater resolution between the lanthanide and americium elution peaks. Ultimately, it was demonstrated that the original TEVAÂ® resin procedure was more reliable than the TRUÂ® resin procedure, however the TRUÂ® GPEC experiments may be relevant for future target material separations. The culmination of this project was the testing of the first stepof the procedure with one-tenth of the amount of Pu-242 that will be in the actual target. This experiment, in conjunction with the americium/lanthanide separation steps that were tested, prompted a re-evaluation and abbreviation of the overall separation procedure, decreasing the required number of total separation steps from four to two. Therefore, the purification of Am-240 could be accomplished much more quickly and easily than originally anticipated,making the determination of the neutron-induced fission cross-section of Am-240 more of a reasonable possibility in the future.Part II of my dissertation describes the investigation of the adsorption and complexation properties of silica-anchored carbamoylmethylphophine oxide (CMPO)-calix[4]arenes. In Part I, it was demonstrated that the difficulty encountered when separating trivalent lanthanides and actinides can be a severe impediment to accomplishing even fairly straightforward benchtop radiochemical procedures. Much greater motivation for developing newtechnologies to achieve this are encountered when considering the mixtures of trivalent actinides and lanthanides produced when irradiated nuclear fuel is reprocessed. CMPO, a ligand that binds indiscriminately with both the trivalent actinides and lanthanides, was attached to a calix[4]arene scaffold, which was then attached via two different methods to a silica surface. In previous studies, it has been shown that the CMPO-calix[4]arene has a greater affinity for americium than europium. The experiments in this work aimed to establishwhether this relationship held for CMPO-calix[4]arenes anchored in two new ways to the silica surface. It was found that, for a very particular set of conditions - high salt concentration and an aqueous solution pH of 3 - that a CMPO-calix[4]arene very rigidly anchored to the surface of the silica displayed extremely high uptake for Eu(III)when sites outnumbered the number of Eu(III) atoms by at least 10 to 1. This was not found when the same material andconditions were tested with Am(III) nor when the more flexibly anchored CMPO-calix[4]arene material and conditions were tested, leading to the conclusion that both the aqueous solution conditions and the rigidity of the grafted site affect the affinity of the CMPO-calix[4]arene toward cations of interest. The optimization of the CMPO-calix[4]arene system could havebeen potentially useful for Part I of this dissertation for separating americium and the lanthanides, had it existed at the time, and could eventually be applied to the separation of trivalent actinides and lanthanides at the conclusion of nuclear fuel reprocessing.Part I and Part II contribute to two different areas of actinide/lanthanide separation research, with Part I focused on optimizing existing resin systems to achieve separation and Part II focused on testing new materials specifically intended for separating trivalent actinides and lanthanides. Part I shows that existing resin systems can be used to rapidly purify a very small amount of americium from much greater amounts of plutonium, the lanthanides, and other elements. Part II demonstrates that the affinity of a grafted CMPO-calix[4]arene site for either trivalent actinides or lanthanides can be tailored based on the rigidity of the grafting to the solid and the aqueous phase conditions. Separately, it is hoped that these studies can be applied to work focused on detailed actinide target material purification and new innovations in actinide/lanthanide separations, respectively. However, it is also hoped that, cumulatively, these studies contribute to a broader understanding of actinide and lanthanide interactions with ligands designed to effect their separation, both for benchtop laboratory separations and nuclear fuel reprocessing.",ucb,,https://escholarship.org/uc/item/0r73v3h1,,,eng,REGULAR,0,0
554,1990,"Creative Destruction: Memory, Public Finance, and the State in New York City","Potluri, Keerthi Choudary","Esmeir, Samera;Watts, Michael J;",2014,"This dissertation demonstrates how the neoliberal state marshals public finance and public memory to incorporate itself into the contemporary urban landscape.  It investigates sites in the built environment where the state deploys public funds to subsidize private construction projects that are fiscally unsound, using the institutional form of the public authority to covertly mediate between itself and the private sector. Combining discursive analysis of news media, funding allocation records, and the sites themselves, it looks at the World Trade Center reconstruction, a bus depot in Harlem, and Freshkills Park in Staten Island, a former landfill, and examines the effects of these projects both on urban space and public memory itself.  The dissertation argues that the state uses public funds to inaugurate sites of memory in the city's cultural and political landscape in order to assert its ethical and political legitimacy in a moment when that legitimacy is bound up in its relationship to private capital. This is an unexpected manifestation of what Marx and Schumpeter describe as ""creative destruction"" - the destruction of capital necessary to the continuation of capitalism - but what the dissertation shows is that this process not only sustains capitalist economy, but also removes democratic participation from planning the built environment while regenerating the state's failing legitimacy in times of fiscal crisis.  The dissertation employs historical and archival research on public authorities, including the Port Authority of New York and New Jersey, and qualitative research, such as personal interviews with project planners, neighborhood residents, tourists, state officials, and activists.  By showing that state projects of memorialization are central to mediating between private interests and the capitalist state, it contributes to scholarship in state theory, urban planning, and material culture.",ucb,,https://escholarship.org/uc/item/0sq8t5vq,,,eng,REGULAR,0,0
555,1991,"At the Crossroads of Empire: The United States, the Middle East, and the Politics of Knowledge, 1902-2002","KHALIL, Osamah F.","Doumani, Beshara B;",2011,"This dissertation examines how U.S. foreign policy shaped the origins and expansion of Middle East studies and expertise.  For over sixty years the United States has considered the area called the ""Middle East"" to be vital to its national security interests, and governmental and academic institutions have been essential pillars in support of this policy.  America's involvement in the Middle East has matched its rise as a global superpower and I argue that U.S. foreign policy significantly influenced the production and professionalization of knowledge about the region.  I demonstrate that passage of the National Defense Education Act (NDEA) of 1958 ultimately led to the growth and diversification of the field.  Moreover, my dissertation contends that an unintended consequence of this expansion was strained relations between academia and the government, which contributed to and was compounded by decreased federal funding for area studies.  By the late and post-Cold War periods, I assert that these factors led to a perceived decline in the field while private think tanks garnered increased attention and influence.Drawing on research completed at national, university, and foundation archives, I explain how key governmental and non-governmental institutions collaborated to promote Middle East studies and expertise.  I examine early American attempts to produce contemporary regional expertise through different wartime agencies and programs during the First and Second World Wars.  In particular, I focus on the Inquiry, a group of scholars created to help President Woodrow Wilson prepare for the Versailles Peace Conference, as well as the Office of Strategic Services and the Army Specialized Training Program.  I assert that the example of these initial efforts and their alumni helped establish the institutional precursors for the development of area studies.  During and after the Cold War, I analyze how the Department of State and the Central Intelligence Agency coordinated with the Middle East studies programs at Princeton and Harvard and supported the American Universities of Beirut and Cairo.  I also discuss the coordination of private foundations and academic societies with governmental agencies as well as their funding and support of area studies programs before and after the NDEA.  This includes the activities of the Rockefeller and Ford Foundations, the Social Science Research Council, and the American Council of Learned Societies. I conclude that different regimes of knowledge production and cultures of expertise related to the Middle East have emerged over the past century.  While these regimes have often intersected and competed for supremacy, I contend that U.S. foreign policy interests and goals have had a predominant influence on the contested ways knowledge is produced, communicated, and consumed.  I demonstrate that the terminology and associated geographical representations inherent in U.S. foreign policy discourse has been adopted and promulgated by academic scholarship on the Middle East.  Thus, revealing that even when Washington's policies are contested by area experts its interests have already been subsumed into existing discourse on the region.  While university-based Middle East studies were successful in expanding and enhancing the U.S.'s knowledge about the region and producing potential candidates for government service, I assert that the foreign policy and intelligence establishments developed their own processes for collecting and analyzing information and trends which benefited from but were independent of academic scholarship on the Middle East.  Furthermore, I argue that think tanks emerged at the expense of university-based Middle East studies programs by actively pursuing research agendas in support of U.S. foreign policy objectives in the region.",ucb,,https://escholarship.org/uc/item/0t00c8pw,,,eng,REGULAR,0,0
556,1992,"Viewpoints in the Korean Verbal Complex: Evidence, Perception, Assessment, and Time","Kwon, Iksoo",,2012,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0jq329pw,,,eng,REGULAR,0,0
557,1993,Approaches to Plans in Public Health Emergency Preparedness and Response,"Hunter, Mark David","Keller, Ann;",2017,"In public health emergency preparedness and response (PHEP), plans are a central matter of concern. Understanding plans' role in establishing, facilitating, or securing preparedness and response objectives is of enduring practical and theoretical warrant. Drawing from practice-based perspectives and treating plans as sociotechnical objects, this project aims to describe ways in which theoretical approaches to plans shape their use, meaning, and value in actual practice. Employing empirically-grounded and interpretative methods, it investigates divergent accounts of plans' role for the purpose of identifying assumptions about the mechanisms by which plans' achieve their practical effects. The consequences of these assumptions on how plans are written and used are explored in separate case studies. The first focuses on the modalities for coordinating functional response activities within local health departments' preparedness and response plans in the U.S. The second focuses on the modalities for coordinating information globally within the International Health Regulations (2005). Both cases demonstrate that the capacities of plans to realize functional or operational objectives i.e., the mechanisms of how plans work, depend on the dominant theoretical approaches in the relevant setting.",ucb,,https://escholarship.org/uc/item/0mj764qc,,,eng,REGULAR,0,0
558,1994,Multilinear and Sharpened Inequalities,"O'Neill, Kevin","Christ, Michael;",2019,"Multilinear integral inequalities, such as HÂ¨olderâ€™s inequality and Youngâ€™s convolution inequality, play a large role in analysis. In [6] and [5], Bennett, Carbery, Christ, and Tao provide a classification of such inequalities of the formîš â†’ R. Rdî™n j=1fj(Lj(x))dx â‰¤ Cî™n j=1||fj||pjfor constant C > 0, exponents pj âˆˆ [1,âˆž], and surjective linear maps Lj : Rd â†’ Rdj with fj : Rdj, In Chapter 2, we discuss a generalization of the above related to work by Ivanisviliand Volberg [23]. Specifically, we provide a classification of functions B : Rn â†’ [0,âˆž) such thatîš B(f1(L1(x)), ..., fn(Ln(x))dx â‰¤ CB Rd î€’îš f1, ...,îš î€“ fn.In some cases, it will be shown that maximizers of the above inequality exist. Tuples of Gaussians are not always maximizers, differing from the usual multilinear theory. Chapter 3 focuses its attention on the trilinear form for twisted convolution:îšîš f(x)g(y)h(x + y)eiÏƒ(x,y)dxdy. RdÃ—Rd While existence of maximizers can shed light on the structure of an operator,sometimes it is useful to establish more refined information. For twisted convolution, we show a quantitative version of the statement that if a triple of functions nearly maximizes the form, then it must be close to a maximizing triple. Such a statement2may be referred to as a sharpened inequality. Here, the proof of a sharpened inequality is complicated by the fact that no maximizers exist for twisted convolution; however, one may vary the amount of oscillation and compare to the case in which there is zero oscillation. In Chapter 4, we establish a sharpened version of the following inequality due toBaernstein and Taylor [3]: îšîšSdÃ—Sd f(x)g(y)h(x Â· y)dÏƒ(x)dÏƒ(y) â‰¤ îšîš fâˆ—(x)gâˆ—(y)h(x Â· y)dÏƒ(x)dÏƒ(y) SdÃ—Sdwhere f, g, h are restricted to the class of indicator functions and h is monotonic on [âˆ’1, 1]. In the above, fâˆ— refers to the symmetric decreasing rearrangement of f, and likewise for g and gâˆ—.",ucb,,https://escholarship.org/uc/item/1298t85k,,,eng,REGULAR,0,0
559,1995,"Bioconjugate materials for the study of pigment mobility in light-harvesting systems, protein-based formulations for hydrophobic actives, and conformational changes in conjugate vaccines","Jaffe, Jake","Francis, Matthew B;",2016,"Techniques for the preparation, purification, and characterization of protein-based materials have allowed for advances in fields ranging from medicine to materials science. While great attention has been paid to the chemistries used in the attachment of molecules of interest to proteins, the importance of the linking group is often overlooked. Each chapter of this dissertation describes the use of linkers as functional design elements in three distinct projects. In Chapter One, the development and characterization of a minimal model for investigating the role of pigment mobility in photosynthetic light-harvesting antenna systems will be discussed. In this model system, pigment-protein and pigment-pigment interactions were altered through the use of pigment-protein linkers of various lengths and rigidities. Chapter Two explores new biocompatible bond cleavage reactions for the preparation of a new class of general protein-based formulations for hydrophobic actives, wherein the linker group imparts the critical properties to the system. Chapter Three will examine conformational changes in peptide-protein conjugate vaccines. The roles of specific methods of covalent modification and linker composition in these conformational changes are emphasized.",ucb,,https://escholarship.org/uc/item/1637m4hd,,,eng,REGULAR,0,0
560,1996,Inhibition of the T-cell Kinase ZAP-70,"Visperas, Patrick Ramos","Kuriyan, John;",2013,"The adaptive immune system responds to foreign antigens to produce a highly specific immune response. T-cells, a type of white blood cell of the adaptive immune system, can either directly kill infected cells or aid in sending signals that regulate the immune response. The T-cell receptor is expressed on the surface of T-cells and recognizes foreign antigens presented by antigen presenting cells. Unlike transmembrane receptor tyrosine kinases, the T-cell receptor does not contain its own kinase activity. Rather, the T-cell receptor recruits a number of non-receptor tyrosine kinases upon extracellular antigen binding to trigger intracellular signaling cascades. Zeta-chain associated protein of 70-kDa (ZAP-70) is one such non-receptor tyrosine kinase that is recruited to regions of the T-cell receptor known as Immuno-receptor Tyrosine-based Activation Motifs (ITAMs). Clinical observations, as well as chemical genetic studies, suggest that ZAP-70 is a possible target for T-cell inhibition.Small molecule inhibitors of ZAP-70 may prove to be therapeutically useful for the treatment of autoimmune disease or for organ transplant rejection. Because no such molecules exist today, we hope to exploit the structural insights gleaned from the recently solved autoinhibited structure of ZAP-70 to discover an allosteric inhibitor. In this dissertation, I describe a method of inhibiting the interaction between ZAP-70 and the T-cell receptor. I performed a high throughput screen for inhibitors of ZAP-70:ITAM binding and identified a collection of hit compounds. Inhibition by these compounds was verified in an orthogonal assay and was dose-dependent. I found that the compounds targeted the tandem-SH2 domains of ZAP-70 and were thiol-reactive. I go on to show that covalent cysteine modification underlies the inhibition of ZAP-70 and that individual compounds are specific for different cysteine residues.",ucb,,https://escholarship.org/uc/item/16j1b7gb,,,eng,REGULAR,0,0
561,1997,Internet Self-Injury Forums as Communities of Social-Cognitive Literacy Practice,"Brett, Jeremy","Mahiri, Jabari;",2010,"This exploratory study provides an interpretive framework and empirical evidence supporting the proposition that internet forums devoted to intentional self-injury may fruitfully be conceptualized as communities of social-cognitive literacy practice. This conceptualization may facilitate the development of theory, research, and clinical practice involving individuals for whom the practice bears psychological meaning, while also providing theoretical surplus value for research into psychology, digital media, critical pedagogy, and the study of virtual lives. I present a selection of discussion threads drawn from seven of the most visible Internet forums devoted to self-injury, providing a range of ethnographic and social-cognitive observations concerning the forms and apparent functions of these sites, as articulated by their members. The guiding theoretical framework emerges from points of instructive overlap between social- cognitive psychology and the developing field of digital media and learning. Forums appear to offer members interactive, real-time community as well as a shared, dynamic repertoire of social-cognitive constructs with which to interpret lived experiences and explore their implications for socialization and identity development. Through ethnographic observation and discourse analysis of forums and discussion threads, I reveal forum discussions to be social and cognitive ecologies in which members represent and reflect collectively on experiences, thoughts, and social categories, but in which such discussions tend to remain at a generally concrete level of cognitive operations.",ucb,,https://escholarship.org/uc/item/16q4c5r5,,,eng,REGULAR,0,0
562,1998,Reinventing the PN Junction: Dimensionality Effects on Tunneling Switches,"Agarwal, Sapan","Yablonovitch, Eli;",2012,"Tunneling based field effect transistors (TFETs) have the potential for very sharp On/Off transitions.  This can drastically reduce the power consumption of modern electronics.  They can operate by either electrostatically controlling the thickness of the tunneling barrier or by exploiting a sharp step in the density of states for switching.  We show that current TFETs rely on controlling the thickness of the tunneling barrier but they do not achieve the desired performance.  In order to get better performance we need to also exploit a sharp step in the density of states.In order to have a sharp density of states turn on, a variety of non-idealities need to be accounted for.  A number of effects such as thermal vibrations, heavy doping, and trap assisted tunneling are analyzed and engineered.After accounting for the various non-idealities, the ideal density of states will determine the on state characteristics.  The nature of the quantum density of states is strongly dependent on dimensionality. Hence we need to specify both the n-side and the p-side dimensionality of pn junctions. For instance, we find that a typical bulk 3d-3d tunneling pn junction has only a quadratic turn-on function, while a pn junction consisting of two overlapping quantum wells (2d-2d) would have the preferred step function response. Quantum confinement on each side of a pn junction has the added benefit of significantly increasing the on-state tunnel conductance at the turn-on threshold.  We analytically demonstrate these effects and then give a numerical non-equilibrium greens function (NEGF) model to verify the key results.  Finally we introduce some new device designs that will take advantage of the benefits of 2d-2d tunneling.",ucb,,https://escholarship.org/uc/item/18h0497c,,,eng,REGULAR,0,0
563,1999,Effects of osteoporosis therapies on bone biomechanics,"Easley, Sarah Kathleen","Keaveny, Tony M;",2010,"Anti-fracture therapies for the treatment of osteoporosis have been shown clinically to reduce the incidence of fracture; however, standard clinical measurements of bone density cannot sufficiently explain these large reductions. Therefore, the overall goal of this research is to develop a better understanding of the mechanisms through which anti-fracture therapies improve bone strength -- a critical determinant of fracture risk -- which should lead to improved assessment of treatment efficacy.Combining the latest advances in micro-computed tomography and high-resolution micro-CT-based finite element modeling, we used repeated measures and parameter variations to isolate specific biomechanical effects of various bone characteristics that can be altered by disease and treatment. Specifically, we found that simulated microcavities in trabecular bone from a wide range of bone volume fraction and microarchitecture reduced the strength and altered the relationship between strength and bone volume fraction. While this effect was greater in low-density bone and when the microcavities were targeted to regions of high tissue strain, an appreciable biomechanical effect persisted for all types of bone. Since previous work with antiresorptive-treated canine bone did not find such an effect, questions remain regarding accurate representation of the morphology and micromechanics of actual resorption-induced cavities. Despite these uncertainties, our results provide new insight into the clinical relevance of stress risers caused by resorption cavities, suggesting that antiresorptive therapies may be most effective via mitigation of stress risers in a subset of patients with low bone volume fraction and high bone turnover.Studying vertebrae from treated rats revealed that any treatment-induced changes in intra-specimen variations in tissue mineralization, as detected by quantitative micro-CT, had a negligible biomechanical effect at the whole bone level and in isolated trabecular bone. Intra-specimen variations in tissue mineralization did have a role in general biomechanical behavior, but this role was remarkably uniform across the four different treatment groups: sham control, ovariectomized (OVX), OVX+PTH, and OVX+raloxifene. Finite element results showed that biomechanical treatment effects were dominated by treatment-induced changes in geometry and microarchitecture.This research also produced an efficient pre-clinical framework for characterizing bone quality which should provide considerable insight into the mechanisms of biomechanical effects in a broad range of bone research applications, including aging, diseases, and pharmaceutical and genetic therapy. The approach takes advantage of the hierarchical structure of bone by evaluating the most biomechanically relevant characteristics at each physical scale to isolate the source of bone quality effects and prescribing subsequent analysis only when such effects are found. Using this framework, we found that neither ovariectomy nor PTH treatment had a net effect on bone quality of rat vertebrae during compressive loading suggesting that the observed changes in vertebral strength were primarily due to changes in bone quantity.In closure, this dissertation research has increased knowledge regarding the mechanisms through which osteoporosis therapies improve bone strength without appreciably increasing bone mass. Further, it provides new methods for pre-clinical assessment of treatment efficacy. This dissertation also outlines areas of research to further advance our understanding of the effects of disease and drug therapies on bone biomechanics in human bone.",ucb,,https://escholarship.org/uc/item/18s6w0dd,,,eng,REGULAR,0,0
564,2000,Characterizing rare and transient conformations of proteins using amide hydrogen exchange and thiol exchange,"Bolin, Eric","Marqusee, Susan;",2018,"The dynamic nature of proteins is often an underappreciated aspect of biology necessary for understanding protein function and turnover in the cell. This is in part because within the multitude of states that a protein occupies throughout its lifetime, at most a few are significantly populated and have long enough lifetimes to study using traditional structural biology methods. In this thesis, I develop and explore experimental methods (amide hydrogen-deuterium exchange and cysteine labeling) to selectively label and characterize these rare and transient conformations methods. Specifically, I used cysteine labeling as a means to validate computational predictions of rare conformational fluctuations that expose potential drug-binding sites, I used pulse-labeling amide hydrogen exchange coupled with mass spectrometry detection to identify intermediates formed during protein folding and developed new data analysis procedures for analyzing hydrogen exchange rates measured determined by mass spectrometry on proteolytic fragments. The first project used thiol labeling rates to validate computational predictions of cryptic binding sites. Analysis of millisecond-long molecular dynamics (MD) simulations of Î²-lactamase uncovered several potential conformational fluctuations that expose hidden, or cryptic binding pockets. Targeting these rare conformations presents a potential avenue for drug development. I validated the existence of these pockets by introducing cysteine residues in select positions and characterizing their accessibility to chemical modifications. In addition to validating the existence of these fluctuations, modification of cysteines at these sites modulates activity allosterically.In the second project, I used pulsed labeling hydrogen exchange to follow the folding pathway of a protein family over evolutionary time (>3 billion years). I determined the conformations populated during folding (folding intermediates) for a family of RNase H proteins, including two extant and seven ancestral proteins. Each of these proteins was shown to populate a similar folding intermediate prior to the rate-limiting step in folding; however, the details of the steps leading up to this intermediate varied. We further showed that we can alter these early folding steps for a given protein using rationally designed mutations.The third project compared and characterized the refolding and co-translational folding pathway of the protein HaloTag. We found that HaloTag aggregates during refolding but not during cotranslational folding, and it adopts at least one intermediate during refolding that is suppressed during cotranslational folding.Finally, in the fourth project I developed a modification of a recently published approach for analyzing data from mass spectrometry-based detection of hydrogen exchange. This modification allows me to obtain multiple protection factors per peptide monitored and obtain a quantitative measurement of the free energy of hydrogen exchange in different regions of the protein.",ucb,,https://escholarship.org/uc/item/1cd158xw,,,eng,REGULAR,0,0
565,2001,Restoring Ecological Function with Invasive Species Management,"Hanna, Cause","Kremen, Claire;",2012,"Mutually beneficial interactions between pollinators and flowering plants represent a critical but threatened component of ecosystem function that can underlie the success of ecological restoration. The management and removal of invasive species may give rise to unanticipated changes in plant-pollinator mutualisms because they can alter the composition and functioning of plant-pollinator interactions in a variety of ways. In an attempt to incorporate a functional framework into invasive species management, we conducted a large-scale manipulative experiment to examine the restoration of the plant-pollinator mutualisms and the pollination of a functionally important endemic tree species, Metrosideros polymorpha, following the removal of a competitively dominant invasive floral visitor and arthropod predator, Vespula pensylvanica. The invasive western yellowjacket wasp, Vespula pensylvanica, is an adept and aggressive nectar thief of the partially self-incompatible and pollen limited M. polymorpha. A management strategy utilizing 0.1% fipronil chicken bait with the addition of heptyl butyrate reduced the abundance of V. pensylvanica by 95 Â± 1.2% during the 3 months following treatment and maintained a population reduction of 60.9 Â± 3.1% a year after treatment in the managed sites when compared with unmanaged sites. The large-scale management of V. pensylvanica demonstrated that V. pensylvanica through both superior exploitative and interference competition inhibits resource partitioning and displaces native and non-native M. polymorpha pollinators. Correspondingly, the removal of V. pensylvanica resulted in the competitive release and restructuring of the pollinator community and the re-establishment of the plant-pollinator mutualisms and pollination of M. polymorpha. This research elucidates the competitive mechanisms and contrasting implications of introduced species on ecological function and provides a framework from which future invasive species management can preserve ecological function and maintain ecosystem resilience.",ucb,,https://escholarship.org/uc/item/1cn915mt,,,eng,REGULAR,0,0
566,2002,"Decolonizing Being, Knowledge, and Power: Youth Activism in California at the Turn of the 21st Century","Banales, Samuel","Briggs, Charles L;",2012,"By focusing on the politics of age and (de)colonization, this dissertation underscores how the oppression of young people of color is systemic and central to modernity.  Drawing upon decolonial thought, including U.S. Third World women of color, modernity/coloniality, decolonial feminisms, and decolonizing anthropology scholarship, this dissertation is grounded in the activism of youth of color in California at the turn of the 21st century across race, class, gender, sexuality, and age politics.  I base my research on two interrelated, sequential youth movements that I argue were decolonizing: the various walkouts organized by Chican@ youth during the 1990s and the subsequent multi-ethnic ""No on 21"" movement (also known as the ""youth movement"") in 2000.  Through an interdisciplinary activist ethnography, which includes speaking to and conducting interviews with many participants and organizers of these movements, participating in local youth activism in various capacities, and evaluating hundreds of articles--from mainstream media to ""alternative"" sources, like activist blogs, leftist presses, and high school newspapers--I contend that the youth of color activism that is examined here worked towards ontological, epistemological, and institutional decolonization.  This study, which addresses negative social understandings about youth in general and young people of color specifically, highlights how the oppression of youth is systemic and central to modernity/coloniality, and calls attention to the necessity of incorporating age, power, and their theorization into the discourses on decolonization.  Along with making youth's politics and political identities essential to the research, this dissertation aims to contribute to, not only knowledge production, but also the unfinished project of decolonization--which most literature on young people or youth activism has yet to do.",ucb,,https://escholarship.org/uc/item/1g51b118,,,eng,REGULAR,0,0
567,2003,Tropical Linear Spaces and Applications,"Rincon, Edgard Felipe","Sturmfels, Bernd;Holtz, Olga;",2012,"Tropical geometry is an area of mathematics that has enjoyed a quick development in the last 15 years. It can be seen as a tool for translating problems in algebraic geometry to combinatorial problems in convex polyhedral geometry. In this way, tropical geometry has proved to be very succesful in different areas of mathematics like enumerative algebraic geometry, phylogenetics, real algebraic geometry, mirror symmetry, and computational algebra.One of the most basic tropical varieties are tropical linear spaces, which are obtained as tropicalizations of classical linear subspaces of projective space. They are polyhedral complexes with a very rich combinatorial structure related to matroid polytopes and polytopal subdivisions. In Chapter 1 we give a basic introduction to tropical geometry and tropical linear spaces, and review the basic theory of tropical linear spaces that was developed by Speyer. In Chapter 2 we study a family of functions on the class of matroids, which are ``well behaved'' under matroid polytope subdivisions. In particular, we prove that the ranks of the subsets and the activities of the bases of a matroid define valuations for the subdivisions of a matroid polytope into smaller matroid polytopes.The pure spinor space is an algebraic set cut out by the quadratic Wick relations among the 2^n principal subPfaffians of an n x n skew-symmetric matrix. Its points correspond to n-dimensional isotropic subspaces of a 2n-dimensional vector space. In Chapter 3 we tropicalize this picture, and we develop a combinatorial theory of tropical Wick vectors and tropical linear spaces that are tropically isotropic. We characterize tropical Wick vectors in terms of subdivisions of Delta-matroid polytopes. We also examine to what extent the Wick relations form a tropical basis. Our theory generalizes several results for tropical linear spaces and valuated matroids to the class of Coxeter matroids of type D. In Chapter 4 we study tropical linear spaces locally: For any basis B of the matroid underlying a tropical linear space L, we define the local tropical linear space L_B to be the set of all vectors v in L that make B a basis of maximal v-weight. The tropical linear space L is the union of all its local tropical linear spaces, which we prove are homeomorphic to Euclidean space. We also study the combinatorics of local tropical linear spaces, and we prove that they are combinatorially dual to mixed subdivisions of a Minkowski sum of simplices. We use this duality to produce tight upper bounds on their f-vectors. We introduce a certain class of tropical linear spaces called conical tropical linear spaces, and we give a simple proof that they satisfy the f-vector conjecture. Along the way, we give an independent proof of a conjecture of Herrmann and Joswig.In Chapter 5 we introduce the cyclic Bergman fan of a matroid M. This is a simplicial polyhedral fan supported on the tropical linear space T(M) of M, which is amenable to computational purposes. It slightly refines the nested set structure on T(M), and its rays are in bijection with flats of M which are either cyclic flats or singletons. We give a fast algorithm for calculating it, making some computational applications of tropical geometry now viable. We develop a C++ implementation, called TropLi, which is available online. Based on it, we also give an implementation of a ray shooting algorithm for computing vertices of Newton polytopes of A-discriminants.",ucb,,https://escholarship.org/uc/item/0nw8b6kj,,,eng,REGULAR,0,0
568,2004,Mixed Conduction in Rare-Earth Phosphates,"Ray, Hannah Leung","De Jonghe, Lutgard C;",2012,"AbstractMixed Conduction in Rare-Earth PhosphatesbyHannah Leung RayDoctor of Philosophy in Materials Science and EngineeringUniversity of California, BerkeleyProfessor Lutgard C. De Jonghe, ChairThe goal of this work is to gain a better fundamental understanding of mixed electronic and protonic conduction in rare earth phosphates. Specifically, the controlled introduction of a small amount of electronic conductivity is found to enhance the proton conductivity of the (La,Ce) orthophosphates. This discovery could change the way that materials for electrolyte applications are designed. Rare earth orthophosphates for the past 20 years have been investigated for their applications as intermediate temperature fuel cell electrolytes. They are known to conduct protons, electron-holes, and oxygen vacancies, under different conditions. The rare earth phosphates are tunable in many different ways. The materials can be aliovalently doped; their metal to phosphorous ratio can be varied; and using different synthesis methods can have a large effect on their conduction properties. In addition, the (La,Ce) orthophosphates are an ideal model system for investigating mixed conduction. Systematically varying the Ce content tunes the contribution of electronic component of the total conduction. In this work, the defect chemistry of the system is discussed; several synthesis methods for the rare earth phosphate compositions are carried out; different methods for identifying the dominant charge carriers are described; and the effect of increasing the Ce content upon the total conductivity in single atmospheres and in the fuel cell configuration is measured.",ucb,,https://escholarship.org/uc/item/0ps754qc,,,eng,REGULAR,0,0
569,2005,"Agreement, case, and switch-reference in Amahuaca","Clem, Emily","Deal, Amy Rose;",2019,"This dissertation probes the nature of the syntactic operation of Agree through the lens of the morphosyntax of Amahuaca, an endangered Panoan language of the Peruvian Amazon. I take as my empirical focus two interrelated case studies in Amahuaca syntax: 1) the split ergative case system, and 2) the extensive switch-reference system. In the domain of case, I argue that overt ergative case morphology in Amahuaca expones agreement of the transitive subject with multiple functional heads. This leads to a distinction between the features needed for abstract ergative case (agreement only with v), and the features needed to trigger overt ergative case (agreement with both v and T). This distinction between abstract and morphological case factors into the analysis of the switch-reference system of Amahuaca, which I argue is sensitive to abstract case. In addition to case-sensitivity, Amahuacaâ€™s switch-reference system shows the typologically unusual property of tracking the reference of all arguments of the verb, not only subjects. I propose that this system arises through adjunct complementizer agreement that probes both the adjunct and matrix arguments directly through cyclic expansion of the probe. Through these two investigations, I conclude that Amahuaca provides support for a narrowly cyclic model of Agree in which each instance of Merge defines a new cycle of Agree (Rezac 2003, 2004; BÃ©jar and Rezac 2009). Further, the empirical facts can be most straightforwardly accounted for if we assume that some probes are insatiable, agreeing with all possible goals in their search space (Deal 2015b). Finally, despite the fact that some agreement in Amahuaca appears to be long distance, I argue that the data can be captured under the fairly conservative assumption that Agree is always under c-command and is always phase-bound.",ucb,,https://escholarship.org/uc/item/0rg4s64p,,,eng,REGULAR,0,0
570,2006,"The Far Reach of Megathrust Earthquakes: Evolution of Stress, Deformation and Seismicity Following the 2004 Sumatra-Andaman Rupture","Wiseman, Kelly Grijalva","Burgmann, Roland;",2012,"Starting with the 2004 Mw 9.2 megathrust event, Southeast Asia has been home to an exceptional amount of seismic activity over the past eight years. The series of megathrust earthquakes have been imperfect dominoes, rupturing the northernmost section of the Sunda subduction zone in 2004, then the Nias segment next in line to the south in 2005, followed by the Bengkulu earthquake âˆ¼750 km further south in 2007.  The Bengkulu earthquake skipped over the northern Mentawai segment, which has not ruptured in a great event since 1797. However, the subduction zone has not been silent is this section. Analysis of focal mechanisms and geodetic data reveals the reactivation of the Mentawai backthrust system in the overriding plate, and a large, deep earthquake near the city of Padang in 2009 is shown through finite fault inversions and aftershock analysis to have obliquely ruptured the subducting slab.  At the same time, the entire region spanning from the Indian Ocean, through the trench and forearc islands, and throughout Thailand has been aseismically deforming in response to the stress changes in the mantle following the megathrust earthquakes. Geodetic observations of postseismic deformation during the first five years following the 2004 earthquake have shown that the far-field regions of Thailand and the Malay Peninsula have moved more postseismically than coseismically, peaking at âˆ¼0.4 m of horizontal displacement in Phuket.  In 2012, the stress changes associated with this continued postseismic deformation, along with the initial push from the megathrust earthquakes, appears to have triggered the largest instrumentally recorded strike-slip earthquake.  This was a complex earthquake, consisting of four conjugate fault segments, that ruptured the diffuse India-Australia plate boundary zone. Understanding how the faults interact throughout the subduction system, from the incoming plate, to the slab, to the megathrust interface, and overriding plate is an essential part of determining the future seismic hazard for Southeast Asia.",ucb,,https://escholarship.org/uc/item/0rw50461,,,eng,REGULAR,0,0
571,2007,Characteristics of the Emergent Disease Batrachochytrium dendrobatidis in the Rana muscosa and Rana sierrae Species Complex,"Tunstall, Tate Scott","Briggs, Cherie J;Moritz, Craig;",2012,"The fungal pathogen Batrachochytrium dendrobatidis, or Bd,  has been a major driver of amphibian extinctions world wide. This dissertation investigates the effects of Bd on the mountain yellow legged frogs, Rana muscosa and Rana sierrae. These two species occur in the Sierra Nevada in California, and have under gone dramatic declines in part due to the invasion of Bd. Even before the invasion of Bd, populations of Rana muscosa/sierrae have faced habitat loss and fragmentation from introduced predators and habitat fragmentation. The fist chapter of this dissertation investigates how genetic diversity effects $R_0$, the threshold for invasion of a pathogen. I model the case where a naive population faces multiple disease, as well as a single population faced with invasion of a single disease. I also model the effect of overdominance and genetic diversity on R_0. In my second chapter, I use both models and simulations to investigate how genetic diversity effects the final size of an epidemic, and whether or not populations with higher genetic diversity maintain that diversity after an epidemic. In my third chapter, I present data on a series of experiments on how variation in the external source of Bd, or zoospore pool, affects the growth rate of Bd on its amphibian host, as well as host mortality. In my final chapter, I use microsatellite markers to measure genetic diversity in serveral populations of Rana muscosa and  Rana sierrae, and test whether or not these populations have experienced historic bottlenecks.",ucb,,https://escholarship.org/uc/item/0w45b27v,,,eng,REGULAR,0,0
572,2008,Three Essays in Operations and Marketing,"Ke, Te","Shen, Zuo-Jun;Villas-Boas, Miguel;",2015,"My thesis consists of three essays in the field of operations management and marketing.In the first essay, I study the problem of consumer search for information on multiple products. When a consumer considers purchasing a product in a product category, the consumer can gather information sequentially on several products. At each moment the consumer can choose which product to gather more information on, and whether to stop gathering information and purchase one of the products, or to exit market with no purchase. Given costly information gathering, consumers end up not gathering complete information on all the products, and need to make decisions under imperfect information. I solve for the optimal search, switch, and purchase or exit behavior in such a setting, which is characterized by an optimal consideration set and a purchase threshold structure. It is shown that a product is only considered for search or purchase if it has a sufficiently high expected utility. Given multiple products in the consumer's consideration set, the consumer only stops searching for information and purchases a product if the difference between the expected utilities of the top two products is greater than some threshold. Comparative statics show that negative information correlation among products widens the purchase threshold, and so does an increase in the number of the choices. Under my rational consumer model, I show that choice overload can occur when consumers search or evaluate multiple alternatives before making a purchase decision. I also find that it is optimal for sellers of multiple products to facilitate information search for low-valuation consumers, while obfuscate information for those with high valuations.In the second essay, I conduct an empirical study of peer effects of iPhone adoptions on social networks. I use a unique data set from a provincial capital city in China, in a span of over four years starting from iPhone's first introduction to mainland China. I construct a social network using six month's call transactions between iPhone adopters and all other users on a carrier's network. Strength of social ties is measured by duration of calls. Based on the network structure, I test whether an individual's adoption decision is influenced by his friends' adoptions. A fixed-effect model shows that, on average, a friend's adoption increases one's adoption probability in next month by 0.89%, and the marginal effect decreases in the size of his current neighboring adopters. To further control for potential time-varying correlated unobservables, I instrument adoptions of one's friends by their birthdays, based on the fact that consumers are more likely to adopt iPhones on birthdays. The IV estimation shows a slightly smaller peer effect at 0.75%. I also investigate how network structures modulate the magnitude of peer influence. My results show that peer effect is stronger when the influencer has more friends or has a stronger relationship with the influence.In the third essay, I study the problem of coordination of operations and marketing decisions for new product introductions. In the industry with radical technology push or rapidly changing customer preference, it is firms' common wisdom to introduce high-end product first, and follow by low-end product line extensions. A key decision in this ""down-market stretch"" strategy is the introduction time. High inventory cost is pervasive in such industries, but its impact has long been ignored during the presale planning stage. This essay takes a first step towards filling this gap. I propose an integrated inventory (supply) and diffusion (demand) framework, and analyze how inventory cost influences the introduction timing of product line extensions, considering substitution effect among successive generations. I show that under low inventory cost or frequent replenishment ordering policy, the optimal introduction time indeed follows the well-known ""Now"" or ""Never"" rule. However, sequential introduction becomes optimal as the inventory holding gets more substantial or the product life cycle gets shorter. The optimal introduction timing can increase or decrease with the inventory cost depending on the marketplace setting, requiring a careful analysis.",ucb,,https://escholarship.org/uc/item/0zm332rp,,,eng,REGULAR,0,0
573,2009,"Response of Liquefiable Sites in the Central Business District of Christchurch, New Zealand","Markham, Christopher Stephen","Bray, Jonathan D;",2015,"The strong shaking of the 2010-2011 Canterbury earthquake sequence caused widespread liquefaction in much of the city of Christchurch, NZ, including large parts of the central business district (CBD). The most intense event of the earthquake sequence with regards to shaking in the CBD was the 22 FEB 2011 Mw6.2 Christchurch event. A reported 185 casualties were caused by the destruction from this event. The financial damage was immense as many of the structures, especially in the CBD, had to be demolished due to damage sustained from the Christchurch earthquake as well as some of the other less intense events. The performance of structures during these events was often related to liquefaction of foundation soils. Understanding the effects of liquefaction on soil and building response is an area of earthquake engineering that continues to challenge practitioners and researchers. This challenging topic makes the study of case histories, such as those provided by the Canterbury earthquakes, an essential component to characterizing and understanding the effects of soil liquefaction on building performance. This thesis focuses on providing insights regarding the seismic response of liquefiable soils through the use of information and data collected in Christchurch, NZ following the Canterbury earthquake sequence.     Nonlinear effective stress site response analyses are often used by engineers to model the dynamic response of potentially liquefiable soils during strong shaking. For the presented research, a widely used one-dimensional nonlinear effective stress site response analysis program is used to perform this modelling. Ground motions recorded during six events of the 2010-11 Canterbury earthquake sequence are used in conjunction with the extensive site investigation data that has been obtained in Christchurch to complete site response analyses at several strong motion station sites in the greater Christchurch area. Deconvolved Riccarton Gravel input motions were generated, because representative, recorded rock or firm layer base-motions were not available in the Christchurch area. Nonlinear effective stress seismic site response analyses are shown to capture key aspects of the observed soil response through the comparison of acceleration response spectra of calculated surface motions to those of recorded surface motions; however, equivalent-linear and total stress nonlinear analyses are shown to capture these aspects as well. Biases in the computed motions compared to recorded motions were realized for some cases, but they can be attributed primarily to the uncertainty in the development of the input motions used in the analyses.     The study of the consequences of liquefaction on building performance is a complex soil-structure interaction problem that requires the use of well-documented case histories for validation purposes. An extensive site investigation and advanced laboratory testing program was carried out in Christchurch, NZ from April to October of 2014. The aim of this work was to provide characterization of the liquefaction resistance of foundation soils from building sites affected by liquefaction during the Canterbury earthquakes. In-situ penetration tests, such as CPT, are valuable methods for gaining an initial understanding of a siteâ€™s characteristics and the expected performance of critical soil layers. However, to understand fully the complex response of soil at the element level, laboratory testing of relatively undisturbed soil specimens provide unique insights. To accomplish these goals, â€œundisturbedâ€ sampling and triaxial testing (monotonic and cyclic) were performed on soils from key building sites in Christchurchâ€™s CBD. High quality sampling and testing could be achieved for most of the predominantly silty and sandy soils in the CBD. Test results indicate, though, that loose clean sand specimens were densified significantly during the sampling with the Dames & Moore hydraulic fixed-piston sampler (an Osterberg-type thin-walled sampler). The cyclic resistances measured in the tests on â€œundisturbedâ€ specimens were generally consistent with those estimated using empirical simplified liquefaction triggering procedures.      Important insights regarding the cyclic response of the shallow CBD soils were obtained through the laboratory testing carried out as a part of the research presented in this thesis. Triaxial testing of â€œundisturbedâ€ soil specimens proved important in understanding not only the stress-strain response of the studied soils, but also allowed for further knowledge regarding the pore water pressure response of the tested soils during both cyclic and monotonic loading. Importantly, insights into how various soil types of the CBD responded to cyclic loading were gained through comparisons of cyclic triaxial (CTX) tests performed on a variety of sand and silty sand soils. It was seen through CTX results comparisons that silty sands (soils classified as SM) and clean sands (SP and SP-SM) responded similarly in cyclic loading, even when the fines content of the tested specimens differed. Monotonic triaxial testing was also performed on reconstituted specimens to characterize the steady state response of several soil units in the CBD. The extensive insight garnered from laboratory testing is critical for informing researchers and engineers studying the case histories provided by the Canterbury earthquakes of buildings founded on liquefiable soils, especially those using numerical-based soil-structure interaction analyses. The results of the monotonic and cyclic tests performed as part of this study provide useful data for calibrating advanced numerical models.      Appendices are included as a part of this dissertation to provide supporting information and data not included in the main body of the thesis. A majority of these appendices are included at the end of this thesis; however, there are several additional appendices included as electronic attachments to this dissertation, which provide supplementary information regarding the work presented in Chapters 2 through 4 of this dissertation. Electronic appendices that support the work presented in Chapter 2 include: rotated (fault normal and fault parallel) seismic records for events studied at the strong motion station sites of interest (zip file that contains text files), deconvolved Riccarton Gravel motions for all events studied (zip file that contains text files), and selected results for completed site response analyses (PDF file). Electronic appendices that support the work presented in Chapters 3 and 4 include triaxial test data and results for â€œundisturbedâ€ soil specimens (zip file that contains text, data, and PDF files).",ucb,,https://escholarship.org/uc/item/0zs5q7j3,,,eng,REGULAR,0,0
574,2010,Using Multiple Representations to Resolve Conflict in Student Conceptual Understanding of Chemistry,"Daubenmire, Paul Lemuel","Stacy, Angelica M;",2014,"Much like a practiced linguist, expert chemists utilize the power and elegance of chemical symbols to understand what is happening at the atomic level and to manipulate atoms and molecules to effect an observable change at the macroscopic level.  Unfortunately, beginning chemistry is often taught in a way that emphasizes memorizing the symbolic representations of equations and reactions without much opportunity to meaningfully connect the observable macroscopic phenomena with an understanding of the chemistry taking place at the atomic level.  The compartmentalized manner of chemistry instruction in most chemistry classrooms further nullifies the efficacy of the triplet relationship to connect between macroscopic observations, symbolic representations, and atomic scale views. If symbolic representations are presented as the goal of instruction, rather than as the means to gain understanding, then students will be impaired in developing a coherent understanding of chemical principles.This dissertation describes the development and implementation of an interview study to examine how undergraduate students interpreted multiple representations of a chemical equilibrium.  To establish a baseline of ideas, students first were coached to verbally generate successive representations.  They were then cued to think about the chemistry occurring between atoms and ions at the molecular level.  Next, an experiment involving a change in states of matter and color was performed which paralleled the symbolic representations.  Through self-explanations and verbalizing of conjectures, students were encouraged to explore, interpret, and refine their understanding of the observations related to the chemical symbols presented to them.  Finally, with the goal of fostering a deeper understanding of the process of equilibrium, a dynamic visualization of the molecular level was introduced as a tool for helping students connect these multiple representations.This study revealed that one way in which students develop conceptual understanding and resolve conflicts between different representations of the same phenomena is by verbalizing their ideas as a conjecture (as a verbal explanation to advance towards a hypothesis).  Thus, it is proposed that symbolic representations are most effective viewed not as an end goal but as a bridge for connecting macroscopic, visible phenomena with what is occurring at the molecular, invisible level. When the focus on merely memorizing chemical equations and symbols is removed, students can gain a coherent understanding of the meaning available when multiple representations are viewed together.",ucb,,https://escholarship.org/uc/item/10c4h3dn,,,eng,REGULAR,0,0
575,2011,Teacher Educators: Addressing the Needs of All Learners,"Cook, Ellen","Little, Judith W;",2017,"This qualitative dissertation examines how teacher preparation programs take up policy messages from two state agencies. These questions guided the study: 1) What are the messages about RTI and MTSS from the California Department of Education and the California Commission on Teacher Credentialing; and, 2) How are RTI and MTSS taken up by preparation programs and understood by teacher educators? The research sites were two large public universities in urban areas in California with general and special education teacher preparation programs. Data was obtained from the websites of the CDE and CCTC, interviews with teacher educators, and preparation program materials.Findings support that the CDE is the major proponent of RtI and MTSS but lacks the regulatory power to impact teacher preparation. While the CCTC did have regulatory authority over teacher preparation its guidelines were written in broad language and did not require that programs include specific practices necessary to implement RtI and MTSS.  Without this regulatory press incorporation of RtI and MTSS was at the discretion of faculty. Special education was the source of knowledge about RtI and MTSS despite the fact that the CDE clearly situates these practices in general education. General education teacher educators were ambivalent about RtI and MTSS and were unwilling to sacrifice precious time during initial preparation to these topics. These findings have implications for policy makers, teacher educators, and research on policy implementation.",ucb,,https://escholarship.org/uc/item/10k957bt,,,eng,REGULAR,0,0
576,2012,Molecular and Stochastic Biophysical Modeling of mRNA Export and Quality Control,"Soheilypour, Mohammad","Mofrad, Mohammad R.K.;",2019,"Molecular systems orchestrating the biology of the cell typically involve a complex web of interactions among various components and span a vast range of spatial and temporal scales. Export and quality control of messenger ribonucleic acids (mRNAs) feature a prominent example of such an intricate molecular system. Export of mRNAs into the cytoplasm is a fundamental step in gene regulation processes, which is meticulously quality controlled by highly efficient mechanisms in eukaryotic cells. Despite extensive research on how mRNAs are quality controlled prior to export into the cytoplasm, the exact underlying mechanisms are still under debate. Specifically, it is unclear how aberrant mRNAs are recognized and retained inside the nucleus. Computational methods have advanced our understanding of the behavior of molecular systems by enabling us to test assumptions and hypotheses, explore the effect of different parameters on the outcome, and eventually guide experiments. In this dissertation, I present my research on mRNA quality control using different computational techniques.Using the agent-based modeling (ABM) approach, which is an emerging molecular systems biology technique for exploring the dynamics of molecular systems/pathways in health and disease, we first developed a minimal model of the mRNA quality control (QC) mechanism. Our results suggested that regulation of the affinity of RNA-binding proteins (RBPs) to export receptors along with the weak interaction between the RBPs and nuclear basket proteins, namely myosin-like protein-1 (Mlp1) or translocated promoter region (Tpr) protein, are the minimum requirements to distinguish and retain aberrant mRNAs. In addition, we demonstrated how the length of mRNA may affect the QC process.The interaction between Mlp1 with one of the Saccharomyces cerevisiae RBPs, namely the nuclear polyadenylated RNA-binding protein (Nab2), was then investigated. Mlp1 plays a substantial role in mRNA quality control by interacting with other proteins involved in this process, specifically the RBPs. Yet, the mechanism of the interaction between Mlp1 and RBPs is still elusive. Using an array of integrated computational approaches including protein structure prediction, protein-protein docking, and molecular dynamics simulations, we dissected Mlp1-Nab2 interaction. Our results were consistent with experimental observations, which suggested that Nab2 residue F73 is essential for Mlp1 binding and further uncovered an indirect role of Nab2-F73 in this interaction.",ucb,,https://escholarship.org/uc/item/0wd4k702,,,eng,REGULAR,0,0
577,2013,"Soil Formation and Transport Processes on Hillslopes along a Precipitation Gradient in the Atacama Desert, Chile","Owen, Justine J.","Amundson, Ronald;",2009,"The climate-dependency of the rates and types of soil formation processes on level landforms has been recognized and documented for decades. In contrast, methods for quantifying rates of soil formation and transport on hillslopes have only recently been developed and the results suggest that these rates are independent of climate. One explanation for this discrepancy is that hillslopes and their soil mantles are dynamic systems affected by local and regional tectonic effects. Tectonics can change local or regional baselevel which affects the hillslope through stream incision or terrace formation at its basal boundary. Another explanation is that in most of the world hillslope processes are biotic, and biota and their effects vary nonlinearly with climate. The effects of both tectonics and life can obscure climatic effects. Recent studies have been made to isolate the climatic effect on hillslope processes, but they are few and focus on humid and semiarid hillslopes.In order to isolate the effects of boundary condition, precipitation, and life, I studied pairs of hillslopes in northern Chile in semiarid, arid, and hyperarid climates. In each pair, one hillslope was bounded by an incising (bedrock-bedded), first-order channel, and the other was bounded by a low-slope, non-eroding surface. This precipitation gradient spans the transition from biotic to abiotic landscapes. The guiding framework for this study is a hillslope soil mass balance model in which the soil mass is controlled by the balance of soil production from bedrock and from atmospheric input, and soil loss through physical and chemical erosion. My objectives were to quantify the components of the mass balance model, identify the processes driving soil production from bedrock and soil transport, and interpret this data in the context of climate and hillslope morphology. In the field, I made observations of the processes driving soil formation and transport, surveyed the hillslopes to produce high-resolution topographic maps, and sampled soils and rock for chemical analysis and particle size analysis. Dust collectors were erected to measure atmospheric input. Bedrock and surface gravel samples were collected in order to calculate the rate of soil production from bedrock, the incision rate of the channels, the age of the non-eroding surfaces, and the exposure history of surface gravels using the concentrations of in situ-produced 10Be and 26Al. Rates of physical and chemical erosion were calculated using the soil mass balance model, based on the rate of soil production from bedrock, the atmospheric deposition rate, and the concentrations of an immobile element in the soil, bedrock, and atmospheric input. In addition, to understand the effect of precipitation on the landscape and to quantify the infiltration rate of the soil, sprinkling experiments were conducted in each climate region and infiltrometer measurements were made in the hyperarid region.The effect of boundary condition on soil thickness was observed in all climate zones, with thicker soils on hillslopes with non-eroding boundaries compared to hillslopes bounded by channels. However, the expected effect of boundary condition on the rates of soil production from bedrock, with slower erosion rates on hillslopes with non-eroding boundaries, decreased as precipitation decreased. In contrast to previous work on wetter hillslopes which showed little climatic sensitivity, rates of soil production from bedrock increase with precipitation following a power law, from ~ 1 m My-1 in the hyperarid region to ~ 40 m My-1 in the semiarid region. A geomorphic and pedologic threshold was observed at mean annual precipitation (MAP) ~100 mm, marked by changes in soil chemistry and thickness, types of erosion mechanisms, and rates of soil production. In the semiarid region, where MAP = 100 mm, the hillslopes are soil-mantled with a relatively thick, chemically-weathered soil where MAP is high enough to support coastal desert vegetation. Soil formation and transport is primarily through bioturbation. As MAP decreases to 10 mm in the arid region, the hillslopes are nearly soil- and plant-free, and soil transport is through overland flow, rather than bioturbation. In the hyperarid region, where MAP is <2 mm, the hillslopes are mantled with salt-rich soils which are primarily derived from atmospheric input rather than bedrock erosion. Soil transport is through overland flow and likely some salt shrink-swell.The spatially-explicit physical erosion rates were used to test the applicability of four soil transport models. Where bioturbation is active, soil transport is slope- and depth-dependent. In the plant-free regions, soil transport is a function of slope and distance downslope. The transport coefficients in the transport models increase several orders of magnitude with increasing MAP. A comparison of these values with those determined on wetter hillslopes suggests that at MAP<100 mm, transport coefficients are a function of MAP. Where MAP>100 mm, they are a function of the types of organisms driving bioturbation and other soil properties. This threshold corresponds to the MAP below which there is a dramatic decrease in net primary productivity (NPP), and suggests that hillslope process rates are sensitive to MAP where the effect of life is small.A unique feature on the hyperarid hillslopes was darkly-varnished, contour-parallel bands of gravels on the soil surface which I call ""zebra stripes"". Based on cosmogenic radionuclide concentrations in surface gravel and bedrock, as well as salt deposition rates from the atmosphere and content in the soils, I propose that the salt-rich soils began accumulating >0.5-1 Ma and the zebra stripes formed in the last 103-105 y. The zebra stripe pattern has been preserved due to the self-stabilization of the gravels within the stripes and the continued absence of life (which would disturb the surface, as seen at the arid site). The accumulation of the salt-rich mantle and the formation of zebra stripes suggest a profound climatic change occurred sometime between the late Pliocene and early Holocene.The Atacama Desert provides a multi-million year-old experiment testing the effect of water and life on geophysical and geochemical processes. In contrast with portions of the planet where biota modulates soil production and erosion through complex and rapid feedbacks, this work shows that the absence of biota in the driest parts of the Atacama Desert results in the rates and mechanisms of geomorphic processes being extremely precipitation-sensitive. This unusual environment, for Earth, illuminates the uniqueness and complexity of a planet whose surface bears the indelible imprint of life.",ucb,,https://escholarship.org/uc/item/0wm2q0dx,,,eng,REGULAR,0,0
578,2014,Essays in Development Economics,"Hicks, Joan Hamory","Miguel, Edward;",2009,"According to the World Bank's World Development Report 2007, there are 1.3 billion young adults aged 12-24 living in less-developed countries today.  Individuals in this age group are going through a period of tremendous flux in their lives as they embark on marriage, job searches or higher education, and their experiences during this time will shape the next generation of decision makers.  Research focusing on the choices of these individuals, as well as the circumstances under which they are made, is urgently needed.  The present collection of essays seeks to advance such research by utilizing a recent longitudinal survey to examine the decisions of young adults in rural Kenya as they relate to education, migration and behavior in the wake of violent civil conflict.Chapter 1 explores the extent to which individual academic and cognitive ability is factored into household decisions concerning education.  Panel information on schooling for nearly 1,900 rural Kenyan youth over the period 1998-2008 is combined with satellite precipitation data in order to examine the effects of agricultural income variability on school attendance.  A unique early-age academic test score proxies for child ability.  Regression analysis indicates that during times of plenty, there is an 11 percent increase in attendance of high ability relative to low ability individuals, suggesting households recognize and value ability when making schooling decisions. This finding is framed using a model of human capital accumulation in which schooling decisions are a function of individual ability. Surprisingly, although youth on the whole are less likely to attend school during negative income shocks, there are no differential attendance changes across individuals of different ability levels.  Instead, credit constraints and income shocks may work together in this setting to limit desirable human capital investments.  Such consumption smoothing behavior could imply negative long-term effects on household welfare.Chapter 2 studies selective migration among 1,500 Kenyan youth originally living in rural areas.  In particular, this essay examines whether migration rates are related to individual â€œabilityâ€, broadly defined to include cognitive aptitude as well as health, and then uses these estimates to determine how much of the urban-rural wage gap in Kenya is due to selection versus actual productivity differences. Whereas previous empirical work has focused on schooling attainment as a proxy for cognitive ability, the present research employs an arguably preferable measure, a pre-migration primary school academic test score. Pre-migration randomized assignment to a deworming treatment program provides variation in health status.  Results suggest a positive relationship between both measures of human capital (cognitive ability and deworming) and subsequent migration, though only the former is robust at standard statistical significance levels. Specifically, an increase of two standard deviations in academic test score increases the likelihood of rural-urban migration by 17%. In an interesting contrast with the existing literature, schooling attainment is not significantly associated with urban migration once cognitive ability is accounted for. Accounting for migration selection due to both cognitive ability and schooling attainment does not explain more than a small fraction of the sizeable urban-rural wage gap in Kenya, suggesting that productivity differences across sectors remain large.Finally, Chapter 3 examines the socioeconomic impacts of two months of protests and violent, primarily ethnic-based clashes that erupted across central and western Kenya in late 2007 following the controversial conclusion of a heavily-contested presidential election.  Although not an epicenter of the conflict, Busia District experienced sporadic unrest, an influx of refugees from other parts of Kenya, inflation, supply shortages, and local market closures.  Unique and timely survey data collected from young adults living primarily in this district of rural western Kenya in the months surrounding the election permits the use of both differences-in-differences and propensity score matching methodologies to estimate the short- to medium-run impacts of this conflict, and both approaches yield broadly similar findings.  Despite little support for lasting effects on labor, migration and nutritional outcomes within weeks of cessation of the violence, there do appear to be persistent consequences for social cohesion and informal financial activities.  While there is little indication of change in survey respondents' self-reported attitudes regarding trust of others, analysis confirms large declines in attendance at religious services, participation in community and bible groups, and utilization of non-family members as points of contact for future survey enumeration efforts.  These findings highlight a disconnect between reported attitudes and observable behavior.  Furthermore, respondents are between 29 and 53 percent less likely to engage in informal lending and transfers post-conflict.  Given the key role played by social networks in informal financial markets in less-developed countries, these results indicate that even brief civil unrest may have lasting negative consequences.",ucb,,https://escholarship.org/uc/item/0x91n32b,,,eng,REGULAR,0,0
579,2015,Improving child survival with biannual distribution of azithromycin: an exploration of optimal program design,"O'Brien, Kieran S","Reingold, Arthur L;",2020,,ucb,,https://escholarship.org/uc/item/0xx4p7,,,eng,REGULAR,0,0
580,2016,Understanding the effect of modifying elements in supported vanadia bilayered catalysts for methanol oxidation to formaldehyde,"Vining, William Collins","Bell, Alexis T;",2011,"The field of heterogeneous catalysis has long been interested in understanding the role of site structure on reactivity and selectivity for the rational design of catalysts.  Vanadia is of particular interest because of its potential to be highly active and selective for a variety of reactions, such as oxidative dehydrogenation of alkanes to alkenes, or the oxidation of n-butane to maleic anhydride.  When supported, vanadia can exist in several environments depending on its surface coverage.  At the lowest loadings, below 2 V/nm2, the vanadium exists predominantly in well dispersed, tetrahedral structures with 3 V-O-support bonds and 1 V=O bond.  At higher loadings, above 2 V/nm2, V-O-V bonds form on the surface, and at loadings above 7 V/nm2, the vanadia begins to form 3 dimensional domains of V2O5. Methanol oxidation rates over catalysts with varying vanadia loadings have shown no significant effect of the V surface density on the formaldehyde formation rate. However, significant differences in the formaldehyde production rates are observed for different supports. Changing the support from silica to titania or zirconia, will result in increases in the production of formaldehyde from methanol by over an order of magnitude for similar vanadia surface coverages. These differences in rate are observed even though the reaction mechanism is believed to be the same regardless of the support. The mechanism is thought to proceed as follows. First methanol dissociatively adsorbs across a V-O-support bond, producing V-OCH3 and M-OH (M = Si, Ti, Zr, Ce) in a quasi-equilibrated step.  Next a surface oxygen abstracts hydrogen from the methoxy group in the rate determining step, and formaldehyde desorbs.  The final steps are fast and involve the production of H2O from neighboring hydroxyls and the reoxidation of the catalyst by gas-phase oxygen. When vanadia is supported on bulk TiO2, ZrO2, or CeO2, the support surface area is relatively small (~200 m2/g at its highest), and the bulk support causes side reactions which make it difficult to understand the role of the vanadia.  Furthermore, by using a bulk support, only the vanadia surface coverage can be varied, such that the effect of different V structures can be elucidated, but not that of V-O-M bonds. Therefore, high surface area silica with a variable coverage of two-dimensional TiO2, ZrO2, and CeO2 layers are used to support isolated vanadate structures to vary the quantity of vanadia bound to the modifying layer. These bilayered catalysts can be used to determine the effect of the V-O-support bonds on the formaldehyde production rate.Three mesoporous silica supports, MCM-48 (1550 m2/g), MCM-41 (1353 m2/g), and SBA-15 (700 m2/g) were used as the high surface area silica.  Ti was grafted to the MCM-48 surface using Ti(OiPr)4 and a maximum surface coverage of 2.8 Ti/nm2 was obtained after 3 graftings. The grafting of zirconium was performed using Zr-2-methyl-2-butoxide on MCM-41, and a maximum loading of 2.1 Zr/nm2 was achieved after 3 graftings. The final modifying element, cerium, was grafted onto SBA-15 using Ce(OtBu)4 for a maximum surface coverage of 0.9 Ce nm-2. After treating the MOx/SiO2 (M = Ti, Zr, Ce) samples in air to remove any organic ligands, OV(OiPr)3 was grafted onto the MOx/SiO2 (M = Ti, Zr, Ce) support to achieve the desired V surface coverage of approx. 0.7 V/nm2.The resulting catalysts contain amorphous two-dimensional layers of TiO2, ZrO2, or CeO2 with V existing in a pseudo-tetrahedral structure on the surface. As the surface density of the modifying element layer increases, the quantity of vanadia bound to TiO2, ZrO2, or CeO2 increases. For the VOx/ZrO2/SiO2 catalysts, the fraction of vanadia bound to the zirconia layer was able to be quantified and determined to be 35% of all V for a Zr surface density of 2.8 Zr nm-2. Even for small quantities of modifying elements (0.2 M nm-2), the apparent rate constant for formaldehyde production on VOx/MO2/SiO2 (M = Ti, Zr, Ce) is an order of magnitude higher than for VOx/SiO2 catalysts at the same V surface density. Regardless of the modifying element used, the increase in apparent rate constant is comparable for all catalysts. As the modifying element surface density is increased, the apparent rate constant also increases, which is a result of an increasing fraction of V bound to the MOx layer. Each of these bilayered catalysts can be described using a two-site model of VOx/SiO2 and VOx/MO2 (M = Ti, Zr, Ce) with the latter being responsible for the increased apparent rate constant. This higher activity for the VOx/MO2 site is due to a lower apparent activation energy. For VOx/SiO2, the apparent activation energy is 23 kcal mol-1, but is approximately 17 kcal mol-1 for VOx/MO2 sites. The apparent activation energy can be expressed as the sum of the heat of methanol adsorption and the activation energy for H-abstraction. My results indicate that the lower apparent activation energy observed for the bilayered catalysts is a result of a decrease in the activation energy for H-abstraction. This lower energy pathway occurs because the MOx layer can abstract H from surface methoxy groups. For VOx/SiO2, however, the vanadyl oxygen abstracts H in a higher energy step.",ucb,,https://escholarship.org/uc/item/0zd9w5sw,,,eng,REGULAR,0,0
581,2017,Developing Small-Area Health and Exposure Data for the Use in Environmental Public Health Tracking,"Ortega Hinojosa, Alberto Manuel","Jerrett, Michael;",2013,"The turn of the millennium has been accompanied by a rapid growth in data collection along with an increasing ability to store, manipulate and analyze it. In tandem with this development in technology and surveillance, there has been a growing understanding of the importance of the socio-physical environment on population behavior and human health. We capitalize on this progress to develop a methodology by which we develop two macro scale datasets, one national and one state-wide, to support the efforts of the Centers for Disease Control and Prevention's Environmental Public Health Tracking Network (EPHTN) in understanding two important health risks: smoking and obesity. Moreover, we use new geographic information science techniques, spatial statistics methodologies and machine learning algorithms to gain a better understanding of the relationship between spatial patterns in physical and socio-demographic characteristics and health risks. Specifically, we address three specific aims: (1) To use current data systems to develop national small-area predictions of adult smoking and obesity for the EPHTN and research; (2) To evaluate current data systems and spatial analysis tools available to describe the within-school environment and the school-neighborhood socio-physical characteristics of California's public schools thought to influence childhood obesity, and use these to develop a comprehensive multilevel dataset for the EPHTN and research; (3) To test the utility of the dataset developed in aim 2 by applying it to an analysis used to increase the understanding of the relationship between the school environment and childhood obesity by examining the relative importance of school attributes. This analysis determines that current data collection systems provide a valuable resource which we combine for the ease of future research use. We demonstrate that using the spatial structure and socio-demographic patterns of health risks, we are able to downscale adult smoking and obesity prevalence to the ZIP code and census tract levels for the conterminous United States, and develop five-year predictions for these for the four quinquennia in the 1991-2010 time period. Lastly, we confirm the utility of the school dataset and determine through the third aim that individual demographics and the social environment seem to be the predominant determinants of childhood obesity.",ucb,,https://escholarship.org/uc/item/11n7g50q,,,eng,REGULAR,0,0
582,2018,"The Revolution Will Be Televised: Identifying, Organizing, and Presenting Correlations Between Social Media and Broadcast Television","Riley, Patrick F","Braunstein, Yale;",2011,"Many popular facets of live information, known collectively as communication technology, deliver ongoing, socially-relevant narrations of our world. Traditionally, different types of communication media were considered to be in competition, but recently they have been discovered to be complementary and synergistic. This paper will concentrate on the role, influence, and potential of the integration of various types of communication media with particular attention to the recent phenomenon of social media and broadcast television.This research demonstrates how to develop a listening platform that enables one to capture, parse, analyze, cluster and present correlations between social media and broadcast television.  The listening platform, called LiveDash for this research, uses a number of innovative media real-time media capturing, link analysis, clustering, and social networking analysis techniques to provide an immediate system of record of what is being mentioned on television, social networks, and journalism wires. Using media servers designed specifically for this project, LiveDash captures and records all national television channels from the United States, and incrementally indexes what is said on Broadcast Television in real-time. LiveDash also crawls the links people share on Twitter, Digg, and other social sharing services, and indexes the content on those pages as soon as they are shared, allowing users to find the newest, most socially-relevant content in real-time.Due to this sophisticated system, it is now possible to identify and comprehend the correlations of information between social media and broadcast television.  Moreover, by conducting annual surveys and user experience research, this research explores the simultaneous usage of social media and broadcast television by users.  With over two million unique visits to Livedash.com last year alone, this research demonstrates that not only are users increasingly using social media and broadcast television simultaneously, but also value interactivity and contextual, correlated information to augment their broadcast television experience.  LiveDash and its mobile device component, LiveDash Mobile, show how users react, process, and value additional computer recommended social media while watching broadcast television. A REST API for the LiveDash system has also been made available for academic uses and further research.",ucb,,https://escholarship.org/uc/item/1212g99h,,,eng,REGULAR,0,0
583,2019,Differentiation of Exposure and Disease-Related Biomarkers Associated with Colorectal Cancer,"Perttula, Kelsi Michel","Rappaport, Stephen M.;",2017,"Chronic diseases such as cardiovascular disease, diabetes, and cancer are the leading causes of death among developed and developing countries, and account for approximately 75 percent of deaths worldwide. With the sequencing of the human genome and subsequent genomic studies, we now know genetic factors alone are responsible for a relatively small portion of these diseases. Specifically, cancer risk attributed to genetic factors is typically about eight percent. Thus, the vast majority of cancer risk likely lies within the realm of exposures (non-genetic factors) or a combination of genetic factors and exposures.  The collection of exposures over an individualâ€™s lifetime comprise the concept of the exposome, an epidemiological complement to the genome. The exposome is defined by measurement of both endogenous (inflammation, lipid peroxidation, microbiota) and exogenous (air pollutants, pesticides, drugs, diet, etc.) exposures within an individual.Much exposure data is from non-individualized sources, such as air quality monitors or other spatial-temporal data, which have limited use in epidemiology.  Individual exposure assessment consists largely of self-reported dietary and lifestyle data from interviews or questionnaires. In recent years, advances in analytical chemistry have permitted the simultaneous detection of thousands of molecules in biological fluids including urine, whole blood, plasma, and serum.High resolution liquid chromatography mass spectrometry (LCMS) is a powerful technique to measure the accurate masses of molecules in biological fluids for high-throughput epidemiological studies. Chapter 1 of this dissertation details a method for the analysis of lipophilic molecules in plasma using specimens from 158 healthy volunteer subjects. The resulting data revealed levels of lipids and other molecules that differed between smoking and nonsmoking, white and black, and male and female subjects. A modified version of this LCMS method was used in the analysis of serum from subjects in a nested case-control study, described in Chapters 2 and 3.Colorectal cancer (CRC) accounts for one fourth of all cancer deaths worldwide and less than about 15 percent of CRC risk is attributable to genetic factors alone. To investigate possible influences of exposures on CRC risk, serum from 190 subjects in the European Prospective Investigation of Cancer and Nutrition (EPIC) were extracted for lipophilic molecules and analyzed with high resolution LCMS. These prospective samples â€“ collected up to 22 years prior to diagnosis - offered a unique opportunity to differentiate between CRC biomarkers related to disease causes and those that result from disease progression. Chapter 2 describes the testing of one class of lipids, ultra-long chain fatty acids (ULCFAs), that had been reported as a probable protective factor of CRC. Paired case-control differences were assessed with respect to the time period from when the serum was collected (study enrollment) to when the case was diagnosed. Since, case-control differences decreased with increasing time prior to case diagnosis, ULCFAs were likely depleted by cancer progression rather than by protective exposures. Many of the features in LCMS profiling are unannotated (identity unknown) chemicals. Rather than relying on hypothesis-driven analyses of only known compounds, data-driven analyses of reliably detectable features can result in the generation of new hypotheses of possible disease-causing exposures. This untargeted methodology, used in Chapters 1 and 3, makes lipidomic and other exposure-related profiling a powerful tool in exposure assessment. In Chapter 3, the untargeted analysis of features in EPIC CRC serum samples revealed potentially relevant molecules associated with CRC causes and disease progression. As opposed to traditional p-value-centric analysis used in Chapters 1 and 2, a combination of regularized regression, random forest, and t-tests were employed in the feature selection for this untargeted analysis.In Chapter 4, the lipophilic data from the healthy volunteer samples of Chapter 1 are studied once again. Using a method similar to the regularized regression technique described in Chapter 3, we determined which lipids were associated with levels of adductomic biomarkers (another methodology developed in our laboratory), which had also been measured in plasma from the same healthy volunteers. Analysis of the combined data from these two OMIC datasets found interesting correlations between particular lipids and adducts in these samples.",ucb,,https://escholarship.org/uc/item/1394p43q,,,eng,REGULAR,0,0
584,2020,Finding the Optimal Shape of an Object Using Design-By-Morphing,"Oh, Sahuck","Marcus, Philip S.;",2016,"We present a new design method, which we call design-by-morphing, for the optimal design of the shape of an object. The surface of one or more objects (or the sub-objects from which it is composed) is represented as a truncated series of exponentially-convergent spectral basis functions multiplied by spectral coefficients. A morphed object or sub-object is obtained from a new set of spectral coefficients, which are a weighted average of the spectral coefficients of the original objects or sub-objects from which it is morphed. Optimized designs are created by choosing the weights such that a cost function of the new morphed shape is minimized. The boundaries of an object and the interfaces between sub-objects can be forced to satisfy geometric constraints on their shapes, slopes, curvature, etc. With these constraints, sub-objects can be seamlessly attached to each other to create a complex object. Because design-by-morphing has the flexibility to adjust independently the weights of sub-objects, users or an automated algorithm can choose some subset of sub-objects to be optimized while preserving or restricting the changes of other sub-objects. Our design-by-morphing method can be automated and is computationally efficient, so it requires much less human input than traditional design methods and is therefore not only inexpensive but also free from human bias in finding optimal designs that are radical and non-intuitive.We have applied optimization via design-by- morphing to aircraft and a turbine-99 draft tube, and reduced drag-to-lift ratio and maximized mean pressure recovery factor by 23.1% and 10.9%, respectively. We believe that this optimization method is applicable to a wide variety of engineering applications in which the performance of an object depends on the aerodynamic or hydrodynamic",ucb,,https://escholarship.org/uc/item/13v059vh,,,eng,REGULAR,0,0
585,2021,The Role of Spatial Frequency Selection in Local versus Global Perception,"Flevaris, Anastasia V.","Robertson, Lynn C;",2009,"The aim of this research is to investigate the extent to which selective attention to spatial frequency (SF) mediates local versus global perception in general and in the context of face perception. Previous research has suggested a relationship between processing high versus low SFs and local versus global perception, respectively, but the nature of this relationship is debated. The experiments reported here demonstrate that attention to local and global aspects of a hierarchical display biases the flexible selection of relatively higher and relatively lower SFs during image processing. Moreover, the attentional selection of relative SF mediates the perceptual integration of the identity of elements in a hierarchical display with the level (local/global) at which they occur. Finally, the attentional selection of SF is shown to modulate early stages of face perception reflected in the N170-effect, a neurobiological index of face categorization that is particularly sensitive to face features. The N170-effect is found to be equally robust in response to the selection of both HSFs and LSFs in face-related stimuli, but the reliance on one SF scale or another is contingent upon the nature of the attended face-related stimulus, and whether its configuration is intact. Taken together, this investigation provides clear evidence that the flexible, top-down selection of low-level SF channels mediates the perception of local and global elements of visual displays, both for tightly controlled experimental stimuli as well as faces, a natural and frequently viewed hierarchical object category.",ucb,,https://escholarship.org/uc/item/143509fk,,,eng,REGULAR,0,0
586,2022,"Of Maras and Mortal Doubt: Violence, Order, and Uncertainty in Guatemala City","Fontes, Anthony Wayne IV","Kosek, Jake G;",2015,"Everyday brutality in Guatemala City shocks and numbs a society that has suffered generations of war and bloodshed. Much of this violence is blamed on maras, gangs bearing transnational signs and symbols, that operate in prisons and poor urban communities. I will explore how the maras' evolution in post-war Guatemala has made them what they are today: victim-perpetrators of massive and horrifying violence, useful targets of societal rage, pivotal figures in a politics of death reigning over post-war society.  However, while maras and mareros play starring roles in this account of extreme peacetime violence, they are not the problem. They are a hyper-visible expression of a problem no one can name, a deafening scream, a smokescreen obscuring innumerable and diffuse sources of everyday brutality.The maras will be my entry-point into a world defined by mortal doubt, and my guides as I navigate the rumors, fantasies, fears, and trauma swirling about criminal violence in post-war Guatemala City. The specter of violence has become so utterly entwined with the making of lived and symbolic landscapes that it cannot be extricated from the very fibers of everyday life. I will illuminate the myriad of spaces violence infiltrates and reorders to expose the existential uncertainty haunting efforts to confront, contain, and overcome violence. In the process, I provide an alternative, intimate understanding of the violence and suffering for which maras speak, or are made to speak, and the ways this violence and suffering affects individual consciousness and communal life, orders urban space, and circulates in public discourse.   The veins of uncertainty fracturing this account are meant to rupture the pretense of knowing, and so break through into the treacherous and largely unmapped territory that is life lived in the shadow of constant violence.",ucb,,https://escholarship.org/uc/item/16c477pk,,,eng,REGULAR,0,0
587,2023,A Team-Based Behavioral Economics Experiment on Smoking Cessation,"White, Justin S.","Dow, William H;",2013,"Tobacco use is a leading cause of death worldwide, yet smoking cessation services are not widely available in many low-resource settings. Popular approaches also fail to help smokers to maintain self-control and motivation. The degree to which peer pressure promotes self-control in team-based health interventions remains largely untested. Moreover, peer pressure and cash incentives have rarely been mobilized in tandem. To this end, we conducted a randomized controlled trial in 42 villages in Thailand to test a novel intervention that combines commitment contracts for smoking cessation with team incentives that activate peer pressure. We randomly assigned 201 participants, 11% of all smokers in the study area, to a control group that received smoking cessation counseling or a treatment group that received counseling plus a commitment contract, team incentives, and text message reminders for smoking cessation. We find that, relative to the control group, the intervention increased biochemically verified smoking abstinence by 25% points at six months (three months post-intervention). Moreover, the intervention cost about $300 per marginal quitter, less than half that of common smoking cessation aids in Thailand. We find evidence that exogenously selected teammates had a large causal effect on each other's outcomes. The team effects are heterogeneous with respect to participants' ex ante quit predictions: the success of less confident smokers increases with a teammate's degree of self-confidence whereas the success of more confident smokers does not change. Further analyses indicate that heterogeneous teams result in higher aggregate quitting than do homogeneous teams. Our team commitment intervention may offer a viable cost-effective alternative to smoking cessation approaches in low-resource settings.",ucb,,https://escholarship.org/uc/item/1773n4j0,,,eng,REGULAR,0,0
588,2024,Studies on the Cave- Spider Family Leptonetidae,"Ledford, Joel M.","Griswold, Charles E;Roderick, George R;",2011,"The spider family Leptonetidae Simon, 1890 includes 17 genera and 213 species worldwide. They are broadly distributed in the Holarctic and typically associated with cool, moist habitats such as leaf litter, moss, rotting logs, and caves. The North American fauna is divided into two subfamilies, the Archoleptonetinae and Leptonetinae, with representatives in California through the Southern U.S. and Mexico. Five genera are currently recognized (Platnick, 2010), the most diverse of which is Neoleptoneta Brignoli, 1972 with a center of diversity in Texas where most species are known only from caves (Gertsch, 1974). Their restricted distributions and specialized biology have made them conservation priorities and two Texas species, Neoleptoneta microps (Gertsch, 1974) and N. myopica (Gertsch, 1974), are listed under the U.S. Endangered Species Act.	The impetus for this study was a series of collections produced over the past thirty years which have dramatically increased the number of records for the family, including the discovery of several unknown sexes and new species. Additionally, detailed morphological study using scanning electron microscopy (SEM) has revealed a wealth of new characters many of which have implications for relationships within the family and among spiders as a whole. Lastly, fresh collections for several genera in Alabama, California, Mexico, and Texas have facilitated the use of molecular data to develop phylogenetic hypotheses within the family for the first time. The primary objective for the study is to revise the systematics of the North American genera, with particular emphasis on the taxonomy and relationships within the Archoleptonetinae and the Texas cave fauna. The study is divided into three chapters, the results of which are briefly summarized below.	In the first chapter, a detailed morphological study of the genus Archoleptoneta revealed the presence of a cribellum an calamistrum representing the first cribellate member of the Leptonetidae. The morphology and relationships for the family are reviewed and the genus Darkoneta is described to include the ecribellate archoleptonetines. Three new species are also described from California, Mexico, and Guatemala.	The second chapter uses molecular sequence variation from three genes to produce a phylogeny for the North American Leptonetidae, with emphasis on the relationships of Neoleptoneta Brignoli, 1972. The placement of two incertae sedis species, Leptoneta brunnea (Gertsch, 1974) and Leptoneta sandra (Gertsch, 1974) are also considered and four new genera are described. Morphological data are traced on the molecular phylogeny and patterns of cave evolution are discussed.	The third chapter revises the taxonomy of the genus Tayshaneta, including the descriptions of ten new species from Texas. All Tayshaneta species are diagnosed and keyed, and comparative images using scanning electron and compound light microscopy are provided. Relationships among Tayshaneta are also discussed, including detailed descriptions of their morphology. Lastly, the karst faunal region (KFR) conservation strategy in Central Texas is evaluated using revised species distributions.",ucb,,https://escholarship.org/uc/item/17t4g7dn,,,eng,REGULAR,0,0
589,2025,Kavva and Kavya: Hala's Gahakosa and Its Sanskrit Successors,"Chiarucci, Bergljot Julie","Goldman, Robert P.;Rospatt, Alexander v.;",2014,"The study begins by looking at how the compilers of later Sanskrit and Prakrit Kavya anthologies select, interpret, and impose meaning on poetry and how, in so doing, they tie it back into their own wider cultural and ideological mi- lieus. The dissertation then considers the early recensions of the Prakrit Gahakosa (Treasury of Gathas), as it was known in the centuries following its creation, or the Sattasai; (Seven Hundred Poems), as it came to be known later, an anthology of lyrical Maharastari Prakrit poetry commonly attributed to the Satavahana king Hala, whose arrangements, themes, and concerns reveal an ofentimes different poetic universe. The study contrasts the Gahakosa with the later anthologies and the Sanskrit categories on which they are based, reflecting, in particular, on the Prakrit anthology's aesthetics of surprise, its inscription of gendered voice, and its representations of naturalness and the natural world. In the second part of the dissertation the discussion of the earlier chapters is brought to bear on Tribhuvanapala's Chekoktivicaralila, the Gahakosa's earliest extant commentary. Part II contains an edition and annotated translation of Chekoktivicaralila 1-28 based on a Kesar Library Manuscript (NGMPP C6/12, nak acc. no. 76). The third part of the dissertation presents further materials for the study of the Gahakosa.",ucb,,https://escholarship.org/uc/item/18s7k792,,,eng,REGULAR,0,0
590,2026,Constructional Morphology: The Georgian Version,"Gurevich, Olga",,2006,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/1b93p0xs,,,eng,REGULAR,0,0
591,2027,Rational Design and Preparation of Organic Semiconductors for use in Field Effect Transistors and Photovoltaic Cells,"Mauldin, Clayton Edward","FrÃ©chet, Jean;",2010,"The goal of this research was to develop methods to control the material properties of organic semiconductors, like solubility, stability, charge mobility, and self-assembly, through structural design. Investigations of structure-property relationships were conducted to optimize the properties of organic semiconductors for applications in organic field effect transistors (OFETs) and organic photovoltaics (OPVs).Chapter 1 gives an introduction to charge transport in organic semiconductors, and describes how the structure of conjugated molecules can affect their electrical performance and facilitate facile solution deposition. Furthermore, factors that can affect the stability of organic semiconductors to ambient conditions are discussed. Also, the device characteristics of OFETs and OPVs are summarized as a reference for subsequent chapters.Chapter 2 discusses the investigation of the air stability of distyryl oligothiophenes in OFETs. This work made use of thermally labile solubilizing groups to facilitate solution deposition of the oligothiophenes. In addition to device characterization, an extensive analysis of the thin film morphology using AFM, NEXAFS and GIXD is presented. This work revealed the general stability of distyryl oligothiophenes to oxidative degradation, and the high degree of crystallinity in our thin films.In Chapter 3, the charge transporting properties of pentathiophene monolayer islands is analyzed using current sensing AFM. The pentathiophenes were prepared with carboxylic acid moieties for self assembly, and the sub-monolayer films were transferred onto conductive substrates using the Langmuir-Blodgett technique. The morphology of the monolayers was observed to be sensitive to the alkyl substitution pattern of the pentathiophenes, which in turn affected charge transport. Hierarchical supramolecular assemblies of oligothiophenes and block copolymers are studied in Chapter 4. The structure of the assemblies is studied by TEM and small angle X-ray scattering, confirming successful formation of supramolecular assemblies. Thin films are studied further by grazing incidence X-ray scattering, and their charge transporting properties are evaluated in OFETs. This work demonstrates that non-covalent self-assembly can be used to access nanostructured thin films of functional organic semiconductors.Finally, Chapter 5 discusses the photophysical, morphological, and electronic properties of boron(subphthalocyanine)s with conjugated axial ligands. The tendency of these materials to form polycrystalline films instead of amorphous films was shown to be very sensitive to axial ligand structure and coordination geometry. The morphology of thin films was also shown to affect OPV device characteristics, with crystalline films supplying higher photocurrents.",ucb,,https://escholarship.org/uc/item/1bs2h6sn,,,eng,REGULAR,0,0
592,2028,Engaging High School Science Teachers in Personalized Professional Learning: A Design Development Study,"Reinhardt, Thomas Arthur","Mintrop, Heinrich;",2018,"The current educational climate, featuring standards-based reforms and reliance onsummative assessment and managerial control has strained the professional work of teachers by simultaneously demanding compliance with reform effort and control of classrooms while expecting in-depth professional learning to shift their pedagogical strategies to provide deeper cognitive experiences for students. Currently, the managerial and high-stakes accountability forces on teachers are prevailing, leaving teachers in a state of low motivation with limited time and resources to further their pedagogical content knowledge. In addition, the educational system lacks the resources and technical ability to provide high quality professional learning experiences for teachers. This design development study attempts to motivate high school science teachers into a state of learning through classroom experimentation and reflection on student learning.This research involved seven science teachers at high-poverty, urban high school with a large portion of English learners and student of color. During a series of seven 90-minute workshops, the participating teachers created personalized professional learning plans based on their self-identified teaching characteristics and goals for their own learning. Subsequently, they attempted pedagogical strategies and presented artifacts of students learning to their colleagues. Being highly contextualized research, design development studies provide the great insight into the specific context where implemented but also illuminate deep challenges facing our educational system, such as motivating teachers to learn advanced pedagogical strategies in response to student learning.Results from participant responses to structured and semi-structured pre/post interview questions combined with analysis of process data collected during the implementation of the workshop series yielded salient trends in the participating teachersâ€™ learning. While most participants attempted new strategies and began tracking learning by reviewing student artifacts, the pedagogical strategies attempted focused on general engagement strategies rather than deeper science pedagogical strategies. This suggested a willingness to try new strategies and a possible need for explicit examples of high-quality science teaching strategies during professional development experiences. Additionally, the teachers deepened their reflective conversation throughout the project and began to focus more on their learning and control of classroom experiences over the perceived deficits of the student population. However, the language used to describe teaching and learning did not advance to the desired technical level, suggesting a need for additional time repeating learning cycles with a learning experience to bolster technical analysis of learning. This design development study reinforces much of the theorized suggestions for high quality professional learning in science education. Questions remain regarding sustainability and the effects of prolonged and engaging teacher learning opportunities.",ucb,,https://escholarship.org/uc/item/1c04c69t,,,eng,REGULAR,0,0
593,2029,Fourth-order conservative Vlasov-Maxwell solver for Cartesian and cylindrical phase space coordinates,"Vogman, Genia","Colella, Phillip;Shumlak, Uri;",2016,"Plasmas are made up of charged particles whose short-range and long-range interactions give rise to complex behavior that can be difficult to fully characterize experimentally.  One of the most complete theoretical descriptions of a plasma is that of kinetic theory, which treats each particle species as a probability distribution function in a six-dimensional position-velocity phase space.  Drawing on statistical mechanics, these distribution functions mathematically represent a system of interacting particles without tracking individual ions and electrons.  The evolution of the distribution function(s) is governed by the Boltzmann equation coupled to Maxwell's equations, which together describe the dynamics of the plasma and the associated electromagnetic fields.  When collisions can be neglected, the Boltzmann equation is reduced to the Vlasov equation.  High-fidelity simulation of the rich physics in even a subset of the full six-dimensional phase space calls for low-noise high-accuracy numerical methods.  To that end, this dissertation investigates a fourth-order finite-volume discretization of the Vlasov-Maxwell equation system, and addresses some of the fundamental challenges associated with applying these types of computationally intensive enhanced-accuracy numerical methods to phase space simulations.  The governing equations of kinetic theory are described in detail, and their conservation-law weak form is derived for Cartesian and cylindrical phase space coordinates.  This formulation is well known when it comes to Cartesian geometries, as it is used in finite-volume and finite-element discretizations to guarantee local conservation for numerical solutions.  By contrast, the conservation-law weak form of the Vlasov equation in cylindrical phase space coordinates is largely unexplored, and to the author's knowledge has never previously been solved numerically.  Thereby the methods described in this dissertation for simulating plasmas in cylindrical phase space coordinates present a new development in the field of computational plasma physics.A fourth-order finite-volume method for solving the Vlasov-Maxwell equation system is presented first for Cartesian and then for cylindrical phase space coordinates.  Special attention is given to the treatment of the discrete primary variables and to the quadrature rule for evaluating the surface and line integrals that appear in the governing equations.  The finite-volume treatment of conducting wall and axis boundaries is particularly nuanced when it comes to phase space coordinates, and is described in detail.  In addition to the mechanics of each part of the finite-volume discretization in the two different coordinate systems, the complete algorithm is also presented.The Cartesian coordinate discretization is applied to several well-known test problems. Since even linear analysis of kinetic theory governing equations is complicated on account of velocity being an independent coordinate, few analytic or semi-analytic predictions exist.  Benchmarks are particularly scarce for configurations that have magnetic fields and involve more than two phase space dimensions.  Ensuring that simulations are true to the physics thus presents a difficulty in the development of robust numerical methods.  The research described in this dissertation addresses this challenge through the development of more complete physics-based benchmarks based on the Dory-Guest-Harris instability.  The instability is a special case of perpendicularly-propagating kinetic electrostatic waves in a warm uniformly magnetized plasma.  A complete derivation of the closed-form linear theory dispersion relation for the instability is presented.  The electric field growth rates and oscillation frequencies specified by the dispersion relation provide concrete measures against which simulation results can be quantitatively compared.  Furthermore, a specialized form of perturbation is shown to strongly excite the fastest growing mode.  The fourth-order finite-volume algorithm is benchmarked against the instability, and is demonstrated to have good convergence properties and close agreement with theoretical growth rate and oscillation frequency predictions.  The Dory-Guest-Harris instability benchmark extends the scope of standard test problems by providing a substantive means of validating continuum kinetic simulations of warm magnetized plasmas in higher-dimensional 3D (x,v_x,v_y) phase space.  The linear theory analysis, initial conditions, algorithm description, and comparisons between theoretical predictions and simulation results are presented.The cylindrical coordinate finite-volume discretization is applied to model axisymmetric systems.  Since mitigating the prohibitive computational cost of simulating six dimensions is another challenge in phase space simulations, the development of a robust means of exploiting symmetry is a major advance when it comes to numerically solving the Vlasov-Maxwell equation system.  The discretization is applied to a uniform distribution function to assess the nature of the singularity at the axis, and is demonstrated to converge at fourth-order accuracy.  The numerical method is then applied to simulate electrostatic ion confinement in an axisymmetric Z-pinch configuration.  To the author's knowledge this presents the first instance of a conservative finite-volume discretization of the cylindrical coordinate Vlasov equation.  The computational framework for the Vlasov-Maxwell solver is described, and an outlook for future research is presented.",ucb,,https://escholarship.org/uc/item/1c49t97t,,,eng,REGULAR,0,0
594,2030,Challenges of Evaluating the Causal Effects of Early Child Development Programs,"Weber, Ann","Tager, Ira B;Fernald, Lia H;",2012,"Over 200 million children under five years old in low- and middle-income countries (39% of preschool children in developing countries) are estimated as not achieving their potential across multiple domains of development (including sensori-motor, cognitive, language, and social-emotional development).  Although there is evidence of benefit to child development for a wide range of interventions, results from assessments of scaled-up programs are less conclusive.  Therefore, the assessment of large-scale early child development (ECD) programs in developing countries is a priority.  This dissertation focuses on several methodological issues in evaluating large-scale ECD interventions that threaten the validity of finding a program benefit.  Specifically, I address two important areas of evaluation: 1) the challenge of obtaining an unbiased measure of language development in a setting for which the test was not developed; and 2) the analytic process of determining whether the ECD intervention had a benefit that actually is the result of the intervention, given an unbiased developmental outcome.  To demonstrate these challenges, I make use of data collected over a 14 year period for a national nutrition program in Madagascar.  First implemented in 1999 by the National Office of Nutrition (ONN) in Madagascar, the program has expanded to include 5550 sites with coverage of approximately 1.1 million children.  The program takes a comprehensive approach to improving early child nutritional status, targeting children less than 5 years of age and including multiple activities that have been found to be associated with better child outcomes.  A wide spectrum of developmental outcomes was assessed in four national surveys in Madagascar, including physical growth (height and weight), and motor, cognitive, language, and behavioral skills.  In my dissertation, I focus on only two of these measures: weight-for-age (a measure of short-term nutritional status) and receptive language (understanding of words, gestures or phrases), as assessed by an adaptation of the U.S. version of the Peabody Picture Vocabulary Test, 3rd edition (PPVT-III).Tests of early child cognition and language that were developed and carefully validated in one country are not guaranteed to maintain their properties when adapted and translated for use in another.  The risk of censoring is high, and bias from differential item functioning (DIF) can be introduced when administering the test to different subgroups (e.g., ethnicities) within the same country.  Using longitudinal data from two rounds of testing (when children were 3-6 years and 7-10 years of age) I apply item response theory (IRT) models to assess the performance of the PPVT in Madagascar.   My analysis uncovers problem items (e.g., bias from DIF by dialect spoken in 55% of items), censoring in a large proportion of the children, and patterns of responses related to test fatigue.  This information can be used to identify items that need to be dropped before estimating the program effect (e.g., items with strong, significant DIF); and items that should be replaced, modified, or re-ordered in future work (to avoid censoring and test fatigue).  Although my analysis focuses on a test of vocabulary, many of the issues apply to any multi-item instrument intended to capture a latent construct.   Such multi-item measures are commonly used in ECD intervention research and include other tests of language and memory, as well as non-verbal tests of cognition and socio-emotional behavior scales.  I present lessons learned from working with the PPVT in Madagascar and make recommendations for how these lessons can be applied in other developing country settings.  Presuming that the developmental outcome is assessed without bias, there remains the analytic challenge of determining whether an ECD intervention has a benefit that actually is the result of the intervention.  I make use of a detailed, step-by-step roadmap for estimating the average treatment effect (ATE) of Madagascar's program on children's mean weight-for-age in a community between 1997 and 2004.  The evaluation of the Madagascar program is complicated by the fact that the selection into the program was non-random and strongly associated with the pre-treatment (lagged) outcome.  The availability of pre-program data allows me to define the outcome as either the post treatment value or the change from pre-treatment to post-treatment.  Using these two outcome definitions, I contrast identification results for three common statistical parameters that under different assumptions are equivalent to my target parameter, the ATE.  These statistical parameters are a post-treatment estimand commonly used in epidemiology that adjusts for measured confounders, and two difference-in-differences estimands (one of which is popular in econometrics) that can address certain types of unmeasured confounders.  For identification, I make the assumptions underlying each of these estimands explicit and demonstrate the consequences of alternate choices using directed acyclic graphs and data simulations.  Finally, I describe and compare three methods of estimation for each of the three estimands: traditional parametric regression, inverse probability of treatment weights (IPTW or propensity score weighting), and targeted maximum likelihood estimation (TMLE).  Throughout, I avoid imposing parametric model assumptions unless they are firmly supported by knowledge, and deliberately keep the process of identification separate from the process of estimation in order to avoid the common confusion of the two.My findings show that I am faced with a serious bias trade-off when choosing an estimand for the ATE of the Madagascar nutrition program.  A post treatment estimand controls for confounding due to the lagged outcome but not from possible unmeasured confounders.  The difference-in-differences estimands do not control for confounding by lagged outcome, but have the potential to adjust for a certain type of unmeasured confounding.   However, the difference-in-differences estimands have the potential for introducing bias if the additional assumptions they require (beyond those needed for the post-treatment estimand) are not met.  The three estimands result in very different estimates of effect in the Madagascar study, regardless of method of estimation.  The estimates for the ATE from the post-treatment estimand are less than one tenth of a standard deviation (SD) improvement in community mean weight-for-age z-score (as compared to the WHO reference population).   The two difference-in-differences estimands are comparable to each other, with estimates of the ATE ranging from 0.24 to 0.28 SD increase in mean weight-for-age z-score (statistically significant with all estimation methods).  However, since I am unable to estimate either the magnitude or direction of possible confounding from unmeasured factors, or the magnitude or direction of bias from the failure of the assumptions to hold, I conclude that my best choice is the post treatment estimand.  This simple estimand adjusts optimally for known measured confounders and is equal to the ATE under the fewest assumptions.Given this choice of estimand, the choice of estimator can still make a difference.  In fact, the only significant effect for the post-treatment estimand was obtained with TMLE (estimate of the ATE = 0.066 SD, CI: 0.001, 0.146 SD).  TMLE has specific advantages over either parametric regression or IPTW, and improves on both by implementing a bias reduction step to estimate the target parameter of interest.  In addition, TMLE is considered doubly robust to model misspecifications.   Therefore, I conclude that TMLE is a better choice for estimation over the other two methods, and that my best estimate of the ATE is small, but statistically significant.  Alternate target parameters and alternate estimation approaches are unlikely to resolve the uncertainty of the choice of estimand for the Madagascar evaluation.  However, future analytic work on other nutritional outcomes (e.g., height-for-age) and longer term effects of the program (e.g. from a third wave of data in 2011) may provide an accumulation of evidence of a causal benefit of Madagascar's nutrition program.There is mixed evidence of the effectiveness of large-scale nutrition programs on early child development outcomes.  Although the lack of consistent results is generally attributed to possible problems of implementation and governance of the program, the failure to find a statistically significant effect (or alternatively, the success of finding one) may, in fact, be due to the types of problems described in my dissertation.  There may be bias in the outcome (or other) measure, failure of the causal assumptions to hold, or bias from the method of effect estimation.  Misleading estimates of a program's benefit (in either direction) have significant policy and funding implications for the program.  More importantly, the decisions made based on an evaluation have consequences for the children the programs are trying to help.  I present tactics for addressing several methodological challenges to evaluation and urge investigators to update and/or reconsider their analytic approaches to evaluations.  Since ECD intervention research is often inter-disciplinary, I recommend learning new methods from other disciplines and to use the best methods that are at our disposal.",ucb,,https://escholarship.org/uc/item/1cd4d6f7,,,eng,REGULAR,0,0
595,2031,Transforming trash: reuse as a waste management and climate change mitigation strategy,"Vergara, Sintana Eugenia","Nelson, Kara L;",2011,"Waste reflects the culture that produces it and affects the health of the people and environment surrounding it. As urbanization and waste production increase on a global scale, cities are faced with the challenge of how to manage their waste effectively to minimize its negative impacts on public and environmental health. Using waste as a resource can offer a variety of environmental benefits, including climate change mitigation, though these benefits are variable and uncertain. My work begins with an overview of the relationship between solid waste and the environment, focusing on two trends over time and space: regionalization and formalization of waste management. Recognizing that appropriate waste management must be determined locally, I then focus on two places, one in the Global North, and one in the Global South, whose waste production and management differ tremendously, and quantify the climate benefits from reuse strategies at different scales using life-cycle assessment (LCA). In California, USA, where waste production and access to technology are abundant, I ask: how can the state minimize the greenhouse gas (GHG) emissions from its municipal waste management? I conclude that source reduction and anaerobic digestion are the methods by which CA could most greatly and robustly reduce its waste emissions. I also find that waste LCA results are very sensitive to model assumptions, about system boundary, landfill behavior, and electricity generation, though the emissions from source reduction are robust to these inputs. In BogotÃ¡, Colombia, where the municipal government is in the process of modernizing their recycling system, I ask: what are the GHG emission implications of this modernization? I find that the unregulated recycling system is more financially sustainable, more socially inclusive, and abates more greenhouse gas emissions than does the municipal system. The municipal system, on the other hand, conforms to aesthetic visions of a modern city, and provides workers with steady employment and benefits. A hybrid model could combine the incentives and efficiency of the informal system with the working conditions of the municipal one. In BogotÃ¡ and in California, modes of reuse - technologies or behaviors that use waste as a resource - offer waste management, environmental and climate benefits.",ucb,,https://escholarship.org/uc/item/1cn990v0,,,eng,REGULAR,0,0
596,2032,Two-color Three-pulse Photon Echo Studies on the Photosynthetic Bacterial Reaction Center,"Lee, Hohjai","Fleming, Graham R;",2009,"Photosynthesis begins with absorbing the sun light by the light harvesting complexes. The solar energy is then funneled into the reaction center (RC) via the energy transfer between the light harvesting complexes at ultrafast rates (~1/100fs ) with extremely high quantum efficiency (~100 %). Most of the complexes are composed of pigments and protein matrices that tightly bind them. The pigments are responsible for absorbing and transferring the energy. The roles of the protein environment of photosynthetic pigment-protein complexes have been suggested, but the detailed mechanisms are still not fully understood.      In this dissertation, non-linear spectroscopic methods using ultrashort pulses (~ 40-fs FWHM), three-pulse photon echo studies are presented to investigate the roles of protein environment of the photosynthetic bacterial RC. The technique characterizes the protein dynamics around the pigments (a bacteriochlorophyll a, B and a bacteriopheophytin a, H) in the RC. In particular, two-color three-pulse electronic coherence photon echo technique is used to observe the quantum coherence between the excited states of coupled H and B, whose life time is sensitive to the protein dynamics. I found a long-lasting quantum coherence suggesting that the protein actively preserves the quantum coherence. A scenario in which the long-lasting coherence can accelerate the rate of energy trap is described with a simple Bloch model simulation.       In addition, one- and two-color three-pulse photon echo peak shift (1C- and 2C3PEPS) techniques are used to measure the coupling strength between H and B in the wild type RC. The coupling strength is facilitated from the geometry between the pigments governed by the protein environment. The simulation based on the standard response function formalism is used to obtain the coupling strength. 2C3PEPS signal from H and B of the oxidized RC is reproduced to extract the coupling constant between them by quantum-master equation which efficiently incorporates pulse overlap effect and bath memory effect. The values will enable the molecular level of studies on the photosynthetic energy and electron transfer.",ucb,,https://escholarship.org/uc/item/1d23j386,,,eng,REGULAR,0,0
597,2033,Electron microscopy methods to overcome the challenges of structural heterogeneity and preferred orientations in small (sub-500 kDa) macromolecular complexes,"Lipscomb, Dawn Michelle","Nogales, Eva;",2017,"While cryo-EM imaging technology and software suites have led to a â€œresolution revolutionâ€, the poor reproducibility and inefficiency offered by current specimen preparation techniques remain challenges largely unexamined and under-prized. Regardless of camera or software proficiency, a poor-quality specimen will always produce an equivalently substandard 3D reconstruction. To further advance the high-resolution EM pipeline and establish quality control for grid specimens, we tested three techniques to assess optimum conditions for vitrification of small (<500kDa), multi-subunit biological macromolecular complexes. 	We performed specimen optimization for a 300kDa, five-component, macromolecular complex, PRC2. PRC2 is a key regulator of gene silencing in eukaryotes and mutations in its catalytic subunit, Ezh2, are linked to a number of human cancers and degenerative diseases. To date, no high-resolution structure of the complete complex has been solved. PRC2 presents a number of unique challenges for the structural biologist, including its small size, highly flexible regions, and conformation heterogeneity. A high-resolution structure of PRC2 would provide unparalleled insight into the biochemical mechanisms that mediate gene silencing by methylation of histone tails. For these reasons, PRC2 is an ideal candidate for optimizing cryo-EM grid preparation techniques translatable to similar complexes and for future research.	Previous EM studies of PRC2 revealed a distinct preferred orientation when bound to carbon substrates, necessitating the collection of tens of thousands of images to generate a complete structure containing high-resolution features. To overcome this impediment, we experimented with two other grid specimen preparation techniques. First, we vitrified solution-suspended PRC2 in open holes on carbon mesh support. Despite modifications of chemical and physical buffer and substrate parameters to stabilize the complex, denaturation upon surface binding at the air-water interface and dissociation of the complex during vitrification proved insurmountable. We next tested a method of affinity binding to a streptavidin monolayer substrate covering the holes of a carbon mesh support. We found that chemically biotinylated PRC2 was intact, monodisperse, and bound the grid in random orientations, all factors critical to high-resolution cryo-EM structure determination. Negative stain EM analysis revealed a more even distribution of views for 3D reconstruction. From these results, we conclude that streptavidin monolayer substrates are an option for overcoming the obstacle of preferred orientations for small complexes and provide an easily reproducible protocol for grid preparation.",ucb,,https://escholarship.org/uc/item/1d81t93b,,,eng,REGULAR,0,0
598,2034,The Effects of Agricultural Contaminants on Amphibian Endocrine and Immune Function,"Falso, Paul Gerald","Hayes, Tyrone B;",2011,"Amphibian populations are dramatically reduced from historical numbers on a global scale. Amphibians in agricultural regions experience a diverse set of environmental stressors that may disrupt immune function and increase susceptibility to infection. The draining of wetlands for water and land usage leads to desiccation, crowding, and ultimately temperature extremes. Fertilizers and pesticides further degrade the quality of the available water. American bullfrogs (Lithobates catesbeiana) were collected throughout a gradient of agricultural land use in the Sacramento, San Joaquin, and Salinas River watersheds of California, USA. Animals collected downstream of intense agricultural land use had increased stress hormone (corticosterone) concentrations and altered white blood cell differentials and activity relative to those in upstream locations. An individual stressor of global importance, an agrichemical mix, as well as the amphibian immune response to chronic stress were furthered examined in the laboratory. Exposure to a mixture of pesticides and nutrients commonly applied in California did not affect plasma corticosterone or immune function of adult L. catesbeiana, suggesting that additional factors or exposure scenarios influence amphibians in the wild. The effects of chronic stress on blood cell differentials and function were characterized in lab-bred African clawed frogs (Xenopus laevis) and wild-caught L. catesbeiana. In both species, increased plasma corticosterone was associated with increased whole blood oxidative burst activity and blood neutrophil concentrations, while blood lymphocyte and eosinophil concentrations were decreased. Taken together, our results suggest that wild amphibians experience stressful conditions in agricultural habitats and that stressors may alter immunity and result in disease.",ucb,,https://escholarship.org/uc/item/1dx7d63k,,,eng,REGULAR,0,0
599,2035,Transcription Activator-Like (TAL) Effectors of the Cassava Bacterial Blight Pathogen Xanthomonas axonopodis pv. manihotis,"Shybut, Mikel","Staskawicz, Brian J;",2015,"Cassava is an essential food crop relied on by hundreds of millions of people worldwide. Xanthomonas axonopodis pv. manihotis (Xam) is the causal agent of cassava bacterial blight (CBB) and the leading destructive bacterial pathogen of cassava. Xam utilizes a unique class of type three effectors known as transcription activator-like (TAL) effectors (TALEs) to activate specific host genes that contribute to virulence and bacterial growth. In Xam, TALEs are often localized to plasmids. TALE-containing plasmids of other Xanthomonas ssp. have been shown to be conjugative, providing a mechanism for the horizontal transfer of TALE virulence components. Here, I characterize the two-TALE containing plasmid pXam46 of Xam isolate CIO151, providing a full draft sequence, TALE virulence assays, and evidence for its mobilizing ability. The potential horizontal transfer of TALEs suggests that TALEs which confer a strong virulence phenotype may be well conserved amongst Xam communities. I screened a subset of 22 global Xam isolates spanning 28 years of evolution from 5 countries over 3 continents for their TALE repertoires. I identified one pair of highly conserved TALEs, including a single repeat variable diresidue (RVD) variant of a pXam46 localized TALE, and 3 additional well-conserved TALEs. Of the two highly conserved TALEs, both contribute to bacterial growth in planta and one is associated with a water soaking disease phenotype. The remaining 3 well-conserved TALEs did not show any measurable contributions to virulence. Some plants contain an evolutionary mechanism to defend against TALEs, carrying executor resistance (R) genes containing TALE binding elements upstream of disease resistance genes. I found that the highly conserved TALE of pXam46 triggers a specific, transcriptionally dependent HR-like phenotype in the non-host Nicotiana benthamiana. Employing RNA-seq, I have identified a list of candidate TALE-upregulated genes that may be involved in the defense response of N. benthamiana. Identifying TALE-triggered R genes as well as conserved TALEs and their susceptibility targets can assist in the design of durable resistance strategies against Xam. Successful strategies may include stacking promoters of multiple conserved TALEs in front of R genes or modifying cassava susceptibility gene promoters to abrogate TALE binding. It is my hope that the basic biology described herein may assist in those efforts.",ucb,,https://escholarship.org/uc/item/1g52w36q,,,eng,REGULAR,0,0
600,2036,Understanding and Controlling the Optical Properties of Quasi Two-Dimensional Materials,"Cao, Ting","Louie, Steven G.;",2018,"In this thesis, I discuss the understanding and control of the optical properties of quasi-two-dimensional materials, an emerging field since the discovery of graphene. This thesis not only aims to understand and predict the distinct optical properties of quasi-two-dimensional materials from theoretical and numerical approaches, but also incorporates and quantitatively explains relevant experimental data when available. This thesis is organized as follows:In the first chapter, I give a brief background overview on 1) research on excited states in general, 2) first-principles GW-BSE method that calculates the electron quasiparticle bands and exciton properties, and 3) recent progress on the optical properties of two-dimensional semiconductors and light-matter interactions in these materials.In the second chapter, I review the valley physics in transition metal dichalcogenide monolayers, which builds the foundation of the more advanced topics that we discuss in the next chapters.In the third chapter, I present several studies on the unusual optical properties of transition metal dichalcogenide monolayers arising from the novel exciton physics, including strongly-bound non-hydrogenic exciton series, light-like exciton dispersion, and magnetic brightening of the dark states. These results show the distinct optical properties of two-dimensional semiconductors compared with those in other dimensions.In the fourth chapter, I demonstrate some consequences of topological effects on optical transitions in two-dimensional semiconductors, which leads to a new set of optical selection rules dictated by the winding number of interband optical matrix elements. The new selection rules go beyond the selection rules for conventional semiconductors which have been used for over 6 decades, and explains the experimental results on the photo-current spectroscopy of gapped bilayer graphene.In the last chapter, I present materials engineering aspects of two-dimensional materials via van der Waals interfacial engineering. We show that by changing the interlayer stacking configurations and by applying out-of-plane electric fields, the electronic and optical properties of van der Waals layers can be rationally engineered and controlled.",ucb,,https://escholarship.org/uc/item/1g99q98r,,,eng,REGULAR,0,0
601,2037,Efficient Sequential Decision Making,"Malek, Alan","Bartlett, Peter L;",2017,"This thesis studies three problems in sequential decision making  across two different frameworks. The first framework we consider is  online learning: for each round of a $T$ round repeated game, the  learner makes a prediction, the adversary observes this prediction  and reveals the true outcome, and the learner suffers some loss  based on the accuracy of the prediction. The learner's aim is to  minimize the regret, which is defined to be the difference between  the learner's cumulative loss and the cumulative loss of the best  prediction strategy in some class. We study the minimax strategy,  which guarantees the lowest regret against all possible adversary  strategies. In general, computing the minimax strategy is  exponential in $T$; we focus on two setting where efficient  algorithms are possible.  The first is prediction under squared Euclidean loss. The learner  predicts a point in $\Reals^d$ and the adversary is constrained to  respond with a point in some compact set. The regret is with respect  to the single best prediction in the set. We compute the minimax  strategy and the value of the game for any compact set and show that  the value is the product of a horizon-dependent constant and the  squared radius of the smallest enclosing ball of the set. We also  present the optimal strategy of the adversary for two important  sets: ellipsoids and polytopes that intersect their smallest  enclosing ball at all vertices. The minimax strategy can be cast as  a simple shrinkage of the past data towards the center of this  minimum enclosing ball, where the shrinkage factor can be  efficiently computed before the start of the game. Noting that the  value does not have any explicit dimension dependence, we then  extend these results to Hilbert space, finding, once again, that the  value is proportional to the squared radius of the smallest  enclosing ball.  The second setting where we derive efficient minimax strategies is  online linear regression. At the start of each round, the adversary  chooses and reveals a vector of covariates. The  regret is defined with respect to the best linear function of the  covariates. We show that the minimax strategy is an easily computed  linear predictor, provided that the adversary adheres to some  natural constraints that prevent him from misrepresenting  the scale of the problem.  This strategy is horizon-independent:  regardless of the length of the game, this strategy incurs no more  regret than any strategy that has knowledge of the number of  rounds.  We also provide an interpretation of the minimax algorithm  as a follow-the-regularized-leader strategy with a data-dependent  regularizer and obtain an explicit expression for the minimax  regret.  We then turn to the second framework, reinforcement learning. More  specifically, we consider the problem of controlling a Markov  decision process (MDP) with a large state-space. Since it is  intractable to compete with the optimal policy for large scale  problems, we pursue the more modest goal of competing with a  low-dimensional family of policies. Specifically, we restrict the  variables of the dual linear program to lie in some low-dimensional  subspace, and show that we can find a policy that performs almost as  well as the best policy in this class. We derive separate results  for the average cost and discounted cost cases. Most importantly,  the complexity of our method depends on the size of the comparison  class but not the size of the state-space. Preliminary experiments  show the effectiveness of the proposed algorithms in a queuing  application.",ucb,,https://escholarship.org/uc/item/0qm524f5,,,eng,REGULAR,0,0
602,2038,"Genocide, Nuptiality, and Fertility in Rwanda and Bosnia-Herzegovina","Staveteig, Sarah Elizabeth","Evans, Peter;Johnson-Hanks, Jennifer;",2011,"How does exposure to genocide affect nuptiality and fertility among the surviving population? Genocides in Rwanda and in Bosnia-Herzegovina in the early 1990s caused high levels of population displacement, trauma, and death, along with a dramatic decline in the standard of living. In Rwanda, genocide also reduced the sex ratio of the marriage-aged population, while in Bosnia, despite the high proportion of male casualties, the overall sex ratio of the marriage-age population did not decline substantially. Contrary to the expected delay in first marriage throughout the process of demographic transition, there was clear evidence of a post-genocide marriage boom among younger cohorts in Rwanda. Meanwhile in Bosnia--a country that had already achieved the demographic transition and fairly steady marriage rates more than a decade before the war started--first marriage rates peaked slightly during and after the war and then continued to decrease during the decade after the war ended, never returning to prewar levels. Patterns in fertility echoed this trend, with a small baby boom in Rwanda contrasted with a fall in birth rates in Bosnia.Drawing from a multi-method, multi-site study involving data analysis from nationally representative household surveys, 117 qualitative life-history interviews and 34 interviews with key informants in both countries, I assess the relative importance of several factors on marriage and fertility trends during and immediately after genocide in both countries. By examining the Bosnian and Rwandan cases in tandem, I situate demographic changes due to genocide in a broader social and historical framework. It is difficult to compare the demographic effects of war and genocide in two countries that had widely divergent pre-war nuptial and reproductive contexts. I explore four broad sets of causal factors linking genocide with demographic outcomes: (1) involuntary factors, such as mortality, separation of partners, and survival bias; (2) material and economic factors, including loss of income, changes in employment, and `transactional marriage' during acute periods of crisis; (3) sex ratio and gender role factors, which encompass the change in the marriage market due to differential mortality by sex, changing gender roles during and after wartime, and norms of widowhood remarriage; and (4) psychosocial factors, including the duration and severity of conflict, the effects of rape and post-traumatic stress disorder [PTSD] on the surviving population, and the role of political recovery and social cohesion in demographic decisions and outcomes. Whereas many journalists emphasized ethnic hatreds as the motivation behind both genocides, the instrumentalist view of ethnic conflict--which claims that ethnic divisions are exploited by elites during moments of economic and political crisis--has emerged as a powerful counter-narrative in both Bosnia and Rwanda. In areas where it is possible to measure ethnic differences my findings tend to support the instrumentalist view--namely, that economic and situational factors were more salient than ethnic differences in determining the demographic response to genocide. Broadly speaking, the mortality effects of genocide dominated in Rwanda, and this helped fuel an increase in marriages and births after war. Meanwhile in Bosnia the economic effects of genocide and the accompanying transition to a market economy tended to dominate the demographic response to genocide, suppressing marriage and fertility rates during and after the war. To classify the dominant character of these genocides in this way is not to ignore the massive economic collapse in Rwanda, nor the extraordinary death toll in Bosnia, but rather to suggest the relative primacy of economic and mortality effects as mediating channels through which genocide affected demographic outcomes for the majority of the population in each country.I postulate the need for a typology of the demographic effects of crisis. Famine, economic collapse, political instability, natural disaster, and armed conflict each affect population dynamics in different ways. Given the variation and complexity between the Bosnian and Rwandan genocides, it would be difficult to extrapolate directly from these two cases. Yet, when considered in tandem, the Bosnian and Rwandan case studies suggest a need to consider several important axes of variation--factors such as the intensity and duration of the crisis, economic effects, differential age structure effects, displacement, mortality, background conditions, and post-crisis political and social stability--when predicting the effects of future crises on nuptial and reproductive trends.",ucb,,https://escholarship.org/uc/item/0t58z112,,,eng,REGULAR,0,0
603,2039,Using outcomes to inform social decision-making in schizophrenia: Implications for motivation and functioning,"Campellone, Timothy Ryan","Kring, Ann M;",2015,"The outcomes of decisions we make are integral for guiding our behavior. In this study, we investigated if and how people with and without schizophrenia use positive and negative social outcomes and social partnersâ€™ emotional displays to inform decisions to trust as well as whether they could detect reversals in behavior even as emotion displays remained unchanged. Thirty-two people with schizophrenia and 29 control participants completed a task where they decided how much trust to place in social partners showing either a dynamic emotional (smiling, scowling) or neutral display. Interactions were designed to result in either positive (trust reciprocated) or negative (trust abused) outcomes, allowing us to model changes in decisions to trust over the course of repeated interactions. Compared to controls, people with schizophrenia were less sensitive to positive social outcomes as evidence by their placing less trust in trustworthy social partners during initial interactions. On the other hand, people with schizophrenia were more sensitive to negative social outcomes during initial interactions with untrustworthy social partners, placing less trust in these partners compared to controls. Interestingly, people with schizophrenia were just as able as controls to detect reversals in social partnersâ€™ behavior as it changed from trustworthy to untrustworthy, but were less able to detect changes in the untrustworthy to trustworthy direction. Importantly, decisions to trust were associated with real-world social functioning. We discuss the implications of these findings for understanding social engagement among people with schizophrenia and the development of psychosocial interventions for social functioning.",ucb,,https://escholarship.org/uc/item/0v73d5w9,,,eng,REGULAR,0,0
604,2040,Quantum Simulation of the Bose-Hubbard Model with Ultracold Atoms in Triangular Optical Superlattices,"Barter, Thomas Hamish","Stamper-Kurn, Dan M;",2018,"Quantum simulation is the study of one quantum mechanical system via analog with another. In this thesis we explore a simple model of interacting bosons on a lattice, known as the Bose-Hubbard model, by experimental investigation of ultracold rubidium atoms in an optical lattice made from laser light. We describe the construction and stabilization of an optical superlattice with threefold symmetry, and its use in studying the Bose-Hubbard model on triangular and trimerized kagome lattices. We study the short range phase coherence of a Mott insulator on the triangular lattice, and develop a scheme to mitigate out-of-equilibrium effects arising from the state preparation. We show the first experimental realization of an optical  trimerized kagome lattice for cold atoms, and discuss experiments characterizing this lattice. Finally, we provide evidence for a Mott insulating state with fractional average particle number per site with measurements of the nearest-neighbor phase coherence of strongly interacting atoms in the trimerized kagome lattice.",ucb,,https://escholarship.org/uc/item/0hs0r409,,,eng,REGULAR,0,0
605,2041,Numerical Simulation of Equilibrium Liquid Configurations On and Between Rough Surfaces,"Bartlow, Christopher Clayton","Glaeser, Andreas M.;",2011,"A numerical model of the interaction between liquids and rough solids is presented in this work. Partial-Transient-Liquid Phase (PTLP) bonding has proven an effective method of joining ceramic materials. The joining process is facilitated through the development of a thin-liquid layer between ceramic and metallic solids. Successful joining requires a liquid that will spread to fill interfacial voids, which can act as critical flaws that decrease the strength of the joined assembly. A full understanding of this method requires a model of the liquid behavior between dissimilar, rough solids. This work discusses the effect of wetting angle and surface roughness on the behavior of the liquid layer. 	Numerical simulations are presented that test possible liquid configurations on rough surfaces, determining the preferred geometry based upon the total interfacial energy. Use of computational methods allows independent control of surface roughness, liquid volume, and interfacial energies. In this way, the effect of the liquid contact angle and the amplitude and wavelength of surface-roughness features on the liquid behavior is examined. Emphasis is placed upon surface-roughness parameters and the correlation with preferred liquid configurations. Models are presented and discussed for liquids in contact with one rough surface and for a liquid entrapped between two dissimilar surfaces. Comparison of the one-surface simulation to previous studies of liquid/roughness behavior is provided. 	In liquid-based joining methods, the ability of the liquid layer to fill interfacial voids is strongly affected by surface roughness. Numerical simulations of a liquid flowing to fill an inter-solid gap are presented. The importance of dissimilar surface roughness, dissimilar contact angles, and inter-solid distance is discussed. It is found that increasing surface roughness can act to aid liquid-based joining in certain systems.",ucb,,https://escholarship.org/uc/item/0jx0f9z1,,,eng,REGULAR,0,0
606,2042,"The Spell of the Barricade: Art and Politics in France, 1830-1852","Rose, Jordan Marc","Clark, Timothy J;",2013,"This dissertation is about barricades and image production in the 1830s and `40s.  It comprises three chapters, each of which centers on a single work of art: Auguste PrÃ©ault's Tuerie (1834), Ernest Meissonier's Souvenir de guerre civile (1849-1850), and HonorÃ© Daumier's L'Ã‰meute (c. 1848-1852).  Together, the three chapters describe how the barricade oriented the period's multiple, often contradictory conceptions of class, revolution, history, and art.  A new grammar of democratic politics took shape in the `30s and `40s, and it was one in which the street figured as both the site and medium of social transformation.  Some would say, as I do, that the barricade was the 19th century's most poignant and meaningful ""symbolic form.""  It was also the most enchanting. Triumph is rare in these pages; tragedy sets their key.  They turn on PrÃ©ault's bas-relief, the political re-entrenchments of 1833-1834, horror, defeat, and disillusion rather than, say, the affirmative image of the barricade EugÃ¨ne Delacroix's La LibertÃ© guidant le peuple is often thought to have engendered; the uncertainties, ambivalences, and violence of artistic production in times of radical disintegration anchor their pivot.  Not infrequently, they emphasize paintings, lithographs, and sculptures that evoke the barricade only in its absence.  Yet while this dissertation reconfigures the image-map formerly in place, it does so not in pursuit of greater comprehensiveness - with the hope, as it were, of producing a total picture - but to capture something of the dividedness and fragmentariness that the barricade asserts even as it prefigures a world in which reconciliation is possible.  Much was at stake in the `30s and `40s, and this dissertation aims to sharpen our sense of the true diversity and depth of the contemporary response, both to the limbo of the July Monarchy and the ""dark times"" (Brecht's phrase) after June 1848.",ucb,,https://escholarship.org/uc/item/0kx3g616,,,eng,REGULAR,0,0
607,2043,Self-Assembly and Mass Transport in Membranes for Artificial Photosynthesis,"Modestino, Miguel Antonio","Segalman, Rachel A;",2013,"Recent environmental factors have triggered a strong interest towards the development of scalable technologies that can increase the share of renewable sources into our energy mix. Artificial photosynthesis systems are a promising alternative as they can simultaneously capture and store solar energy in the form of a fuel. These systems are based on photoelectrochemical (PEC) cells that can take low energy density reactants such as water and/or carbon dioxide and transform them into energy dense hydrogen or carbon containing molecules via light-driven processes. Deployable solar-fuel generators need to be able to produce fuels in a robust, scalable, and efficient manner. Despite the large number of studies focusing on this technology since its inception in the early 1970's, a system that can satisfy those three requirements does not exist. Significant innovation is required to develop cost-effective components that can perform the light-absorption, catalytic redox reaction, ion transport and product separation requirements. Additionally, understanding of component performance in integrated devices is crucial for developing high efficiency solar-fuel generators.This dissertation focusses on several aspects of component integration in solar-hydrogen generators.  The initial focus involves the development of self-assembly techniques of nanometer scale units to obtain architectures necessary for solar-fuel devices.  Starting with solutions of semiconducting nanorods and polymers, this work demonstrates that by controlling the evaporation rate during solvent casting, arrays of vertically aligned nanorods embedded in polymer films can be obtained over large areas (> 1 cm2). This architecture is desirable for the integration of hydrogen generating nanorods into integrated water splitting membranes, where H2 and O2 are evolved at physically separated sites. This work also describes how the structure of proton conducting membranes (NafionÂ®) is affected at inorganic interfaces such as the ones present in solar-fuels devices. The effects of thin-film confinement and wetting interactions are studied in Nafion thin-film model systems using a combination of X-ray scattering and mass transport characterization techniques. These studies show how confinement of Nafion films to thicknesses below 10 nm results in significant limitations to self-assembly, disruption of phase separation in the material and ultimately decrease in ionic conductivity. Wetting interaction also play a role in the orientation of conducting domains in the material. Hydrophobic surfaces results in a parallel orientation of ionic domains while films cast on hydrophilic substrates result in an isotropic orientation of domains. The differences in domain orientation also impact the mass transport behavior of the material.Additionally, this dissertation covers several topics related with the integration of components for the fabrication of practical hydrogen generators. Here we describe the development of a microfluidic test-bed for the incorporation of catalytic and membrane components in scales amenable for research. This tool allows for the simple exchange and quantitatively assessment of the performance of integrated electrochemical fuel generating devices as well as each of the individual components that participate in the process. Lastly, this work also describes engineering solutions that allow both membrane-separated water electrolyzers and solar hydrogen generators to operate under buffered electrolytes. This is achieved by using supporting electrolytes to carry the ionic current through the membrane while controlled convective streams around the membranes are implemented to avoid the formation of large concentration gradients between reaction sides. This development opens up the space of operating electrolytes for the incorporation of wide range of components that are not stable under strong basic or acidic conditions.",ucb,,https://escholarship.org/uc/item/0n82t3fn,,,eng,REGULAR,0,0
608,2044,"""Resocialize to Conquer the Future"": Incarceration and Reform in Rio de Janeiro","Thompson, David C","Holston, James;",2020,"This dissertation examines the interventions made in the lives and futures of those incarcerated in Rio de Janeiroâ€™s menâ€™s prisons in the name of â€œresocialization.â€ I argue that resocialization â€“ the network of practices oriented towards â€œreformingâ€ or rehabilitating those in prison â€“ animates the prison system and structures how both prison workers and the incarcerated apprehend and navigate the institution. That is, prisons come to understand, treat, analyze, punish, and ultimately release, those within their custody based on a simultaneous promise and demand for resocialization. They do so despite the prevalence of violence, overcrowding, and an entrenched culture of punitivism both within the criminal justice system and in Brazil more broadly. At the same time, many of those in prison are invested within the same normative trajectory of reform against which they are held accountable by prison workers. As such, the motto of the state prison administration â€“ â€œResocialize to Conquer the Futureâ€ â€“ represents a legal and moral injunction to reform that structures prison life and governance.The analytical focus of the dissertation centers on the points of encounter between incarcerated people and those workers or volunteers whose work falls under the broad mandate to resocialize. I examine the engagements of public defenders, evangelical missionaries, and psychosocial â€œtechniciansâ€ with rehabilitative projects and procedures. Additionally, I follow incarcerated people both during and after their imprisonment, as they endeavor to build and pursue futures for themselves while also struggling to pursue the ideals and expectations set out by resocializationâ€™s narrative. The relationship between these two groups, I argue, channels a series of anxieties surrounding race, gender and kinship, concerns that shape what becomes recognizable within the institution as a properly reformed subject. As a result, resocialization functions through foreclosure as much as potential or possibility; it becomes a tool wielded within prisons towards both punitive and emancipatory ends, often blurring the distinction between the two.Through this analysis, the dissertation offers a new form of engagement with resocialization, and rehabilitation more broadly, that remains open to its variegated effects beyond any ostensible successes or failures. I also demonstrate that the social continues to serve as an anchor for the project of incarceration, since both prisons and the imprisoned come to be understood in reference to â€œsocietyâ€ even as they are positioned as outside it. By underlining the anticipatory nature of prisons, I contend that incarceration constitutes a key site that determines what individual and collective futures are imaginable, particularly in the context of Brazilian democracy and its expanding penal state.",ucb,,https://escholarship.org/uc/item/0ps7f6x7,,,eng,REGULAR,0,0
609,2045,Surface Structure Probe of Transition Metal-Based Oxygen Evolving Systems with Spectroscopy,"Doan, Hoang Quoc","Cuk, Tanja;",2016,"The goal of a carbon free H$_{2}$ economy using a photoelectrochemical cell to split water is a worthwhile endeavor to solve the looming energy crisis. Many scientists have taken up this cause and an abundance of studies exists characterizing, optimizing, and creating materials to integrate into a full artificial photosynthetic system. However, despite widespread attention, a viable industrial-scaled photoelectrochemical device has yet to emerge due to low efficiencies, slow kinetics, and high energetic barriers. To that end, going back to the fundamentals may be a necessary step to understand what is causing the bottleneck, particularly within the heterogeneous water oxidation catalysts. Herein, the surface electronic structure of transition metal-based semiconductors and their 3$d$ valence electrons that actively participate in the oxygen evolution process are investigated using a number of spectroscopic techniques in an effort to unravel the mechanism and uncover important material properties. I focus on three major properties: 1) photo-excited carrier dynamics affecting the excited state electronic structure, 2) ground state electronic structure including covalent and atomic parameters, and 3) surface state-mediated interfacial hole transport.First, the transient electronic structure of Co$_{3}$O$_{4}$, a promising water oxidation catalyst, is probed via transient absorption spectroscopy. With selective excitation of key optical transitions, both inter- and intravalence transitions involving the 3$d$ electrons, the kinetics and spectrum are investigated. A wide range of pump and probe wavelengths, spanning the ultraviolet to the visible to the near infrared, are employed. Despite this range of pump and probe energies, the carrier dynamics were largely unaffected. Additionally, the kinetics and spectra show a unique independence to fluence and sample morphology.The kinetics reveal a photo-excited carrier density that quickly thermalizes when excited across the charge transfer transitions and converts into $d-d$ excitations. The recombination from these localized midgap $d$ states occurs at a longer, nanosecond time scale.In addition to perturbing the system via photo-excitation, the electronic structure of Co$_{3}$O$_{4}$ was tuned using an applied potential in a technique called spectroelectrochemistry. Spectroelectrochemistry reinforced the results of the transient absorption spectroscopy and confirmed the identity of the midgap $d$ states through determination of the energetics of these 3$d$ states as well as assignment of whether electrons or holes induced the absorptions or bleaches observed within the transient spectrum.Taken together, the results suggest a special type of intrinsic hole trap center that is a potentially promising long-lived state for utilization in photo-activation. Further, since the photo-excited hole is efficiently localized at these 3$d$ sites, the most likely water oxidation reaction intermediate is an oxidized cobalt center, \emph{i.e.} a Co(IV)=O species.The second investigation probes the ground state 3$d$ electronic structure of active and non-active water oxidation catalysts using X-ray absorption spectroscopy to determine an electronic structure-activity relationship. A set of molecular, homogeneous cobalt polyoxometalates serves as model systems for extracting electronic structure parameters, such as metal valence state (\emph{i.e.} 2+ or 3+), metal coordination environment (\emph{i.e.} tetrahedral or octahedral environments), structural distortions and covalency between the metal and oxygen ligands, from cobalt L-edge spectra. No definitive structure-activity relationship could be established because X-ray absorption spectroscopy could not distinguish between the number of metal atoms within the molecular structure or the identity of the heteroatom surrounding the metal(s) and ligands. These properties are what defines the extent of catalytic activity and point to the importance of using more sensitive techniques such as resonant inelastic X-ray scattering. However, ligand field multiplet theory was able to simulate well the experimental Co L-edge spectra for the well-defined model systems and report the ligand field parameter 10Dq to within an accuracy of $\pm$1~eV and the strength of electron-electron interactions to within $\pm$5$\%$ of atomic values. These parameters were subsequently applied to characterize a lesser-defined heterogeneous sample, a Co$_{3}$O$_{4}$ thin film.Lastly, the final chapter addresses the surface structure dynamics under \emph{in situ} conditions, \emph{i.e.} during the influence of water, on GaN. GaN, a widely studied semiconductor for integration as a photoanode in the water oxidation reaction, possesses a unique surface chemistry and a mobility that is experimentally accessible. Using surface sensitive transient grating spectroscopy, a quantitative value for the hole mobilities of both an undoped and a n-doped GaN film were determined in air and at a semiconductor/electrolyte interface. It was found that interfacial carrier mobility is highly dependent on the surface intermediates. For n-doped GaN, a large density of dark surface states in the form of various adsorbed species exist that can localize holes and, further, allow the holes to hop from metal site to metal site along the surface, possibly assisted by proton-coupled electron transfer with the water molecules. This charge transfer pathway during dark equilibration between the semiconductor electrochemical potential and water oxidation potential more than doubles the interfacial hole diffusivity compared to its value in air. This was confirmed under both acidic and neutral conditions, 0.1~M HBr (pH = 2) and 0.1~M Na$_{2}$SO$_{4}$ (pH = 7). In contrast, the hole diffusivity of undoped GaN with no significant surface state density is unchanged with introduction of an electrolyte solution.The results suggest that charge transport and surface reactivity, which are generally treated independently, are connected phenomena.",ucb,,https://escholarship.org/uc/item/0hs3s799,,,eng,REGULAR,0,0
610,2046,Spontaneous Symmetry Breaking in Nonrelativistic Systems,"Watanabe, Haruki","Vishwanath, Ashvin;",2015,"The subject of condensed matter physics is very rich â€” there are an infinite number of parameters producing a diversity of exciting phenomena. As a theorist, my goal is to distill general principles out of this complexity â€” to construct theories that can coherently explain many known examples altogether.This thesis is composed of several attempts to develop such theories in topics related to spontaneously symmetry breaking. A remarkable feature of many-body interacting systems is that although they are described by equations respecting various symmetries, they may spontaneously organize into a state that explicitly breaks symmetries. Examples are numerous: various types of crystalline and magnetic orders, Bose-Einstein condensates of cold atoms, superfluids of liquid helium, chiral symmetry in QCD, neutron stars, and cosmic inflation.These systems with spontaneously broken continuous symmetries have gapless excitations, so called Nambu-Goldstone bosons (NGBs). Although the properties of NGBs are well understood in Lorentz-invariant systems, surprisingly, some basic properties of NGBs such as their number and dispersion in nonrelativistic systems have not been discussed from a general perspective. In the first part of this thesis, we solve this issue by developing and analyzing an effective Lagrangian that coherently captures the low-energy, long-distance physics of many different symmetry-breaking states all at once.Next, we examine whether these NGBs originating from spontaneous symmetry breaking remain to be well-defined excitations inside a metal, where low-energy electrons near Fermi surface can collide with them. Our result is a one equation criterion that specifies whether the interactions between electrons and NGBs can be ignored, or whether it completely changes their character. In the latter case, unusual phases of matter such as non-Fermi liquids may arise; in that case, NGBs are overdamped and cannot form particle-like excitations in spite of the assumed symmetry breaking.In the last part of this thesis, we investigate the possibility of spontaneously breaking of Hamiltonian itself. The homogeneity of time is one of the most fundamental symmetries of nature, underlying the conservation of the energy. The question is whether this symmetry can be spontaneously broken, as suggested recently by Wilczek, in analogy with ordinary crystals. Contrary to his proposal that attracted a significant attention and stimulated many further studies, we prove a no-go theorem that rules out spontaneously breaking of time translation, in the ground state or in the canonical ensemble of a general Hamiltonian.",ucb,,https://escholarship.org/uc/item/0jb153bk,,,eng,REGULAR,0,0
611,2047,Numerical methods for the Landau-Lifshitz equation in micromagnetics : the mimetic finite difference method and the mass-lumped finite element method,"Kim, Eugenia Hail","Wilkening, Jon;",2017,"Micromagnetics is a continuum theory describing magnetization patterns inside ferromagnetic media. The dynamics of a ferromagnetic material are governed by the Landau-Lifshitz equation. This equation is highly nonlinear, has a non-convex constraint, has several equivalent forms, and involves solving an auxiliary problem in the infinite domain, which pose interesting challenges in developing numerical methods. In this thesis, we first present a low order mimetic finite difference method for the Landau-Lifshitz equation, that works on general polytopal meshes on general geometries, preserves non-convex constraint, is energy (exchange) decreasing, requires only a  linear solver at each time step and is easily applicable to the limiting cases. Secondly, we present a high order mimetic finite difference method for the Landau-Lifshitz equation which is third order in space and second order in time. In fact, it can be arbitrarily high order in  space. This method works on general polytopal meshes, and preserves the non-convex constraint in a certain sense. Lastly, we present a  new class of convergent mass-lumped finite element methods to solve a weak formulation of the Landau-Lifshitz equation. The scheme preserves a non-convex constraint, requires only a linear solver at each time step and is easily applicable to the limiting cases. We provide  a rigorous convergence proof that the numerical solution of our finite element method for the Landau-Lifshitz equation converges weakly to a weak solution of the Landau-Lifshitz-Gilbert equation.",ucb,,https://escholarship.org/uc/item/0mk6q9h9,,,eng,REGULAR,0,0
612,2048,"The Study of Disorder in Amorphous Silica, Alkali-Silica Reaction Gel and Fly Ash","Meral, Cagla","Monteiro, Paulo J.M.;Ostertag, Claudia P.;",2012,"Significant progress was achieved with the application of Rietveld method to characterize the crystalline, and nanocrystalline phases in portland cement concrete. However, to obtain detailed information on the amorphous phases present in concrete, it is necessary to analyze the total scattering data. The pair distribution function (PDF) method, that takes the Sine Fourier transform of the measured structure factor over a wide momentum transfer range, has been successfully utilized in the study of liquids and amorphous solids. The method provides a direct measure of the probability of finding an atom surrounding a central atom at a radial distance away. The obtained experimental characteristic distances can be used to validate the predictions by the theoretical models, such as, molecular dynamics simulations. This research focuses on obtained results of PDF analysis on silica fume, rice husk ash, fly ash, alkali-silica reaction gel, and opals.",ucb,,https://escholarship.org/uc/item/0n101652,,,eng,REGULAR,0,0
613,2049,Clinging to their Guns? The New Politics of Gun Carry in Everyday Life,"Carlson, Jennifer","Ray, Raka;",2013,"Alongside a series of high-profile massacres over the past decade, Americans continue to turn to guns as the solution to, rather than the cause of, violent crime. Since the 1970s, most US states have significantly loosened restrictions on gun carrying for self-defense, and today, over 8 million Americans hold permits to carry guns concealed. Contrary to popular images of gun culture, this is not a white-only affair: in Michigan, whites and African Americans are licensed to carry guns at roughly the same rate. And while women are licensed at rates far lower than men, their numbers are increasing. This dissertation presents an in-depth analysis of the new politics of concealed carry and asks: Why do Americans not just own guns but also carry them? What role does the NRA playin enabling people to carry guns? And finally, how do different kinds of gun carriers enact the model of citizenship advocated by the NRA?Through intensive ethnographic fieldwork and interviews with gun carriers and pro-gun advocates in the Metro Detroit area, it shows how suspicion of the state's power to police, combined with the embodied practice of gun carry, sustains a new politics of policing, crime and insecurity. Situating the appeal of guns in contexts of neoliberal decline facing Michigan, this book analyzes how, with the help of required NRA training, gun carry becomes an embodied, everyday practice through which gun carriers embrace a moral duty to protect not only oneself but also others (usually family, but sometimes strangers). I show how this moral duty is enacted differently by different groups of gun carriers: while all of the gun carriers I interviewed turned to guns to supplement what they viewed as inadequate police protection, gun carriers of color also mobilized gun rights as a way to defend against police abuse and assert their political rights, echoing the anti-statist position of groups such as the Black Panthers. Meanwhile, male gun carriers embraced their duty to protect self and others as a way to assert their social relevance as male protectors amid their declining status as breadwinners, while female gun carriers tended to emphasize their individual right to self-defense as an act of empowerment.Overall, this dissertation argues that for pro-gun Americans, the carrying of guns is a means of practicing good citizenship amid perceptions of social disorder. This understanding of American gun politics helps to clarify both why Americans so vociferously 'cling to their guns' as practical and symbolical tools of policing, and it also sheds light on the NRA's hidden power as the primary organization that trains and certifies Americans to carry guns.",ucb,,https://escholarship.org/uc/item/0nx042hb,,,eng,REGULAR,0,0
614,2050,Rule by Aesthetics: World-Class City Making in Delhi,"Ghertner, David Asher","Watts, Michael J;",2010,"This project addresses the cultural and environmental politics of slum demolitions in the making of a ""world-class city."" If ""modern"" cities are supposed to be built through techno-scientific procedures of urban planning and government--such as maps, censuses, and surveys--the conspicuous absence of such techniques in the world-class redevelopment of Delhi raises the question of how rule there is achieved. Based on an ethnography of the judiciary, state, and property owners' associations, I find that what I call a world-class aesthetic--an idealized vision of a privatized, ""green,"" and slum-free city assembled from transnationally circulating images of ""global cities""--has replaced these techniques as the key instrumentality of rule in contemporary Delhi. To explore how this aesthetic regime of planning operates, I begin by demonstrating how the city's new ""good governance"" initiative, called Bhagidari, has reconfigured state space, providing property owners' with privileged channels of access to the judiciary and bureaucracy. By tracing the circulation of key representations of the slum through and beyond these channels, I show how discourse depicting slums as ""nuisances""--i.e., as illegal environments--constructs an aesthetic grid that demarcates the ""world-class"" on the one hand, and the ""polluting"" on the other. I further reveal how the judiciary has codified this world-class aesthetic through a reinterpretation of nuisance law, recalibrating the axes of legality and planning such that those spaces appearing polluting and dirty (e.g., slums) are deemed unplanned and illegal, regardless of their statutory basis in planning law or actual environmental impact. Conversely, spaces that have a world-class ""look"" (e.g., shopping malls, sports stadia), despite violating land-use codes and environmental standards, are deemed planned and legal. More than just reconfiguring state power, this aesthetic regime of planning alters how citizens see and engage the state, as well as how they envision their place in the city. Based on a year of ethnographic fieldwork in a Delhi slum, I show how residents have begun to shift the basis on which they advance citizenship claims away from an earlier idiom of historical entitlement to public land and toward one of potential self-improvement via home ownership. This shift, however, cannot be reduced to an overarching neoliberal rationality, but must rather be located in residents' changing affective ties to place and city. Specifically, I trace how a series of political technologies--including government-run slum surveys, media campaigns, and a broad set of changes in the meaning of landscape--train slum residents to see the city through the lens of world-class aesthetics. In arguing that projects of rule are secured as much through embodied practices and aesthetic dispositions as through reason or ideology, the dissertation argues for the importance of everyday aesthetic practices as a key terrain on which political possibilities and urban visions are produced.",ucb,,https://escholarship.org/uc/item/0pt7w0c5,,,eng,REGULAR,0,0
615,2051,"Contaminating Conversions: Narrating Censorship, Translation, Fascism","Escolar, Marisa Abby","Spackman, Barbara G;",2011,"In an idealized formulation, translation and censorship mark opposite points on the spectrum of signification: if translation works to raze boundaries between text and reader, censorship strives to raise them. In scholarship on fascist Italy, this polarized definition was particularly widespread: censorship was cast exclusively as the tool of the powerful, and translation, as a way for intellectuals to smuggle subversive ideas into a xenophobic society. However, despite their apparently antithetical aims, literary historians have found archival evidence to support a more nuanced view of censorship and translation under fascism, and seminal theoretical contributions by Sigmund Freud, Michel Foucault and Jacques Derrida have led to wide recognition of a chiastic relationship between â€œrepressiveâ€ censorship and â€œproductiveâ€ translation. If, today, in light of the numerous fields which take translation and censorship as their objects of study (literature, history, psychoanalysis, cultural studies), it were possible to make a single claim about these processes, it would speak to their ability to destabilize notions of discipline, author and text. This dissertation uses these transgressive, polyvalent phenomena to unsettle a rather stubborn boundary in the postwar Italian literary panorama: â€œfascistâ€ and â€œantifascistâ€ literature, specifically as it relates to fictionalized wartime narratives by Curzio Malaparte (1898-1957), Elio Vittorini (1908-1966) and Beppe Fenoglio (1922-1963), intellectuals who are key for different yet convergent reasons: Malaparte, who abandoned fascism in 1931, has been depicted as an unscrupulous chameleon; Vittorini, who became antifascist in 1936, has become a symbol of redemptive conversion; Fenoglio, a partisan in the Resistance, has become synonymous with antifascist literature. In bringing together these texts, traditionally understood as reflections of their authorsâ€™ politics, my dissertation examines how the rise and fall of fascism is narrativized and suggests that these political conversionsâ€”never before considered comparativelyâ€”share common anxieties as they attempt to renegotiate boundaries not just between â€œfascistâ€ and â€œantifascistâ€ but also between other categories of identity, including race and gender.While my dissertation locates such overlaps in â€œfascistâ€ and â€œantifascistâ€ texts, it also identifies a preoccupation with renegotiating boundaries shared by these fascist era texts and their postwar criticism. Perceived as a site for struggle between intellectuals and the regime, censorship and translation maintain their tantamount importance today, as their oppositional definitionsâ€”still widely held in the scholarship on Malaparte, Vittorini and Fenoglioâ€”contribute to the reification of the emblematic positions of these intellectuals and their texts. My readings explore the ways in which censorship and translation have shaped perceptions of the textsâ€™ production and afterlife, but also how they inform narrative dynamics. After an introduction to my theoretical methods in Chapter One, in Chapter Two, I look at how categories of sexual and political â€œintegrityâ€ and â€œviolationâ€ intertwine in the plots of two novels by Vittorini, Garofano rosso [The Red Carnation], a famously censored fascist bildungsroman, and Conversazione in Sicilia  [Conversation in Sicily], a story of an antifascist awakening, whose stylistic innovations were held to be the result of his experience translating Anglophone literature. As I offer new readings of previously marginalized scenes, I show how the criticsâ€™ rhetorical emphasis on the censorâ€™s â€œviolationâ€ of Garofano and the â€œintegrityâ€ of Conversazione has helped construct Vittoriniâ€™s emblematic position as a redemptive antifascist convert, obscuring the representations of sexual violence on which the protagonistsâ€™ conversion depends. In Chapter Three, I focus on Fenoglioâ€™s Partigiano Johnny [Johnny The Partisan], purportedly born through self-translation from English to Italian, and show how its status as a literary monument to antifascism is predicated upon philological â€œconversionâ€ efforts to restore the text to its uncensored form and thus tell the â€œcompleteâ€ story of the Resistance. I argue that these efforts paradoxically censor a number of suggestive tensions that speak to the trauma of Italyâ€™s civil war. In Chapter Four, I turn to Malaparteâ€™s La pelle [The Skin], a polemical novel still caught in a debate on whether it deserves to be freed of its â€œcultural censorship.â€ While the critics debate its â€œfidelityâ€"" to the â€œtruthâ€ of the war, I explore the textâ€™s construction of its own truth claims, as it represents moments of â€œtranslationâ€ between the Allies and the Italians.",ucb,,https://escholarship.org/uc/item/0r7943w7,,,eng,REGULAR,0,0
616,2052,A Tale of a Tail,"Chang-Siu, Evan Heng Seng","Tomizuka, Masayoshi;Full, Robert;",2013,"Rapid terrestrial locomotion is a fascinating but difficult problem that roboticists face. Considerations must be made towards dynamics, controls, estimation, the environment, electronics, and mechanical design. Wheeled and legged robots have been heavily researched and have gained much success, however they still have their limitations, especially when ground contact is lost.Inspired by the high performance of lizards actively using their tails to stabilize perturbations and rapidly reorient their body, we propose to add an active inertial appendage to terrestrial robots that will greatly improve their stability and maneuverability.In this dissertation we present three main focus areas. We first develop the capabilities of a novel tailed robot with an active single degree-of-freedom (DOF) tail. We then discuss the single degree of freedom orientation sensing developed for this robot. Finally, we construct a nonlinear controller to enable reorientation of the body of a 2-link robot in three-dimensions while being constrained to a tail that only has two DOF of actuation.The first 177 (g) active tailed robot, Tailbot, has a single DOF of actuation and contains attitude inertial sensors. By utilizing both contact forces and zero net angular momentum maneuvering, this tailed robot can rapidly right itself in a fall, avoid flipping over after a large perturbation, and smoothly transition between surfaces of different slopes. We used a modeling approach to show that a tail-like design offers significant advantages to other alternatives, including reaction wheels, when the speed of response is important.In order to provide the reliable orientation sensing for the single DOF robot we design a novel time varying complementary filter. Complementary filtering (CF) is a signal processing methodthat is commonly used for the fusion of gyroscope and accelerometer measurements in order to robustly estimate the attitude of a rigid body. The traditional CF uses linear time invariant filters to combine the two different measurements of the same physical quantity. Here we present an extension to the CF method with time-varying filters. A fuzzy logic method is developed to adjust the parameters. Stability analysis as well as experimental results are presented to verify the proposedmethod.Finally, we propose a nonlinear control scheme for attitude control of a falling, two link active tailed robot with only two DOF of actuation. We derive a simplified expression for the robot's angular momentum and invert this expression to solve for the shape velocities that drive the body'sangular momentum to a desired value. By choosing a body angular velocity vector parallel to the axis of error rotation, the controller steers the robot towards its desired orientation. The proposed scheme is accomplished through feedback laws as opposed to feedforward trajectory generation, is fairly robust to model uncertainties, and is simple enough to implement on a low power microcontroller.We verify our approach by implementing the controller on a small 175 (g) robot platform, enabling rapid maneuvers approaching the spectacular capability of animals.",ucb,,https://escholarship.org/uc/item/0t05b8tp,,,eng,REGULAR,0,0
617,2053,"Causal attribution: insights from developmental, cross-cultural work on social and physical reasoning","Seiver, Elizabeth","Gopnik, Alison;",2013,"This series of studies examines the relationship between causal inference and attribution from a developmental and cross-cultural perspective. In the first study, we consider how children at the ages of four and six reason about person by situation covariation information (Kelley, 1967), both younger than previously demonstrated trait biases or spontaneously using trait words. We then compare children's explanations of people's behavior to the actual behavioral evidence. Next, we extend the paradigm to include physical causation and the potential overhypotheses that guide the formation of domain specific reasoning. We will determine whether there are domain differences in perceiving virtually identical data construed as physical events or intentional actions. And finally, in the last study, we examine the role of culture in shaping the developmental trajectory of attributional style. As in physical causal inference, social causal inference combines covariational evidence and prior knowledge. We can use the tools of causal inference to understand how culturally-mediated prior beliefs affect the construal of person-by-situation information in a domain specific manner and trace the origins of attributional biases in adults.",ucb,,https://escholarship.org/uc/item/0vx671jb,,,eng,REGULAR,0,0
618,2054,Design of Reconfigurable Radio Front-Ends,"Xiao, Xiao","Nikolic, Borivoje;",2016,"Modern and future mobile devices must support increasingly more wireless standards and bands. Currently, multi-band coexistence is enabled by a network of discrete, off-chip components that are bulky, expensive, and narrowband. As transceivers are required to accommodate an increasing number of wireless bands, the required number of discrete components increases accordingly, resulting in greater bill of materials (BoM) cost and front-end module (FEM) area. This work focuses on design techniques to enable front-end integration and reconfigurability in multi-band radios.In the first part of this work, we present a wideband spectrum sensing receiver with high sensitivity, wide dynamic range, and low power overhead. Reconfigurability in multi-band radios requires environmental awareness, and spectrum sensing can be used for optimal channel selection and adaptive interference suppression. The 300MHz-700MHz spectrum sensing receiver uses subsampling downconversion and digital-analog hybrid correlation to achieve -104dBm sensitivity and 84dB dynamic range for a 6MHz channel while consuming only 28mW of power. In the second part of this work, we present a wideband time division duplex (TDD) front-end with an innovative transmit/receive (T/R) switching scheme. T/R switches conventionally are off-chip components, and existing integrated designs have been narrowband or high loss. We propose a wideband integrated T/R switching technique in which the PA is re-used as an LNA during receive mode, and we demonstrate this in a 20dBm polar transmitter that can be re-purposed into a 3.4GHz-5.4GHz LNA achieving -6.7dBm P1dB and 5.1dB NF. The two systems presented in this thesis contribute key innovations towards fundamental aspects of future reconfigurable radios - greater front-end integration, wideband transceiver design, and spectrum awareness.",ucb,,https://escholarship.org/uc/item/0xg9b3v4,,,eng,REGULAR,0,0
619,2055,A Platform-Based Approach to Verification and Synthesis of Linear Temporal Logic Specifications,"Iannopollo, Antonio","Sangiovanni-Vincentelli, Alberto L;",2018,"The design of Cyber-Physical Systems (CPS) is challenging as it requires coordination across several domains (e.g., functional, temporal, mechanical). To cope with complexity, rarely a CPS is built from scratch. Instead, it is assembled by reusing available components and subsystems. If a component is not available, then it is made to order according to a specification which ensures its compatibility with the rest of the system.To achieve design goals faster while guaranteeing system safety, the correct instantiation of modules and subsystems is essential. Formal specifications, such as those expressed in Linear Temporal Logic (LTL), have the potential to drastically reduce design and implementation efforts by enabling rigorous requirement analysis and ensuring the correct composition of reusable designs. Composing formal specifications, however, is a tedious and error-prone activity, and the scalability of existing formal analysis techniques is still an issue. In this dissertation, we present a set of techniques and algorithms that leverage compositional design principles to enable faster verification and correct-by-construction, platform-based synthesis of LTL specifications. In our framework, a design is a composition of several components (which could describe both hardware and software elements) represented through their specifications, expressed as LTL assume/guarantee interfaces, or contracts. The collection of all the available contracts, i.e., a library, describes the design platform. The contracts in the library are the building blocks of different possible designs, and they are simple enough that their correctness can be easily verified, yet complete enough to guarantee the correct and safe use of the components they represent. Our contribution is two-fold. On the one hand, we address the verification task: given an existing composition of contracts from the library, we want to check whether it satisfies a set of desired requirements. We improve the scalability of existing verification techniques by leveraging pre-verified relations between contracts in the library. On the other hand, we enable specification synthesis: given a (possibly incomplete) set of desired system properties, we are able to automatically generate a composition of contracts, chosen from a library, that satisfies them. We do so by devising a set of algorithms based on formal inductive synthesis, where a candidate is either accepted as correct or is used to infer new constraints and guide the synthesis process towards a solution. Additionally, we show how to increase the scalability of our approach by leveraging principles from the contract framework to decompose a synthesis problem into several independent tasks, which are simpler than the original problem. We validate our work by applying it to several industrial-relevant case studies, including the problem of verification and synthesis of a controller for the electrical power system of an aircraft.",ucb,,https://escholarship.org/uc/item/0xx972bn,,,eng,REGULAR,0,0
620,2056,"Displaying ""The Natural World"" for Public Curiosity: U.S. Science Museum Transformations, from Lewis & Clark to the Exploratorium","Holzmeyer, Cheryl Ann","Swidler, Ann;",2012,"This dissertation analyzes the U.S. science museum field over time in order to examine institutional emergence, institutional transformation, and changing patterns of science boundary-work in displaying ""the natural world"" to publics. It investigates the constitution of the U.S. science museum field via 19th century natural history museums and world's fairs, and the 20th century transformation of the field by industrial science museums and science center museums. Its theoretical contribution is to underscore the significance of material culture and its spatial dimensions to analyzing institutions, including patterns of boundary-work between publics and science. It argues that industrial science museums and their novel exhibitionary conventions arose as industrial material culture became framed as ""applied science,"" and as distinct from artifacts in the existing field of natural history museums. In addition, it argues that science center museums distinguished themselves from and proliferated more rapidly than industrial science museums due not only to new constituencies mobilized on their behalf, but also due to their de-emphasis on collections, particularly of rare and historical artifacts. These changes again facilitated new exhibitionary conventions. Thus the emergence, transformation and proliferation of institutions hinge on their material cultural dimensions, on multiple levels.",ucb,,https://escholarship.org/uc/item/0zf2x0k3,,,eng,REGULAR,0,0
621,2057,The Development and Application of Homogeneous and Heterogeneous Late Metal Complexes in Catalysis,"Witham, Cole","Toste, Dean;",2010,"In recent years, gold(I) complexes have seen increased use as catalysts for the activation of alkynes toward nucleophilic addition.  Now, studies show that carbeneâ€“like intermediates are accessible, often depending on the substrates and catalyst ligands utilized.  While metal carbenoid behavior for gold(I) has been proposed in other transformations, our research has expanded the reactivity of gold beyond simple electrophilic activation, based on evidence that gold can function as both a Ï€â€“acid and as an electron donor.  For instance, electron donation from gold dâ€“orbitals can serve to facilitate the formation of gold carbenoid intermediates after initial cycloisomerization.  The gold carbenoid species is then capable of undergoing subsequent reactivity to generate synthetically useful products that can also be further functionalized.  Chapter 1 will discuss the development of new gold(I) carbenoid reactivity whereby an oxidation even results in oxygen atom transfer to the carbenoid center to generate carbonyl functional groups.  The broad range of reactions for which this methodology can be applied will be discussed as will catalyst and ligand effects for the transformations.  During the course of our investigations into homogeneous gold catalysis, several relevant questions consistently arise.  Among them are whether the catalyst can be recovered and recycled, what the cost is of the catalyst, and how load the catalyst loading can be.  In order to address these issues, we initiated a research program focused on studying the translation of homogeneous catalytic processes to heterogeneous ones, all in the solution phase.  In doing so, we could obtain catalytic activity with heterogeneous catalysts for reactions that were previously only obtained homogeneously.  Chapter 2 details our efforts in pursuit of this goal and successes in developing oxidatively modified, electrophilic platinum nanoparticles that display activity for a range of Ï€â€“bond activation/cyclization reactions.  Our characterization studies for the designed nanoparticle system and examination of catalyst activity are presented, as are catalyst leaching investigations.  Initial efforts into flow reactors and new dendrimer capping agents are also discussed.  The discovery of treatments for nanoparticles that induce the desired homogeneous catalytic activity should lead to the further development of reactions previously inaccessible in heterogeneous catalysis.",ucb,,https://escholarship.org/uc/item/10471465,,,eng,REGULAR,0,0
622,2058,Modeling and Simulation of Electrical Breakdown in DC for Dielectric-Loaded Systems with Non-Orthogonal Boundaries Including the Effects of Space-Charge and Gaseous Collisions,"Aldan, Manuel Thomas Pangelinan","Leung, Ka-Ngo;Verboncoeur, John P;",2015,"Improved modeling of angled-dielectric insulation in high-voltagesystems is described for use in particle-in-cell (PIC) simulations.Treatment of non-orthogonal boundaries is a significant challenge inmodeling angled-dielectric flashover, and conditions on boundaries aredeveloped to maintain uniform truncation error in discretized spaceacross the dielectric angles studied. Extensive effort was expended inisolating particular operating regimes to illustrate fundamentalphenomenological surface effects that drive the discharges studiedherein; consequently, this document focuses on the phenomenology oftwo specific dielectric angles at 6.12Â° for multiplicative breakdown(the so-called single-surface multipactor) and 22.9Â° for anon-multiplicative discharge that evolves into a dark current atsteady state.Phenomenological results for simulations in vacuum through ""ultra-lowpressures"" on the order of a few hundred mTorr are presented. Amultipactor front forms via net emission of electrons from impact onthe dielectric surface, where emission leads to saturated fieldconditions in the wake of the front, producing a well-definedforward-peaked wave.  A treatment of the gain and saturationcharacteristics is presented, isolating the surface electric fields asthe driving contributor to both metrics. Physical models includeoftenneglected effects such as space-charge, dielectric-surfacecharging, and particle distributions in energy and space. For thedischarges treated in this study, breakdown voltages of the typicalPaschen form are not applicable, since multiplicative conditions aredriven primarily by surface effects.Phenomenological results are also presented for simulations at lowpressure (~ 1Torr), which is shown to be a transitional limit wherevolume effects become appreciable compared to surface effects. Acoupling between space charge and surface charge is shown to lead tooscillatory effects in otherwise DC discharges. Surface multipactorleads to increased ionization and space charge, and the ensuingspace-charge momentum alters what would have been a steady-statesaturation as in the case of vacuum-like discharges. Models fordiffusive outgassed species are developed and implemented, extendingthe capabilities of the PIC suite.The overarching theme of this study is to communicate the dependenceof multiplicative discharges dominated by surface effects onnear-surface electric field conditions. It is shown through variousexamples from vacuum through low pressures, and in diffusive gases,that single-surface multipactor conditions can be expressed solely interms of the dielectric surface field angles. This treatment lays thefoundation for a novel extension of RF breakdown susceptibility theory[1] to the DC regime, grounding breakdown susceptibility to thewell-established fundamentals on secondary emission [2, 3]. Thistheory shows that breakdown characteristics can be modeled in ana-priori framework, hence the lack of a Paschen-type curve.Finally, the effect of the seed source on discharge characteristics isalso studied. A comparison between a constant-waveform source, aFowler-Nordheim source, and an application of a modified source basedon theoretical treatment from [4] are presented, showing that the seedis a necessary but insufficient condition for surface flashover, wherethe dominant contributor is the configuration of the surface fieldsdownstream of the seed source. While the seed can influence upstreamconditions to alter the injected current, the gain characteristics ofthe downstream region are still well described by the frameworkdeveloped in the remainder of this document.",ucb,,https://escholarship.org/uc/item/11d438fc,,,eng,REGULAR,0,0
623,2059,Reliability-Based Optimization for Maintenance Management in Bridge Networks,"Hu, Xiaofei","Madanat, Samer;Daganzo, Carlos;",2014,"This dissertation addresses the problem of optimizing maintenance, repair and reconstruction decisions for bridge networks. Incorporating network topologies into bridge management problems is computationally difficult. Because of the interdependencies among networked bridges, they have to be analyzed together. Simulation-based numerical optimization techniques adopted in past research are limited to networks of moderate sizes.       In this dissertation, novel approaches are developed to determine the maintenance policies that best balance network performance and agency cost. For two different types of networks, two performance metrics are adopted, and the research is divided into two parts accordingly.       The first part focuses on moderate-size networks with limited redundancy. The network performance is quantified by a graph-theoretic indicator of network connectivity, since connectivity is the fundamental service function of a network. The objective is to ensure an adequate level of network connectivity at the lowest possible life-cycle maintenance cost. A novel two-stage approach is developed, which makes it possible to solve the problem by using standard optimization tools (with guaranteed convergence to optimality), as opposed to the heuristic algorithms used in related literature.       The second part studies large and redundant networks, and the network performance is quantified by the total user costs due to potential bridge failures. The objective is to minimize the total user costs, specifically, the extra travel distance over a planning horizon and under a budget constraint. It is conjectured and then verified that the expected increase in vehicle-miles travelled due to failures can be approximated by the sum of expected increases due to individual failures. This allows the network-level problem to be decomposed into single-bridge problems and tackled efficiently. The computational effort increases linearly with the number of bridges.",ucb,,https://escholarship.org/uc/item/11v668vt,,,eng,REGULAR,0,0
624,2060,Risk Dynamicsâ”€An Analysis for the Risk of Change,"Huang, Tailin","Bea, Robert G.;Kastenberg, William E.;",2010,"The concept of risk has evolved over the centuries of human history. People care about risk because much of our property and human lives are constantly at stake in the face of unforeseeable future. Unlike the fixed, known past, the future is always uncertain to us. In fact, such uncertainty is where risk arises. Thus, people assess risk by identifying sources of uncertainty and manage risk by trying to reduce those uncertainties. Indeed, existing risk analysis may be reduced to an endless anticipation of hazardous events, followed by a quantification of how likely those events are to happen and what their consequences are. Those approaches were originally developed for relatively well structured mechanical problems. However, our society inexorably marches towards greater complexity. On top of such natural progression, the advance of information and communication technology has made the rate of society's development faster than ever. Everything in society changes over time and the complexity of change brings us the uncertain future that perplexes our decision-making processes. Our current conceptual framework for risk analysis is now facing serious challenges due to the rapid pace of change in today's societies. How can we analyze risk when systems are constantly changing? To answer this question, this research will reexamine the concept of risk and investigate the uncertainty and complexity of change, to understand the very nature of risk in the ever-changing systems. In Newton's laws of motion and the ways of traditional Chinese medicine, we have found a new perspective of risk and a new way to analyze it.",ucb,,https://escholarship.org/uc/item/12m4t24n,,,eng,REGULAR,0,0
625,2061,The Relationship Between Mating Systems and Selection on Immunogenes in the Genus Peromyscus,"MacManes, Matthew David","Lacey, Eileen A;",2011,"Understanding the selective forces responsible for the generation and maintenance of genetic and phenotypic diversity is one of the fundamental questions facing evolutionary biologists.  Indeed, understanding how both biotic and environmental interactions lead to observable differences in the natural world is one of the most exciting, yet challenging of problems. This work has focused on the analysis of one specific type of biotic interaction--that of sexual behavior.  In nature, patterns of sexual behavior are diverse, and include instances of monogamy, polygyny, and even promiscuity.  Historically, these classes of behavior have been considered distinct entities, yet a growing number of studies indicate that variation within each category is continuous.  The first chapter of this work introduces the problem, and introduces the idea of a quantitative index that allows for a more detailed understanding of pattern and process.Differences in mating system, specifically the contrast between `true' monogamy and promiscuity, provide the context for the remaining work. The relationship between sexual behavior and pathogen transmission is well documented in humans.  Specifically, the risk of sexual transmission of disease increases with the number of sexual partners. Because pathogens have a negative effect on fitness, genes that underlie immune function should experience enhanced selection in species characterized by frequent transmission. While this relationship is supported by common sense, it had not been formally tested.Using rodents of the genus Peromyscus, I tested the relationship between sexual behavior and vaginal bacterial diversity.  Specifically I studied vaginal bacterial from sympatric populations of the promiscuous P. maniculatus and the monogamous P. californicus. Using molecular methods, I show that bacterial diversity, and thus the number of pathogens and potential pathogens, is greater in promiscuous species.  Next, in the same population, I examined patterns of natural selection at the immunogene MHC-DQÎ±.  Although I expected selection to be enhanced in the promiscuous species, I found the opposite--that selection was stronger in the monogamous P. californicus.  I then attempted to generalize these findings, using an evolutionarily isolated origin of monogamy within the genus, P. polionotus as well as 19 other Peromyscus species.  Using a phylogenetically controlled maximum likelihood analysis, I show that selection is enhanced in both monogamous lineages, thus providing convincing evidence that although vaginal bacterial diversity is greater in promiscuous taxa, selection on immunogenes is stronger in monogamous.  Although the reason for this finding is currently unknown, I argue that mate choice, which is thought to be more important in monogamous species, may be responsible.Last, and conceptually unrelated to the other works, I describe the genetic underpinnings of social behavior in the social tuco-tuco, Ctenomys sociabilis, which has been studied both in the field and in the lab by Professor Eileen Lacey.  Using Illumina sequencing of mRNA from the hippocampus of wild caught animals, I identified several hundred genes that appeared to be highly expressed in social tucos.  Although the function of many of these genes is unknown, several, including GABA, calcineurin, and septin genes have known function related to social behavior or aggression. While these studies are currently in their infancy, novel sequencing techniques may allow us insight into the genomic correlates of complex phenotypes (e.g. social behavior) that until now have been inaccessible.",ucb,,https://escholarship.org/uc/item/1394w97z,,,eng,REGULAR,0,0
626,2062,Performance-Based Decision-Making in Post-Earthquake Highway Bridge Repair,"Gordin, Eugene","Stojadinovic, Bozidar;",2010,"Post-earthquake highway bridge repair is an ever-present part of the lifecycle of transportation systems in seismic regions. These repairs require multi-level decisions involving various stakeholders with differing values. The improvement of the repair decision process, repair decision itself, and repair decision outcomes, requires an evaluation of current practices in post-earthquake repair decision-making.This dissertation assesses these current practices within the California Department of Transportation (Caltrans), outlines areas where the current process is ineffective, and highlights areas for improvement. Current repair decision-making practice is focused on the repair of individual bridges given a limited set of established repair methods. To improve upon these practices, this dissertation presents the Bridge Repair Decision Framework (BRDF), a new and unique methodology that allows for simultaneous consideration of all earthquake-damaged bridges as individual elements of a larger regional transportation system. This systematic approach enables the achievement of short- and long-term transportation system performance objectives while accounting for engineering, construction, financing, and public policy constraints. Furthermore, the BRDF allows for continuous refinement of the decision-making process to incorporate engineering and construction innovations, changes in the financial and public policy environment and, most importantly, changes in transportation system performance goals. While existing methodologies allow the incorporation of some of these changes, the BRDF provides a flexible structure that can account for all of these changes simultaneously. This is accomplished through a rigorous, performance-based, and risk-informed decision-making approach that presents repair decisions using a traditional engineering demand-capacity inequality. As a result, the BRDF empowers decision-makers with a holistic understanding of the transportation network condition on a microscopic (bridge) as well as macroscopic (overall system) level. The BRDF also accounts for the probabilistic nature of the earthquake hazard, bridge seismic capacity, and subsequent repair decisions, providing decision-makers with transparency regarding the uncertainties of system condition, repair method reliability, construction workforce availability, and public and business risks. BRDF decision-outcomes are technology-neutral as a result, greatly expanding the range of repair method alternatives that a decision-maker may consider while allowing for tradeoffs to be made between performance, cost, and time in light of transportation system condition and constraints. The BRDF is validated using a simulated bridge system case study that requires post-earthquake repair. This study was designed to demonstrate the functionality of the framework and to examine two alternate decision-making strategies: one with complete and the other with incomplete post-earthquake bridge damage state information.  This case study led to refinements in the framework and insights about the benefits of additional information on the damage state of bridges in terms of overall repair time and cost of the regional transportation system. Additionally, the validation revealed areas where the current BRDF can be improved in future studies. The BRDF was created for large public transportation organizations such as the California Department of Transportation (Caltrans), where implementation of the BRDF requires several important prerequisites, including new database creation and additional training for engineers. Once implemented however, the BRDF allows decision-makers to potentially reduce repair costs and times, minimize system downtime, make better investments, and account for transportation system performance goals given current financial and public policy constraints.",ucb,,https://escholarship.org/uc/item/1435v6hp,,,eng,REGULAR,0,0
627,2063,CMOS Magnetic Particle Flow Cytometer,"Murali, Pramod","Boser, Bernhard E;",2015,"Neutrophils, a class of white blood cells, are our bodyâ€™s first line of defense against invading pathogens. When the number of neutrophils in blood drops to 200cells/$\mu$L, it leads to a critical clinical condition called neutropenia. Currently, optical flow cytometry is the most common and powerful technique used to diagnose neutropenia, but the centralized nature of the test, time-consuming sample preparation and high cost prevent real-time modification of treatment regimens. In this thesis, we  propose an  approach of using  magnetic  labels to tag and detect cells that allows us to design a low cost point-of-care flow cytometer to diagnose neutropenia. The cytometer cartridge integrates a gravity driven microfluidic channel with a CMOS sensor chip that detects magnetically labeled cells as they flow over it. The sensor combines an on-chip excitation coil that magnetizes the labels and a pick-up coil that detects them. The high frequency RF signal from the sensor is down-converted and amplified by on-chip receiver circuitry, which has been optimized to maximize the signal to noise ratio. The functionality of the cytometer is demonstrated with SKBR3 cancer cells labeled with 1$\mu$m magnetic labels using streptavidin-biotin chemistry. The SKBR3 cells are used in lieu of neutrophils as they can be cultured in a laboratory setting and pose minimal bio-hazard.  Furthermore, the high frequency operation of the sensor enables classification of two types of magnetic labels, which is necessary to obtain absolute cell-counts.",ucb,,https://escholarship.org/uc/item/14m1895k,,,eng,REGULAR,0,0
628,2064,"The Cold War Poetics of Muktibodh: a Study of Hindi Internationalism, 1943-1964","Goulding, Gregory Young","Dalmia, Vasudha;",2015,"My dissertation deals with the poetry, short fiction, and critical writing ofGajanan Madhav Muktibodh (1917-1964). In my dissertation I argue that Muktibodh's workâ€”inspite of or even because of its conscious affiliation to a Marxist idea of international revolutionand progressâ€”presents a critique of postcolonial modernity rooted in the experience of thepost-Independence middle class. Through an account of the ways in which his writings wereable to bring into poetic life a range of issuesâ€”not least the imagination of an alternativeglobality in the wake of Indian independenceâ€”a picture emerges of the hopes and anxieties ofan emerging lower middle class beginning to speak on a global stage. My work relies on amethodology of close readings of Muktibodh's work with an attention to the formal qualities oflanguage, situated in a theoretical understanding based upon Critical and Postcolonial Theory.In my first chapter, I discuss the life and reception of Muktibodh, who has been a central figure in Hindi literary criticism since his early death in 1964. In the first part of this chapter I examine Muktibodh's life and career, with special attention to the influence of Central India and its distinct literary culture. I then examine Muktibodh's reception, focusing on two main stages. In the first, roughly within the decade after his death, Muktibodh's works are seen as a referendum on debates over aesthetics and politics that dominated Hindi criticism in the 1950s. In the second, which takes place in the late 1970s, Muktibodh is seen as a uniquely prophetic figure, even as he is reinterpreted to fit contemporary concerns. The second chapter of my dissertation examines Muktibodh's interest in science and technology. This interest, typically overlooked in criticism of Muktibodh, brings into light Muktibodh's engagement with the discourse of science and technology in the context of the Cold War. In this chapter, through the examination of his poetry and fiction, I show how Muktibodh used the language and thematics of science and technology to raise questions about India's new relationship to the international.The third and fourth chapters of this dissertation analyze in detail the formal evolution of Muktibodh's poetry towards the ""Long Poem"" forwhich he is most known. By tracing the sources of the Long Poem both to the influence of the ChÄyÄvÄd [Romantic] poet Jayshankar Prasad, as well as the influence of modernist Marathi poetry on the bilingual Muktibodh, I show how Muktibodh's long poem emerges as a unique form in Hindi modernism, one rooted in the literary culture of Central India in which he worked. This chapter therefore demonstrates the complex networks through which Hindi poetry developed. In my fourth chapter, I compare ``Aá¹dhere meá¹,'' which is Muktibodh's most well-known long poem, with the more obscure ``Bhaviá¹£yadhÄrÄ.'' This chapter shows how ``Aá¹dhere meá¹,'' even as it depicts a nightmarish version of contemporary reality, takes as its subject the role of the autonomous imagination in engaging with the world. These chapters, in analyzing Muktibodh's long poem, point towards the possibilities of a formal analysis of modernist Hindi poetry, and its importance for understanding larger trends in intellectual, cultural, and literary history. The final two chapters take up Muktibodh's views towards realism and genre, issues that are present in the discussion of the long poem, but emphasized here in an investigation of his short fiction and criticism. In the fifthchapter, I use an analysis of several of Muktibodh's short stories in order to examine his ideas about genre, and the problem of a post-Independence reality in which social relations seemed to resist a straightforward, realist depiction. Rather than view Muktibodh's short fiction as unfinished or stagnant, I examine the moments in these stories in which fables, frame stories, and metanarrative devices interrogate ideas of realism. The final sixth chapter of my dissertation analyzes Muktibodh's criticism, showing how he used a critique of Realism to develop an aesthetics centered on the critical imagination. Through an examination of Muktibodh's criticism across his career, I show how a contradiction between narrative and the poetic image eventually develops into an critique of realism, based on readings in Marxist and romantic aesthetic theory.My research not only contributes to an emerging scholarship of post-Independence Hindi, but also develops an understanding of the contribution of Hindi and other South Asian languages to postcolonial literature. I argue that taking account of Muktibodh's writings is crucialtowards forming a model of World Literature that accounts for the experience of India following independence, and the unique perspectives on the international developed by South Asian literatures. My dissertation therefore acts as a corrective to studies of global modernism that privilege literatures written in European languages and view non-European literatures asfundamentally local in scope. In addition, by analyzing the unique way in which Muktibodh combined Marxism with a global Hindi internationalism, my dissertation reveals new ways of understanding the literary and intellectual history of the Cold War. Research that situates writers such as Muktibodh in an international context can thus help to illuminate networks of alternative, postcolonial modernisms in the twentieth century.",ucb,,https://escholarship.org/uc/item/16r585zd,,,eng,REGULAR,0,0
629,2065,Advanced Source/Drain and Contact Design for Nanoscale CMOS,"Vega, Reinaldo","King Liu, Tsu-Jae;",2010,"The development of nanoscale MOSFETs has given rise to increased attention paid to the role of parasitic source/drain and contact resistance as a performance-limiting factor.  Dopant-segregated Schottky (DSS) source/drain MOSFETs have become popular in recent years to address this series resistance issue, since DSS source/drain regions comprise primarily of metal or metal silicide.  The small source/drain extension (SDE) regions extending from the metallic contact regions are an important design parameter in DSS MOSFETs, since their size and concentration affect contact resistance, series resistance, band-to-band tunneling (BTBT), SDE tunneling, and direct source-to-drain tunneling (DSDT) leakage.  This work investigates key design issues surrounding DSS MOSFETs from both a modeling and experimental perspective, including the effect of SDE design on ambipolar leakage, the effect of random dopant fluctuation (RDF) on specific contact resistivity, 3D FinFET source/drain and contact design optimization, and experimental methods to achieve tuning of the SDE region.  It is found that DSS MOSFETs are appropriate for thin body high performance (HP) and low operating power (LOP) MOSFETs, but not low standby power (LSTP) MOSFETs, due to a trade-off between ambipolar leakage and contact resistance.  It is also found that DSDT will not limit DSS MOSFET scalability, nor will RDF limit contact resistance scaling, at the end of the CMOS roadmap.  Furthermore, it is found that SDE tunability in DSS MOSFETs is achievable in the real-world, for an implant-to-silicide (ITS) process, by employing fluorine implant prior to metal deposition and silicidation.  This is found to open up the DSS process design space for the trade-off between SDE junction depth and contact resistance.  Si(1-x)Ge(x) process technology is also explored, and Ge melt processing is found to be a promising low-cost alternative to epitaxial Si(1-x)Ge(x) growth for forming crystalline Si(1-x)Ge(x) films.Finally, a new device structure is proposed, wherein a bulk Tri-Gate MOSFET utilizes high-k trench isolation (HTI) to achieve enhanced control over short channel effects.  This structure (the HTI MOSFET) is shown, through 3D TCAD modeling, to extend bulk LSTP scalability to the end of the CMOS roadmap.  In a direct performance comparison to FinFETs, the HTI MOSFET achieves competitive circuit delay.",ucb,,https://escholarship.org/uc/item/1707173c,,,eng,REGULAR,0,0
630,2066,Out of the Shadows: An Inquiry into the Lives of Undocumented Latino AB540 Students,"Martinez-Calderon, Carmen","Baquedano-LÃ³pez, Patricia;",2010,"AbstractThis research project lies at the intersection of immigrant incorporation, academic institutions, urban politics and U.S. law. I am interested on the role of local, state, and federal laws and policies in creating institutional conditions and fostering social networks that influence democratic politics and levels of immigrant assimilation and incorporation. To that end, I investigate the social networking ability of academic institutional actors (specifically undocumented AB540 students) based on conditions created and fostered by state and federal policy within schools. I examine the role of schools and school activities in offering opportunities and creating (or not) fertile conditions for social networking that may ultimately lead to segmented patterns of academic achievement and/or social incorporation of immigrant students and I analyze the role of undocumented AB540 students within these networks in democratic politics, more specifically in creating, re-creating, and/or re-defining legality. For over three years, I conducted brief and in-depth interviews with 20 undocumented AB540 students and executed monthly shadowing sessions and participant observations of 6 of these student participants. I also conducted archival research, including legislative histories of immigration and education policies, and analyzed the content of coverage in local mainstream and ethnic media, including newspapers and talk-shows. Based on this multi-method research design, I argue that local, state, and federal policies create institutional conditions that offer opportunities for undocumented immigrants to latch on to social networks that may affect their levels of academic achievement and social incorporation. This in turn, helps us to understand the varying and segmented patterns of academic achievement and social incorporation of immigrant youth that continue to maintain structures of social inequality. This project expands upon the literature on immigrant assimilation and incorporation by analyzing the benefits and drawbacks of local, state and federal laws like Plyler vs. Doe and AB540 that grant undocumented youth opportunities for inclusion and incorporation through education yet, at the same time they set limitations that often lead to social and economic barriers and consequently end up sending mixed and conflicting messages. This project also contributes to the literature on the schooling of immigrant children and youth, particularly Latino youth. In the last half-century, schooling has emerged as both - ""the first sustained, meaningful, and enduring participation in an institution of the new society"" and ""the surest path to well-being and status mobility"" (Suarez-Orozco, C., Suarez-Orozco, M., Todorova, p. 2, 2008). In schools, immigrant youth forge new friendships, create and solidify social networks, and acquire the academic, linguistic, and cultural knowledge that ultimately sustains them throughout their journey in the U.S. This said, my project also contributes to literature on social capital as it investigates how people's social capital responds to organizational conditions and supports research that argues that social networks are ""sets of context-dependent relations resulting from routine processes in organizational context"" and as a result ""individuals receive distinct advantages from being embedded in effective broker-organizations that both, intentionally and unintentionally, connect people to other people, organizations, and their resources"" (Small, p.vi, 2009). This research also shows that many practices, resources, and information available and offered to undocumented AB540 students often result from larger factors such as policies of the state, something far removed from these students' daily lived experiences. Furthermore, this project makes a contribution to the urban politics literature by highlighting that undocumented AB540 students are a distinct type of urban political actors with a presence and influence in local politics that is different from that of other immigrants, minorities, and underrepresented groups. Finally, I believe the results of this project will help shape our knowledge of the possibilities and challenges local, state, and federal legislations provide for how we define legality, citizenship, and belonging as well as how we analyze immigrants' processes of assimilation and/or incorporation to address the diversity challenges of America's sizable undocumented Latino population.",ucb,,https://escholarship.org/uc/item/19b2n9mc,,,eng,REGULAR,0,0
631,2067,"Stream, for solo violin with variable ensemble","Cullen, Daniel Shannon","Bedrossian, Franck;",2012,"Stream is a composition for solo violin with variable ensemble. The score consists of a fixed part for solo violin along with a collection of one-page parts for the variable ensemble. This includes parts for flute, oboe, saxophone, clarinet, percussion, piano, violin/viola, cello, contrabass and a multi-channel electronic part, to be performed using a program designed for performance in Max/ MSP.This piece addresses an assortment of compositional practices, thereby blurring the lines between them. Elements of the notation vary in specificity, creating an ambiguity of constrained improvisation and detailed instruction. Similarly, the relationship between soloist and accompaniment suggests a simultaneously fixed composition, improvisation and open-form work. The variability of the ensemble is an extension of this modular, open-form practice.Stream was written for the sfSound ensemble and premiered on August 28, 2011 at ODC Theater, San Francisco, using a subset of six instrumental parts. The sfSound ensemble is comprised of musicians who specialize in both improvised music and meticulously notated scores. It was written with their abilities in mind, but also with the ability to be approachable to ensembles of non- improvisers.",ucb,,https://escholarship.org/uc/item/19h14835,,,eng,REGULAR,0,0
632,2068,Site-Specific Immobilization of Living Cells in 2 and 3 Dimensions on Surfaces and in Fluidic Devices,"Twite, Amy","Francis, Matthew B;Mathies, Richard A;",2014,"There is a substantial need for new chemical tools with the capability to arrange multiple cell types in 2- and 3-dimensions on surfaces in a rapid and precise manner. Such tools would provide scientists with models for the study of mono and cocultures of cells leading to a variety of applications including the development and screening of drugs, cellular synthetic biology, and the assembly and mechanistic study of in vitro tissue and biofilm mimics. Current live-cell patterning tools do not have the ability to produce assemblies of multiple cell types rapidly and repeatedly with good control over dimensional cell localization.  However, by exploiting DNA complementarity, I have developed a patterning method that overcomes the previous limitations. In my approach, surfaces are patterned with single strands of DNA enabling hybridization-directed deposition of cells displaying complementary DNA sequences on their surface. Successive rounds of photolithography and etching provide a means to develop a glass or silicon surface selectively. Following this, the exposed areas are covalently modified with unique single-stranded DNA (ssDNA) capture probes. This method has enabled cell patterning of multiple cell types in a site-specific manner, with good resolution and feature sizes of less than 10 Âµm (the size of a single mammalian cell).  In previous work, the DNA-patterning method was limited to mammalian cells or to cells with chemically accessible amines. Herein, a protocol is developed to modify oxidized cell surface diols with hydrazide-ssDNA; this new chemical biology technology expands DNA-directed cell patterning to include Gram positive and negative bacteria, eukaryotes with cell walls, and protists like algae. My method is the first for directly and site-specifically patterning multiple microorganisms on one substrate using ssDNA. Together with the previous mammalian cell modification technique, DNA-directed cell patterning now accommodates all kingdoms of life, and can be performed on a variety of materials and in microfluidic devices. Compared to previous photolithographic-based ssDNA patterning methods, my direct photopatterning method for covalently modifying a substrate surface reduces the process complexity as well as the amount of time and reagent needed to pattern multiple DNA sequences on a surface. This is the first demonstration of a direct, chemoselective photopatterning method capable of incorporating multiple unique chemical species on one surface. Using an oxidative coupling reaction between aniline and an azidophenol, which acts as a photocaged iminoquinone, UV initiated chemical reactivity is demonstrated for functionalization of surfaces with ssDNA. By irradiating through a photomask, the mask features are transferred to a surface of interest for creation of complex patterns directly and covalently on an aniline substrate. This procedure can be expanded to encompass the patterning of any molecule type that displays an azidophenol moiety, allowing for the prospective use of this technique for the creation of complex patterns of molecules ranging from small molecules to protein assemblies.In summary, I developed a new method to derivitize the surface of living cells with cell walls with ssDNA for site-specific patterning. To reduce the time and complexity of multi-sequence DNA patterning, a new photopatterning method was also developed to allow for the direct and covalent modification of surfaces with multiple ssDNA sequences. Finally, these methods were used to arrange cells in complex designs on glass surfaces and in fluidic channels. Together, these procedures have enabled the assembly of on-chip cocultures for optical and impedance monitoring of tissue models consisting of endothelial cells, vascular smooth muscle cells, T-cells, and/or basophils for the real-time study of allergy response or vasculature assembly and growth. Overall, these advancements in ssDNA-directed patterning methods provide heretofore-unprecedented tools for rapid and site-specific assembly of any desirable combination of cell type and molecule and for the functional study of these mono or coculture on surfaces and in microfluidic devices.",ucb,,https://escholarship.org/uc/item/1b93x2x4,,,eng,REGULAR,0,0
633,2069,Statistical learning models of sensory processing and implications of biological constraints,"Dodds, Eric McVoy","DeWeese, Michael R;",2018,"Despite progress in understanding the organization and function of neural sensory systems, fundamental questions remain about how organisms convert visual, auditory, and other sensory input into useful representations to understand the world and guide behavior. An important and fruitful line of work models the brain as an unsupervised statistical learner, examining how a sensory system may optimize for efficient representation of the natural environment or for explicit representation of useful structure in that environment. This dissertation explores efficient coding and sparse coding models of the visual and auditory systems, the data these systems process, and how these models are affected by the constraints imposed by implementation in biological neural systems. First, I show that both natural images and natural sounds have statistical structure amenable to a sparse coding model but that the sparse structure of these two types of natural data also differ in interesting ways that may be relevant to extending the success of sparse coding in describing primary visual cortex (V1) to analogous regions of the auditory system. I also discuss how a related model may shed light on how the neurons in these sensory systems are organized in space based on coding for related stimulus properties. Second, I show that a sparse coding model with biological constraints requires its inputs to be whitened in order to learn sparse features using synapse-local learning rules. This observation provides a novel explanation for the separation of sparse coding and spatial decorrelation into, respectively, V1 simple cells and preceding areas including retina. Third and finally, I turn back to the auditory system and extend existing work on efficient coding in the cochlea to account for the requirement of causality, i.e., determining a code without knowledge of the future of a signal.",ucb,,https://escholarship.org/uc/item/1c05d34t,,,eng,REGULAR,0,0
634,2070,Development of Energy Models for Production Systems and Processes to Inform Environmentally Benign Decision-Making,"Diaz-Elsayed, Nancy","Dornfeld, David;",2013,"Between 2008 and 2035 global energy demand is expected to grow by 53%. While most industry-level analyses of manufacturing in the United States (U.S.) have traditionally focused on high energy consumers such as the petroleum, chemical, paper, primary metal, and food sectors, the remaining sectors account for the majority of establishments in the U.S. Specifically, of the establishments participating in the Energy Information Administration's Manufacturing Energy Consumption Survey in 2006, the non-energy intensive"" sectors still consumed 4*10^9 GJ of energy, i.e., one-quarter of the energy consumed by the manufacturing sectors, which is enough to power 98 million homes for a year. The increasing use of renewable energy sources and the introduction of energy-efficient technologies in manufacturing operations support the advancement towards a cleaner future, but having a good understanding of how the systems and processes function can reduce the environmental burden even further. To facilitate this, methods are developed to model the energy of manufacturing across three hierarchical levels: production equipment, factory operations, and industry; these methods are used to accurately assess the current state and provide effective recommendations to further reduce energy consumption.First, the energy consumption of production equipment is characterized to provide machine operators and product designers with viable methods to estimate the environmental impact of the manufacturing phase of a product. The energy model of production equipment is tested and found to have an average accuracy of 97% for a product requiring machining with a variable material removal rate profile. However, changing the use of production equipment alone will not result in an optimal solution since machines are part of a larger system. Which machines to use, how to schedule production runs while accounting for idle time, the design of the factory layout to facilitate production, and even the machining parameters â€” these decisions affect how much energy is utilized during production. Therefore, at the facility level a methodology is presented for implementing priority queuing while accounting for a high product mix in a discrete event simulation environment. A baseline case is presented and alternative factory designs are suggested, which lead to energy savings of approximately 9%.At the industry level, the majority of energy consumption for manufacturing facilities is utilized for machine drive, process heating, and HVAC. Numerous studies have characterized the energy of manufacturing processes and HVAC equipment, but energy data is often limited for a facility in its entirety since manufacturing companies often lack the appropriate sensors to track it and are hesitant to release this information for confidentiality purposes. Without detailed information about the use of energy in manufacturing sites, the scope of factory studies cannot be adequately defined. Therefore, the breakdown of energy consumption of sectors with discrete production is presented, as well as a case study assessing the electrical energy consumption, greenhouse gas emissions, their associated costs, and labor costs for selected sites in the United States, Japan, Germany, China, and India.By presenting energy models and assessments of production equipment, factory operations, and industry, this dissertation provides a comprehensive assessment of energy trends in manufacturing and recommends methods that can be used beyond these case studies and industries to reduce consumption and contribute to an energy-efficient future.",ucb,,https://escholarship.org/uc/item/1cp046qq,,,eng,REGULAR,0,0
635,2071,A Constructional Approach to Japanese Internally Headed Relativization,"Ohara, Kyoko",,1996,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/1d82f0c3,,,eng,REGULAR,0,0
636,2072,One and Two Locus Likelihoods Under Complex Demography,"Kamm, John Arthur","Song, Yun S.;",2015,"The coalescent is a random process that describes the genealogy relating a sample of individuals, and provides a probability model that can be used for likelihood-based inference on genetics data. For example, coalescent models may include recombination, natural selection, population size crashes and growth, and migrations, and thus can be used to learn the strength of these biological and demographic forces. Unfortunately, computing the likelihood of data remains a challenging problem in many of these coalescent models.In this dissertation, I develop new equations and algorithms for computing coalescent likelihoods at one or two loci, and apply them to inference problems in a composite likelihood framework. I begin by developing an algorithm for the one-locus case, computing the site frequency spectrum (the distribution of mutant allele counts) under complex demographic histories with population size changes (including exponential growth), population splits, population mergers, and admixture events. This method improves on the runtime and numerical stability of previous approaches, and can successfully infer demographic histories that would otherwise be too computationally challenging to consider. I then consider the two- locus case, and derive a formula for the likelihood at a pair of sites under a variable population size history; this formula scales to tens of individuals. In addition to this exact formula, I also develop a highly efficient importance sampler to compute the same likelihood. I apply these results to the problem of inferring recombination rates under variable population size.",ucb,,https://escholarship.org/uc/item/1dp6w9q8,,,eng,REGULAR,0,0
637,2073,Multi-scale Modeling of Heat Transfer,"Hashemi Ghermezi, Seyed Hossein","Zohdi, Tarek;",2016,"The dynamics of a complex fluid or fluid close to interfaces is affected by its microscopic structure. Because the molecular level dynamics is playing a role, and macroscopic scale is effected by lower levels, in these situations the fluid cannot be completely described by continuum theory and Navier-Stokes equation. However, molecular dynamics also is not a practical approach; the large number of particles in the large scales makes it computational impossible. I solve the problem with a Mesoscopic simulation methods by making use of information from different length scale, macroscopic and microscopic, and solve the problem in middle ground in the mesoscale level. However, by using the coupling of continuum and a mesoscale method, I could solve an industry size problem with accuracy of lower scale problem. For the first time, I present a novel coupling method of dissipative particle dynamics to continuum model to calculate heat transfer. The results of the method shows an excellent agreement with a broad range of experimental results in various cases.",ucb,,https://escholarship.org/uc/item/1f6903rv,,,eng,REGULAR,0,0
638,2074,Applications of Integer Programming Methods to Solve Statistical Problems,"Higgins, Michael","Sekhon, Jasjeet J;Nolan, Deborah;",2013,"Many problems in statistics are inherently discrete. When one of these problems also contains an optimization component, integer programming may be used to facilitate a solution to the statistical problem. We use integer programming techniques to help solve problems in the following areas: optimal blocking of a randomized controlled experiment with several treatment categories and statistical auditing using stratified random samples.We develop a new method for blocking in randomized experiments that works for an arbitrary number of treatments.  We analyze the following problem: given a threshold for the minimum number of units to be contained in a block, and given a distance measure between any two units in the finite population, block the units so that the maximum distance between any two units within a block is minimized. This blocking criterion can minimize covariate imbalance, which is a common goal in experimental design. Finding an optimal blocking is an NP-hard problem. However, using ideas from graph theory, we provide the first polynomial time approximately optimal blocking algorithm for when there are more than two treatment categories. In the case of just two such categories, our approach is more efficient than existing methods. We derive the variances of estimators for sample average treatment effects under the Neyman-Rubin potential outcomes model for arbitrary blocking assignments and an arbitrary number of treatments. In addition, statistical election audits can be used to collect evidence that  the set of winners (the outcome) of an election according to the machine count is correct---that it agrees with the outcome that a full hand count of the audit trail would show. The strength of evidence is measured by the p<\italic>-value of the hypothesis that the machine outcome is wrong. Smaller p<\italic>-values are stronger evidence that the outcome is correct. Most states that have election audits of any kind require audit samples stratified by county for contests that cross county lines. Previous work on p<\italic>-values for stratified samples based on the largest weighted overstatement of the margin used upper bounds that can be quite weak. Sharper $p$-values than those found by previous work can be found by solving a 0-1 knapsack problem. We also give algorithms for choosing how many batches to draw from each stratum to reduce the counting burden.",ucb,,https://escholarship.org/uc/item/1g55c77q,,,eng,REGULAR,0,0
639,2075,A Theory of MiXtopia: Critical Performance Life/Art in Three MiXtopias,"Ikehara, Ariko Shari","Perez, Laura E;",2016,"ABSTRACT: The heart of my project lies at the praxis of life/art. I call miXtopia the sites where performance art blurs the line between life/art, where life is self-consciously in performative mode. At the life/art crossroad (X), life turns from mundane to marvelous in critical and creative performances. The X in miXtopia opens up passages between narrative and body through which I enter and exit at the crossroad of extra-text and inter-text between the real and fiction. MiXtopia is a concept I have coined based on the practice of â€œart as life itselfâ€ where the mundane crosses into the marvelous, and the â€œsouls of folkâ€ meet at these and other crossroads. MiXtopia is conceptualized by fusing theories of third, non-binary spaces and methodologies taken from performance art. Like Allan Kaprow (1960s) and Judith Butler (1990), I stretch the definition of performance art by thinking of life as performance. Moreover, I reconfigure the third as mixed or hybrid being, object, or space to embody and describe a performative mode that operates through what I identify and name as a mixed-multiplying principle. The principle operates in the transnational and translational borders of Okinawa as other sites that are within the contexts of colonial, imperial and racial legacies of the world order. In this study, decolonial Okinawan cultural practices are explored through the non-binary theoretical and methodological framework of miXtopia, which brings into view the colonial, imperial, and racial elements of the tripartite relationship between Okinawa, America, and Japan, and complex forms of resistance and cultural persistence with respect to these. My theoretical neologisms aim to render elements of Okinawan culture legible as decolonial projects. MiXtopia merges critical ethnic studies analysis and performance art praxis to illuminate life as the crossroads of critical and creative performance in Okinawa, against histories of cultural and political imperialisms.",ucb,,https://escholarship.org/uc/item/1gz8p47p,,,eng,REGULAR,0,0
640,2076,Development of a multi-knife-edge slit collimator for prompt gamma ray imaging during proton beam cancer therapy,"Ready, John","Vetter, Kai;",2016,"Proton beam usage to treat cancer has recently experienced rapid growth, as it offers the ability to target dose delivery in a patient more precisely than traditional x-ray treatment methods. Protons stop within the patient, delivering the maximum dose at the end of their trackâ€”a phenomenon described as the Bragg peak. However, because a large dose is delivered to a small volume, proton therapy is very sensitive to errors in patient setup and treatment planning calculations. Additionally, because all primary beam particles stop in the patient, there is no direct information available to verify dose delivery. These factors contribute to the range uncertainty in proton therapy, which ultimately hinders its clinical usefulness. A reliable method of proton range verification would allow the clinician to fully utilize the precise dose delivery of the Bragg peak.Several methods to verify proton range detect secondary emissions, especially prompt gamma ray (PG) emissions. However, detection of PGs is challenging due to their high energy (2â€“10 MeV) and low attenuation coefficients, which limit PG interactions in materials. Therefore, detection and collimation methods must be specifically designed for prompt gamma ray imaging (PGI) applications. In addition, production of PGs relies on delivering a dose of radiation to the patient. Ideally, verification of the Bragg peak location exposes patients to a minimal dose, thus limiting the PG counts available to the imaging system.An additional challenge for PGI is the lack of accurate simulation models, which limit the study of PG production characteristics and the relationship between PG distribution and dose delivery. Specific limitations include incorrect modeling of the reaction cross sections, gamma emission yields, and angular distribution of emission for specific photon energies. While simulations can still be valuable assets in designing a system to detect and image PGs, until new models are developed and incorporated into Monte Carlo simulation packages, simulations cannot be used to study the production and location of PG emissions during proton therapy.This work presents a novel system to image PGs emitted during proton therapy to verify proton beam range. The imaging system consists of a multi-slit collimator paired with a position-sensitive LSO scintillation detector. This innovative design is the first collimated imaging system to implement two-dimensional (2-D) imaging for PG proton beam range verification, while also providing a larger field of view than compared to single-slit collimator systems. Other, uncollimated imaging systems have been explored for PGI applications, such as Compton cameras. However, Compton camera designs are severely limited by counting rate capabilities. A recent Compton camera study reported count rate capability of about 5 kHz. However, at a typical clinical beam current of 1.0 nA, the estimated PG emission rate would be 6 x 108 per second. After accounting for distance to the detector and interaction efficiencies, the detection system will still be overwhelmed with counts in the MHz range, causing false coincidences and hindering the operation of the imaging system.Initial measurements using 50 MeV protons demonstrated the ability of our system to reconstruct 2-D PG distributions at clinical beam currents. A Bragg peak localization precision of 1 mm (2Ïƒ) was achieved with delivery of (1.7 Â± 0.8) x 108 protons into a PMMA target, suggesting the ability of the system to detect relative shifts in proton range while delivering fewer protons than used in a typical treatment fraction. This is key, as the ideal system allows the clinician to verify proton range when delivering only a small portion of the prescribed dose, preventing the mistreatment of the patient. Additionally, the absolute position of the Bragg peak was identified to within 1.6 mm (2Ïƒ) with 5.6 x 1010 protons delivered. These promising results warrant further investigation and system optimization for clinical implementation. While further measurements at clinical beam energy levels will be required to verify system performance, these preliminary results provide evidence that 2-D image reconstruction, with 1â€“2 mm accuracy, is possible with this design. Implementing such a system in the clinical setting would greatly improve proton therapy cancer treatment outcomes.",ucb,,https://escholarship.org/uc/item/14d0r6zb,,,eng,REGULAR,0,0
641,2077,The Relevant Signals for Sensorimotor Adaptation in Human Reaching,"Parvin, Darius Euge","Ivry, Richard B;",2019,"Motor control is an essential part of what makes us human. Itâ€™s important for learning how to walk and talk, for professional athletes and performers, and for recovery after injury, such as being able to function independently after a stroke. Indeed, one can even argue that the ultimate function of the brain is to dynamically select and execute movements. Without being able to move, we wouldnâ€™t be able to express ourselves or affect the world around us. By improving our understanding the motor system, we may not only be able to optimize rehabilitation schedules or brain machine interfaces, but also discover general principles about how the brain is structured. Our lack of understanding of the motor system is made apparent by the current state of robotics. Humans have a number of mechanical disadvantages compared to robots: The forces generated by our muscles are variable and subject to fatigue, and there is considerable noise and delays in the motor signals sent to our muscles as well as in the processing of sensory feedback from our movements. Despite these challenges, we can perform complex and dexterous tasks, controlling our legs and body to walk along highly variable surfaces or our hands to crack an egg, tasks we do far more gracefully than the most sophisticated robots. Computers can easily outperform humans in games like chess where there are well defined rules and objectives, however, when it comes to learning new movements, itâ€™s unclear what to optimize for. What makes humans great at learning and performing movements is likely not to do with our muscles or processing power, but the learning rules instantiated in our brains. Human motor learning is thought to be comprised of multiple learning processes. Consider a child learning to tie her shoes: The initial step involves declaratively learning a specific sequence of steps. Through trial and error, reinforcement learning would help favor certain actions over other possible actions. And sensorimotor adaptation can fine-tune the movements, allowing the child to accomplish the task for laces of different thickness or when wearing gloves on a wintry day. These different processes are thought to rely on different brain regions, and the extent to which they are independent processes or interact with each other is an open question. In order to move accurately throughout life, the motor system must compensate for changes in the body and environment. Sensorimotor adaptation refers to the automatic and implicit process, one this is dependent on the integrity of the cerebellum, essential for keeping the sensorimotor system calibrated. For this form of supervised learning, the actual sensory feedback is compared to the predicted sensory consequences of a movement, with the difference constituting a sensory prediction error. The sensory prediction error is used to make rapid adjustments in an on-going movement and as a learning signal to alter the next movement in order to reduce the movement error.A key insight concerning sensorimotor adaptation is that this process appears to be impervious to task goals. In a seminal study, Mazzoni and Krakauer (2006) perturbed the visual feedback while participants performed a reaching task. By providing instructions about the perturbation, the participants were able to immediately adjust their behavior such that the cursor hit the target. However, the adaptation system continued to respond to the mismatch between the predicted and observed limb position, even though in this context, the consequences of adaptation were actually detrimental to task performance. This conclusion has been reinforced with a number of different tasks over the past decade, underscoring the automatic nature of adaptation and, computationally, that this learning process is concerned with ensuring that a selected movement is executed properly, rather than that the selected movement is appropriate. This body of work raises the question of the constraints on adaptation: If not task performance, what sources of feedback influence adaptation? The central purpose of this thesis is to characterize the features of sensory feedback that influence sensorimotor adaptation. By understanding the relevant features or inputs that constrain adaptation, we may be able to better understand the information that is communicated to this learning process. From these we could hypothesize how brain regions involved in adaptation interact with other brain regions that contribute to motor control. This should also lead to new insights and testable predictions about how the motor system will respond in various situations, meaning we could design more effective learning environments where these features are pronounced. In this thesis, I study the inputs to sensorimotor adaptation by testing cases with multiple or ambiguous sources of feedback, exploring how these signals affect performance.In Chapter 1, I investigate the potential interaction between sensory prediction errors and reward prediction errors. To study this, I use a task in which participants reach towards targets in order to earn points. I manipulate both the task feedback (reward or no reward) and movement feedback, asking how these different feedback signals interact with each other. Contrary to previous hypotheses, we find that sensorimotor adaptation and reinforcement learning operate in parallel. Furthermore, I show that a key feature determining choice behavior is a sense of agency: Does the participant believe they have control over the outcome. If so, they are more likely to seek higher payoffs (riskier behavior) compared to when they believe that they do not control the outcome. In Chapter 2, I present multiple feedback signals linked to a single movement. By varying which sources of feedback are task relevant and which are task-irrelevant, I ask if adaptation is sensitive to task relevance (even if insensitive to task outcome). The alternative hypothesis is that all feedback information is treated in a similar manner by the adaptation system. The results show that the adaptation system is sensitive to task relevance. However, overall adaptation is attenuated in the presence of irrelevant feedback signals. These results highlight a novel role of task relevance for sensorimotor adaptation, particularly in situations with multiple or redundant sources of feedback.In Chapter 3, I explore the interaction of vision and proprioception in sensorimotor adaptation. Previous experiments have shown that adaptation in response to visual errors has a limited capacity; the system can be recalibrated up a certain point, beyond which accurate performance requires some sort of change in the movement plan. The basis for this limited capacity is unclear. One hypothesis is that the upper bound on adaptation reflects an equilibrium point between signals concerning visual and proprioceptive sensory prediction errors. This hypothesis predicts that, as proprioceptive information becomes less reliable, the sensitivity to visual errors should be relatively greater, and thus produce a larger upper bound on adaptation. I test hypothesis by asking if variation across individuals in terms of their sensitivity to proprioception is predictive of their response to a visual perturbation. The results show a negative correlation between proprioceptive acuity and the magnitude of adaptation to a visual error, consistent with the idea that adaptation entails the integration of different sensory prediction error signals.In summary, by studying the response of adaptation of multiple sources of feedback, we have furthered our understanding of the constraints on sensorimotor adaptation. We find that sensorimotor adaptation not only integrates feedback from vision and proprioception, but that under situations it is also sensitive to the task relevance. Although we did not find evidence for a direct interaction between adaptation and decision making, our finding of adaptationâ€™s sensitivity to task relevance suggests an exciting possibility for the interaction with other goal directed systems.",ucb,,https://escholarship.org/uc/item/14m8q91f,,,eng,REGULAR,0,0
642,2078,Metagenomic and Cultivation-Based Analysis of Novel Microorganisms and Functions in Metal-Contaminated Environments,"Yelton, Alexis Pepper","Banfield, Jillian F;",2012,"Some bacteria and archaea have evolved metabolic strategies that enable them to live in environments contaminated by toxic metals. In fact, many bacteria and archaea take advantage of the redox sensitivity of these very same metals to gain energy via anaerobic respiration. Here, metagenomic techniques were developed and applied along with conventional physiological and ecological methods to elucidate multiple modes of adaptation of bacteria and archaea in metal-contaminated acid mine drainage and aquifer environments contaminated by mine tailings. These approaches provided insight into how these organisms survive and thrive in these environments and how they differentiate themselves from each other.Many of the microbial species in acid mine drainage and mine tailings-contaminated aquifer environments are difficult to culture in the laboratory. Thus, a focus of the research was metabolic analysis of these organisms via analysis of genes and genomes recovered from microbial communities and isolates. Many of the genes are novel, and likely required for specific environmental adaptation, but they are difficult or impossible to functionally characterize based on conventional homology methods. A new method was developed to deal with the challenge of identifying poorly annotated or hypothetical genes of importance in adaptation to metal-contaminated environments. This probabilistic approach is based on conserved gene order between the genomes of interest with distant relatives. The annotation method was used in conjunction with traditional comparative genomics to differentiate a group of co-occurring archaea based on their genetic metabolic potential. These microorganisms grow in biofilm communities in an acid mine drainage system within the Richmond Mine, near Redding in Northern California, USA.  Microbial biofilms growing at the air-solution interface were sampled from solutions with pH values of < 1.2, temperatures of up to 48 Â°C, and mM concentrations of zinc, copper, arsenic, and sub-molar concentrations of dissolved iron. We used a metagenomic approach in which DNA was extracted from biofilm samples, sequenced, and analyzed in order to evaluate differences in the metabolic potential of five closely related Thermoplasmatales archaea and one distant relative. A subset of these organisms appears to be capable of iron oxidation, whereas others appear to live primarily heterotrophically. Another subset is potentially capable of CO oxidation. There are also major differences in motility within the group.A metal-contaminated aquifer adjacent to the Colorado River in Colorado, USA, was studied to investigate microorganisms adapted to high vanadium concentrations. A vanadium-reducing Betaproteobacterium of the genus Simplicispira was isolated (strain BDI). This organism's genome encodes a large number of toxic metal resistance, chemotaxis, motility, and conjugation-related genes that likely allow it to detoxify, avoid contaminants and rapidly adapt in a changing environment. Physiological characterization in the laboratory shows that it is a facultative anaerobic nitrate-reducer capable of reduction of up to 3 mM vanadate.In order to determine the effect of vanadium contamination on the aquifer community structure, soluble vanadium was added to an in-well, flow-through sediment column. Reduction of dissolved vanadate was documented, along with an increase in the number of cells capable of vanadium reduction, and an increase in the relative abundance of strain BDI. An increase in the relative abundance of three families of known vanadium reducing bacteria (Commomonadaceae, Geobacteraceae, and Pseudomonadaceae) was also noted. This experiment confirmed the environmental importance of BDI, and microbial vanadium reduction in response to acetate addition.  Following short-term acetate addition to the aquifer, vanadium remained immobile for at least two years. Because the organisms stimulated by amendment were resident in the aquifer and removal of vanadium from solution persists, the acetate addition approach has significant potential for bioremediation of vanadium.In summary, this research used culture-based and culture-independent techniques to elucidate microbial metabolisms that allow organisms to colonize metal-contaminated environments. Vanadium reduction was linked to specific subsurface bacteria, one of which was isolated and characterized. The findings have significance in the fields of genomics, metagenomics, microbial ecology and biogeochemistry, and have potential application for bioremediation.",ucb,,https://escholarship.org/uc/item/14w6j3nm,,,eng,REGULAR,0,0
643,2079,Californians and their Earthquakes: Post-Earthquake Public Information Infrastructures,"FINN, MEGAN","Saxenian, AnnaLee;Duguid, Paul;",2012,"This dissertation analyzes Californians' information infrastructure after three Bay Area earthquakes: 1868 Hayward Fault Earthquake, 1906 San Francisco Earthquake and Fire, and 1989 Loma Prieta Earthquake. I use qualitative and historical research approaches, focusing on documents produced by state and local governments, newspapers and letters by Californians. In my analysis, I employ the construct of ""information infrastructure"" from the field of Science and Technology Studies to describe the complex constellation of practices, technology and institutions that underpins the public sphere. Four themes help develop the idea of public information infrastructure: continuity, reach, informational authority and multiple infrastructures. First, major disruptions such as earthquakes challenge the continuity of public information infrastructure while making infrastructure visible. For example, after the 1906 earthquake and fire, refugees had to reassemble their social geography. Friends, loved ones, employers and employees all wanted to locate each other and notify others of their well-being. While the physical information infrastructure was destroyed, the ways that people worked and organized was not. Thus, with some work-arounds, information infrastructure within San Francisco was reassembled to working order.  Second, I look at one of the qualities of information infrastructure that is considered fundamental - that of the reach of infrastructure across space. In 1868, the circulation of documents to far away audiences shaped the earthquake narrative locally. Third, I examine claims to informational authority. My dissertation begins in 1868, at a time when there were not shared scientific earthquake descriptors such as magnitude, when it took weeks for a newspaper to travel from San Francisco to New York, and when there was no professionalized class of ""responders"" or specialized government response. The Chamber of Commerce claimed the authority to explain the earthquake. The bureaucratization of disaster response and the rise of scientific explanations for earthquakes shaped infrastructure and information practices, such that by the 1989 earthquake government officials claimed the authority to explain what had happened. The intertwining of science, the state, and infrastructure helped constitute and legitimize a new set of informational authorities, and provide a lens with which to design post-disaster information systems and policy today. Last, I argue that there is not just one information infrastructure, but multiple infrastructures supporting multiple publics. Alternate infrastructures supported Chinese people in 1906 and Spanish-speakers in 1989 when attempting to get aid or find loved ones. My research ties together how technology, media organizations, government institutions, and scientific explanations of earthquakes contribute to a sensemaking epistemology for Californians.",ucb,,https://escholarship.org/uc/item/154229wm,,,eng,REGULAR,0,0
644,2080,Statistical Mechanics and Dynamics of Liquids in and out of Equilibrium,"Hudson, Alexander","Chandler, David;",2016,"Liquids display an astonishing array of phenomena that, at first glance, seem to have little in common. In this thesis, we study two of these phenomena. The first is the hydrophobic effect, the well-known tendency of oil and water to demix and the driving force for biological assembly. The second is the glass transition, the process by which liquids cooled to very low temperatures become increasingly viscous until succumbing to structural arrest and displaying the rigidity of a solid with none of its microscopic ordering.In the first part of this thesis, we expand upon our prior understanding of the hydrophobic effect. We study the behavior of water in the confined volume created by a pair of RNA helices at the inter-subunit interface of the bacterial ribosome, which provides an opportunity to apply our contemporary understanding of the hydrophobic effect, first detailed by Lum, Chandler, and Weeks almost twenty years ago, to a context that is less idealized than previously studied. We also discuss and improve upon computational models of hydrophobic solvation, expanding their applicability to more complicated solute geometries.In the second part of this thesis, we study the glass transition from the perspective of the East model, a kinetically constrained lattice model of glass formers. We review and derive a number of results for East model glasses that we expect to emerge in more realistic, atomistic models of glass formers. We choose one such model and drive it out of equilibrium by cooling, forming a glass. We then check whether our East model predictions are correct. Of particular interest to us is whether inter-excitation correlations emerge out of equilibrium, a key prediction of the East model. Our atomistic results suggest that they do not, forcing us to reconsider our conceptual mapping between the highly abstract East model and more realistic models of glass formers.The common thread in these seemingly disparate parts of the thesis is the importance of collective fluctuations in liquid-state phenomena. The hydrophobic effect is a physical consequence of liquid-vapor interfacial fluctuations near extended hydrophobic surfaces, while dynamics in supercooled liquids, the precursors to glasses, are dominated by small, collective motions of particles in an otherwise jammed material. The importance of fluctuations in these and other liquid-state phenomena make the tools of statistical mechanics particularly suitable to their study. In this thesis, we employ these tools in the service of furthering our understanding of the liquid state.",ucb,,https://escholarship.org/uc/item/15v1r70c,,,eng,REGULAR,0,0
645,2081,Statistical Methods for Predicting Dengue Diagnosis using Clinical and LC-MS Data,"Cotterman, Carolyn Louise","Hubbard, Alan E.;",2015,"Dengue virus is the most widespread arthropod-borne virus affecting humans, with as many as 528 million annual infections each year. Of particular concern are the subset of cases which develop into life-threatening dengue hemorrhagic fever, and those which further progress into dengue shock syndrome. Non-invasive tools that accurately differentiate dengue and its subtypes from other viral infections early in the disease progression are vital for timely therapeutic intervention and supportive care. Unfortunately, such tools are sorely lacking. Using liquid chromatography-mass spectrometry (LC-MS), we detect tens of thousands of molecular features in serum, saliva, and urine of suspected dengue patients in Nicaragua. We then use machine-learning methods to help identify candidate small molecule biomarkers which, along with easily obtainable clinical data, predict dengue diagnosis and prognosis. Our findings should aid in developing a low-cost diagnostic tool for use in the field.",ucb,,https://escholarship.org/uc/item/0jh6q6x9,,,eng,REGULAR,0,0
646,2082,Heterogeneity and Flow in the Deep Earth,"Cottaar, Sanne","Romanowicz, Barbara;",2013,"Over the past half century, study of deep regions in the Earth has revealed them to be complex and dynamic. Much of our knowledge comes from seismological data, and ultimately these observations need to be linked to results from geodynamics and mineral physics in order to make inferences about compositional heterogeneities and flow. One example of a strongly-heterogeneous region is the lower thermal boundary layer of the mantle, i.e. the layer of several hundred kilometers thickness above the core-mantle boundary, commonly referred to as Dâ€™â€™. This region appears to be characterized by two large provinces of distinctive slow shear velocities, 4000-5000 km across, one beneath the Pacific and one beneath Africa. Surrounding these regions, seismic velocities are faster, and often interpreted as corresponding to a graveyard of slabs. A recently discovered phase transition from perovskite to postperovskite may also occur in this depth range and has been associated with an intermittently observed seismic discontinuity at the top of Dâ€™â€™ This study adds to the seismological evidence for complexities both in isotropic seismic velocities as well as in anisotropic velocities and how those can be linked to flow of material. We map a distinctive small ""pile"" of slow shear velocity beneath Russia through direct waveform evidence. This ""pile"" is less than 1000 km across, and thus much smaller than the Pacific and African provinces.  Its height is several hundreds of kilometers and its velocity reduction suggests it is composed of the same material as the large provinces of slow shear velocity. Beneath Hawaii, at the northern edge of the Pacific province, we find an extended thin zone of ultra-low velocities. This is the first time the three-dimensional extent of such a zone is constrained with some accuracy. The constraints on its morphology come from the presence of strong postcursors to  shear waves diffracted along the core-mantle boundary, delayed by 30 to 50 seconds with respect to the main diffracted arrival. The zone is almost 1000 km across and roughly 25 km high. Its shear velocity is  reduced by ~20%  with respect to the global average, which is possible through strong iron enrichment or the presence of partial melt. Given its location, one can speculate that this zone may represent an anchor to a whole-mantle plume reaching the surface of the Earth beneath the Hawaiian hotspot and may also represent the source of geochemical anomalies in Hawaiian basalts.Around the southern margin of the African low shear velocity province, we map strong variations in anisotropic velocity structure. These could be interpreted as evidence for strong flow outside the region of slow velocity, and insignificant flow within. To understand observations of seismic anisotropy in terms of flow in a more general context, we adopt a multi-disciplinary approach. We forward model anisotropic texture development through geodynamical models, reflecting mineral-physical constraints on the deformational behavior and elastic constants of possible compositions. In both two and three-dimensional models, we find that perovskite and post-perovskite result in distinctive anisotropic patterns. Deformation on different crystallographic lattice planes within post-perovskite also results in different signatures. In general, the presence of post-perovskite appears to best explain seismic observations of anisotropy in the deep mantle.One other region of strong heterogeneous and anisotropic velocity variations is the inner core. In the last chapter, we model thermal histories to assess the possibility of episodes of convection over the course of the inner core growth. Our numerical model predicts strains due to convective flow that could cause texturing and seismic anisotropy. An early termination of this convection can explain the stronger anisotropy seen in the innermost inner core (the most central 500 km).",ucb,,https://escholarship.org/uc/item/0jx6h8vx,,,eng,REGULAR,0,0
647,2083,Aspects of Lakhota Syntax,"Van Valin, Robert",,1977,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0k52c122,,,eng,REGULAR,0,0
648,2084,Regulation of mitotic progression in Saccharomyces cerevisiae by the microtubule-associated proteins Slk19 and Stu1,"Faust, Ann Marie Elizabeth","Barnes, Georjana;",2011,"Mitosis is the process by which eukaryotic cells segregate their chromosomes before division. A critical stage of mitosis is anaphase, when the microtubule-based spindle segregates chromosomes into mother and daughter cells prior to cytokinesis. My dissertation research aimed to provide a better understanding of the regulation of anaphase progression and spindle function during mitosis in Saccharomyces cerevisiae. My research focused on two microtubule-associated proteins, Slk19 (CENP-F homolog) and Stu1 (CLASP homolog). These proteins play fundamental roles in anaphase progression, Stu1 in microtubule spindle stability and Slk19 in spindle midzone organization and Cdc14 phosphatase regulation. I found that Stu1 and Slk19 physically interact and that Slk19 regulates Stu1 localization during anaphase. In addition to its interaction with Slk19, I identified a number of other physical and genetic interactors of Stu1 through mass spectrometry, yeast two-hybrid and synthetic genetic analyses. These interactors provide insight into the role of Stu1 at kinetochores. I also investigated Stu1 function through a protein truncation analysis and purification of full-length Stu1 protein. My truncation analysis revealed that the Stu1 C-terminus is dispensable for viability but is necessary for proper protein localization. The N-terminus, however, is essential for viability. My attempts to purify Stu1 from insect cells were partially successful; the protein is extremely sensitive to proteolytic degradation, and under conditions that limit proteolysis, the protein appears to aggregate or oligomerize in solution.I also investigated the role of Slk19 sumoylation in anaphase progression. I determined that the Cdc14 early anaphase release (FEAR) network protein Slk19 is sumoylated in vivo and that sumoylation is important for restricting Cdc14 phosphatase localization to the nucleus at the end of anaphase. A slk19 sumoylation mutant causes premature Cdc14 movement from the nucleus to the bud neck, which affects mitotic exit, as this slk19 sumoylation mutant can partially rescue the spindle disassembly defect of the mitotic exit network mutant cdc15-2. This slk19 mutant also has aberrant spindle elongation dynamics, which might be due to a change in Cdc14 function during anaphase. In conclusion, my dissertation research has uncovered a number of previously unrecognized interactions among mitotic proteins and has revealed a novel function of sumoylation in the regulation of Cdc14 function during anaphase through the FEAR network protein Slk19.",ucb,,https://escholarship.org/uc/item/0kp6r666,,,eng,REGULAR,0,0
649,2085,Excitability of Sensory Cortex in Mouse Models of Fragile X Syndrome and Autism Spectrum Disorders,"Langberg, Tomer","Feldman, Daniel E;",2020,"Distinct genetic forms of autism are hypothesized to share a common increase in excitation inhibition (E-I) ratio in cerebral cortex, causing hyperexcitability and excess spiking. We provide a systematic test of this hypothesis across 4 mouse models (Fmr1-/y, Cntnap2-/-, 16p11.2del/+, Tsc2+/-), focusing on somatosensory cortex. All mouse models with autism risk mutations showed reduced feedforward inhibition in layer 2/3 (L2/3) coupled with more modest, variable reductions in feedforward excitation, driving a common increase in E-I conductance ratio. Contrary to the classic, naÃ¯ve prediction that increased E-I ratio drives spiking hyperexcitability, synaptic conductance modeling suggested that sensory inputs to this circuit would actually evoke stable or reduced firing in L2/3 in vivo. Indeed, spiking evoked by single whisker deflections was normal in the two strains whose modeling predicted stable synaptic depolarization (Cntnap2-/-, 16p11.2del/+) and was reduced in the Fmr1-/y mice, also matching that strain's modeling prediction. In addition, spiking of putative inhibitory neurons was reduced in each strain. Thus, increased E-I ratio does not necessarily yield increased spiking responses to simple sensory stimuli.Stable or reduced spiking was surprising given the dramatically reduced inhibition observed in each strain. Thus, we tested whether the Fmr1-/y mouse exhibits excess spiking in response to multi-whisker sequences, which drive more spiking in S1 compared to single-whisker deflections and may thus engage a different synaptic E-I ratio. Total spiking to whisker sequences, and cross-whisker suppression, were reduced in Fmr1-/y mice. These effects were explained by reduced firing to the anatomically corresponding columnar whisker and increased tuning heterogeneity within each cortical column. This suggests that weak or heterogeneous sensory maps may underlie circuit abnormalities in disorders expressing autism spectrum behaviors.",ucb,,https://escholarship.org/uc/item/0n1264pp,,,eng,REGULAR,0,0
650,2086,Light Absorbers and Catalysts for Solar to Fuel Conversion,"Kornienko, Nikolay","Yang, Peidong;",2016,"Increasing fossil fuel consumption and the resulting consequences to the environment has propelled research into means of utilizing alternative, clean energy sources. Solar power is among the most promising of renewable energy sources but must be converted into an energy dense medium such as chemical bonds to render it useful for transport and energy storage. Photoelectrochemistry (PEC), the splitting of water into oxygen and hydrogen fuel or reducing CO2 to hydrocarbon fuels via sunlight is a promising approach towards this goal. 	Photoelectrochemical systems are comprised of several components, including light absorbers and catalysts. These parts must all synergistically function in a working device. Therefore, the continual development of each component is crucial for the overall goal. For PEC systems to be practical for large scale use, the must be efficient, stable, and composed of cost effective components. To this end, my work focused on the development of light absorbing and catalyst components of PEC solar to fuel converting systems.	In the direction of light absorbers, I focused of utilizing Indium Phosphide (InP) nanowires (NWs) as photocathodes. I first developed synthetic techniques for InP NW solution phase and vapor phase growth. Next, I developed light absorbing photocathodes from my InP NWs towards PEC water splitting cells.	I studied cobalt sulfide (CoSx) as an earth abundant catalyst for the reductive hydrogen evolution half reaction. Using in situ spectroscopic techniques, I elucidated the active structure of this catalyst and offered clues to its high activity.  In addition to hydrogen evolution catalysts, I established a new generation of earth abundant catalysts for CO2 reduction to CO fuel/chemical feedstock. I first worked with molecularly tunable homogeneous catalysts that exhibited high selectivity for CO2 reduction in non-aqueous media. Next, in order to retain molecular tunability while achieving stability and efficiency in aqueous solvents, I aimed to heterogenize a class of molecular porphyrin catalysts into a 3D mesoscopic porous catalytic structure in the form of a metal-organic framework (MOF).To do so, I initially developed a growth for thin film MOFs that were embedded with catalytic groups in their linkers.6 Next, I utilized these thin film MOFs grown on conductive substrates and functionalized with cobalt porphyrin units as 3D porous COÂ¬2 reduction catalysts. This new class of catalyst exhibited high efficiency, selectivity, and stability in neutral pH aqueous electrolytes.Finally, as a last chapter of my work, I explored hybrid inorganic/biological CO2 reduction pathways. Specifically, I used time-resolved spectroscopic and biochemical techniques to investigate charge transfer pathways from light absorber to CO2-derived acetate in acetogenic self-sensitized bacteria.",ucb,,https://escholarship.org/uc/item/0n84h3vb,,,eng,REGULAR,0,0
651,2087,"The chemistry, recognition behaviors, and population genetics of Neotropical parabiotic ants","Emery, Virginia Jayne","Tsutsui, Neil D;",2013,"In my dissertation, I have explored behavioral, chemical and genetic aspects of a unique nesting symbiosis called parabiosis. In parabiosis, two unrelated ant species share a nest and foraging trails in a potentially mutualistic association. I have focused on the Neotropical parabiosis between Camponotus femoratus (Subfamily: Formicinae) and Crematogaster levior (Subfamily: Myrmicinae), which occur in ant- gardens throughout Amazonia. These two ants share a common nest but keep their brood in separate chambers. Behavioral tradeoffs suggest that the relationship is a mutualism: both species build the carton nest and forage, but Cr. levior is superior in finding food sources, and Ca. femoratus carries the epiphyte seeds required to give the nest structural support. Like any mutualism, the relationship is vulnerable to exploiters and cheaters, so reliable recognition systems would help to maintain the relationship.In Chapter 1, I examine the nestmate recognition behaviors of Cr. levior and Ca. femoratus living in parabiotic ant garden nests. By using pairwise behavioral assays in neutral arenas, I assayed the proportion of ants exhibiting aggressive behaviors when paired with nestmate and non-nestmate ants. We expect for ants to aggress non-nestmates by biting, stinging, spraying formic acid or otherwise attacking to exclude these intruders. I also sampled the cuticular hydrocarbon chemistry of ants in these nests to determine whether recognition behavior was related to these compounds. Cuticular hydrocarbons (CHC) are often used as recognition cues amongst social insects. I found that there were three different CHC phenotypes in my study population in French Guiana. There were two sympatric chemotypes of Cr. levior, with very little overlapping chemistry. Within each nest, there was only a single chemotype of Cr. levior, and neither chemotype shared chemical cues with Ca. femoratus. Despite sharing a nest, Ca. femoratus exhibited a single chemotype throughout the population, and did not chemically match its Cr. levior nestmates. Both species maintain intraspecific recognition abilities, and Cr. levior shows some evidence of being able to distinguish amongst its Ca. femoratus nestmates. However, despite their strong chemical divergence, there was noevidence Ca. femoratus could distinguish between the Cr. levior chemotypes. My findings suggest that selection to maintain reliability in conspecific recognition can potentially constrain the evolution of interspecific cooperation.In Chapter 2, I delve further into the details of the cuticular chemistry of these parabiotic ants. Incidentally, there are three species living within the ant garden nests -- Cr. levior, Ca. femoratus and a tiny Solenopsis thief ant, called Solenopsis picea. Do these multispecies nests still form a common colony `gestalt' odor? Are there differences in the chemical integration techniques of social mutualists and social parasites? I sampled individual ant CHCs to look for patterns of similarity within and between ant species, and within and between colonies. Both parabiotic species show some evidence of forming a single-species common colony odor, which is consistent with the gestalt hypothesis that nestmates share chemical cues. However, this cue sharing does not spread to allospecific nestmates. The two parabiotic species, Cr. levior and Ca. femoratus, share very few cues in common. In contrast, the social parasite S. picea shares cuticular chemistry with both of its host species. The specificity of this chemical cue similarity is limited, and S. picea is not chemically different whether it is nesting with Cr. levior Type A and Cr. levior Type B. These findings are the first to examine the CHC patterns in nests with three ant species, and highlight important differences in the chemical integration of social mutualists and social parasites.In Chapter 3, I examine the genetic basis of the chemical phenotypes of Cr. levior and Ca. femoratus. Using the individual profiles of ants from Chapter 2, I compare the chemical phenotypes to genotypic information from both nuclear microsatellite loci and mitochondrial co-1. For both species, there is a correlation between chemical phenotypes and genotypes. In Ca. femoratus, there are positive correlations between genetic distances and both chemical and geographic distances of colony pairs. The genetic basis for chemotype includes a correlation between some alleles and the proportion of straight-chain alkanes, which are shorter than other hydrocarbons in the typical Ca. femoratus profiles. Likewise, there are correlations between chemical phenotypes and genotypes of Cr. levior ants, with a strong genetic distinction between the two Cr. levior chemotypes. There is no geographic partitioning of either chemical or genetic differences, which supports the observation of sympatry of the Cr. levior Type A and Type B. We find correlations between several alleles and the proportion of different chemical compounds. Specifically, there appear to be opposing genetic trends for the alkane and methyl branched compounds that dominate Cr. levior Type A profiles, and the unsaturated alkenes and alkadienes, which typify Cr. levior Type B profiles. Together this evidence supports the hypothesis that Cr. levior Type A and Type B are genetically distinct and potentially different cryptic species.In Chapter 4, I attempt to resolve the relationships between geography, chemistry, genetics, and recognition behaviors of the parabiotic ants. In other systems, cuticular chemistry plays an important role in determining the outcome of recognition assays, with increased chemical dissimilarity usually resulting inincreased aggression. Similarly, more genetically distant non-nestmates are expected to be subject to more aggression. I find that for Ca. femoratus, conspecific aggression is related to genetic differences, but not to the measured chemical differences. This suggests potential kin-informative cues exist, that we have not yet measured. These genetic cues may also be used by Cr. levior to recognize their Ca. femoratus nestmates, and highlight the technical limitations of our current chemical machinery. In contrast, Cr. levior conspecific recognition is more strongly related to chemical differences, but mainly at the level of chemotypes. These results emphasize the importance of considering all levels of chemical and genetic differentiation when assessing nestmate recognition patterns. Both species of parabiotic ant uses chemical and associated genetic information to assess nest membership. The fully functioning recognition systems in parabiotic nests likely maintain the relationship by excluding exploiters of this unique cooperative relationship.In sum, my dissertation research characterizes the recognition behaviors, chemical cues, and population genetics of an uncommon but abundant ant-ant mutualism. My findings support the hypothesis that recognition behaviors, although proximately mediated by chemical similarity, are ultimately controlled by genetic factors. I find that both chemical integration techniques and nestmate recognition behaviors differ for mutualistic and parasitic nesting symbioses. Together my dissertation research highlights the unique properties of the parabiotic nesting association, and supports its status as the only ant-ant mutualism.",ucb,,https://escholarship.org/uc/item/0r79859p,,,eng,REGULAR,0,0
652,2088,Establishing reference in MÃ¡Ã­hÄ©ÌµÌ€kÃ¬,"Farmer, Stephanie Jo","Michael, Lev D.;",2015,"In this dissertation, I investigate the hierarchical nominal lexicon of MÃ¡Ã­hÄ©ÌµÌ€kÃ¬, an endangered Western Tukanoan language spoken in northern Peruvian Amazonia. With data from original fieldwork, I establish eight types of nouns in MÃ¡Ã­hÄ©ÌµÌ€kÃ¬, each of which patterns distinctly from the others with respect to at least seven different morphosyntactic behaviors. I argue that these patterns can be predicted from the inherent semantic properties of the nouns, and provide a formal account of the parameters that govern the structure of the proposed hierarchy. In particular, I establish the notion of a `reference ratio'---the ratio of properties that are requisite to the entities in some set to the properties that are incidental of those entities. The first half of the dissertation provides an in-depth grammatical description of MÃ¡Ã­hÄ©ÌµÌ€kÃ¬. A chapter on MÃ¡Ã­hÄ©ÌµÌ€kÃ¬ phonetics and phonology includes a detailed description of the language's nasal spreading and tone systems, and a chapter on MÃ¡Ã­hÄ©ÌµÌ€kÃ¬ morphosyntax includes extensive treatments of deixis, event structure, and clause-linking devices. The latter half of the dissertation discusses the semantics of MÃ¡Ã­hÄ©ÌµÌ€kÃ¬ nouns and nominal morphology. I provide a summary of the literature on nominal hierarchies, then argue for one such hierarchy in MÃ¡Ã­hÄ©ÌµÌ€kÃ¬ based on the distinct morphosyntactic behaviors of nouns. I look at two of these morphosyntactic behaviors---the availability of plural morphology and the ability to be suffixed with or serve as a nominal classifier---in depth. A chapter on noun classification provides background on the typology of the phenomenon, including a discussion of the features of noun categorization devices common to northwest Amazonia. In addition to the typological survey, I provide a novel semantic analysis of MÃ¡Ã­hÄ©ÌµÌ€kÃ¬ classifiers that has implications for the theory of both classification and nominal compounding. In a chapter on plurality, I provide a formal semantic analysis that addresses the complexities of number marking in MÃ¡Ã­hÄ©ÌµÌ€kÃ¬, including the availability of multiple pluralization strategies and their apparent optionality. The role of both classifiers and plurals in altering the reference ratio of nouns is examined throughout.",ucb,,https://escholarship.org/uc/item/0rn709mg,,,eng,REGULAR,0,0
653,2089,"Beyond the Limit: Gender, Sexuality, and the Animal Question in (Afro)Modernity","Jackson, Zakiyyah Iman","JanMohamed, Abdul;",2012,"""Beyond the Limit: Gender, Sexuality, and the Animal Question in (Afro)Modernity""  demonstrates that key texts of twentieth-century African American and African diasporic literature, performance, and visual culture provide crucial interventions into the racialization of the human/animal border. Pairing African American, South African, and Afro-Caribbean literature, film, visual and performance art with historical representations of animalized blackness found in philosophy, science, and law, ""Beyond the Limit"" highlights African diasporic literary and visual culture's disruptions of the racialization of the human/animal boundary.",ucb,,https://escholarship.org/uc/item/0rw8q9k9,,,eng,REGULAR,0,0
654,2090,Mechanisms of Rickettsia parkeri invasion of host cells and early actin-based motility,"Reed, Shawna","Welch, Matthew D;",2012,"Rickettsiae are obligate intracellular pathogens that are transmitted to humans by arthropod vectors and cause diseases such as spotted fever and typhus. Spotted fever group (SFG) Rickettsia hijack the host actin cytoskeleton to invade, move within, and spread between eukaryotic host cells during their obligate intracellular life cycle. Rickettsia express two bacterial proteins that can activate actin polymerization: RickA activates the host actin-nucleating Arp2/3 complex while Sca2 directly nucleates actin filaments. In this thesis, I aimed to resolve which host proteins were required for invasion and intracellular motility, and to determine how the bacterial proteins RickA and Sca2 contribute to these processes.        Although rickettsiae require the host cell actin cytoskeleton for invasion, the cytoskeletal proteins that mediate this process have not been completely described. To identify the host factors important during cell invasion by Rickettsia parkeri, a member of the SFG, I performed an RNAi screen targeting 105 proteins in Drosophila melanogaster S2R+ cells. The screen identified 34 proteins important for invasion, including a signal transduction pathway involving Abl tyrosine kinase, the RhoGEF Vav, the GTPases Rac1, Rac2, and Cdc42, the WAVE nucleation promoting factor (NPF) complex and the Arp2/3 complex.  In mammalian cells, including HMEC-1 endothelial cells, the natural targets of R. parkeri, the Arp2/3 complex was also crucial for invasion, while requirements for WAVE2 as well as Rho GTPases depended on the particular cell type. I propose that R. parkeri invades S2R+ arthropod cells through a primary pathway leading to actin nucleation, whereas invasion of mammalian endothelial cells occurs via redundant pathways that may involve the activity of host and bacterial proteins. Our results reveal a key role for the WAVE and Arp2/3 complexes, as well as a higher degree of variation than previously appreciated in actin nucleation pathways activated during Rickettsia invasion. 	Most pathogens undergo actin-based motility during a single phase of their life cycle, using the force of actin ""tails"" to push into neighboring cells without accessing the extracellular milieu.  In contrast, I found that R. parkeri undergo two phases of motility, early and late, during the infectious cycle.  Early actin tails are formed between 15 and 60 minutes after infection, have a distinctive short and curved appearance, are decorated with Arp2/3 complex proteins Arp3 and ARPC5, and are associated with polar localization of the Rickettsia Arp2/3 NPF, RickA.  Late actin tails, as previously described (Haglund et al. 2010, Serio et al. 2010) are long, composed of helical bundles of actin, and associated with polar localization of the Rickettsia formin-like actin nucleator, Sca2.  Early motility is Arp2/3-dependent and is significantly slower, and less efficient when compared to late motility. Finally, isolation of R. parkeri strains with transposon insertions in either the rickA or sca2 genes revealed that RickA is required for robust early actin tail formation, while Sca2 is required for late actin tail formation and efficient cell-to-cell spread. Thus, Rickettsia appear to be unique in their ability to promote two temporally and mechanistically distinct phases of actin-based motility during infection.  Continued investigation of invasion and actin-based motility may shed light on the pathogenesis of Rickettsia, the function of actin in the host cell, and the purpose of actin tail formation during intracellular infection.",ucb,,https://escholarship.org/uc/item/0v8134dn,,,eng,REGULAR,0,0
655,2091,Dehydrogenation of Light Alkanes over Supported Pt Catalysts,"Wu, Jason","Bell, Alexis T;",2015,"The production of light alkenes comprises a 250 million ton per year industry due to their extensive use in the production of plastics, rubbers, fuel blending agents, and chemical intermediates. While steam cracking and fluid catalytic cracking of petroleum crude oils are the most common methods for obtaining light alkenes, rising oil prices and low selectivities toward specific alkenes have driven the search for a more economical and efficient process. Catalytic dehydrogenation of light alkanes, obtained from natural gas feedstock, presents an attractive alternative that offers high selectivity and greater flexibility in the alkene pool to address changing demands. Platinum is the most effective metal to catalyze the reaction, but by itself, suffers from catalyst deactivation due to the buildup of carbonaceous deposits. The addition of a secondary metal to form a bimetallic alloy has been of high interest due to its ability to suppress coking and increase selectivity. This dissertation has focused on developing a deeper understanding of promotion effects of various metals and elucidating the mechanism behind coking on Pt catalysts.The use of Sn as a promoter was first investigated, and the effects of particle size and composition on the ethane dehydrogenation performance were determined using a colloidal method to prepare model catalysts. At high conversions, catalyst deactivation from coke formation was a strong function of particle size and Sn/Pt, in agreement with previous high resolution transmission electron microscopy studies (HRTEM) studies. Deactivation decreased significantly with decreasing particle size and increasing addition of Sn. For a fixed average particle size, the activity and selectivity to ethene increased with increasing content of Sn in the Pt-Sn particle. For Pt and Pt3Sn compositions, the turnover frequency increased with increasing particle size, while the selectivity to ethene was not strongly affected.	For uncovering the mechanism by which Pt catalysts deactivate, carbon formation on MgO-supported Pt nanoparticles was studied by in situ HRTEM in order to obtain time-resolved images of a single nanoparticle during the dynamic coking process. An electron dose rate dependence on the rate of carbon growth was found, and a suitable imaging strategy was adopted in order to minimize beam-induced artifacts. Multi-layer carbon growth around the nanoparticle was investigated, and significant restructuring of the particle was also observed. In particular, step formation was captured in various images, supporting evidence that the nucleation and growth of carbon during coking on Pt catalysts often requires low coordination sites such as step sites. This is in agreement with scanning tunneling microscopy (STM) experiments, which illustrate a slight preference for carbon atoms to nucleate at the step sites on a Pt(111) crystal.Other promoters for light alkane dehydrogenation were then investigated. The thermal dehydrogenation of n-butane to butene and hydrogen was carried out over Pt nanoparticles supported on calcined hydrotalcite containing indium, Mg(In)(Al)O. The optimal In/Pt ratio was found to be between 0.33 and 0.88, yielding > 95% selectivity to butenes. Hydrogen co-fed with butane was shown to suppress coke formation and catalyst deactivation, with a ratio of H2/C4H10 = 2.5 providing the best catalytic performance. In addition, a Pt-Ir alloy was investigated for ethane and propane dehydrogenation. Following characterization to confirm formation of a bimetallic alloy, intrinsic rate measurements at low feed residence time revealed the following trend in activity: Pt3Sn > Pt3Ir > Pt. DFT calculations carried out on tetrahedral clusters (Pt4, Pt3Ir, Pt3Sn) show that this trend in activity can be  attributed to variations in the HOMO-LUMO gap of the cluster.",ucb,,https://escholarship.org/uc/item/0x2827c0,,,eng,REGULAR,0,0
656,2092,Semiconductor Nanowires for Photoelectrochemical Water Splitting,"HWANG, YUN JEONG","Yang, Peidong;",2012,"Photolysis of water with semiconductor materials has been investigated intensely as a clean and renewable energy resource by storing solar energy in chemical bonds such as hydrogen. One-dimensional (1D) nanostructures such as nanowires can provide several advantages for photoelectrochemical (PEC) water splitting due to their high surface areas and excellent charge transport and collection efficiency. This dissertation discusses various nanowire photoelectrodes for single or dual semiconductor systems, and their linked PEC cells for self-driven water splitting. After an introduction of solar water splitting in the first chapter, the second chapter demonstrates water oxidative activities of hydrothermally grown TiO2 nanowire arrays depending on their length and surface properties. The photocurrents with TiO2 nanowire arrays approach saturation due to their poor charge collection efficiency with longer nanowires despite increased photon absorption efficiency. Epitaxial grains of rutile atomic layer deposition (ALD) shell on TiO2 nanowire increase the photocurrent density by 1.5 times due to improved charge collection efficiency especially in the short wavelength region. Chapter three compares the photocurrent density of the planar Si and Si nanowire arrays coated by anatase ALD TiO2 thin film as a model system of a dual bandgap system. The electroless etched Si nanowire coated by ALD TiO2 (Si EENW/TiO2) shows 2.5 times higher photocurrent density due to lower reflectance and higher surface area. Also, this chapter illustrates that n-Si/n-TiO2 heterojunction is a promising structure for the photoanode application of a dual semiconductor system, since it can enhance the photocurrent density compared to p-Si/n-TiO2 junction with the assistance of bend banding at the interface. Chapter four demonstrates the charge separation and transport of photogenerated electrons and holes within a single asymmetric Si/TiO2 nanowire. Kelvin probe force microscopy measurements show the higher surface potential on the n-TiO2 (photoanode) side relative to the p-Si (photocathode) side under UV illumination as the result of hole accumulation on the TiO2 side and electron accumulation on the Si side which are desirable charge separation for solar water splitting. In chapter five, TiO2 is replaced with single phase InGaN nanowire in a dual bandgap photoanode to show the potential for solar water splitting with high surface area Si/InGaN hierarchical nanowire arrays and InGaN as a possible candidate for visible light absorber. An enhancement of 5 times in photocurrent was observed when the surface area increased from InGaN nanowires on planar Si to InGaN nanowires on Si wires. Chapter six demonstrates a self-driven water splitting device with the p/n PEC cell which consists of a photocathode and a photoanode. The operating photocurrent (Iop) with the p/n PEC cell is enhanced when n-Si/p-Si photovoltage cell was embedded under an n-TiO2 photoanode by utilizing the photovoltage generated by a Si PV cell. Also, the Si nanowire photocathode surface is modified with Pt and TiO2 to increase hydrogen reducing activity and stability which enhances Iop of the p/n PEC cell as well. When Si/TiO2 nanowire photocathode is linked with n-Si/p-Si photovoltage cell embedded TiO2 nanowire photoanode, the p/n PEC cell shows water splitting without bias voltage confirmed with 2:1 ratio of hydrogen:oxygen gas evolution and a 92 % Faradic efficiency. These studies represent a significant step towards realizing the benefit of the advanced 1D nanowire configuration for efficient solar to energy conversion.",ucb,,https://escholarship.org/uc/item/0xh2x55w,,,eng,REGULAR,0,0
657,2093,"Decarbonizing Chinaâ€™s Power Sector: Potential, Prospects and Policy","He, Gang","Kammen, Daniel M.;",2015,"Chinaâ€™s power sector accounts for 25% of the world coal consumptionâ€“fully about 13% of total global carbon emissions from fossil fuel. Decarbonizing Chinaâ€™s power sector will shape how the country and to a large extent the world uses energy and addresses pollution and climate change. Combining methods of GIS modeling and wind and solar capacity factor simulation, this study utilized 200 representative locations each independently for wind and solar, with 10 years of hourly wind speed and solar irradiation data to investigate provincial capacity and output potentials from 2001 to 2010, and to build wind and solar availability profiles. This study then examined the implications of the solar and wind variability and availability in the context of an overall energy strategy for China by using a system optimization model: SWITCH-China to analyze the feasibility, costs and benefits of Chinaâ€™s clean power transition under three key policy scenarios: Reference Scenario, Low Cost Renewable Scenario, and Carbon Cap Scenario. By optimizing capacity expansion and hourly generation dispatch simultaneously, SWITCH-China is uniquely suited to explore both the value of and synergies among various power system technology options, providing policymakers and industry leaders with important information about the optimal development of the electricity grid.  Chinaâ€™s power sector is in the midst of fast development, and todayâ€™s investment decisions will have a large impact on the countryâ€™s ability to achieve its environmental and carbon mitigation goals. Concerted actions are needed to enable such a transition, including introducing a meaningful carbon price, coordinating the investment decisions, and building the necessary infrastructure for moving energy around.",ucb,,https://escholarship.org/uc/item/0xz2f8jg,,,eng,REGULAR,0,0
658,2094,The Racial Origins of U.S. Domestic Violence Law,"Mahan, Margo","Fox, Cybelle;",2017,"This dissertation investigates the historical emergence of wife-beating laws in the United States. The key questions I investigate are: What were the social conditions in which wife- beating laws emerged in the nineteenth-century South? What do these conditions reveal about the primary functions of these laws? Based on analysis of 19th-century legal and government data, local and appellate case records, federal reports, Freedmanâ€™s Bureau documents, periodical data, and family records, I argue that Southern wife-beating laws were a white supremacist post-Civil War response to the legalization of black family formation. They functioned to control black labor and degrade the status of blackness.My research challenges conventional accounts about the historical origins of U.S. domestic violence legislation. Since the proliferation of early wife-beating laws (1870-1900) coincided with first wave feminism, scholarship assumes that they were the result of feminist agency and borne out of a desire to protect women. These assumptions have led to two important limitations in domestic violence scholarship. First, most scholarship focuses on the North, where first wave feminism flourished. Second, even when research considers the effects of other social factors, such as race and class, it foregrounds the effects of feminist agency. Both limitations are troubling because the first state to legally rescind a husbandâ€™s right to chastise his wife was Alabama, whose 1871 Fulgham v. State ruling was also the countryâ€™s first in which the litigants were black. In fact, anti-wife-beating laws proliferated throughout southern states where, like Alabama, there was neither a feminist movement, nor female collective action against wife-beating. By the early 1900s, the ideological association between wife-beating and black families was so pervasive that denying â€œwife-beatersâ€ the vote was a device some southern states used to disenfranchise black men.In contrast to the feminist narrative, I argue that southern anti-wife-beating laws were a postbellum response to the racialized and gendered convergence of the antebellum Master- slave and Husband-wife relationships. Antebellum socio-legal norms simultaneously advanced marital cruelty protections for wives on the one hand and encouraged the physical chastisement of slaves on the other. This ensured that the authority to beat slave women â€“ to include de facto slave wives â€“ was a specifically white male prerogative; and, it added physical chastisement to a long list of naturalized distinctions between blackness and whiteness. Emancipation exposed the fragility of â€˜domestic relationsâ€™ â€“ and thus the1southern way of life â€“ by highlighting its dependence on racialized gender hierarchies. Wife- beating laws that threatened to punish black men, in the midst of socio-legal norms that kept black women vulnerable to white male violence, helped to restore a southern way of life that simultaneously controlled the labor and degraded the status of black families.The dissertation has six chapters. The introductory chapter provides the theoretical framework for the project. Wife-beating cases, I argue, performed the crucial socio-legal function of reinforcing domestic gender norms â€“ norms that were inextricably articulated through race and class, given the southern householdâ€™s distinctive Master-slave relationship. Chapter Two reveals that the antebellum progression of laws created a racialized double- standard for wife-beating, in which the prerogative to chastise white wives and slave menâ€™s de facto wives, was a privilege exclusive to white men. Chapter Three begins after the Civil war, when the Reconstruction amendments led Southern states to legally recognize black marriages and families. But the antebellum racialized double-standard for wife-beating nevertheless endured, criminalizing black men as wife-beaters in a burgeoning law and order regime that enabled control over black labor. Chapter Four elucidates how, the racialization of wife-beating as a black crime functioned to symbolize and reify a hegemonic ideology of black family dysfunction. Chapter Five examines how wife-beating is eventually used to disenfranchise black men. Chapter Six concludes the dissertation. Situated at the intersection of sociology of family, political economy, criminalization, and 19th-century southern historical literatures, my dissertation reveals how racial projects to symbolically and materially privilege Whiteness motivated the emergence of â€œfeministâ€ laws that scholarship and social policy largely conceptualize as apart from race, class, and market forces.",ucb,,https://escholarship.org/uc/item/0zs890k1,,,eng,REGULAR,0,0
659,2095,Eddy-covariance observations of the atmosphere-biosphere exchange of nitrogen oxides,"Min, Kyung-Eun","Cohen, Ronald C;",2012,"Oxidative forms of reactive nitrogen (NOy) impact air quality, nutrient dynamics, and climate. Our understanding of the sources and sinks of atmospheric NOy that result from exchange between the atmosphere and biosphere is poorly constrained due to the lack of adequate instrumentation.In this dissertation, I apply novel technologies to the question of NOy exchange and describe observations of fluxes at the atmosphere-forest interface. I use thermal-dissociation laser-induced fluorescence, chemiluminescence, and the micrometeorological eddy-covariance method. Simultaneous observations of the rate and magnitude of exchanges and vertically resolved concentration measurements of the NOy species (NO, NO2, RO2NO2, RONO2 and HNO3) were measured in addition to a wide suite of relevant parameters including canopy structures, meteorological parameters, and associated chemical species as part of the Biosphere Effects on AeRosols and Photochemistry EXperiment (BEARPEX), which took place June 15 - July 31, 2009 at a Ponderosa pine plantation on the western slope of Sierra Nevada Mountains in California.I present observational evidence for active within canopy chemistry resulting in simultaneous production and removal of NOX within a forest canopy. My observations provide direct evidence for a NOX canopy reduction factor and for an NO2 compensation point. I also establish that an unusual peroxynitrate is formed within the canopy and then emitted to the atmosphere above underscoring the importance of coupling between biogenic volatile organic compounds and NOX to the fluxes of NOX, NOy and BVOC.",ucb,,https://escholarship.org/uc/item/114865mm,,,eng,REGULAR,0,0
660,2096,Molecular mechanisms of transcription regulation by non-coding RNAs and the DNA helicase RECQL5,"Kassube, Susanne Anke","Nogales, Eva;",2013,"Transcription is the process of copying a fragment of DNA in the cell's nucleus into RNA. This copy is then used as a template to produce proteins, or it functions by itself as an enzyme, structural element or regulator. Transcription of protein-coding genes in eukaryotes is achieved by RNA polymerase II (Pol II), an enzyme that is tightly regulated to allow for the adaptation of transcript levels to both extracellular conditions as well as intracellular needs. My research has focused on understanding transcriptional regulation by two distinct factors: non-coding RNAs (ncRNAs) that are upregulated in response to cellular stress, and the DNA helicase RECQL5, a member of the highly conserved family of RecQ helicases involved in DNA repair. Non-coding RNAs are an important transcriptional regulator when cells adapt to extreme conditions such as heat shock. In mouse and human cells, heat shock triggers an increase in levels of B2/B1 RNA and Alu RNAs, respectively, which regulate expression of protein-coding genes by Pol II. Although it had been shown that ncRNAs interact directly with Pol II to regulate transcription, many important questions remained unanswered: Where is the binding site for ncRNAs located? Does binding of ncRNAs interfere with the binding of DNA to Pol II? How are repressive and non-repressive ncRNAs, which are both upregulated in response to heat shock and which both bind to Pol II with high affinity, distinguished? To address these questions, I employed single-particle cryo-electron microscopy (cryo-EM) to determine the structures of human Pol II in complex with six different repressive and non-repressive ncRNAs from mouse and human. The structural data allowed me to identify a conserved docking site for ncRNAs in the active site cleft of Pol II; the location of this site was later confirmed independently by cross-linking studies in collaboration with the laboratory of James Goodrich. Collectively, my analysis of the cryo-EM reconstructions of ncRNA-Pol II complexes in conjunction with biochemical data from the Goodrich lab suggest that the distinction between repressive and non-repressive ncRNAs is made by the general transcription factor TFIIF based on certain flexible RNA elements that extend beyond the docking site.RECQL5 is a DNA helicase implicated to function at the interface of the cellular DNA replication, DNA repair, and RNA transcription machineries. Although RECQL5 had previously been shown to interact directly with Pol II, its molecular mechanism of action remained elusive. My work aimed to answer the following questions: Where is the binding site for RECQL5 located on the surface of Pol II? Does binding of RECQL5 interfere with the binding of DNA or other transcription factors during transcription initiation or elongation? How is transcriptional repression by RECQL5 achieved at the molecular level? To answer these questions, we employed an integrative experimental approach, combining biochemical assays, X-ray crystallography, cryo-EM and small angle X-ray scattering. The crystal structure of a fragment of RECQL5's Pol II binding domain suggested that the topology of this domain is similar to a domain found in the transcription elongation factor TFIIS, which promotes continued transcription of arrested elongation complexes by stimulating the intrinsic RNA cleavage activity of Pol II. Using pull-down assays, I showed that RECQL5 and TFIIS compete for binding to Pol II, suggesting that the two proteins bind to overlapping sites. I corroborated these initial findings using an in vitro transcription assay, which confirmed that binding of RECQL5 to Pol II interferes with the function of TFIIS to promote read-through of intrinsic blocks to elongation. Using cryo-EM, I obtained a high-resolution reconstruction of an elongating Pol II complex repressed by RECQL5. By docking the known crystal structures of individual components into the EM map, I generated a pseudo-atomic model of the complex. This model confirmed the location of the binding site, and suggests a novel, dual mechanism for the regulation of transcription by RECQL5 that includes structural mimicry of the Pol II-TFIIS interaction. Both ncRNAs and RECQL5 are important regulatory factors in human cells whose molecular mechanisms of transcriptional repression remained unknown. My research has provided important insights into their structure and function and, in the case of RECQL5, uncovered a novel mechanism of transcription regulation that might be employed by a number of other factors involved in transcriptional repression at the interface of the DNA recombination, replication and repair machineries.",ucb,,https://escholarship.org/uc/item/0hs6d20h,,,eng,REGULAR,0,0
661,2097,The Yukian Language Family,"Schlichter, Marie-Alice",,1985,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0zf540nk,,,eng,REGULAR,0,0
662,2098,Connecting Science Concepts and Engineering Practices: Supporting Student Understanding of Energy Transformation,"McBride, Elizabeth Anne","Linn, Marcia F;",2018,"It is often claimed that engineering projects improve student achievement in mathematics and science, but research on this topic has shown that many projects do not live up to the claim (Teacher Advisory Council, 2009). Ideally, undertaking a science project should be motivating, while also helping students to understand the interplay between science concepts (like energy transformation) and engineering design decisions. This dissertation research investigates ways to integrate engineering practices and science concepts (like energy transformation) in classroom settings. I investigate ways to integrate the Next Generation Science Standards (NGSS) science and engineering practices while simultaneously expanding the knowledge integration theory (Linn & Eylon, 2011). I refine knowledge integration design principles in classroom studies, comparing alternative forms of instruction where students integrate engineering design and science disciplinary concepts. I accomplish this by creating new technologies to support students in building solar ovens while testing their design ideas in an interactive computer model that connects science concepts and design decisions. When students build a physical model they may neglect the scientific basis for their decisions, instead focusing on details of construction that may be superficial rather than scientifically based. Educational tools, like interactive computer models, can help students connect science principles and design decisions by making mechanisms such as energy transformation visible. The NGSS envision that instruction would combine practices including modeling, data, analysis, computational thinking, and design to enable students to integrate their scientific and engineering ideas (NGSS Lead States, 2013). This research identifies optimal ways to integrate science and engineering practices by taking advantage of interactive models, automated guidance for student short essays, and supports for making evidence centered decisions. The investigations are guided by the knowledge integration theory and the results expand the theory into the engineering domain. In this dissertation, I present five empirical chapters. Each study uses a solar ovens curriculum in which students use a virtual model to design and explore energy transformation, then build and test a physical solar oven. These studies investigate ways to support students in integrating their ideas about energy transformation with ideas about engineering design. The first empirical chapter investigates how computer models function in hands-on curriculum to aid in the knowledge integration process. The second and third empirical chapters investigate supports for students while they use computer models. These chapters document how students interact with the model. Because the computer model aids in both design and reflection, there are three chapters devoted to investigations of how the computer model aids students in knowledge integration. A fourth empirical chapter investigates the non-normative, yet common, idea that shiny or dark objects â€œattractâ€ light to them, causing them to heat up. I first collect data about the ideas students present around this non-normative idea, then present a method to automatically score student written responses for the presence of this idea. This automatic scoring algorithm could support the development of automated guidance that could then encourage students to refine their ideas. The fifth empirical chapter investigates two ways to frame the curriculum. Since the goal of this curriculum is to integrate both science content ideas and engineering design ideas, I investigate two different frameworks for presenting the curriculum â€“ science-centered or engineering-centered. Together, these chapters suggest guidelines for the structure of hands-on projects that aim to teach both science concepts and engineering design. First, creating dynamic computer models that allow students to test their design ideas has proven useful in helping students integrate science disciplinary ideas and engineering practices. However, students need scaffolding to integrate these ideas and practices. To ensure that the virtual models inform student designs in a meaningful way (and vice versa), there should be careful consideration about when during the curriculum they are introduced. Including science content in a meaningful way and supporting the integration of science ideas is also critical for the success of projects that are intended to support the integration of science and engineering. To help students make sense of key scientific phenomena, designers need to identify ideas that are challenging for students to distinguish among, like that of light propagation (e.g., is light reflected, absorbed, or â€œattractedâ€?). Creating opportunities for students to follow the knowledge integration process is important with these types of ideas, in order to give students the opportunity to integrate their disparate and perhaps contradictory ideas. Specifically, students need to generate multiple ideas so that those ideas can be inspected, added to through the use of inquiry activities, and then they can distinguish among their entire corpus of ideas. This process helps students to make sense of their ideas; the addition of an engineering project provides further evidence for students to reflect upon.It is also important to consider the goals for learning when framing curriculum as either an engineering or a science project. Different ways of framing the same type of project may lead to different learning outcomes. If a project is framed around engineering design, students are likely to develop stronger engineering practices, but their understanding of scientific content may not be as deep. If a project is framed as a scientific investigation, students may integrate their science ideas, but not develop a strong sense of engineering practices.",ucb,,https://escholarship.org/uc/item/1052c1vc,,,eng,REGULAR,0,0
663,2099,New Roads to Capitalism: China and Global Value Chains,"Dallas, Mark Peter","Chaudhry, Kiren;",2010,"The creation of markets in China has been most commonly analyzed through the lens and vocabulary of the new institutional economics in which broad, national-level institutional reforms are seen to be effective because they altered the incentive structures of farmers, local government officials, or factory managers.  Drawing from literature on comparative capitalism which focuses on the processes of production, this dissertation examines markets through deconstructing production to the level of specific commodities.  It utilizes a value chain framework by beginning with the cultivation of cotton, wool and silk agricultural commodities, and tracing them through China's textile and garment industries and into domestic and foreign trade. It considers each of these links along the chain as a locus of conflict between China's many ministries, local governments and economic actors, highlighting the political contestation and the complexity of state policy underlying the institutionalization of markets.  By tracing how the terms of trade become structured along the chain over time, it details the re-creation of economic order and the distribution of resources among different producer groups.        This approach is employed to construct a comparative historical narrative of China's textile agro-industries, starting from the domestic market reforms in the 1980s through to China's international integration in the 1990s, a period which coincided with major transformations in global manufacturing.  In terms of domestic market reforms in the 1980s, it first shows that an institutional economics perspective mistakenly draws too clear a line between China's planned economy and the market reforms over the 1980s.  By examining reforms at the level of concrete commodities and along the value chain, the planned economy and market reforms are re-conceptualized as being deeply interpenetrated such that the vitality of China's nascent market economy grew not simply from the liberation of economic interests through institutional re-engineering, but from the structure of China's version of a planned economy.        Second, it examines China's international integration over the 1990s by analyzing the impact of the contemporary transformations in global manufacturing, in which vertically integrated production along the value chain has been sliced up and re-integrated through cross-national networks of production.  This fragmentation of production introduced a new form of capitalist development in China in terms of the organization and regulation of industry and the composition of its labor force.      Finally, the dissertation's approach offers new insights into the study of China's rapid rise in regional inequality.   Instead of explaining regional inequality through differences in location advantages, it finds that regional inequalities arose more from changes in regulation between direct producers along the production chain.  The dissertation employs a variety of data sources, including fieldwork interviews, Chinese newspapers and trade journals, internal government documents and statistics, yearbooks and local gazetteers, industrial and population censuses and digital mapping techniques.",ucb,,https://escholarship.org/uc/item/10c7r9d4,,,eng,REGULAR,0,0
664,2100,The Development of Semiconducting Materials for Organic Photovoltaics,"Douglas, Jessica D.","FrÃ©chet, Jean M. J.;",2013,"The chemical structure of conjugated semiconducting materials strongly influences the performance of organic photovoltaic (OPV) devices. Thus a good understanding of the structure-function relationships that govern the optoelectronic and physical properties of OPV materials is necessary. In this dissertation, organic polymers and small molecules are evaluated in terms of OPV device output parameters, and molecular design rules are elucidated.The development of molecules with alternating electron-rich and electron-deficient backbone units provides materials with suitable optoelectronic properties for OPVs and favorable modularity for organic semiconductor design. The choice of specific aromatic units and side chains for conjugated materials are shown to modulate the energy levels and architecture of OPV devices, affecting each of the four mechanistic steps of OPV operation.In Chapter 2, the relationship between molecular packing parameters and the bulkiness of aliphatic solubilizing group extending away from a polymer backbone is elucidated, and high-performance OPV devices are achieved. In Chapter 3, the inclusion of a post-processing functionality on a polymer side chain is found to have a positive effect on the bulk morphology and overall performance of OPV devices. In Chapter 4, the influence of electron-withdrawing and quinoidal monomers on the optoelectronic properties of conjugated polymers is established, and energy level modulation is shown to affect the electron accepting and donating capabilities of OPV materials in a blended device. In Chapter 5, small molecules are designed with complementary light absorption properties in order to investigate a rarely observed charge generation mechanism.",ucb,,https://escholarship.org/uc/item/11v7k61v,,,eng,REGULAR,0,0
665,2101,"Between Trade and Legitimacy, Maritime and Continent: The Zheng Organization in Seventeenth-Century East Asia","Hang, Xing","Yeh, Wen-hsin;Pomeranz, Kenneth L.;",2010,"This study examines the Zheng organization, which flourished from 1625 to 1683, during a time when the Ming-Qing transition in China intersected with the formation of an integrated early modern economy in maritime Asia.  This quasi-governmental commercial enterprise reached the apex of its power under Zheng Chenggong (1624-1662), and his son and successor Zheng Jing (1642-1681).  From bases along the southeastern Chinese coast and Taiwan, they relied upon overseas commerce to maintain a sustained resistance against the Manchus, who had taken over most of China in 1644 from the collapsing Ming, the ethnic Chinese dynasty to which both men had pledged their support.  Like their fiercest competitor, the Dutch East India Company (VOC), the organization protected the safety and property of Chinese subjects abroad, engaged in armed trade, and aggressively promoted overseas expansion.  Zheng Chenggong and Jing proved far more successful and profitable at these endeavors than the VOC.  In 1662, shortly before his death, Chenggong even defeated and expelled the company from its colony of Taiwan, and opened the island for Chinese colonization and settlement.Yet, operating within an imperial world order that looked upon overseas contact of any form as a potential source of political instability, the Zhengs, lacking ""native"" maritime sources of legitimacy, had to receive recognition for their authority from continental centers of power.  Father and son skillfully utilized the ranks and titles from the Ming Yongli pretender to rule over territory, develop a civil bureaucracy, and sign treaties with foreign powers, functioning essentially as an autonomous ""state.""  Moreover, by successfully intermediating between continental and maritime Chinese cultural discourse, they forged a complex social unit of traders, militarists, and Ming imperial descendants and loyalist elites.  However, this ambiguous arrangement, which gave the organization maximum autonomy and flexibility, came under threat due to the gradual consolidation of Qing rule.Chenggong's successor, Zheng Jing, turned away from Ming symbols of authority on Taiwan during the 1660s, and tried to institutionalize a new identity based upon Han Chinese customs and Confucian moral values on an island considered by contemporaries to be geographically and culturally outside of ""China.""  In negotiations with the Qing court, he pressed hard for the emperor to recognize Taiwan as a tributary kingdom along the lines of Korea.  The talks broke down, however, over ethnic identity, as Zheng insisted upon keeping his Han Chinese long hair and flowing robes, while the Qing ruler ordered him to shave his head and wear tight riding jackets in the Manchu style.  Despite the failure of negotiations, Zheng took significant steps toward articulating a distinct Han Chinese state.  He traded extensively, signed a commercial treaty with the English East Indies Company, and nearly launched an invasion of the Spanish Philippines.  However, his return to China to participate in the Rebellion of the Three Feudatories (1674-1681) ruined his organization and paved the way for the Qing invasion and occupation of Taiwan in 1683, two years after his death.This project moves beyond the standard Confucian trope of the Zhengs as ardent Ming loyalists or the Western narrative of ruthless pirate entrepreneurs, extreme discourses later appropriated to serve different nationalisms.  Instead, the two men should be viewed as both the initiators and products of a dynamic and internally generated East Asian modernity within an interdependent economic and cultural region that nonetheless enjoyed significant interactions outside the system.  Such an approach imbues maritime China with agency and revises the role commonly attributed to it as a marginalized appendage of its bureaucratic and agrarian continental counterpart.  An examination of interstate relations unique to this East Asian world region also allows one to conceive of communities beyond the nation-state, and make sense of their identity formation and change, especially when combined with shifts in spatial settings.",ucb,,https://escholarship.org/uc/item/133829bz,,,eng,REGULAR,0,0
666,2102,Speeding up distributed storage and computing systems using codes,"Lee, Kang Wook","Ramchandran, Kannan;",2016,"Modern data centers have been providing exponentially increasing computing and storage resources, which have been fueling core applications ranging from search engines in the early 2000's to the real-time, large-scale data analysis of today. All these breakthroughs were made possible only due to the scalability in computing and storage resources offered by modern large-scale clusters, comprising individually small and unreliable low-end devices. Given the individually unpredictable nature of the underlying devices in these systems, we face the constant challenge of securing predictable and high-quality performance of such systems in the face of uncertainty.In this thesis, distributed storage and computing systems are viewed through a coding-theoretic lens. The role of codes in providing resiliency against noise has been studied for decades in many other engineering contexts, especially in communication systems, and codes are parts of our everyday infrastructure such as smartphones, WiFi, cellular systems, etc. Since the performance of distributed systems is significantly affected by anomalous system behavior and bottlenecks, which we call ""system noise"", there is an exciting opportunity for codes to endow distributed systems with robustness against such system noise.Our key observation â€“ channel noise in communication systems is equivalent to system noise in distributed systems â€“ forms the key motivation of this thesis, and raises the fundamental question: ""can we use codes to guarantee robust speedups in distributed storage and computing systems?"". In this thesis, three main layers of distributed computing and storage systems â€“ storage layer, computation layer, and communication layer â€“ are robustified through coding-theoretic tools. For the storage layer, we show that coded distributed storage systems allow faster data retrieval in addition to the other known advantages such as higher data durability and lower storage overhead; for the computation layer, we inject computing redundancy into distributed algorithms that are robust to stragglers or nodes that are substantially slower than the other nodes; for the communication layer, we propose a novel data caching and communication protocol, based on coding-theoretic principles that can significantly reduce the network overhead of the data shuffling operation, which is necessary to achieve higher statistical efficiency when running parallel/distributed machine learning algorithms.",ucb,,https://escholarship.org/uc/item/0jb2g9pp,,,eng,REGULAR,0,0
667,2103,Nuclear Resonance Fluorescence for Nuclear Materials Assay,"Quiter, Brian Joseph","Prussin, Stanley G;",2010,"This dissertation examines the measurement of nuclear resonance fluorescence gamma-rays as a technique to non-destructively determine isotopic compositions of target materials that are of interest for nuclear security applications. The physical processes that can result in non-resonant background to nuclear resonance fluorescence measurements are described and investigated using a radiation transport computer code that relies on the Monte Carlo technique, MCNPX. The phenomenon of nuclear resonance fluorescence is discussed with consideration of the angular distributions of resonance emissions, the effects of nuclear recoil, and the influence of thermal motion.Models describing two ways of measuring nuclear resonance fluorescence rates in materials are considered. First the measurement of back-scattered photons is considered. In this type of measurement, the portion of the interrogating photon beam that is scattered into large relative angles is measured. When the radioactivity of the target can be overcome by shielding or by use of intense photon sources, direct measurement of gamma-rays, emitted during nuclear resonance fluorescence can provide quantitative signatures that appear to be useful for applications such as forensic age-dating of large radiological sources. However, if the target radioactivity is too intense, as in the case for most spent nuclear fuel, a second measurement type, where indirect measurement of transmitted resonant-energy photons can also provide quantitative information. This method allows radiation detectors to be better-shielded from target radioactivity, but suffers from a slower accrual rate of statistical confidence.  The models described herein indicate that very intense photon sources and large high-resolution detector arrays would be needed to measure 239Pu content in spent fuel to precisions desired by nuclear safeguards organizations. However, the rates at which statistics accrue are strongly proportional to the strengths of the resonances, and measurement of a plutonium isotope with stronger resonances may provide more practical measurement rates.The model for predicting relative detection rates of nuclear resonance fluorescence gamma-rays in the transmission measurement was experimentally tested using the 238U in a mixture of depleted uranium and lead as a surrogate for 239Pu in spent fuel. The experiment indicated that the model was approximately correct, but that the process of notch refilling, which was excluded from the initial model, appears to be visible. Data files of the computer code, MCNPX, were modified to allow for nuclear resonance fluorescence to be simulated and a bug in the code was repaired to allow the code to more accurately simulate non-resonant elastic photon scattering. Simulations using this modified version of MCNPX have indicated that the magnitude of the notch refill process is comparable to that of the difference between the analytical model and the experimental data.",ucb,,https://escholarship.org/uc/item/0kc2s2kn,,,eng,REGULAR,0,0
668,2104,"Precision Higgs Physics, Effective Field Theory, and Dark Matter","Henning, Brian Quinn","Murayama, Hitoshi;",2015,"The recent discovery of the Higgs boson calls for detailed studies of its properties. As precision measurements are indirect probes of new physics, the appropriate theoretical framework is effective field theory. In the first part of this thesis, we present a practical three-step procedure of using the Standard Model effective field theory (SM EFT) to connect ultraviolet (UV) models of new physics with weak scale precision observables. With this procedure, one can interpret precision measurements as constraints on the UV model concerned. We give a detailed explanation for calculating the effective action up to one-loop order in a manifestly gauge covariant fashion. The covariant derivative expansion dramatically simplifies the process of matching a UV model with the SM EFT, and also makes available a universal formalism that is easy to use for a variety of UV models. A few general aspects of renormalization group running effects and choosing operator bases are discussed. Finally, we provide mapping results between the bosonic sector of the SM EFT and a complete set of precision electroweak and Higgs observables to which present and near future experiments are sensitive.With a detailed understanding of how to use the SM EFT, we then turn to applications and study in detail two well-motivated test cases. The first is singlet scalar field that enables the first-order electroweak phase transition for baryogenesis; the second example is due to scalar tops in the MSSM. We find both Higgs and electroweak measurements are sensitive probes of these cases.The second part of this thesis centers around dark matter, and consists of two studies. In the first, we examine the effects of relic dark matter annihilations on big bang nucleosynthesis (BBN). The magnitude of these effects scale simply with the dark matter mass and annihilation cross-section, which we derive. Estimates based on these scaling behaviors indicate that BBN severely constrains hadronic and radiative dark matter annihilation channels in the previously unconsidered dark matter mass region MeV < m < 10 GeV. Interestingly, we find that BBN constraints on hadronic annihilation channels are competitive with similar bounds derived from the cosmic microwave background.Our second study of dark matter concerns a possible connection with supersymmetry and the keV scale. Various theoretical and experimental considerations motivate models with high scale supersymmetry breaking. While such models may be difficult to test in colliders, we propose looking for signatures at much lower energies. We show that a keV line in the X-ray spectrum of galaxy clusters (such as the recently disputed 3.5 keV observation) can have its origin in a universal string axion coupled to a hidden supersymmetry breaking sector. A linear combination of the string axion and an additional axion in the hidden sector remains light, obtaining a mass of order 10 keV through supersymmetry breaking dynamics. In order to explain the X-ray line, the scale of supersymmetry breaking must be about 10^{11-12} GeV. This motivates high scale supersymmetry as in pure gravity mediation or minimal split supersymmetry and is consistent with all current limits. Since the axion mass is controlled by a dynamical mass scale, this mass can be much higher during inflation, avoiding isocurvature (and domain wall) problems associated with high scale inflation. In an appendix we present a mechanism for dilaton stabilization that additionally leads to O(1) modifications of the gaugino mass from anomaly mediation.",ucb,,https://escholarship.org/uc/item/0kq093s2,,,eng,REGULAR,0,0
669,2105,"Building for Oil: Corporate Colonialism, Nationalism and Urban Modernity in Ahmadi, 1946-1992","Alissa, Reem IR","AlSayyad, Nezar;",2012,"Located at the intersection of oil and space, this dissertation highlights the role of oil as an agent of political, social and cultural change at the level of the everyday urban experience by introducing the oil company town as a modern architectural and urban planning prototype that has been largely neglected in the Middle East. Using the Kuwait Oil Company (KOC) town of Ahmadi as a case in point this article offers a new history of oil, architecture and urbanism in Kuwait since 1946. Apart from oil dictating Ahmadi's location and reason for being various actors were complicit in the creation and playing out of Ahmadi's urban modernity: British KOC officials, the company's architectural firm Wilson Mason & Partners, nationalism, the process of Kuwaitization, Ahmadi's architecture and urbanism, and, especially, the town's residents.  I argue that Ahmadi's colonial modernity which was initially targeted at the expatriate employees of the company during the 1950s, was later adopted by KOC's Kuwaiti employees after the country's independence in 1961, and in turn mediated a drastically new lifestyle, or urban modernity, during the 1960s and 1970s. The memory of this urban modernity coupled with its gradual erosion ever since have rendered Ahmadi a nostalgic city in the nation's collective imagination.",ucb,,https://escholarship.org/uc/item/0pj4q6w0,,,eng,REGULAR,0,0
670,2106,"Supporting Generative Thinking about Number Lines, the Cartesian Plane, and Graphs of Linear Functions","Earnest, Darrell Steven","Saxe, Geoffrey;",2012,"This dissertation explores fifth and eighth grade students' interpretations of three kinds of mathematical representations: number lines, the Cartesian plane, and graphs of linear functions. Two studies were conducted. In Study 1, I administered the paper-and-pencil Linear Representations Assessment (LRA) to examine students' understanding of the three representations. The LRA had an experimental component that compared performance on routine problems to non-routine problems (problems not amenable to routine solution procedures). I administered the assessment to Grade 5 students (n=126) who had no formal instruction involving function graphs, and I compared their performances with those of Grade 8 students (n=131) enrolled in Algebra 1. A repeated measures ANOVA revealed students in each grade performed better on routine problems compared to non-routine problems, suggesting that routine problems may falsely indicate greater competence. Paired samples t-tests indicated no differences in performance between Grades 5 and 8 students on number line items, though Grade 8 students outperformed fifth graders on Cartesian plane and function graph items. Videotaped interviews with a subset of Grades 5 and 8 students revealed that students in each grade approached tasks across representations in similar ways, suggesting persisting misconceptions.  Interviews also revealed patterns unique to each grade.In Study 2, I examined the efficacy of a tutorial intervention. The intervention introduced written definitions to support principled understandings of the number line, the Cartesian plane, and function graphs. A repeated measures ANOVA that compared pre/posttest scores of Grade 5 students (n=20) to a matched control group (n=20) revealed significant gains from pre- to posttest in the experimental group, with no detectable gains in a control. At posttest, Grade 5 tutorial students performed significantly better on non-routine LRA problems than Grade 8 students who did not receive the tutorial. Video analysis revealed a correlation between tutorial students' appropriate uptake of definitions and gains from pretest to posttest.Analyses across the two studies indicate that instruction that supports students' coordination of linear and numerical units can support students' learning with understanding.  Potential applications include the development of curricula to support students' learning with understanding related to these representations and teacher professional development interventions.",ucb,,https://escholarship.org/uc/item/1826m1v0,,,eng,REGULAR,0,0
671,2107,"Against Arcadia: English Mock-Pastoral and Mock-Georgic, 1660-1740","Boyd, Brad Quentin","Turner, James G.;",2013,"AbstractAgainst Arcadia: English Mock-Pastoral and Mock-Georgic, 1660-1740by Brad Quentin BoydDoctor of Philosophy in EnglishUniversity of California, BerkeleyProfessor James Grantham Turner, ChairAgainst Arcadia: English Mock-Pastoral and Mock-Georgic, 1660-1740 is a study of the receptions of the ancient Greek and Roman genres or modes of pastoral and georgic in the British nations and Ireland by poets of the Restoration and early eighteenth century, in particular Andrew Marvell, John Wilmot, Earl of Rochester, Jonathan Swift, John Gay, and Alexander Pope.  It argues that the traditional and still-dominant literary history of pastoral and georgic in English, which sees these poetic forms in terminal decline after the deaths of the ""last Renaissance poets,"" John Milton and Andrew Marvell, is mistaken, and seeks to reconfigure that history.In the case of pastoral, most readers have proceeded from a mistaken belief that arcadian or soft pastoral, marked by idealizing, sentimental, romance conventions, was the traditional nature of this poetic form and that the waning of poetry of this kind after 1660 thus represented the decline and fall of pastoral.  This study argues on the contrary that such arcadian accretions to the main trunk of Graeco-Roman and medieval pastoral in fact date primarily from the widespread popularity of Jacopo Sannazaro's Arcadia and other ""soft"" pastoral Renaissance texts, and that Rochester, Swift, Gay, and Pope, by their vibrant retrieval of the thematic and contextual reference of ancient pastoral, especially its paradigmatic practitioners Theocritus and Vergil, reactivate the traditional nature of the genre: pastoral had in fact always been highly ironized, philosophically skeptic, and often scabrously sexualized, surprisingly ""modern"" almost two thousand years before modernity.In the case of georgic, this study argues, a similar misprision has traditionally led literary history to suppose that the earnest true georgics of the eighteenth century (didactic and landscape-descriptive poems by Philips, Somervile, Thomson, Dyer, Grainger, Jago) were the direct descendants of Hesiodic and especially Vergilian georgic.  In fact, this study argues, it is the mock-georgics of Marvell, Rochester, Swift, Gay, and Pope that lay the best claim to that identity, marked as they are not only by ancient georgic's irony, skepticism of ideas of natural innocence and ease, and consciousness of the dislocations and losses of civil and foreign war, in sharp contrast to the earnest, naturalist or optimist, and progressive themes of eighteenth-century true georgics (which are not in this sense ""true"" at all).  Instead, informed in Marvell's case by the experience of the defeat of the republican and Whig cause at the Restoration, and in the case of Swift, Gay, and Pope by the aftermath of the Stuart dynasty's major reverses in 1688 and 1714, they imagine and satirize a landscape, and cityscape, that are gradually descending to political and cultural ruin.",ucb,,https://escholarship.org/uc/item/19b3n9nk,,,eng,REGULAR,0,0
672,2108,"The Effect of War and Its Aftermath on Land Use and Land Cover in Jinotega, Nicaragua","Zeledon, Esther Beatriz","Kelly, Maggi;",2010,"In Nicaragua, the northern region east of Jinotega is often described as the lungs of the country.  Cool temperatures, lush forests, wild rivers, and abundant animals fill the rich landscape.  Even though this forested area exists inside the protected Bosawas International Biosphere UNESCO Reserve (Bosawas), anyone navigating east immediately notices this is not the case for the entire region.  Rather, the area outside of the Reserve is comprised of a patchwork of forest, agriculture, and cleared patches.  Over the last 50 years, the area has experienced a tremendous change in landscape and land-use.  Many scientists and conservation groups have observed the patterns of deforestation and tracked the loss of forest in this area.  And while the pattern of land use change began 50 years ago, the area has experienced its most rapid deforestation during the aftermath of the Sandinista/Contra war in the early 1990s.            Through integrated methods combining remote sensing and political ecology, I tell the story of the aftermath of an armed conflict that was fought in the jungles of the Bosawas Nature Reserve, and of the impact this conflict had on the landscape.  I use a combination of top-down view of satellites that observe change over decades as well as oral history across frontier areas to tell the account of Land-Use and Land Change (LULC) in the war stricken area of Jinotega, Nicaragua.  The effects of the Sandinista/Contra war significantly shaped the land and drove large-scale deforestation in Jinotega, Nicaragua.  Ex-combatants who fought in the Bosawas region for a decade, were left in the area after the war with few options.  The Nicaraguan government promised the ex-combatants deeds to land and an opportunity to farm in exchange for disarmament, but the land promised by the Nicaraguan government was rainforest land, poor in nutrients that required unique and proper training to make productive.  This policy led to and encouraged migration to the Bosawas region.  The act of merely giving deeds to land in the Bosawas region changed the traditional trajectory of land-use, and accelerated alterations. Because the ex-combatants lacked training and skills to properly cultivate the land in Bosawas, deforestation in this area did not occur along a linear agricultural frontier as described in the classic ""agricultural frontier"" land use transition model, but rather took a sporadic approach depending on where the ex-combatants resided.           My study suggests a modification to the traditional agricultural frontier model is needed to fully understand these kinds of war-influenced land use patterns: the movement and migration of people during the aftermath of war must be considered.  Thus, this study extends the traditional land-use model by recognizing complex underlying causes of land use change and consequently argues against the often predominant discourse that focuses on the ""encroaching"" peasant.  My analysis demonstrates that cleared patches increased in size and frequency after the war along the frontier zone.  Furthermore, from 1986 to 1996, there was a higher occurrence of clearing patches along transportation routes and rivers, demonstrating the increased migration and opening of the land after the war.  The oral history collected identifies the relationships between socio-economic, political, and historical factors that affected the aftermath of war.  Nicaragua appears to follow the typical agricultural frontier patterns where deforestation is driven by farmers.  However, given a closer look, the deforestation experience in Nicaragua is much more complex due to the aftermath of war in the region.             This study was a successful bridge and collaboration between the quantitative and the qualitative when examining the effects of war and its aftermath: remote sensing gave me the large-scale synoptic view of land use change and deforestation, and political ecology gave me a more nuanced understanding of the human causes of that change.  Wars and conflicts are prevalent worldwide; this study serves as a microcosm for other war-inflicted areas and encourages the study of not only the effect of the conflict itself, but also its aftermath.  An increase in investigations in war areas that integrate methods may help accurately determine the consequences of war on the landscape, resulting in accurate management plans specific to the environmental and human needs.",ucb,,https://escholarship.org/uc/item/1bj5297f,,,eng,REGULAR,0,0
673,2109,Robust Scheduling for Queueing Networks,"Pedarsani, Ramtin","Walrand, Jean C;",2015,"Queueing networks are used to model complicated processing environments such as data centers, call centers, transportation networks, health systems, etc. A queueing network consists of multiple interconnected queues with some routing structure, and a set of servers that have different and possibly overlapping capabilities in processing tasks (jobs) of different queues. One of the most important challenges in designing processing systems is to come up with a low-complexity and efficient scheduling policy.In this thesis, we consider the problem of robust scheduling for various types of processing networks. We call a policy robust if it does not depend on system parameters such as arrival and service rates. A major challenge in designing efficient scheduling policies for new large-scale processing networks is the lack of reliable estimates of system parameters; thus, designing a robust scheduling policy is of great practical interest.  We develop a novel methodology for designing robust scheduling policies for queueing networks. The key idea of our design is to use the queue-length changes information to learn the right allocation of service resources to different tasks by stochastic gradient projection method. Our scheduling policy is oblivious to the knowledge of arrival rates and service rates of tasks in the network. Further, we propose a new fork-join processing network for scheduling jobs that are represented as directed acyclic graphs. We apply our robust scheduling policy to this fork-join network, and prove rate stability of the network under some mild assumptions. Next, we consider the stability of open multiclass queueing networks under longest-queue (LQ) scheduling. LQ scheduling is of great practical interest since (a) it requires only local decisions per group of queues; (b) the policy is robust to knowledge of arrival rates, service rates and routing probabilities of the network. Throughput-optimality of LQ scheduling policy for open multiclass queueing network is still an open problem. We resolve the open problem for a special case of multiclass queueing networks with two servers that can each process two queues, and show that LQ is indeed throughput-optimal.Finally, we consider transportation networks that can be well modeled by queueing networks. We abstractly model a network of signalized intersections regulated by fixed-time controls as a deterministic queueing network with periodic arrival and service rates. This system is characterized by a delay-differential equation. We show that there exists a unique periodic trajectory of queue-lengths, and every trajectory or solution of the system converges to this periodic trajectory, independent of the initial conditions.",ucb,,https://escholarship.org/uc/item/1c54x9b9,,,eng,REGULAR,0,0
674,2110,Circadian Rhythms and Clock Genes in Inter-episode Bipolar Disorder,"McGlinchey, Eleanor Louise","Harvey, Allison G;",2012,"Objectives: Circadian rhythms are hypothesized to be disturbed in bipolar disorder (BD). However, the empirical evidence for this hypothesis is mixed. Hence, the goals of the current investigation were to extend and contribute to clarifying the literature on circadian rhythms in inter-episode BD by comparing proxies for circadian rhythm functioning in BD I and II individuals, relative to healthy controls.Methods: Thirty-five adults diagnosed with inter-episode BD I and II were compared to 37 healthy controls of similar age and gender. All participants completed a questionnaire assessing chronotype, reported daily sleep and wake times for four weeks, and wore wrist actigraphy for four weeks in order to assess objective estimates of sleep and wake timing and to calculate the cosinor variables, MESOR, amplitude and acrophase, as proxies for circadian modulation of activity. Participants also provided a saliva sample in order to analyze DNA for polymorphisms of the clock genes PER3 and CLOCK.Results: There were no significant differences between the BD and control groups on questionnaire reported chronotype, although there was a trend for significance such that BD individuals were more likely to report an evening chronotype. BD individuals exhibited more instability in sleep and wake timing as assessed by both sleep diary and actigraphy. There were no significant differences between the BD and control groups on any of the cosinor analysis variables; namely, MESOR, amplitude and acrophase. There was a trend for differences in MESOR among BD individuals who were carriers of the C-allele of the CLOCK gene. There were no differences in circadian cosinor variables for any of the genotype groups of PER3. Conclusions: The current findings are not consistent with previous hypotheses that abnormal circadian activity rhythms are enduring (trait) characteristics of BD individuals. However, instability in sleep timing may be characteristic of the inter-episode period.",ucb,,https://escholarship.org/uc/item/1cp2g5x0,,,eng,REGULAR,0,0
675,2111,Continuum approach for modeling and simulation of fluid diffusion through a porous finite elastic solid,"Zhao, Qiangsheng","Papadopoulos, Panayiotis;",2013,"The diffusion of liquid and gas through porous solids is of considerable technological interest and has been investigated for decades in a wide spectrum of disciplines encompassing chemical, civil, mechanical, and petroleum engineering. Porous solids of interest are made of either natural materials (e.g., soil, sand) or man-made materials (e.g., industrial filters, membranes). In both cases, liquids (e.g., water, crude oil) and gases (e.g., air, oxygen, natural gas) are driven through the voids in the porous solid by naturally or artificially induced pressure. NafionâƒR is an important example of a well-characterized man-made porous medium due to its extensive use in proton- exchange membrane fuel cells. Here, while the fuel cell is in operation, a mixture of air and water diffuses through the pores of a NafionâƒR membrane. The efficiency of the fuel cell is affected by the variation in water concentration. In addition, high water concentration has been experimen- tally shown to cause substantial volumetric deformation (swelling) of the membrane, which may compromise the integrity of the device.In this dissertation, a continuum approach for modeling diffusion of fluid through a porous elastic solid is proposed. All balance laws are formulated relative to the frame of a macroscopic solid resulting from the homogenization of the dry solid and the voids. When modeling only liquid diffusion through the macroscopic solid, the displacement of the macroscopic solid and the liquid volume fraction are chosen to characterize the state of the porous medium, and Fick's law is used as the governing equation for liquid flow. When modeling multiphase diffusion through the macroscopic solid, the displacement of the solid, the gas pressure and the liquid saturation are chosen as state variables, and both fluid diffusions are assumed to follow Darcy's law. Both single phase and multiphase diffusion models are implemented in the finite element method, and tested with various loading conditions on different types of materials. Numerical simulation results are presented to show the predictive capability of the two models.",ucb,,https://escholarship.org/uc/item/1cp328r3,,,eng,REGULAR,0,0
676,2112,Last Passage Percolation and the Slow Bond Problem,"Sarkar, Sourav","Hammond, Alan;",2019,"Last passage percolation models are fundamental examples in statistical mechanics where the energy of a path is maximized over all directed paths with given endpoints in a random environment, and the maximizing paths are called {\em geodesics}. Here we consider the Poissonian last passage percolation (LPP) and the exponential directed last passage percolation (DLPP), the latter having a standard coupling with another classical interacting particle system, the totally asymmetric simple exclusion process or TASEP. These belong to the so-called KPZ universality class, for which exact algebraic formulae have led to precise results for fluctuations and scaling limits. However, such formulae are not very robust and studying the geometry of the geodesics can often provide new insights into these models. Here we consider three problems in each of these three models; exponential DLPP, TASEP and Poissonian LPP, and see how geometric and probabilistic techniques solve such problems.In the first problem, we study finer properties of the coalescence structure of finite and semi-infinite geodesics for exactly solvable models of last passage percolation. We consider directed last passage percolation on $\Z^2$ with i.i.d. exponential weights on the vertices. Fix two points $v_1=(0,0)$ and $v_2=(0, \lfloor k^{2/3} \rfloor)$ for some $k>0$, and consider the maximal paths $\Gamma_1$ and $\Gamma_2$ starting at $v_1$ and $v_2$ respectively to the point $(n,n)$ for $n\gg k$. Our object of study is the point of coalescence, i.e., the point $v\in \Gamma_1\cap \Gamma_2$ with smallest $|v|_1$. We establish that the distance to coalescence $|v|_1$ scales as $k$, by showing the upper tail bound $\P(|v|_1> Rk) \leq R^{-c}$ for some $c>0$. We also consider the problem of coalescence for semi-infinite geodesics. For the  almost surely unique semi-infinite geodesics in the direction $(1,1)$ starting from $v_3=(-\lfloor k^{2/3} \rfloor , \lfloor k^{2/3}\rfloor)$ and $v_4=(\lfloor k^{2/3} \rfloor ,- \lfloor k^{2/3}\rfloor)$, we establish the optimal tail estimate $\P(|v|_1> Rk) \asymp R^{-2/3}$, for the point of coalescence $v$. This answers a question left open by Pimentel(2016) who proved the corresponding lower bound.  Next, we study the ``slow bond"" model, where the totally asymmetric simple exclusion process (TASEP) on $\Z$ is modified by adding a slow bond at the origin. The slow bond increases the particle density immediately to its left and decreases the particle density immediately to its right. Whether or not this effect is detectable in the macroscopic current started from the step initial condition has attracted much interest over the years and this question was settled recently in Basu-Sidoravicius-Sly (2014) where it was shown that the current is reduced even for arbitrarily small strength of the defect. Following non-rigorous physics arguments in Janowsky-Lebowitz(1992,1994) and some unpublished works by Bramson, a conjectural description of properties of invariant measures of TASEP with a slow bond at the origin was provided by Liggett in his book (1999). We establish Liggett's conjectures and in particular show that, starting from step initial condition, TASEP with a slow bond at the origin, as a Markov process, converges in law to an invariant measure that is asymptotically close to product measures with different densities far away from the origin towards left and right. Our proof exploits the correspondence between TASEP and the last passage percolation on $\Z^2$ with exponential weights and uses the understanding of geometry of maximal paths in those models. Finally, we study the modulus of continuity of polymer fluctuations and weight profiles in Poissonian LPP. The geodesics and their energy in Poissonian LPP can be scaled so that transformed geodesics cross unit distance and have fluctuations and scaled energy  of unit order, and we refer to scaled geodesics as {\em polymers} and their scaled energies as {\em weights}. Polymers may be viewed as random functions of the vertical coordinate and, when they are, we show that they have modulus of continuity whose order is at most  $t^{2/3}\big(\log t^{-1}\big)^{1/3}$. The power of one-third in the logarithm may be expected to be sharp and in a related problem we show that it is: among polymers in the unit box whose endpoints have vertical separation $t$ (and a horizontal separation of the same order),the  maximum transversal fluctuation has order $t^{2/3}\big(\log t^{-1}\big)^{1/3}$.Regarding the orthogonal direction, in which growth occurs, we show that, when one endpoint of the polymer is fixed at $(0,0)$ and the other is varied vertically  over $(0,z)$, $z\in [1,2]$, the  resulting random weight profile has sharp modulus of continuity of order $t^{1/3}\big(\log t^{-1}\big)^{2/3}$. In this way, we identify exponent pairs of $(2/3,1/3)$and $(1/3,2/3)$ in power law and polylogarithmic correction, respectively for polymer fluctuation, and polymer weight under vertical endpoint perturbation.The two exponent pairs describe [Hammond(2012, 2012, 2011)] the fluctuation of the boundary separating two phases in subcritical planar random cluster models.",ucb,,https://escholarship.org/uc/item/1cw491jk,,,eng,REGULAR,0,0
677,2113,"Cache decisions, competition, and cognition in the fox squirrel, Sciurus niger","Delgado, Mikel","Jacobs, Lucia F;",2017,"Caching is the movement and storage of food items by animals for future use. Caching facilitates survival during periods of scarcity, may reduce foraging time during future searches for food, and allows animals to take advantage of periods when available food exceeds current needs. Scatter-hoarding animals store one item per cache, and must employ cognitive strategies to protect their caches. These strategies include assessing the relative value of food items, carefully hiding food items, deceptive behaviors to thwart potential pilferers, and remembering each cache location. Such decisions should be driven by economic variables, such as the value of the individual food items, the scarcity of these items, and competition and risk of pilferage by conspecifics.My dissertation begins with a general overview of the food-storing literature and the natural caching behavior of the scatter-hoarding fox squirrel (Sciurus niger). I then describe several experiments that explored the decisions fox squirrels make when storing food. A study examining how fox squirrels adjust effort assessing and caching food based on the food itemâ€™s value (weight, perishability and nutritional content) using two different foods, hazelnuts and peanuts, is described in Chapter 2. Squirrels (n = 23) were observed during natural periods of food scarcity (summer) and abundance (fall). Assessment and investment per cache increased when resource value was higher (hazelnuts) or resources were scarcer, but decreased as experimental sessions continued. This study showed that fox squirrelsâ€™ assessment and caching behaviors were sensitive to both daily and seasonal resource abundance.Another important problem facing scatter-hoarding animals is how to maximize the retrieval of stored food items while minimizing the risk of pilferage by competitors. One defense against theft could be the spatial placement of caches. I describe a study examining whether the spatial distribution of caches is dependent on nut species in Chapter 3. I measured four key variables of the cache decision: distance and direction traveled, the use of distinct cache areas by nut species, and density of caches. Fox squirrels (n = 48) were tested in 50 sessions, and the geographical coordinates of over 900 cache locations were recorded. Results suggested that squirrels distribute caches using three heuristics: matching the distance traveled before caching to the value of the food item, systematically covering a caching area, and matching cache density to minimize pilferage risk to the highest valued food items. Squirrels spatially chunked their caches by nut species, but only when foraging from a single location. This first demonstration of spatial chunking in a scatter-hoarder underscores the cognitive demand of scatter-hoarding. I describe a final field study in Chapter 4. A pilot study revealed that there was a high level of pilfering (25%) among a population of fox squirrels. Nineteen fox squirrels cached 294 hazelnuts with passive integrated transponder tags implanted in them. Variables collected included assessment and cache investment and protection behaviors; cache location, substrate, and conspicuousness of each cache; how long each cache remained in its original location, and the location where the cache was finally consumed. polymer chain reaction (PCR) analysis of hair samples obtained from 14 of the subjects was used to determine relatedness among this group of squirrels, and its potential impact on behavior. Results suggest that cache protection behaviors and the lifespan of a cache are dependent on the conspicuousness of a cache. Squirrels may mitigate some of the costs of pilfering by caching closer to the caches of related squirrels than to those of non-related squirrels. In Chapter 5, I describe a model of the antagonistic relationship between food storing animals and their competitors using agent-based simulations where caching, memory size, and pilfering co-evolve. During periods of food abundance and scarcity, individuals could consume or store found items, retrieve old caches, or pilfer othersâ€™ caches. In the absence of pilfering, selection is strongest for longer memory. As pilfering increases, shorter memory may be more adaptive, because old caches are likely to be depleted. Contrary to common thought that social interactions enhance cognition, these findings demonstrate how competition may constrain rather than promote some cognitive abilities.Finally, in Chapter 6, I argue that my research demonstrates that food assessment and cache investment strategies of fox squirrels represent a complex suite of behaviors. These behaviors allow squirrels to maximize the benefits of periods of excess food in the environment, while increasing the likelihood of retrieving nuts later, when food in the environment is scarce. Competition via pilfering influences these food-storing decisions and outcomes, and in some cases, may impair the cognitive abilities of food-storing animals. I discuss the overall implications of this work, and potential directions for future research.",ucb,,https://escholarship.org/uc/item/1d23x2mk,,,eng,REGULAR,0,0
678,2114,Linear and Nonlinear Electromagnetic Responses in Topological Semimetals,"Zhong, Shudan","Moore, Joel E;",2018,"The topological consequences of time reversal symmetry breaking in two dimensional electronic systems have been a focus of interest since the discovery of the quantum Hall effects. Similarly interesting phenomena arise from breaking inversion symmetry in three dimensional systems. For example, in Dirac and Weyl semimetals the inversion symmetry breaking allows for non-trivial topological states that contain symmetry-protected pairs of chiral gapless fermions. This thesis presents our work on the linear and nonlinear electromagnetic responses in topological semimetals using both a semiclassical Boltzmann equation approach and a full quantum mechanical approach. In the linear response, we find a ``gyrotropic magnetic effect"" (GME) where the current density $j^B$ in a clean metal is induced by a slowly-varying magnetic field. It is shown that the experimental implications and microscopic origin of GME are both very different from the chiral magnetic effect (CME). We develop a systematic way to study general nonlinear electromagnetic responses in the low-frequency limit using a Floquet approach and we use it to study the circular photogalvanic effect (CPGE) and second-harmonic generation (SHG). Moreover, we derive a semiclassical formula for magnetoresistance in the weak field regime, which includes both the Berry curvature and the orbital magnetic moment. Our semiclassical result may explain the recent experimental observations on topological semimetals. In the end, we present our work on the Hall conductivity of insulators in a static inhomogeneous electric field and we discuss its relation to Hall viscosity.",ucb,,https://escholarship.org/uc/item/1dj0d749,,,eng,REGULAR,0,0
679,2115,"Forbidden Fruit: Contested Policy Change, Organizational Resources, and the Teaching of Evolution in Public Schools","Gonzales, Angelo James","Weir, Margaret;",2011,"For over a century, American religious organizations have waged a battle against scientists and their allies over the idea of human evolution. What began as a dispute about the scientific theory of evolution by natural selection has, over time, developed into a long-running policy conflict over the teaching of evolution and creationism in public schools. At the heart of the matter is a puzzle: Despite a nationwide shift in the policy status quo favoring evolution and two U.S. Supreme Court decisions that placed creationists at a severe institutional and political disadvantage relative to their opponents, what accounts for the ability of creationists to keep the dispute alive and to continue to score policy victories; and conversely, why have scientists and their allies failed to end the conflict? This outcome, called ""contested policy change,"" raises big questions about policy sustainability and the relationship of political and non-political actors to the policy process. Specifically, how can a new policy grow stronger over time, while the winners who advocated for the policy change get weaker, and the losers actually manage to get stronger? To answer these questions, we must first reconceptualize the conflict in two dimensions. The first dimension is the policy conflict between pro- and anti-evolution organizations. At stake is the question of whether evolution or creationism (in its various forms) should be taught in public schools. The second, often overlooked, dimension is the ""ideational"" conflict between religious authorities and scientists. Motivating this dispute is the question of how human life began. Both conflicts are being waged by individuals and organizations--political and non-political--which occupy two distinct organizational fields. In this dissertation, I argue that perpetuation of the policy conflict can only be explained in relation to the battle of ideas. Specifically, creationists were successful because they engaged in the practice of ""field bridging,"" drawing resources from the organizational field associated with the battle of ideas (i.e., ""the ideational field"") to sustain and advance their policy agenda. Field bridging is a general mechanism of policy change, which can be found in any policy conflict in which non-political actors are major participants. There are three general mechanisms by which field bridging can advance an organization's policy goals. First, organizations can secure needed material resources from their organizational fields to stay alive and press their policy demands. Second, organizations can supply new ideas to actors in the policy field. Third, organizations can recruit external support from the organizational fields in which they are embedded. In the case at hand, creationists employed all three mechanisms in the wake of their 1960s-era policy defeats. By reframing their policy demands under the banner of ""creation science,"" securing new material resources, and recruiting ""creation scientists"" and conservative Protestants to the cause, creationist policy activists were able to garner the attention of numerous policy makers during the 1970s and 1980s, while securing a few high-profile victories in several states.On the other hand, scientists let down their guard after the Supreme Court ruled in their favor in 1968. Although some scientists recognized the growing threat posed by the reinvigorated creationism movement, it would take an entire decade for scientists to begin to organize themselves at the state and local level to challenge creationists in the policy field. Although they eventually found their organizational footing, scientists' most decisive policy victories only came about because of their alliance with church-state separationist organizations, such as the American Civil Liberties Union and Americans United for Separation of Church and State. Despite a second decisive Supreme Court victory in 1987, scientists continue to find themselves fighting a seemingly neverending policy conflict against the organizations of the creationism movement, now operating under the banner of ""intelligent design."" Until one side or the other is able to conquer the battle of ideas, policy strife is likely to persist.",ucb,,https://escholarship.org/uc/item/1dp7h36t,,,eng,REGULAR,0,0
680,2116,"Promote Chemistry Learning with Dynamic Visualizations: Generation, Selection, and Critique","Zhang, Zhihui","Linn, Marcia C.;",2011,"Dynamic visualizations can strengthen chemistry instruction by illustrating atomic level phenomena. Visualizations can help students add ideas about unseen phenomena involving atomic particles. They allow students to interact with phenomena that cannot be investigated in hands-on laboratories. Connecting dynamic, atomic representations with associated observable phenomena and symbolic representations has the potential to increase the coherence and comprehensiveness of student understanding. Consistent with the knowledge integration framework, students enter chemistry courses with multiple non-normative ideas about chemical reactions. Following the knowledge integration framework, an inquiry project entitled Hydrogen Fuel Cell Cars engaged students in making predictions, exploring new ideas, distinguishing among ideas, and reflecting while connecting atomic interactions to everyday issues such as cars and fuel. To add normative ideas, a dynamic visualization that shows bond breaking and formation during hydrogen combustion was embedded in the unit. Using an iterative design process, a series of studies compared four approaches to distinguishing ideas: unguided exploration of the visualization, generating drawings of the sequence of events in the visualization, critiquing sequences of drawings attributed to a peer, and selecting among alternatives sequences. Progress was assessed using assessments that required students to articulate coherent accounts of chemical reactions.The dissertation describes the design and development of the instruction, reports on a series of comparison studies conducted in typical middle schools that compare student performance in each of the four conditions: exploration, drawing, critique, and selection. The results reveal that visualizations can be deceptively clear so students may ignore important details when exploring a visualization. When learners generate, select, or critique drawings of atomic interactions, they recognize gaps in their knowledge, develop criteria for distinguishing among ideas, and increase in ability to select normative ideas. The dissertation demonstrates the importance of encouraging students to distinguish ideas when learning with visualizations. It suggests design principles for creating instructions featuring visualizations that can succeed in typical classrooms.",ucb,,https://escholarship.org/uc/item/1fh429d3,,,eng,REGULAR,0,0
681,2117,Fatty acid transport proteins mediate fatty acid uptake in vivo and play a role in the development of metabolic diseases,"Dietz, Brittney Nicole","Stahl, Andreas;",2016,"The prevalence of obesity and diet-related diseases is widespread and increasing.  Fatty acids have vital biological functions in the body, but can become pathophysiological in many contexts.  It is well established that fatty acid uptake into cells is a regulated process mediated by membrane-bound fatty acid transporters.  Gain- and loss-of-function studies of fatty acid transport proteins (FATPs) have demonstrated the essential role of fatty acid transporters in maintaining normal fatty acid metabolism and have also indicated that these proteins are involved in the development of diet-related diseases.  We do not fully understand the physiological roles of each FATP in every metabolic context.  The work presented here focuses on FATP1 and FATP6 activity in the heart and endocrine pancreas, particularly in the context of diabetes.  In Chapter 1, the creation of a FATP6 knockout mouse model that we utilized to study FATP6 activity and function is reported.  We found that FATP6 mediated fatty acid uptake in vivo and was specifically responsible for fatty acid availability in the heart.  Deletion of FATP6 resulted in reduced cardiac lipid levels, cardiac dilation, reduced systolic function, and elevated rates of apoptosis in cardiomyocytes.  This phenotype was rescued by high-fat diet feeding.  While the location and mechanism of FATP6 activity have not been fully defined, we hypothesize that lack of FATP6 expression leads to reduced cardiac fatty acid utilization, which in turn leads to reduced cardiac function.  In Chapter 2, our efforts to determine the role of FATP1 and FATP6 in the development of diabetic cardiomyopathy are described.  Diabetic cardiomyopathy is a heart condition characterized by enhanced fatty acid utilization.  We induced diabetes in wild-type and FATP1 and FATP6 knockout mice by feeding them a high-fat diet and injecting them with two low-doses of streptozotocin.  This protocol produced hyperglycemia but did not result in a robust model of diabetic cardiomyopathy.  Due to the lack of a strong phenotype in the heart, we did not detect significant differences in cardiac metabolism or function with deletion of FATP1 or FATP6. In Chapter 3, a novel role for FATPs in the endocrine pancreas is presented.  FATPs are differentially expressed in pancreatic islets, with FATP1 localizing to beta cells.  We used islets from FATP1 knockout mice to analyze how FATP1 expression affected the susceptibility of beta cells to lipotoxicity.  Deletion of FATP1 protected beta cells from palmitate-induced apoptosis, despite normal levels of fatty acid uptake and palmitate-induced ER stress in FATP1 knockout islets.  Although the mechanism explaining this outcome is not clear, beta cells from FATP1 knockout mice may differentially store neutral lipid and this may make the cells less susceptible to palmitate-induced cell death.In Chapter 4, our attempts to characterize small molecule inhibitors of FATPs are portrayed.  We tested the ability of two classes of potential FATP inhibitors to reduce FATP activity in FATP-overexpressing cells and in cells that endogenously express FATPs.  Phospholipid-based inhibitors did not reduce FATP-mediated fatty acid uptake at feasible concentrations.  A dihydropyrimidone-based inhibitor effectively reduced FATP1- and FATP4-mediated fatty acid uptake in cell models overexpressing these proteins but did not affect endogenous FATP1 or FATP4 activity in other cell models.  Taken together, these results contribute to the growing body of evidence that establishes a role for FATPs in fatty acid transport in the body as well as in the development of diet-related diseases.  Specifically, this work shows novel roles for FATP6 in cardiac metabolism and function and FATP1 in pancreatic beta cell metabolism and function.  Clearly, FATPs are important regulators of metabolism, are implicated in metabolic disorders, and should be extensively studied as potential therapeutic targets.",ucb,,https://escholarship.org/uc/item/1fr2g0fq,,,eng,REGULAR,0,0
682,2118,"Spherical aberration, accommodation and myopia","Tarrant, Janice","Wildsoet, Christine F;",2010,"Myopia is a condition in which the eye grows too long to match its optical focal length and represents a failure in both structure and function. Because of the potential risks of vision loss associated with high myopia, and also with corrective treatments such as refractive surgery and occasionally also with contact lenses, myopia should not be considered a benign condition. The hypothesis that hyperopic defocus from under-accommodation during near work is the driving stimulus in the progression of myopia, motivated studies of bifocal spectacles and progressive addition lenses, as optical treatments for the control of myopia progression, with limited success. In contrast, multifocal (MF) soft contact lenses (SCL) and orthokeratology (ortho-k) have proven to be more beneficial although the mechanisms underlying their anti-myopia effects are not understood. This dissertation, which describes 4 main studies, represents efforts to understand how spherical aberration influences the accommodative response and examines as an explanation for the myopia control effects of MF SCLs and ortho-k the possibility that induced changes in ocular spherical aberration decreases the lag of accommodation.First the effects of bifocal (BF) SCLs on the accommodative responses of young adult emmetropes and myopes were measured using a refractometer. Interpretation of these results proved to be problematic because direct measurements could not be made through the BF SCLs necessitating an assumption to be made about the effective add provided by the lenses.To address the above issues, in a follow-up study MF SCLs were used in conjunction with a wavefront sensor, allowing direct measurements of accommodative responses through the lenses. To analyze the collected data, the problem of determining a suitable method for calculating accommodative responses from wavefront aberrations had to be solved. Thus a second complementary study evaluated some of the methods used to calculate objective refractions from wavefront aberrations. The best results were obtained with a through-focus procedure, which used an optical quality metric to determine the best image plane and then calculated the accommodative error relative to this plane. The latter findings enabled a comprehensive analysis of the accommodative response data obtained in the MF SCL study, which demonstrated that spherical aberration and pupil diameter independently influence the accommodative response. Both center-distance and center-near MF lenses produced myopic shifts in the best image plane, the former by adding positive spherical aberration and the latter with the added power of the near addition. For pupils larger than approximately 5 mm, both MF lenses resulted in increased accommodative responses determined by a neural sharpness metric compared with those for a single vision distance lens.A fourth study measured the change in ocular aberrations induced by ortho-k and assessed the long term effect of ortho-k on the accommodative response of young adult myopes. This study found that ortho-k had similar effects to the center-distance MF SCL on aberrations and accommodative responses. An intriguing long-term outcome of this treatment was a dramatic increase in pupil size for all tested vergences. Explanations in terms of changes in both the pupillary light and near reflexes were considered. In summary, the studies reported in this dissertation point to complex interactions between spherical aberration, pupil size and the accommodative response, which may be deliberately manipulated in designing novel optical treatments for the control of myopia progression.",ucb,,https://escholarship.org/uc/item/1g0559kd,,,eng,REGULAR,0,0
683,2119,Signaling and Morphogenesis of the Ascidian Neural Tube,"Navarrete, Ignacio Andres","Levine, Michael S;",2016,"Formation of the vertebrate neural tube represents one of the premier examples of morphogenesis in animal development. Here, we investigate this process in the simple chordate, Ciona intestinalis. Previous studies have implicated Nodal and FGF signals in the specification of lateral and ventral neural progenitors. We show that these signals also control the detailed cellular behaviors underlying morphogenesis of the neural tube. Live imaging experiments show that FGF controls the intercalary movements of ventral neural progenitors, while Nodal is essential for the characteristic stacking behavior of lateral cells. Ectopic activation of FGF signaling is sufficient to induce intercalary behaviors in cells that have not received Nodal. In the absence of FGF and Nodal, neural progenitors exhibit a default behavior of sequential cell divisions, and fail to undergo the intercalary and stacking behaviors essential for normal morphogenesis. Thus, cell specification events occurring prior to completion of gastrulation coordinate morphogenetic movements underlying the organization of the neural tube.",ucb,,https://escholarship.org/uc/item/0r27f2p8,,,eng,REGULAR,0,0
684,2120,Developing a Curriculum for a Formal Education Setting to Prevent Child Marriage in Rural Areas of Honduras: A Design-Based Research Study,"pacheco, Diana","Murphy-Graham, rin;",2020,"Latin America and the Caribbean is the only region in the world where child marriage (CM) is not decreasing. Growing levels of education and legislation to ban this practice have not been associated with CM reduction. This dissertation is grounded on the belief that the educational sector can contribute to CM prevention by going beyond expanding access to secondary education. Schools can also contribute to CM prevention by focusing on challenging social norms, understanding the role of the different changes that occur during adolescence, and recognizing the structural limitations that drive CM. This design-based research study provides a deeper understanding of the factors that shape adolescentsâ€™ decisions to marry in rural areas of Honduras and uses this knowledge to develop a theory of action to design, implement, and evaluate a school-based curriculum to prevent CM. The results and experience of this investigation advanced the development of design principles that can guide the implementation of interventions in similar contexts throughout Latin America. Keywords: child marriage, Latin America, design-based research",ucb,,https://escholarship.org/uc/item/0r80n3zw,,,eng,REGULAR,0,0
685,2121,Investigating Silicon-Based Photoresists with Coherent Anti-Stokes Raman Scattering and X-ray Micro-spectroscopy,"Caster, Allison G.","Leone, Stephen R;",2010,"Photoresist lithography is a critical step in producing components for high-density data storage and high-speed information processing, as well as in the fabrication of many novel micro and nanoscale devices.  With potential applications in next generation nanolithography, the chemistry of a high resolution photoresist material, hydrogen silsesquioxane (HSQ), is studied with two different state-of-the-art, chemically selective microscope systems.  Broadband coherent anti-Stokes Raman scattering (CARS) micro-spectroscopy and scanning transmission XÂ­ray microscopy (STXM) reveal the rate of the photoinduced HSQ cross-linking, providing insight into the reaction order, possible mechanisms and species involved in the reactions.Near infrared (NIR) multiphoton absorption polymerization (MAP) is a relatively new technique for producing sub-diffraction limited structures in photoresists, and in this work it is utilized in HSQ for the first time.  By monitoring changes in the characteristic Raman active modes over time with ~500 ms time resolution, broadband CARS micro-spectroscopy provides real-time, in situ measurements of the reaction rate as the HSQ thin films transform to a glass-like network (cross-linked) structure under the focused, pulsed NIR irradiation.  The effect of laser power and temporal dispersion (chirp) on the cross-linking rate are studied in detail, revealing that the process is highly non-linear in the peak power of the laser pulses, requiring ~6 photons (on average) to induce each cross-linking event at high laser power, which opens the possibility for high resolution MAP lithography of HSQ.  Reducing the peak power of the laser pulses, by reducing average laser power or increasing the chirp, allows fine control of the HSQ cross-linking rate and effective halting of the cross-linking reaction when desired, such that broadband CARS spectra can also be obtained without altering the material. Direct-write XÂ­ray lithography of HSQ and subsequent high resolution STXM imaging of line patterns reveals a dose and thickness dependent spread in the cross-linking reaction of greater than 70 nm from the exposed regions for 300 nm to 500 nm thick HSQ films.  This spread leads to proximity effects such as area dependent exposure sensitivity.  Possible mechanisms responsible for the reaction spread are presented in the context of previously reported results.  XÂ­ray lithography and imaging is also used to assess the XÂ­ray induced cross-linking rate, and similarities are observed between NIR MAP and XÂ­ray induced network formation of HSQ.",ucb,,https://escholarship.org/uc/item/0t083608,,,eng,REGULAR,0,0
686,2122,Spontaneous Activity and Intrinsic Photosensitivity in the Developing Zebrafish Spinal Cord,"Friedmann, Drew Robert","Isacoff, Ehud Y;",2016,"The process of perception is one of the most complicated and compelling biological phenomena, capable of inspiring thousands of years of philosophers, physicians, and scientists. One of these researchers, Walter Freeman of Berkeley once stated, â€œThe brain reaches out into the environment and sees something which is then interpreted according to its own past experiences. First you look, then you see.â€ While the neural computations involved in â€œseeingâ€ may not be understood for many years to come, we have made much progress in understanding how the brain â€œlooksâ€ into its environment. And by studying animal behavior, we can hope to infer some understanding of the cells that transform these sensory inputs into motor outputs. Working with simple neural circuitsâ€”be they in model organisms or at early stages in developmentâ€”the problem seems more tractable, yet new findings can shift our understanding of perception in unexpected ways.	In this thesis, I present research on motor circuits of the embryonic zebrafish spinal cord. These spinal neurons directly drive the earliest muscle contractions in the fish and are a great model for understanding how activity begins in a nervous system. In the course of these studies, we discovered that the activity within this circuit is strongly inhibited by environmental light at an age before vision and before the spinal cord is connected to brain circuitry. Not all photoreceptors are for sight and there are many examples of deep brain photoreception in invertebrates and basal vertebrates, usually driving circadian and seasonal behaviors. Our finding in zebrafish is surprising due to the direct photodetection by motor neurons in the spinal cord, the developmentally early appearance of this photosensitivity, the possible role for primary cilia in sensing light, and the acute affect on behavior. Additionally, by manipulating spontaneous activity within this circuit, we see effects on the development of neural activity in spinal interneurons. These results change how we think about motor circuits and development. No longer are motor neurons simply passive relay cells, we now can see them as sensory inputs. No longer is development in the spinal cord governed solely by genetic programs, but activity dependent processes can be regulated by the outside world. The existence of this category of nonvisual photoreceptor across taxa indicates a new way for the brain to â€œreach out into the environment.â€ Discovering whether and how it alters our perception of the world will hopefully be a focus of future research.",ucb,,https://escholarship.org/uc/item/0vg5991c,,,eng,REGULAR,0,0
687,2123,Essays in Environmental & Resource Economics,"Biardeau, LÃ©opold Temoana Marc","Auffhammer, Maximilian;Hsiang, Solomon;",2020,"At its core, environmental & resource economics seeks to identify and correct market failures, i.e. situations in which markets fail to allocate resources in a way that maximizes societyâ€™s economic welfare. Some of the greatest issues of our time such as water and air pollution, the over-exploitation of natural resources, or climate change all result from market failures. While market failures can take many forms, they are often associated with time-inconsistent preferences, negative externalities, or the difficulty to provide and manage common goods. In this dissertation, I investigate three key issues relative to each of these types of market failure.In the first chapter, coauthored with Solomon Hsiang and SÃ©bastien Annan-Phan, we propose that the probability that individuals focus attention on a moment a fixed temporal distance from their present moment is stable. We call Kernel of Attention to Time (KAT) the resulting probability distribution across moments in relative time, which directs human attention across the past, present, and future. We then analyze how populations across the world query Google Search for information related to specific moments in time and provide the first evidence of a coherent KAT for most humans on Earth. We discover consistent structure to the distribution of attention across time, regardless of populationsâ€™ language or country, with the present strongly dominating all other moments and capturing roughly 25% of time-related attention on average. Attention to the past and future decays rapidly with increasing temporal distance, much faster than exponentially. Despite consistency in the form of the KAT around the world, we find regional patterns in attention to the past, present, and future. Furthermore, it appears that over the last decade, attention to the present has been increasing at the expense of attention to the past. Together, these findings suggest that human populations exhibit strong common patterns of thought with respect to time, but some non-biological factors that vary across space and over time can alter these patterns.While the KAT does not capture time-related economic tradeoffs directly (e.g. foregoing present consumption to increase future consumption), the structure of its future-oriented portion could enable better understanding of the origin of time-based preferences and provide new insights on time-inconsistent behavior.In the second chapter, coauthored with Lucas Davis, Paul Gertler and Catherine Wolfram, we develop new measures of global air conditioning potential using temperature data from more than 14,000 monitoring stations around the world. We combine this information with disaggregated global population estimates to calculate cooling degree days (CDDs) and other measures of air conditioning potential by region, country, and city. Overall, the evidence points to enormous potential growth in air conditioning, particularly in low-income and middle-income countries. India, China, Indonesia, Nigeria, Pakistan, Brazil, Bangladesh, and the Philippines all have greater air conditioning potential than the United States, a country where a staggering 400 terawatt hours of electricity are currently used annually for air conditioning. We find, moreover, that a significant portion of total global air conditioning potential comes from the earthâ€™s largest cities. Mumbai, for example, has by itself the air conditioning potential of 25% of the entire United States. Our estimates imply that, were global air conditioning usage to reach U.S. levels, total electricity consumption worldwide for air conditioning could reach 20,000 terawatt hours annually, which roughly corresponds to the current net global electricity consumption. If unmitigated by improvements in air conditioner energy-efficiency or updated power network infrastructures, this rise in overall electricity demand could generate two negative externalities. First, it could lead to blackouts around the globe, especially in low and middle income countries. What is more, most electricity worldwide continues to rely on fossil fuels. Consequently, growing air conditioner adoption could lead to hundreds of millions of tons of increased carbon dioxide emissions, further aggravating climate change.In the third and last chapter, coauthored with David Zilberman, we rely on satellite-based data tracking vessel fishing hours to investigate the extent to which Very Large Marine Protected Areas (VLMPAs), â€“ Marine Protected Areas spanning at least 100,000 sq. km , â€“ prohibiting all types of fishing have been successful at deterring fishing effort. These VLMPAs have been created in an attempt to protect and replenish fish stocks, 34.5% of which have fallen below biologically sustainable levels, partly as a result of overfishing. Indeed, most fisheries have traditionally been open access resources, leading individually-acting fishers to collectively extract more than the efficient level and threatening the viability of the resource over time, a situation known as the tragedy of the commons.In spite of their large size which may constitute a challenge for enforcement, we find that VLMPAs have on average been able to deter fishing effort. However, a case-by-case analysis reveals varying levels of success, with the most successful VLMPA managed by the Republic of Kiribati and the worst performing one managed by the United States.To better understand the nature of illegal fishing effort in these VLMPAs, we focus on the characteristics of the vessels infringing the fishing bans in these VLMPAs and find that most of the infractions can be traced back to a few industrialized countries.",ucb,,https://escholarship.org/uc/item/0wv5w333,,,eng,REGULAR,0,0
688,2124,Phonetic and Cognitive Bases of Sound Change,"Kataoka, Reiko","Ohala, John J.;Johnson, Keith;",2011,"In this dissertation I investigated, by using coarticulatory /u/-fronting in the alveolar context for a case study, how native speakers of American English produce coarticulatory variations and how they perceive and reproduce continuously varying speech sounds that are heard in coarticulatory and non-coarticulatory contexts.  The production study addressed the question of whether in American English coarticulatory fronting of /u/ in alveolar contexts is an inevitable consequence of production constraints or if it is produced by active speaker control. The study found that: (1) the relative acoustic difference between the fronted /u/ and the non-fronted /u/ remained across an elicited range of vowel duration; and (2) the degree of acoustic variability was less for the fronted /u/ than the non-fronted /u/.  These results indicate that speakers of American English have a distinct and more narrowly specified articulatory target for the fronted /u/ in the alveolar context than for the non-fronted /u/.  The perception study addressed the issue of individual variation and compensation for coarticulation.  The study found within-subject consistency in classification of /CVC/ stimuli both in compensatory and non-compensatory contexts.  The study found no evidence for a within-subject perception-production link, but did find positive evidence for the relationship between linguistic experience and speech perception--the similarity between the distributional characteristics of the fronted and the non-fronted variants of /u/ in production data (a proxy for ambient language data) and the ranges of variation in perceptual responses toward /CVC/ stimuli in the fronting and the non-fronting contexts.  Together, these results suggest that the source of individual variation in speech perception is the differences in the phonological grammar (perceptual category boundary) that guide speech perception, and that this perception grammar emerges in response to the ambient language data.  Finally, the vowel repetition study examined how perceptual compensation for coarticulation and individual differences in speech perception affect vowel repetition performance.  This study found that: (1) ambiguous vowels were repeated with a significantly lower F2 when the vowels were heard in the fronting context than in the non-fronting context; (2) a given stimulus was repeated by some listeners un-ambiguously as the vowel belonging to the speaker's /i/ category for all trials, yet the same stimulus was repeated by other listeners un-ambiguously as vowels belonging to that speaker's /u/ category for all trials; and (3) the perceptual category boundary was a significant predictor for the repeated vowel's F2 value.  Based on these results, it was hypothesized that one source of pronunciation variation in a given community is individual variation in speech perception that contributes variable mental representations across listeners when they encounter ambiguous speech.One general pattern that was found in all experiments was vowel-specific variability: responses to /i/ were less variable than responses to /u/ in a production task, and /i/-like stimuli were repeated less variably than /u/-like stimuli in a vowel repetition task.  Similarly, between /u/ in fronting and non-fronting contexts, /u/ elicited less variability in the fronting context than in the non-fronting context consistently in the production, perception, and vowel repetition tasks.  More broadly, I contend that speech forms a dynamic system, characterized by mutual dependency and multiple causal loops between and among speech perception, speech production, knowledge about pronunciation norm, and ambient language data.  These properties in language use govern the output of communicative interactions among members in a speech community, and one such output is member's knowledge of multiple sub-phonemic pronunciation categories that exist in any speech community.  Additionally, I argue that any speech community is in a constant state of readiness to respond to an innovative pronunciation as a new community norm, because members have a variable but rich pronunciation repertoire even when there is no observable community-level sound change.",ucb,,https://escholarship.org/uc/item/0j39f7vc,,,eng,REGULAR,0,0
689,2125,Tropical Rainforest Food Webs in the Anthropocene,"Luskin, Matthew Scott","Potts, Matthew D;",2016,"Tropical forests throughout the world are rapidly being converted to agriculture. Remaining forests are often fragmented, threatening area-demanding species, such as apex predators and mega-herbivores (e.g. elephants). The loss of predators can trigger trophic cascades, whereby prey species increase in abundance, altering food web dynamics. Fragmentation also increases huntersâ€™ and poachersâ€™ access to previously remote areas, adding an additional threat to megafauna. A pressing challenge in conservation biology is to understand where, why, and over what time scales these unintended secondary processes are degrading remaining forests. My dissertation seeks to address this challenge by exploring how forest loss, predator loss, hunting, and rapid oil palm agricultural expansion are affecting tropical forest floral and faunal communities in Southeast Asia.My first chapter, an introduction, discusses the process and theories on how land use change affects species and drives ecological cascades. In my second chapter, I grapple with the approaches used to study the impacts of agricultural expansion on biodiversity, and how different methods can shape outcomes for conservation planning. In this chapter, I explore the land sparing versus land sharing framework for conservation planning. Land sparing advocates meeting production targets through increasing yields on existing farmland (intensification) in order to protect natural areas from further conversion (set asides).  Land sharing promotes conservation within agricultural landscapes using wildlife-friendly farming practices. Using a literature review, I argue that there is an emerging consensus of ecological theory, empirical data, and direct case studies supporting the â€œland sparingâ€ approach to conserve biodiversity in tropical forested landscapes. Studies of land sharing landscapes routinely report the complete absence of over 50% of forest species, even in wildlife-friendly agroforestry systems. This indicates most species are sensitive to any farming activities, and so conservation should focus on minimizing forest loss. I conclude by exploring three important considerations to effectively implement a land sparing strategy: (i) the tradeoffs and synergies with other ecosystem services (e.g. carbon, water quality); (ii) the economic benefits of forest offsets (e.g. from REDD+ carbon payments) and; (iii) the sociopolitical obstacles to insuring effective forest offsets are created. In the third chapter, I turn to the abiotic impacts of agricultural expansion, comparing and contrasting the microclimate of agricultural lands and forest. Specifically, I examine how abiotic conditions in oil palm plantations vary throughout the life cycle of a plantation. I present a chronosequence study on microclimate and vegetation structure in protected forest and its surrounding oil palm plantations surrounding in Peninsular Malaysia. My results indicate that understory vegetation is twice as tall in young plantations, but leaf litter depth and total epiphyte abundance is twice as high in old plantations. Plantations are also substantially hotter (+2.84 C) and drier (+0.80hPa vapor pressure deficit) than forests during the day, but there are no nocturnal microclimate differences between the two. These findings are important to revealing the habitat heterogeneity throughout the 25-year plantation lifecycle. Based on these results, I develop environmental guidelines to improve the spatio-temporal planning of oil palm plantations for wildlife, drawing heavily on timber management practices. I conclude that oil palm plantations can be developed to create a permeable matrix in order to connect remaining forests habitats and increase the conservation value of the landscape. In Chapter 4, I explore how agricultural expansion is affecting faunal communities, focusing on predators. Accurately monitoring predator populations is an essential but difficult challenge for conservation biologists. Until very recently, many of the methods employed by conservationists overestimated population densities and sizes. This is particularly true for apex predators like tigers, which are important to maintaining ecosystem function and are especially sensitive to habitat loss and poaching. To accurately and defensibly estimate current Sumatran tiger populations, in this chapter, I present new data collected from camera trap arrays across three expansive landscapes (843-999km2), which together make up the UNESCO Tropical Rainforest World Heritage Sites of Sumatra. I use these data to estimate tiger densities using the spatial-capture-recapture approach. Then, to compare my results with other studies, I develop a new approach to correct for biases in previous research and conduct a meta-analyses to draw inferences about the determinants of densities. I find that traditional mark-recapture techniques published before 2010 inflated estimates of tiger densities by 63.3% on average. Controlling for this bias, tiger densities increased 4.2%/yr from 1999 to 2014 and primary forest densities were 58% higher than disturbed forests. Based on my study, I estimate there are 734 Â± 304 tigers within or connected to source landscapes. These results highlight that Sumatran tiger densities in remaining forest may be increasing. However, forest loss, fragmentation, and poaching have reduced the total population and threaten the subspecies with extinction. In Chapter 5, I delve deeper into the impacts of agricultural expansion by focusing on changes in faunal and floral community dynamics. There has been significant debate over whether herbivores are regulated from the â€œtop-downâ€ by predators or from the â€œbottom upâ€ as plants adapt and limit herbivoresâ€™ consumption. In altered ecosystems, such as forests fragmented by agriculture expansion, the widespread loss of predators is thought to be the main destabilizing force. I challenge this status quo by examining the effect of bottom-up agricultural resource subsidies (palm oil fruits) in controlling forest faunal populations as well as the effect on tree communities. To determine these impacts, my fifth chapter takes advantage of a landscape-scale manipulation of agricultural resources adjacent to a 2500 ha forest reserve in Peninsular Malaysia. First, I evaluate whether resource subsidies affect forest wildlife populations by compiling two decades of wild boar (Sus scrofa, a crop-raiding species) abundance data at the Pasoh Research Forest in Peninsular Malaysia. Second, I evaluate if altered wild boar abundance had cascading impacts on the vegetation community by re-censusing seven wildlife exclosures established in 1996. Finally, I evaluate how wild boar predation shaped tree sapling diversity using 24 years of tree census data from the Smithsonian Instituteâ€™s Center for Tropical Forest Science 50-ha research plot at the Pasoh. I find that the forest density of wild boar was 100 times higher when palm oil plantations were fruiting  (1996-2000 and 2007-2014) than when plantations were cleared and fruit was unavailable  (2001-2006). Second, wild boarsâ€™ predation of tree saplings (100-200 cm in height) led to a to 46% reduction in the density of 1-2cm dbh saplings over a 23-year period. Third, while all species of tree saplings had lower abundances, there was a 13% increase in tree diversity as measured by the Fisherâ€™s alpha diversity index. This suggests wild boars exhibit density dependent selective mortality on trees that disproportionately reduces the abundance of common tree species. My final chapter turns to the role of humans in these new forest-plantation landscapes. In this chapter, I examine how the immigration of farmers alters hunting practices. Using information from in-depth interviews with hunters, agricultural workers, and wild meat dealers in Jambi province, Sumatra, I first describe how plantations have affected local human demography. Then, I explore how wildlife hunting and consumption rates vary between different indigenous and immigrant ethnic groups. I also uncover how hunting near palm oil plantations has become primarily a commercial endeavor for managing crop-raiding wild boars. These results also indicate that wild boars may be experience ecological release, either from the loss of predation by tigers, or increased food available in agricultural fields. I discuss how proper management could reduce crop damage and also yield large amounts of wild boar meat with relatively little by-catch of threatened wildlife. Taken together, these studies indicate that oil palm habitat is unsuitable for most forest species, and that forest fragments in oil palm landscapes are undergoing a loss of predators and thus potentially suffer from trophic cascades and subsidy cascades. I found that cross-border agricultural resource subsidies shape wildlife communities and devastate tree sapling communities. These results also suggest that predator loss and agricultural expansion can lead to combined trophic and subsidy cascades with heightened effects, and that this form of indirect forest degradation may be widespread in the region and globally. As such, my results indicate that protecting large continuous forests is necessary to preserve functioning Southeast Asian food webs. This conclusion has direct applicability across the tropical rainforests regions where oil palm agriculture is rapidly expanding, and more generally informs the discussion on how to achieve conservation goals.",ucb,,https://escholarship.org/uc/item/0jj4f343,,,eng,REGULAR,0,0
690,2126,Articulatory uniformity through articulatory reuse: insights from an ultrasound study of SÅ«zhÅu Chinese,"Faytak, Matthew D",,2018,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0jr0010h,,,eng,REGULAR,0,0
691,2127,"Constructing Resilience: Real Estate Investment, Sovereign Debt and Lebanonâ€™s Transnational Political Economy","Tierney, Julia","Caldeira, Teresa;",2017,"In urban studies scholarship, Lebanon is often theorized on the frontiers of sectarian conflict as well as on the frontlines of neoliberalism. Entangling real estate investment, sovereign debt and transnational financial circulations from Arab Gulf investors and the Lebanese diaspora, managed by the Banque du Liban and scrutinized by the United States Treasury, the Lebanese political economy was â€“ and still is â€“ swayed by the fortunes of war. According to literature on the political economy of violence, profits are often made in times of war, a context appropriate to the civil war and postwar eras, during which spoils of war enriched the pockets of warlords-turned-politicians. Yet as the effects of the Syrian conflict spill across the border, encumbering Lebanonâ€™s long paralyzed politics, straining its already deteriorated infrastructure and intensifying uncertainty with punctuated bombings, certain sectors prosper not because of violence but in spite of it. The skyline of Beirut is covered in construction cranes erecting affluent, if empty, apartments; the banks are infused with deposits invested in the debt of a sovereign bankrupt in ways not simply financial. Both real estate and banking are said to be resilient, a discourse so often repeated that resilience has become the dominant mode by which Lebanon is broadly understood, and not only by the developers and bankers with a financial interest in this depiction. Resilience has taken on the status of a self-evident truth. Tracing transnational investment into these two sectors, the pillars of the economy, this dissertation excavates the history around which resilience arose as a concept and thereafter endured. Rather than a description or explanation, resilience is instead â€“ this dissertation argues â€“ a dispositif organizing an entire political economy around the attraction of foreign financial inflows and ongoing emigration, ensuring the resilience for which Lebanon is renowned also perpetuates much of its underlying instability.",ucb,,https://escholarship.org/uc/item/0mm2c8d5,,,eng,REGULAR,0,0
692,2128,"Entanglement, Complexity, and Holography","Moosa, Mudassir","Bousso, Raphael;",2018,"It was found, by studying black holes, that the compatibility of the theory of gravity and the laws of quantum mechanics demands that the universe must act like a hologram. That is, all the information inside a region of the universe should be encoded on a so-called holographic screen of one-lesser dimension. This is termed the holographic principle. In a generic spacetime, such as an expanding universe, holographic screens depend on the choice of an observer, which is consistent with the notion that the observer is part of the system in cosmology. In the first part of this dissertation, we study the observer-dependence of holographic screens. This will help us understand how the fundamental description of the universe depends on the choice of the observer. Furthermore, we study the dynamics of the holographic screens and their geometry in the first part of this dissertation.An example where the holographic principle is manifest is the AdS-CFT correspondence. This correspondence connects quantum information quantities, like entanglement and complexity, to geometric quantities, like area and volume, respectively. In the second and the third part of this dissertation, we use the AdS-CFT correspondence as a tool to calculate the entanglement entropy and the computational complexity of a quenched CFT state. Studying the time evolution of the entanglement entropy following a quantum quench teaches us how a CFT state thermalizes. On the other hand, studying the time evolution of the computational complexity allows us to check the validity of various recent conjectures involving black holes and complexity.",ucb,,https://escholarship.org/uc/item/0n1489nd,,,eng,REGULAR,0,0
693,2129,The Geographic Polarization of American Politics,"Hopkins, David Allen","Schickler, Eric;",2010,"This project addresses the question of whether American elections became more geographically polarized between 1972 and 2008. It finds that variation in partisan voting increased substantially over that time at both the state and regional levels. In particular, the Northeast and Pacific Coast became more strongly Democratic after the 1980s in both presidential and congressional elections, while the South and interior West remained solidly Republican.I employ quantitative analysis of survey data to demonstrate that this trend can be largely explained by the increasing electoral salience of social and cultural issues, which divide Americans along regional lines to a greater extent than economic issues. The growing association of the national Republican Party with social conservatism has produced an electoral advantage in most of the South bolstered by an increasing edge over the opposition Democrats in aggregate party identification within ""red"" America. In more socially liberal regions of the United States, the Republican electoral position weakened substantially after the 1970s and 1980s, with Democratic identifiers becoming much less likely to defect to Republican presidential candidates in 1992 and thereafter.I argue that these trends have significant consequences for American parties and the operation of Congress. Specifically, the growth of Democratic electoral strength outside the South has greatly reduced the number of moderate Republicans in both the Senate and House of Representatives, while centrists--elected mostly from the South and rural West--continue to constitute a sizable proportion of the congressional Democratic Party. This ideological asymmetry, though not often noted by previous studies of party polarization, suggests that the congressional parties do not operate as mirror images but instead maintain distinct strategic positions, with Republican congressional leaders able to command a higher degree of ideological unity among their members than their Democratic counterparts. The challenge faced by the Obama administration in pursuing an ambitious legislative agenda in 2009-2010, including reform of the American health care system, was a visible consequence of this distinction between the congressional parties: the presence of a large moderate bloc on the Democratic side complicated efforts to enact liberal initiatives despite large nominal Democratic majorities in Congress, while the lack of a significant number of moderate Republican officeholders largely frustrated the new president's attempts to gain bipartisan support for his proposals.",ucb,,https://escholarship.org/uc/item/0n90w5bh,,,eng,REGULAR,0,0
694,2130,MEMS Resonant Strain Sensor Integration,"Myers, David Richard","Pisano, Albert P.;",2010,"Despite commercial availability since the 1950's, silicon strain sensors have not experienced the same success as other microdevices, such as accelerometers, pressure sensors, and inkjet heads.  Strain sensors measure mechanical deformation and could be used in many structural components, improving safety, controls, and manufacturing tolerances.  This thesis examines major strain sensing techniques and highlights both advantages and disadvantages of each.  MEMS resonant strain gauges are identified to have superior performance over many traditional strain gauges in terms of sensitivity, resolution, stability, and size.  To use these gauges, additional issues such as harsh environment survivability,  strain transfer, temperature stability, and encapsulation must be solved, as detailed in this thesis.Concerning harsh environment survivability, this work presents a MEMS resonant strain gauge fabricated from silicon carbide, which operates at 600Â°C, and has been tested to 64,000 G, while still resolving 0.01 microstrain in a 10 kHz bandwidth.  Specific details on how to create harsh environment testing equipment are presented. Additionally, this original work identifies a unique temperature stability method based on purposely mismatched device and substrate layers.  Full analytical equations are presented, and experimental confirmation of the scheme shows that temperature stability is improved from 23 ppm/Â°C to 3.6 ppm/Â°C. All MEMS devices are created on flat substrates, which are useful when integrating electronics, but can be difficult to use when measuring strain in structural components, especially round objects.  Furthermore, no work has been presented for gauges operating at high strain.  To address this issue, this thesis contains the first demonstration of a MEMS resonant strain gauge operating at 1000 microstrain on a static automobile halfshaft.  Details on joining the substrate to the circular halfshaft are presented, as well as how to treat the issue of strain transfer.   To protect the device, encapsulation is designed specifically to not change the strain sensitivity of  the gauge.  The encapsulation utilizes directional ion beam sputtering, which is experimentally shown to deposit spatially confined, extremely thin material through the release holes.  Typical depositions were nanometers in thickness (<0.5% of deposited material) and on the order of tens to hundreds of femtograms.",ucb,,https://escholarship.org/uc/item/11v8j97m,,,eng,REGULAR,0,0
695,2131,Development of a Silicon Semiconductor Quantum Dot Qubit with Dispersive Microwave Readout,"Henry, Edward Trowbridge","Siddiqi, Irfan;",2013,"Semiconductor quantum dots in silicon demonstrate exceptionally long spin lifetimes as qubits and are therefore promising candidates for quantum information processing. However, control and readout techniques for these devices have thus far employed low frequency electrons, in contrast to high speed temperature readout techniques used in other qubit architectures, and coupling between multiple quantum dot qubits has not been satisfactorily addressed.This dissertation presents the design and characterization of a semiconductor charge qubit based on double quantum dot in silicon with an integrated microwave resonator for control and readout. The 6 GHz resonator is designed to achieve strong coupling with the quantum dot qubit, allowing the use of circuit QED control and readout techniques which have not previously been applicable to semiconductor qubits. To achieve this coupling, this document demonstrates successful operation of a novel silicon double quantum dot design with a single active metallic layer and a coplanar stripline resonator with a bias tee for dc excitation.Experiments presented here demonstrate quantum localization and measurement of both electrons on the quantum dot and photons in the resonator. Further, it is shown that the resonator-qubit coupling in these devices is sufficient to reach the strong coupling regime of circuit QED. The details of a measurement setup capable of performing simultaneous low noise measurements of the resonator and quantum dot structure are also presented here.The ultimate aim of this research is to integrate the long coherence times observed in electron spins in silicon with the sophisticated readout architectures available in circuit QED based quantum information systems. This would allow superconducting qubits to be coupled directly to semiconductor qubits to create hybrid quantum systems with separate quantum memory and processing components.",ucb,,https://escholarship.org/uc/item/13v970kt,,,eng,REGULAR,0,0
696,2132,Three Essays in Political Economy and Public Policy,"Liao, Sanny Xiao Yang","Tadelis, Steven;",2010,"Chapter 1: In the last two decades, public agencies have started to include performance pay into their compensation structure.  Using a survey data of all law enforcement agencies in the Unites States, this chapter investigates: (1) if the adoption of performance pay by agencies affected their ability to fight crime and (2) whether agencies responded to performance pay adoption by shifting their policing strategies to game the change?  We find that despite increases in the pay gap, performance pay adoption resulted in no change in the police's ability to fight crime.  We find little evidence that adopting agencies attempted to game the incentive structure by shifting efforts away from less profitable tasks.Chapter 2: There has been much debate over whether interest groups act as ideologues or investor when they contribute to candidates.  In a seminal work, Snyder (1990) finds that economic interest groups behave as investors and there is a one to one correlation between candidates' share of contribution from economic interest groups and their probability of winning.  This chapter expands on Snyder's work by proposing a new strategy to empirically identify investor interest groups, that is, by examining whether an interest group has ever given to competing parties in a race, we call this group ""diversifiers"".  We find that there is indeed a significant correlation between diversifier contribution share and election outcome.  Furthermore, the correlation between diversifiers contribution share to date and election outcome remains significant as early as approximately 48 weeks before election day.Chapter 3: Participation of interest groups in public policy making is ubiquitous and unavoidable.  In this final chapter, we try to understand the mechanisms through which interest groups attempt to influence the implementation of public policies from an Institutional Economics perspective.  We recognize that while it is legislatures that enact and supervise statures, it is often bureaucracies that implement policies.  We survey a collection of papers that analyze how the vast power invested in bureaucracies influence the strategic choice of interest group in means to exert influence - buying, lobbying and suing.  We further generalize our analysis to understand how differences in the institutional environment impact the role of interest groups in public policy making.",ucb,,https://escholarship.org/uc/item/16d642pb,,,eng,REGULAR,0,0
697,2133,"Tools, strategies, and applications of synthetic biology in Saccharomyces cerevisiae","Lee, Michael Eun-Suk","Dueber, John E;",2015,"Synthetic biology is founded on the idea that cells are living machines that execute genetically encoded programs, and that we as engineers can reprogram them to perform new functions. Unlike man-made machines that are designed from the ground up, cells have been shaped and molded by evolution, and this makes them much more difficult to engineer. As synthetic biology has grown and matured, the field has shifted from focusing primarily on simpler bacterial chassis to engineering more complex and more powerful eukaryotic hosts, especially the budding yeast, Saccharomyces cerevisiae. Here we present technologies and strategies for efficiently engineering yeast, with an emphasis on metabolic engineering. First, we describe a practical framework for designing and constructing DNA for expression in yeast. This framework standardizes--and as a consequence, accelerates--the process of building new strains, enabling more rapid iteration and experimentation. We then develop a strategy for optimizing metabolic pathways by assembling combinatorial libraries that simultaneously titrate the expression of many genes. We identify strains with improved pathway flux for violacein biosynthesis and xylose utilization using mathematical modeling and selection, respectively. Finally, we attempt to alter the specificity of a native hexose transporter to exclusively import xylose without inhibition by glucose. In summary, this work is a combination of developing fundamental tools for engineering yeast and the application of those tools for lignocellulosic biomass fermentation.",ucb,,https://escholarship.org/uc/item/19v186m8,,,eng,REGULAR,0,0
698,2134,Explainable and Advisable Learning for Self-driving Vehicles,"Kim, Jinkyu","Canny, John;",2019,"Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-to-interpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers, etc., can understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. Our work has focused on the challenge of generating introspective explanations of deep models for self-driving vehicles. In Chapter 3, we begin by exploring the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). In the first stage, we use a visual attention model to train a convolution network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. In Chapter 4, we add an attention-based video-to-text model to produce textual explanations of model actions, e.g. ""the car slows down because the road is wet"". The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. These explainable systems represent an externalization of tacit knowledge. The network's opaque reasoning is simplified to a situation-specific dependence on a visible object in the image. This makes them brittle and potentially unsafe in situations that do not match training data. In Chapter 5, we propose to address this issue by augmenting training data with natural language advice from a human. Advice includes guidance about what to do and where to attend. We present the first step toward advice-giving, where we train an end-to-end vehicle controller that accepts advice. The controller adapts the way it attends to the scene (visual attention) and the control (steering and speed). Further, in Chapter 6, we propose a new approach that learns vehicle control with the help of long-term (global) human advice. Specifically, our system learns to summarize its visual observations in natural language, predict an appropriate action response (e.g. ""I see a pedestrian crossing, so I stop""), and predict the controls, accordingly.",ucb,,https://escholarship.org/uc/item/1b97h2dg,,,eng,REGULAR,0,0
699,2135,"Light, Nearwork, and Visual Environment Risk Factors in Myopia","Alvarez, Amanda Aleksandra","Wildsoet, Christine F;",2012,"Myopia, or nearsightedness, is a form of visual impairment in which distant objects appear blurry due to excessive axial eye growth that is mismatched to the eye's refractive power. This condition, though treatable with spectacles, contact lenses, or refractive surgery, continues to increase in prevalence, particularly in some Asian countries, where up to 80-90% of young people and students are myopic. High myopia (< -6.00 D) is associated with greater risk of glaucoma, retinal detachment, and other blinding complications.Myopia is a complex disease with both genetic and environmental components. Rising myopia prevalence rates have mirrored lifestyle shifts that include reduced outdoor and light exposure. The directionality and impact of environmental risk factors, particularly light exposure, on myopia, continue to be poorly understood, partly due to the lack of in vivo and realtime instruments for measuring these effects. This dissertation examines the role of environmental risk factors in myopia, and introduces two new methods for quantitatively studying light and nearwork in humans. Evidence from animal studies suggests short bursts of bright light may be sufficient to retard myopic eye growth. Recent questionnaire-based studies have found increased exposure to sunlight or outdoor environments to be correlated with reduced myopia in children. We supplemented the questionnaire approach with objectively gathered data from light sensors, and compared the accuracy of the two approaches. Maximum intensity, cumulative light exposure, frequency of intensity change, or time spent in bright light were not correlated with refractive error. Subjects overestimated time spent outdoors, and these estimates were in poor agreement with time reported by the sensor data. This is the first multi-season study to use both the questionnaire and light sensor methods coupled with local weather data to investigate light and outdoor effects in myopia.The duration and degree of another myopia risk factor, nearwork, are typically estimated retrospectively through questionnaires that assess reading, computer use, and other visual behaviors. There are, however, no comprehensive methods of measuring working or fixation distance in realtime during natural tasks. Here we present a new approach for studying the dioptric environment in humans. A head-mounted eye tracking device was adapted to be fully mobile for the realtime measurement of eye movements, including convergence. This device was validated in a small sample of young adults. We conducted exploratory analyses of possible task-related trends in fixational behavior, fixation distance, horizontal eye movements, blinks, and saccades. We found large differences in some of these metrics between reading and walking tasks; these task-dependent changes in visual behavior may underlie the nearwork effect in myopia progression.Light sensing and eye tracking are new techniques for quantifying behaviors that are thought to be involved in myopia development. Unlike questionnaires, these methods provide realtime, unbiased data at the temporal resolution that is relevant to refractive error development. Environmental pressures may be a tipping point toward pathological eye growth for genetically susceptible individuals, and further work in this vein could lead to simple behavioral interventions to curb myopia progression.",ucb,,https://escholarship.org/uc/item/1bj5m20w,,,eng,REGULAR,0,0
700,2136,Routing Along DAGs,"Liu, Junda","Shenker, Scott J;",2011,"Since the invention of packet switching networks, routing is an important, if not the most fundamental, component of networking. Over decades, both academia and industry put huge efforts to improve routing in various scenarios. However, with the explosive growth of Internet, wide adoption for critical business and services, and new environments like data center networks, routing has more difficulties to meet the increasingly stringent requirements.We examine current routing schemes and discover that the problem is more fundamental, because they rely on many assumptions and trade-offs that are no longer applicable for today's networks. Believing that more radical changes are necessary to effectively address the challenge, we start the design process from scratch, with assumptions, trade-offs and design philosophies which better reflect the networks of today and tomorrow. The outcome of the design process is a unified routing framework which utilizes directed acyclic graph (DAG) as routing topology. And we name it Routing Along DAGs (RAD).RAD separates route optimization and connectivity maintenance, handles both failures and congestions, and recovers fast and locally. We explain the details of RAD design and test its performance on various real world topologies. Then we improve RAD to achieve even better performance and cover more use cases. With these extensions, RAD becomes both complete and practical.",ucb,,https://escholarship.org/uc/item/1c5705w4,,,eng,REGULAR,0,0
701,2137,"Moral Obligation, Mutual Recognition, and Our Reasons to be Moral","French, Nicholas Ian","Wallace, R. Jay;Kolodny, Niko;",2020,"AbstractMoral Obligation, Mutual Recognition, and Our Reasons to be MoralbyNicholas I. FrenchDoctor of Philosophy in PhilosophyUniversity of California, BerkeleyProfessor R. Jay Wallace, Co-ChairProfessor Niko Kolodny, Co-ChairMost of us recognize that some ways of treating other people are wrong. It is generally wrong to make a promise to someone with no intention of keeping it; to steal from others; to do physical harm to them. These things are not â€œwrongâ€ in the sense of being socially disapproved of or legally prohibited â€” they are morally wrong. To put it another way, we acknowledge moral prohibitions against these sorts of actions, or moral obligations or requirements to avoid such actions. And we typically think that we should respect these requirements; that an actionâ€™s being wrong means we have significant reason to avoid it.My aim in this dissertation is to articulate our reasons to comply with these demands, which belong to the domain of interpersonal morality. I develop a view, the Attitudinal Relationship View, which grounds our reasons to comply with the demands of interpersonal morality in the value of a particular sort of relationship, which (following T.M. Scanlon) I call mutual recognition . We stand in relations of mutual recognition insofar as we treat facts about what is morally required or forbidden, as we see them, as decisive reasons for action and other attitudes. This thesis represents a development of Scanlonâ€™s view that compliance with moral requirements is constitutive of relations of mutual recognition. But, pace Scanlon, I hold that this relationship is constituted by our attitudes toward one another, not by actual compliance with moral requirements. In holding the attitudes constitutive of mutual recognition, we respect one another as fellow rational creatures, capable of giving, asking for, and acting in the light of reasons. Relating to one another on terms of mutual recognition, I argue, makes our own lives go better. The value of mutual recognition does not explain only why we ought to comply with moral requirements. It also sheds light on the point and value of blame and associated practices of interpersonal accountability. In addition, we can understand the value of mutual recognition as animating many emancipatory social struggles, making the notion essential for a recognition-based critical theory of the sort developed by Axel Honneth.Chapter 1 sets out the animating question of the dissertation, the question of moralityâ€™s reason-giving force, and outlines constraints on an adequate answer. I present the Attitudinal Recognition View in detail, and clarify the view by responding to a number of potential objections.Chapter 2 further clarifies the Attitudinal Recognition View in response to another objection, the Problem of Moral Reciprocity. The objection is: if mutual recognition involves my having certain attitudes toward you and your having certain attitudes toward me, then it seems that I will not have reasons to comply with my obligations if you do not manifest the appropriate attitudes. But that result is intuitively unacceptable. I argue that this objection is misplaced. My response relies crucially on the view that the general moral obligations which the Attitudinal Relationship View is supposed to explain should be understood as bipolar obligations.Given the connection between bipolar obligations and mutual recognition, on the one hand, and between bipolar obligations and resentment, on the other, we should expect theoretically interesting connections between resentment and mutual recognition. Chapter 3 explores these connections, arguing that the value of mutual recognition sheds light on our practices of moral accountability. My main thesis is that blame should be understood as an emotional response to deficiencies or failures in our relationships which aims at correcting such deficiencies. Blame in response to moral violations, in particular, should be understood as an emotional response to the absence of relations of mutual recognition, a response which aims at establishing or restoring such relations. An upshot of the discussion is that our reasons to blame others for moral failures are not really distinct from our reasons to care about fulfilling our moral obligations, as articulated by the Attitudinal Relationship View.In chapter 4 I turn to a discussion of social theory. The notion of mutual recognition plays a fundamental role in the critical social theory of Axel Honneth, who has developed an account of emancipatory social movements as struggles to achieve mutual recognition. I explore the connections between Honnethâ€™s recognition-based social theory and the notion of mutual recognition I have developed in earlier chapters. I argue for a modified version of Honnethâ€™s theory on which the desire for mutual recognition, as construed by the Attitudinal Relationship Viewâ€”the relationship in which persons properly acknowledge their (bipolar) moral obligations to one another in deliberationâ€”plays a fundamental role in explaining emancipatory social struggles. Locating the role of mutual recognition in emancipatory social movements in turn illuminates important connections between mutual recognition, blame, and self-respect.",ucb,,https://escholarship.org/uc/item/1cw4v9kk,,,eng,REGULAR,0,0
702,2138,Mapping Latino Racialization: White Attitudes Towards Latinos and Policy Preferences in Orange County California,"Lacayo, Celia Olivia","Small, Stephen;Omi, Michael;",2013,"The dissertation develops our theorizing about the dynamics of racialization, and the role of race and ethnicity, in the United States, particularly in order to account for the dynamics and processes unique to Latinos.  It does so by examining white attitudes towards Latinos in Orange County, California through public discourse analysis of the ""Ask a Mexican"" column, a survey instrument and a series of in-depth interviews to triangulate whites' use and logic of racial stereotypes and policy preferences.  Orange County is a good testing ground for contemporary Latino racialization because it is a majority minority area,  which has a long history of migration from Mexico, with deep racial segregation that reflects racial inequalities between whites and Latinos. While Latinos in the United States as a whole are a heterogeneous group my data demonstrates how the current racialization of Latinos in the United States has a homogenizing effect. Empirically my data maps racial stereotypes whites have had and continue to reference as the Latino population has increased.  These include: Latinos are inherently criminal; do not value education; abuse public assistance; and do not assimilate.  They further diametrically oppose what it is to be an American and from being Mexican.  Whites use the illegality frame, but frequently assign characteristics to all Latinos regardless of their status and generation.  The findings demonstrate how the perceptions of whites towards Latinos are used when whites make daily decisions and also form their larger policy preferences. While some supported a pathway to citizenship most of the respondents overwhelmingly held negative and frequently racist views of Latinos.Theoretically, my work interrogates how Latinos/Mexicans have their own complex, multifaceted dynamics, including the implications of proximity to Mexico, the torrid race relations between whites and Latinos that included labor exploitation, segregation and lynching as well as the ongoing dominant discourse centered around Latinos being ""illegal,"" a threat, and undeserving of citizenship. My research reveals that whites often use ""ethnic"" terms to really mean biological, racially fixed terms. Thus, the extent to which Latinos are imagined to be a race or ethnicity among populations outside the academy have major implications.  Importantly, whites consistently express the belief that Latinos are not assimilating and consistently assert that an alleged ""backward"" culture is passed down from generation to generation, leading whites to believe Mexicans are an inferior group.  Furthermore, my data account for context of reception, measured by white attitudes towards Latinos, that the literature does not take into account.  My data also supports Ngai's and other scholarship about Latinos treated as perpetual foreigners, but also adds how other parts of the Latino community are labeled and treated as racial minorities.  Thus, my data challenges past literature that examines the Latino experience solely through an immigrant paradigm.  Lastly, my findings challenge Black exceptionalism and a black/non-black color line, because it proves in fact that whites have racist sentiments towards Latinos and do not see them similar to themselves.",ucb,,https://escholarship.org/uc/item/1dj0h4pp,,,eng,REGULAR,0,0
703,2139,Engineering of Novel Adeno-Associated Virus Vectors for Gene Therapy Applications,"Santiago Ortiz, Jorge Luis","Schaffer, David V;",2016,"Gene therapy â€“ the introduction of genetic material into cells and tissues of interest for a therapeutic purpose â€“ has emerged as a very promising treatment for many diseases. Recent advances in genomics and proteomics, coupled with the advent of genome editing technologies, have generated an immense pool of potential nucleic acid cargoes that could be delivered as therapies for a wide array of diseases, ranging from monogenic disorders to cancer. However, before such therapies can be successful, a major hurdle must be overcome: the development of gene-carrying vehicles â€“ also referred to as vectors â€“ that can safely, efficiently, and specifically deliver those therapeutic payloads to the desired cells. The goal of this dissertation was therefore to address a major need in the field: the development of improved gene delivery vectors. To date, more than 2,000 clinical trials employing gene transfer have taken place, establishing the safety of a number of vectors. Non-viral vectors can be easily produced at a large-scale and are amenable to the engineering of their chemical and physical properties via chemical modifications, but they suffer from a low delivery efficiency and cell toxicity. On the other hand, viral vectors harness the highly evolved mechanisms that viruses have developed to efficiently recognize and infect cells and offer several advantages that make them suitable candidates for use in gene delivery, both for therapeutic application and as tools for biological studies. In fact, gene therapy has enjoyed increasing success in clinical trials for numerous disease targets in large part due to the gene delivery capabilities viral vectors. Vectors derived from viruses have been used in the majority (over 68%) of gene therapy clinical trials to date, and the most frequently used have been based on adenovirus, retrovirus, vaccinia virus, herpesvirus, and adeno-associated virus (AAV). AAV vectors are non-pathogenic and can transduce numerous dividing and non-dividing cell types. Because of these characteristics, AAV vectors have been utilized for gene therapy in various tissues. The amino acid composition of the viral capsid affects tropism (tissue specificity), cell receptor usage, and susceptibility to anti-AAV neutralizing antibodies â€“ properties that influence efficacy in therapeutic gene delivery. However, AAV vectors can still encounter formidable impediments to efficacious gene delivery, including poor transduction (infection and expression of delivered gene) of some cell types, off-target transduction, difficulties with biological transport barriers, and potential risks associated with the integration of their genetic load. Extensive engineering of the AAV capsid promises to overcome these delivery challenges and improve numerous clinically relevant properties. To this end, the overarching goal of my work in the Schaffer Laboratory, which is presented in this thesis dissertation, was to advance current gene delivery methods through the engineering and characterization of novel adeno-associated virus vectors for gene therapy and research applications. To access new viral capsid sequences with potentially enhanced infectious properties and to gain insights into AAVâ€™s evolutionary history, we computationally designed and experimentally constructed an ancestral AAV capsid library. We performed selection for infectivity on the library, studied the resulting amino acid distribution, and characterized the selected variants, which yielded viral particles that were broadly infectious across multiple cell types. Ancestral variants displayed higher thermostability than modern (extant) natural AAV serotypes, a property that makes them promising templates for protein engineering applications, including directed evolution. Additionally, some variants displayed high in vivo infectivity on a mouse model, highlighting their potential for gene therapy. Motivated by the success of directed evolution in the engineering of proteins with novel or enhanced properties, I worked in the engineering of AAV vectors for gene delivery to glioblastoma multiforme (GBM), a highly aggressive type of brain cancer. For this, I conducted directed evolution to select AAV variants with selective localization to and infectivity on GBM tumor cells and tumor initiating cells (TICs). Using an accurate GBM mouse model, I performed in vitro and in vivo selection, recovering viral particles that successfully trafficked to tumor cells and TICs in the brain after systemic administration to tumor-bearing animals. Following three rounds of in vivo selection, convergence was achieved upon several variants, the most abundant of which emerged from the ancestral reconstruction library. The selected variants are currently being characterized and assessed for their ability to deliver reporter and therapeutic genes, hopefully resulting in improved suppression of tumor progression compared to delivery with existing AAV serotypes. These novel vectors could enable new, potent therapies to treat GBM tumors and pave the way for engineering AAV vectors for other cancer targets.In summary, this dissertation presents work on the development and characterization of a novel AAV capsid library, as well as on the implementation of this and of other libraries towards the engineering of novel AAV variants with selective gene delivery properties for brain tumors. The work herein presented aims to advance both the field of AAV vector engineering as a whole and the specific application of AAV vectors towards next generation cancer therapies.",ucb,,https://escholarship.org/uc/item/1fh714qz,,,eng,REGULAR,0,0
704,2140,Heterogeneous responses to environmental change: contrasting behavior and physiology in two California chipmunks,"Hammond, Talisin Tess","Lacey, Eileen A;",2017,"Biotic responses to environmental change can vary markedly, even among closely related, ecologically similar species. Such responses may be conspicuous (e.g., climate-associated range shifts) or they may be subtler and more challenging to detect. In the latter case, organisms may use individually variable mechanisms, including modifications of behavior and physiology, to cope with environmental change in situ. Further, in addition to providing mechanisms of response to environmental change, behavioral and physiological traits may be indicators of habitat suitability. Thus, to understand and, ideally, to predict how species will respond to environmental change, it is necessary to determine which traits are associated with vulnerability and to identify which factors constrain range limits for vulnerable species. My dissertation focuses on the behavior and physiology of the alpine chipmunk (Tamias alpinus) and the lodgepole chipmunk (T. speciosus), two co-occurring, closely related species that have been characterized by very distinct spatial responses to environmental change in Yosemite National Park, CA. Over the past century, T. alpinus has contracted its range upward in elevation; during the same period, T. speciosus has displayed no significant elevational range shift. To assess the role of behavioral and physiological variability in generating these responses, I explored interspecific differences in baseline stress hormone (glucocorticoid, GC) levels and behavioral activity budgets with the goal of identifying the environmental factors that are most important for determining range limits in the study species. First, I validated a non-invasive method to measure fecal GC metabolites (FGMs) in both study species. By exposing captive individuals to a series of controlled challenges, I also identified interspecific differences in stress reactivity, with T. alpinus being generally more stress-responsive. Next, I validated the use of accelerometers to remotely document the behavioral activity budgets of the study species, demonstrating that these sensors can be employed to collect behavioral data from free-living animals. I then deployed accelerometers across broader spatial and temporal scales. I used the resulting data to construct models that integrate intrinsic biological and environmental parameters to identify key predictors of activity in each species. I found that, compared to T. alpinus, activity in T. speciosus was characterized by generally greater inter-individual variance and greater variability in response to environmental parameters. Finally, I used FGM data collected over three years and at multiple sites in and around Yosemite National Park in conjunction with data regarding multiple extrinsic (environmental) and intrinsic (life history) parameters to identify the factors that best predict FGMs in the study species. These analyses revealed FGM levels are more strongly related to environmental parameters in T. alpinus than in T. speciosus. In summary, my research indicates that T. alpinus is more stress-responsive to external, environmental challenges, and potentially less flexible in responding behaviorally to environmental conditions than T. speciosus. Overall, these results indicate that behavior and physiology are likely to be important determinants of a speciesâ€™ response to environmental change. These findings also suggest that individual species vary in their general sensitivity to environmental change, with some species being more change-responsive than others.",ucb,,https://escholarship.org/uc/item/1fr5z4zw,,,eng,REGULAR,0,0
705,2141,Searching for the invisible: how dark forces shape our Universe,"Schutz, Katharine","Murayama, Hitoshi;",2019,"Astrophysical observations on a wide range of scales indicate that the majority of matter in our Universe seems to be approximately inert and non-luminous. The existence of this dark matter implies the existence of an undiscovered particle, since there is no viable dark matter candidate within the Standard Model. Many terrestrial searches for dark matter particles are underway; however, there is no evidence to date that the dark matter interacts with particles in the Standard Model except through gravity. It highly conceivable that the dark matter exists as part of a rich hidden sector with diverse matter content and its own dark forces (in analogy to the Standard Model) which would imply that terrestrial searches may not pose an optimal path to discovering dark matter. Instead, observing astrophysical systems â€” where dark matter is known to be present through its gravitational influence â€” would be the best available way to test theories of dark matter where dark forces play a role in altering the properties of those systems. The complementarity of observing various astrophysical systems is a powerful asset for exploring the physics of dark sectors: one can explore broad classes of theories with dark forces in different environments, on different length scales, and at different epochs in the history of our Universe. This dissertation explores several scenarios where astrophysical observations inform our understanding of dark forces in a way that would not necessarily be possible on Earth. In particular, we consider dark sector energy dissipation, dark matter self-interaction, and the early Universe production of dark matter through dark channels. We propagate the implications of these effects for stars, supernovae, the Milky Way stellar disk, dwarf galaxies, galaxy clusters, large-scale structure, the epoch of reionization, and the cosmic microwave background.",ucb,,https://escholarship.org/uc/item/1h03b2kc,,,eng,REGULAR,0,0
706,2142,The Specter of Violence: Perceptions of Violence and Political Behavior in Mexico,"Buss, Tara","Stoker, Laura;Collier, Ruth B;",2020,"Violence stemming primarily from organized crime plagues Mexico. The sharp increase in criminality and insecurity, particularly since the start of the war on drugs in 2006, is one of the most significant developments for the country since the turn of the century. Officially recorded crimes, which are estimated to represent only one tenth of all crimes committed, have grown by 70% in the last two decades. The homicide rate in particular has exploded: between 2000 and 2018 the rate nearly tripled, climbing from eleven murders per 100,000 inhabitants to twenty-nine. Two hundred and fifty thousand deaths since 2006 are considered directly attributable to the drug war. This violence has led to the internal displacement of 345,000 Mexicans as of 2019, an increase of 431% from a decade prior, and more than 35,000 disappearances.A surge of scholarly research on the causes of bloodshed in the country have highlighted the growing intractability of public security issues and have raised questions about the efficacy of various policy initiatives aimed to curtail the violence. Yet, little attention has been paid to how violence is understood by the populace and to the political consequences of those perceptions. This dissertation addresses that deficit by using large-scale survey data with embedded experiments to systematically address two driving questions: 1) how do Mexicans perceive violence? and 2) how do perceptions of violence affect engagement in electoral politics?Using data from an original survey of 17,451 Mexican voters contacted six weeks prior to the 2018 presidential election, I examine the ways in which Mexicans perceive violence, specifically asking a random subsection of respondents to estimate levels of homicide and kidnapping in their state. I find that misperceptions of violence are nearly universal, with underestimation of homicides and overestimation of kidnappings being dominant. Yet, misperception within each type of violence is not uniform: A fifth of respondents overestimated homicides and nearly a quarter underestimated kidnappings. Elite discourse and media coverage of violence interact with different cognitive biases and adaptations to shape the way that information is assimilated. The salience of violence, more than actual, objective levels of violence, drive these misperceptions. Moreover, perceptions of violence in oneâ€™s state often reflect violence levels of years prior, indicating that these perceptions are relatively stable, slow to update, and even dramatic recent changes in violence levels have little effect on peopleâ€™s beliefs. In exploring the heterogeneity in perceptions of violence, I find that a number of state-level and individual-level factors influence how respondents interpret the violence around them. Respondents along both the northern and southern border were more likely to overestimate homicide. Those living along the southern border were substantially more accurate in their assessment of kidnapping than those along the northern border or living in the interior; they still tended to overestimate kidnapping, but not to the same extent as their peers elsewhere. Those living in more wealthy and/or in more economically unequal states were also more substantial overestimators of violence than their peers in either poorer or more equal states. At an individual level, oneâ€™s level of education and the attention paid to political processes drive overestimation of violence, supporting the hypothesis that exposure to media and elite messaging, as well as retention of that information, are key drivers in overestimation of violence. Additionally, support for leftist candidate and current president AndrÃ©s Manuel LÃ³pez Obrador was highly correlated with overestimation of violence.These misperceptions have serious political consequences. I use both experimental and observational data to explore how perceptions of violence, and its two key driving components, actual violence and the salience of violence, alter political attitudes and behaviors. In doing so, a clear picture emerges: the citizens of Mexico who experience violence the most strongly are hiding in fear, expressing uncertainty over their electoral decisions, and withdrawing from political life. Insecurity has dramatically decreased citizensâ€™ the sense of personal safety. It has led them to retreat from electoral politics in clear ways: they are less likely to participate in elections, are more indecisive about who to vote for when they do chose to engage, and feel alienated from political parties. They are also more likely to reject reforms and tolerate societal ills, including corruption and criminal organizations themselves, than those whose overall experience of violence is less heightened. Fear, apathy, and uncertainty drive this withdrawal.While the majority of scholarly research on the effects of violence on political behavior has focused on the relationship between actual violence levels and participation and vote choice, researchers have largely missed that perceptions of violence, not violence itself, are the key driver of these processes. Now, a decade and a half after the start of the militarized war on drugs in Mexico, the death toll has reached new highs for three consecutive years, with no signs of tapering off. The specter of violence hangs over Mexico and is pushing citizens backward in retreat.",ucb,,https://escholarship.org/uc/item/0rx5x99f,,,eng,REGULAR,0,0
707,2143,"Synthesis of Multiscale Transient Analytical, Experimental, and Numerical Modeling of Latent Energy Storage for Asynchronous Cooling","Helmns, Dre","Carey, Van P;",2019,"Novel energy technologies have the potential to address climate change by efficiently using natural resources and reducing greenhouse gas emissions into our shared global atmosphere. In preparation for a future society powered by renewables, engineering solutions must include environmentally conscious and cost-effective methods to store, transport, and convert energy.I have numerically investigated the use of latent thermal energy storage (TES) technology, with solid to liquid phase change material (PCM), to shift cooling loads to off-peak hours. I first non-dimensionalized differential equations describing sensible and latent heat transfer in the PCM, and their finite difference counterparts, in order to facilitate scaling for the wide array of asynchronous cooling applications that could benefit from this technology. Next, I compared closed form analytical solutions and experimental testing of a TES prototype with the numerical prediction of its melting and freezing processes. Both the analytical solutions and the experimental tests matched the predicted results within 10% agreement, validating the computational modelâ€™s capacity to capture the physics governing the transient behavior of the device with high precision and accuracy. As no adjustable parameters were tuned to maximize agreement, the numerical model can be effectively employed to determine the performance of different designs without the need to fabricate, charge, and test them.Using this model, I explored potential improvements to power and refrigeration cycles for power plants and commercial buildings integrated with thermal storage.  I accomplished this task by taking simple representations of these systems in MATLAB and transforming them into declared relationships between complex components using the dynamic programming language of Modelica. Simulations in integrated development environments of both languages demonstrate improvements of up to a 1.4% increase in power plant energy output and a 2.4% decrease in building chiller energy consumption with thermal storage. With thoughtful selection of the phase change material and better charging and discharging control strategies for the thermal energy storage, further performance enhancement of such systems can be achieved.",ucb,,https://escholarship.org/uc/item/0sj9p5k3,,,eng,REGULAR,0,0
708,2144,"Work and Life in the Balance: Ways of Working and Living Among Elite French, Norwegian, and American Professionals","Schulz, Jeremy Markham","Fligstein, Neil;",2010,"The idea that work-shy Western Europeans and work-crazed Americans differ fundamentally in their orientations to working life and private life has gained wide currency on both sides of the Atlantic within the social science community, spawning rafts of studies charting differences in aggregate time use patterns and work value orientations. Taking an experiential perspective on the behaviors and orientations constitutive of working life and private life, my dissertation approaches the question of cross-national and transatlantic difference from a novel standpoint. Drawing on over one hundred and fifty in-depth interviews with comparable elite professionals, the dissertation carries out a three-way case study of the experiential divergences and convergences between the working lives and private lives of comparable French, Norwegian, and American elite professionals working and living in Paris, Oslo, and San Francisco.	The dissertation examines the ways these three groups organize and experience their working lives and their private lives by exploring convergences and divergences relating to a number of analytical dimensions. The study contrasts their daily work routines, their temporal zoning practices, their career pathways and aspirations, their romantic partners' occupational profiles, as well as their ways of talking about work, work effort, and leisure. Capitalizing on my unique body of data, the dissertation reveals the forms which these various practices and orientations take in these three distinctive societal environments. 	The dissertation's findings add a new dimension to the ongoing debates around overwork, extreme work, and work-life strain among managers and professionals. The study's comparative findings reveal important differences in the ways that comparable populations of elite French, Norwegian, and American managers and professionals working in similarly high-stakes, rewarding, and remunerative jobs constitute working life and private life. Relative to their American or French counterparts, elite Norwegian managers and professionals treat their working lives as a less greedy life realm, responding to a social and cultural environment which acts in very specific ways to inhibit the kind of extreme work habits which run rampant in these two other societal contexts. 	While both the French and American elite managers and professionals engage in extreme working, this way of working assumes somewhat different forms in the two societal contexts. The extreme work of the American managers and professionals is driven by a deep-seated desire to perform well in a competition over money and personal status. By contrast, the extreme work of the French managers and professionals issues from an attachment to an occupational identity defined through membership in a recognized social and cultural elite.  This identity is strengthened and reinforced by a surprisingly strong tendency for the male French elite professionals to pair up with occupationally matched women pursuing their own demanding careers.	Just as the dissertation provides a rich and. nuanced picture of working life among these three groups of managers and professionals, it illuminates the complex linkages between extreme work among managers and professionals, on the one hand, and facets of societal context, on the other hand.  Analyzing these connections from a variety of theoretical perspectives, the dissertation reveals the sources of these differences in stratification cultures, gender cultures, systems of elite education, and patterns of romantic and family life.",ucb,,https://escholarship.org/uc/item/0vr6b2n3,,,eng,REGULAR,0,0
709,2145,Role of Retinal Pigment Epithelium in Myopia Development and Control,"Zhang, Yan","Wildsoet, Christine F;",2013,"Myopia (near-sightedness) is one of the most common ocular disorders in humans. Due to the dramatic increases in prevalence of myopia worldwide, especially in children and young adults, myopia has also become a significant public health problem, both socially and economically. While the prevalence and severity of myopia continue to increase, effective therapeutic interventions for myopia remain limited. Currently, management of myopia is largely limited to traditional optical corrections - spectacles, contact lenses, and refractive surgery - which correct distance vision but have no effect on myopia progression. While slowed myopia progression has been reported in clinical studies using the contact lens-based, corneal reshaping therapy and atropine, a pharmaceutical agent, these approaches come with limitations and in the latter case, significant ocular side-effects. Uncontrolled progression may lead to high ""degenerative"" myopia, for which posterior scleral reinforcement surgery remains the only treatment option and a last resort directed at preserving vision. Thus there is a clear need for new myopia control treatments. Understanding more about the molecular and cellular mechanisms underlying myopic eye growth has the potential to uncover novel treatment options. This dissertation presents results from three investigations into the role of the retinal pigment epithelium (RPE) in eye growth regulation, focusing on molecular and cellular mechanisms, and using both in vivo animal models and in vitro cell culture models. In the first in vivo study (Chapter 2), we investigated expression of candidate genes in chick RPE of imposing short-term optical defocus. Specifically, gene expression levels of the three bone morphogenetic proteins (BMP-2, 4 & 7) were examined after 2 and 48 h of treatment, with negative and positive lenses used to impose defocus of opposite sign. These growth factors were observed to be differentially and bidirectionally expressed in RPE, expression generally increasing with imposed myopia, which is associated with ocular growth inhibition. Because eyes had little chance to change their dimensions with such short-term lens treatments, these genes are assumed to play important roles in the onset and early phase of defocus-induced ocular growth changes. For this reason, these genes represent potential targets for molecular-based myopia treatments. In the second study (Chapter 3), high-through gene expression profiling was employed to examine changes in gene expression in chick RPE with long-term imposed hyperopic defocus, which resulted in eyes being longer than normal and highly myopic. This DNA microarray screening revealed changes in the expression of many genes, including BMPs, noggin (NOG), dopamine receptor D4 (DRD4), retinoic acid receptor, beta (RARB), and retinal pigment epithelium-derived rhodopsin homolog (RRH). Some of these genes showed increased expression while others showed decreased expression. It is plausible that some may be linked the ocular pathological complications seen in myopia, while others may be linked to ocular growth regulation, the imposed visual conditions resulting in sustained, increased ocular growth. The third and final study (Chapter 4), addressed the possibility of an RPE site for the anti-myopia action of apomorphine (APO), a dopamine receptor agonist, observed in animal studies. We further investigated the possibility that TGF-Ã¢ secretion from RPE mediates this inhibitory growth effect. APO applied to cultured human fetal RPE cells was found to alter the secretion of both TGFÃ¢1 and TGFÃ¢2, which was biased towards the basal (choroidal) side. These growth factors also exhibited constitutive polarized secretion, albeit biased in the opposite direction to APO-induced paracrine secretion. The results for APO are consistent with its observed inhibitory (anti-myopia) effects in vivo and offer the RPE as a possible site of action. In summary, the research reported in this dissertation provides evidence that RPE plays an important role in postnatal eye growth regulation, (including myopic growth), as a conduit for relaying growth modulatory retinal signals to choroid/sclera. Genes and molecules identified in these studies offer potential directions for novel anti-myopia treatments, with the RPE being a potential target for the same.",ucb,,https://escholarship.org/uc/item/0wf1175t,,,eng,REGULAR,0,0
710,2146,Locally Volume Collapsed 4-Manifolds with Respect to a Lower Sectional Curvature Bound,"Theerakarn, Thunwa","Lott, John;",2018,"Perelman stated without proof that a 3-dimensional compact Riemannian manifold which is locally volume collapsed, with respect to a lower curvature bound, is a graph manifold. The theorem was used to complete his Ricci flow proof of Thurstonâ€™s geometrization conjecture. Kleiner and Lott gave a proof of the theorem as a part of their presentation of Perelmanâ€™s proof.In this dissertation, we generalize Kleiner and Lottâ€™s version of Perelmanâ€™s theorem to 4-dimensional closed Riemannian manifolds. We show that under some regularity assumptions, if a 4-dimensional closed Riemannian manifold is locally volume collapsed then it admits an F-structure or a metric of nonnegative sectional curvature.",ucb,,https://escholarship.org/uc/item/0x37d5vr,,,eng,REGULAR,0,0
711,2147,Studies of the Cosmos Using Spiderweb Absorber Transition Edge Sensor,"Westbrook, Benjamin","Lee, Adrian T;",2014,"Transition edge sensor (TES) bolometer technology has been at the core of advancementsin experimental cosmic microwave background (CMB) science for thepast few decades. Theoretical and experimental work has built a robust model ofthe universe. Despite tremendous progress, there are several key pieces of experimentalevidence missing to complete our understanding of the universe. This dissertationcovers the work done by Benjamin Grey Westbrook at the University of CaliforniaBerkeley between 2007 and 2014. It is centered around the use of spider-web absorbertransition edge sensor (SWATES) bolometers to study the cosmos by theAtcama Pathnder Experiment - Sunyaev Zel'dolvich (APEX-SZ) and the E andB Experiment (EBEX). Both of which help complete our model of the universe incomplimentary ways.APEX-SZ is a ground-based experiment that made observations of galaxy clustersvia the Sunyaev-Zel'dovich Eect from the Chajnantor Plateau in Northern Chilefrom 2005 to 2010. It observed galaxy clusters at 150 GHz with 300 SWATES detectorswith a resolution of 10. Galaxy clusters are the largest gravitationally boundobjects in the present day universe and are excellent for studying the properties ofthe universe. The primary goal of APEX-SZ was to understand the complex physicsof galaxy clusters. By understanding their composition, number density, and evolutionwe can better our understanding of the evolution of the universe into its presentstate.EBEX is a balloon-borne experiment that made observations of the CMB andcosmic foreground during a science ight from the Long Duration Balloon (LDB)facility outside of McMurdo Station, Antarctica over the 2012-2013 austral summer.It made observations of 6000 square degrees of sky using 872, 436, and 256 SWATESbolometers at 150, 250, and 410 GHz detectors (respectively) with 80 resolution",ucb,,https://escholarship.org/uc/item/0xh3r73f,,,eng,REGULAR,0,0
712,2148,Machine Learning to Scale Fault Detection in Smart Energy Generation and Building Systems,"Hu, Rong Lily","Agogino, Agogino M;Auslander, David M;",2016,"Data-driven techniques that extract insights from sensor data reduce the cost of improving system energy performance through fault detection and system health monitoring. To lower cost barriers to widespread deployment, a methodology is proposed that takes advantage of existing sensor data, encodes expert knowledge about the application system, and applies statistical and mathematical methods to reduce the time required for manual configurations. Renewable energy technologies as well as building energy management systems have upwards of hundreds of existing sensor data points used for control and monitoring. Furthermore, innovations in ""Internet of Things"" (IoT) devices have further led to connected power meters, lights, occupancy sensors, and appliances that are capable of data collection and communication. This data presents a valuable opportunity to extract meaningful information and take data-driven action.  The motivation for transforming data from these devices into actionable information is to improve operations, monitor system health, increase energy generation, and decrease energy waste. The development and widespread use of energy conservation and renewable energy technologies are critical to minimizing negative environmental consequences. To that end, increasing profitability for users and decreasing costs of these technologies enables market penetration and widespread adoption. On the energy generation side, operations and maintenance accounts for up to 30% of the cost of wind generation, and unexpected failures on a wind turbine can be extremely expensive. On the energy demand side, commercial buildings consume 19% of US primary energy. Of this, an estimated 15\% to 30% of energy used in commercial buildings is wasted by poorly maintained, degraded, and improperly controlled equipment. However, one cannot achieve scalable deployments of analytics and applications across systems if deploying solutions requires vendors and domain experts to install sensors and information technology infrastructures that require tailoring each solution for each deployment. Today, even well-established commercial offerings are not deployed at scale because costs are prohibitive. Thus, a major challenge to scalability is reducing hardware and software installation costs, manual configuration requirements, and manual monitoring.   To address this challenge, a methodology is proposed that leverages machine learning techniques to configure automated fault detection systems and controls. The approach combines sensor data points and encodes engineering knowledge that is generic to the application system but independent of a particular deployment. The resulting data can be input into numerous machine learning and optimization algorithms. Furthermore, the procedure selects data points and demonstrates that only a small number of sensors are necessary for fault detection with high accuracy rates.  Applications to a wind turbine, a commercial building chiller plant, and residential buildings demonstrate the proposed methodology. Implementation is possible and the results are realizable using off-the-shelf algorithms, libraries, and tools. The goal is to enable an application that can be written once and then widely deployed with little additional cost or effort. The results of analysis can also inform policy decisions for stakeholders.",ucb,,https://escholarship.org/uc/item/0zt3w54b,,,eng,REGULAR,0,0
713,2149,"Universals and variation in language and thought: Concepts, communication, and semantic structure","Carstensen, Alexandra","Regier, Terry;",2016,"Why do languages parcel human experience into categories in the ways they do, and to what extent do these categories in language shape our view of the world? Both language and nonlinguistic cognition vary across cultures, but not arbitrarily, suggesting that there may be universal constraints on how we talk and think. This dissertation explores the sources and consequences of universals and variation in language and thought in four parts.The first study examines a major premise of the universalist view of cognition, that speakers of all languages share a universal conceptual space, which is partitioned by the categories in language. Previous research on color cognition supports this view; when English speakers successively pile-sort colors, their sorting recapitulates an independently proposed hierarchy of color semantics across languages (Boster, 1986). Here I extend that finding to the domain of spatial relations. Levinson et al. (2003) have proposed a hierarchy of spatial category differentiation, and I show that English speakers successively pile-sort spatial scenes in a manner that recapitulates that semantic hierarchy. This finding provides evidence for a specific hierarchy of spatial notions as a model of universals in conceptual structure, and suggests that universal patterns observed across languages reflect general cognitive forces that are available in the minds of speakers of a single language.The second project of this dissertation demonstrates a process by which domain-specific conceptual universals and more general communicative pressures may shape categories in language, extending a previous account (Regier et al., 2015) of semantic universals and variation. In particular, I show that human simulation of cultural transmission in the lab produces systems of semantic categories that converge toward greater informativeness, in the domains of color and spatial relations. These findings suggest that larger-scale cultural transmission over historical time could have produced the diverse yet informative category systems found in the worldâ€™s languages. This work supports the communicative efficiency account of semantic universals and variation and establishes a process through which categories in language become increasingly efficient and increasingly universal.  The third study extends the previous account of categories in language to cognition more broadly, showing that the same principles that govern efficient semantic systems also characterize nonlinguistic cognition. I provide an account of spatial cognition in which conceptual categories optimize the trade-off between informativeness (making for fine-grained and intuitively organized spatial categories) and simplicity (limiting the number of categories). I find that pile sorts made by speakers of diverse languages match this universal account more closely than they match the semantics of the sorterâ€™s native language. These results suggest that across languages, spatial cognition reflects universal pressures for efficient categorization, and observed universals in category structure and granularity result from these pressures.The final project of this dissertation probes the role of language in online spatial reasoning, using linguistic interference to prevent participants from relying on language in solving a spatial task. In previous work, adult English speakers have been shown to use a spatial frame of reference that differs from that of nonhuman primates and toddlers (Haun et al., 2006), suggesting that learning the spatial frame of reference used in English may motivate a switch away from universal modes of spatial thought. I find that under linguistic interference, despite a sharp increase in error, adult English speakers fail to readopt the spatial frame of reference used by nonhuman primates and toddlers. This finding rules out the possibility that language affects spatial frames of reference online and accordingly argues against Kay and Kemptonâ€™s (1984) account, which predicts a removable online role of language. This result raises the stakes of the debate over the role of language in nonlinguistic spatial frames of referenceâ€”either something other than language causes alignment between linguistic and nonlinguistic frames of reference, or language learning fundamentally restructures nonlinguistic spatial cognition in a way that is difficult to reverse.The findings of this dissertation in the domain of space, taken together with parallels in other cognitive domains, reinforce an emerging consensus on the relation of language and thought, by which all people share a universal conceptual foundation that may be altered by language. The research here further elaborates this account, suggesting that universals and variation in both language and thought may derive to some extent from general principles of efficiency. At the same time, it challenges the generality of a classic formulation of this view (Kay & Kempton, 1984), motivating future research. In both complementing and challenging an emerging consensus on language and thought, this dissertation informs our view of language, a defining feature of human cognition, and contributes to a more complete understanding of the nature of thought.",ucb,,https://escholarship.org/uc/item/1h04c13q,,,eng,REGULAR,0,0
714,2150,"True to Life: A Study of Lifelikeness in Fiction through Proust, Austen, Nabokov, and Joyce","Rowan, Jonathan Bricke","Alter, Robert;",2014,"The subject of this dissertation is verisimilitude, or lifelikeness, in fiction: the impression a work of fiction can give a reader that a scene or a character or any other one of its elements is remarkably real-seeming, remarkably ""true to life."" By studying works by four writers who were masters at creating this effect--Marcel Proust, Jane Austen, Vladimir Nabokov, and James Joyce--I attempt to reveal its sources.Chapter one is entirely focussed on a single type of lifelikeness: the ability of a work of fiction to make us more aware of our experience by capturing what Proust calls ""general essences"": subtle general phenomena we have experienced in our own lives but that have never before been the objects of our full conscious awareness. In moments when this capturing takes place, the characters and events it involves are imbued with a striking ""realness."" In the chapter's first half, I show that this kind of lifelikeness is at the heart of Proust's aesthetics; several key scenes of the Recherche--including the famous encounter with the towers of Martinville in ""Combray""--express a philosophy of art in which the capturing of ""general essences"" is art's main source of value. In the chapter's second half, I argue that the Recherche is true to its own philosophy--that it lives up to Proust's ideal of literature as ""a kind of optical instrument that [the author] offers to the reader to enable him to discern what, without this book, he would perhaps never have seen in himself.""From a work that many consider the high point of realism in the novel, we now go back a hundred years to one of its most important early innovators. Chapter two is a study of Pride and Prejudice that attempts to explain the impression so common among Austen's readers that her characters are extraordinarily ""alive."" It does this in the form of a typology of verisimilitude--a study of four different ways a novel can be true to life. In Pride and Prejudice we find again the ""illuminative"" verisimilitude we found in Proust and distinguish from it three other types. First there is ""plausibility,"" in which the writer is akin to the juggler--he keeps the apple of compelling storytelling in the air without dropping the orange of believability. Next there is ""inclusive"" verisimilitude, produced when the representation of a certain object includes features the object has in real life but that are typically left out when the object is represented in art. And finally there is ""rightness,"" which comes from certain writers' great skill at imaginatively inhabiting the minds and bodies of characters.My chapter on Nabokov has two sections. The first is an investigation of the relationship between literature and reality in Nabokov's ideas about literature. We find in his published lectures two important caveats for thinking about verisimilitude in fiction: first, that plausibility means adherence to the rules of a novel's world, which may differ in certain respects from those of the real one; and second, that truth to life in fiction is less a mirror than a prism: not just a reflection of the world, but the world as it appears through the medium of a writer's consciousness. The second section of the chapter is an analysis of Nabokov's novel Pnin that explores the ways fiction can be lifelike in its depictions of the sensory world. Finally, in my chapter on Ulysses, I round out this inquiry by exploring some methods of lifelikeness I haven't yet discussed: verisimilar obscurity; the interweaving of the fictional with the real; verisimilar cross references; the depiction of ""low"" realities; and the ""stream of consciousness"" technique in its ability to bestow on characters a full mental life. Joyce did not invent these techniques, but he carried them much further than any other writer had done; Ulysses, the most ""patterned"" of novels, is also in certain ways the most lifelike, and much of what is distinctive in its style comes from these new extremes.",ucb,,https://escholarship.org/uc/item/12b8b9s9,,,eng,REGULAR,0,0
715,2151,"Unfinished Yarn: Work, Technology, and the Ethical Subject in Kolkata","Paul, Abhijeet","Ray, Raka;Cohen, Lawrence;",2015,"This dissertation explores jute life and community through the repair and reworking of old and analog machines. This, I claim, has given rise to particular brands of microlocal or community practices and politics, including craft revival, tensions between the â€œbody mechanicâ€ and the â€œfragmented mechanic,â€ repair and reuse as ethical work, strategies of dealing with â€œgrayâ€ infrastructure, â€œproverbial ethics,â€ and â€œpension politics,â€ which are different from the politics of unions and the â€œseamlessâ€ production theories of industrial capitalism as well as global capitalism. In short, the dissertation uses the idea of the local, tied to vernacular forms of thought, action, interaction, and interruption, to give a sense of shared values and tensions between individuals and the community in working neighborhoods. The community adapts to the ideas of recycling, flexibility, improvisation, and mobility, redefining the â€œfields of practiceâ€ in the domain of work and everyday life.The five chapters of the dissertation trace the material culture of jute from medieval to neoliberal times. I begin with the many local narratives of raw jute or pÄá¹­ and jute handicrafts or pÄá¹­shilpa, overlooked in most studies of jute. This grounds the ethnography of (machine)-woven jute or choá¹­ and the jute industry or choá¹­shilpa in the first section, â€œJute Works.â€ The section begin with the total works, then moves in to focus on one machine, the hÄti kal (elephant machine), and finally one tiny pinion of this machine. Moving out again from the jute works to â€œJute Publics,â€ the last two chapters explore the spaces and circuits of the mohallÄ (neighborhood/community) bazaar and mohallÄ politics in the context of nonelite globalizations.",ucb,,https://escholarship.org/uc/item/155272r3,,,eng,REGULAR,0,0
716,2152,Essays on behavioral responses to development interventions,"Emerick, Kyle Jared","Sadoulet, Elisabeth;",2014,"This dissertation combines three papers which are all empirical analyses of agricultural interventions in developing countries. I focus on how new policies, technologies, and institutions affect the behavior of small-scale farmers in both Mexico and India. The first paper focuses on the certification of agricultural land in Mexico while the second and third papers focus on technology adoption in rural India.Chapter 1, which is based on joint work with Alain de Janvry, Marco Gonzalez-Navarro, and Elisabeth Sadoulet, shows that removing the link between active land use and ownership through certification leads to a reallocation of labor away from agriculture and towards migration. In particular, we use the rollout of the Mexican land certification program from 1993 to 2006 to show that households obtaining land certificates were subsequently 28% more likely to have a migrant member. This response was differentiated by initial land endowments, land quality, outside wages, and initial land security, as predicted by our model. Effects on land under cultivation were heterogeneous: in high land quality regions land under cultivation increased while in low quality ones it declined.Chapter 2, which is based on joint work with Alain de Janvry, Elisabeth Sadoulet, and Manzoor Dar, shows evidence that risk is an important factor that constrains the decisions made by small farmers. More specifically, the chapter reports results of a field experiment in Odisha India that quantifies the effects of Swarna-Sub1, a promising new rice seed that effectively reduces risk by sharply reducing the susceptibility of the crop to flood damage. In doing so, the chapter offers novel evidence on the effect of a direct reduction in production risk on economic behavior. Specifically, access to this new technology leads to increases in area cultivated, fertilizer used, and the likelihood of using a more modern planting method. Also, the technology reduces precautionary savings of grain for consumption and increases the use of agricultural credit. An important implication from the chapter is that technological progress that directly eliminates weather-induced production variability offers a promising method of advancing agriculture in areas that are prone to extreme weather.Chapter 3 builds on the promising results in Chapter 2 by studying diffusion of Swarna-Sub1. I provide an experimental test of whether informal exchange of Swarna-Sub1 between farmers produces an efficient allocation. I report results on a field experiment, also in Odisha, to compare decentralized trade of Swarna-Sub1 through networks with an approach where demand was revealed via door-to-door sales. While 84% of farmers are expected to gain from Swarna-Sub1, only 7% adopt in networks. Conversely, 40% of farmers adopt when demand is revealed in door-to-door sales. Using variation across the sample in estimated gains in revenue, I show that 63% of the gains from door-to-door sales are lost with decentralized trade through networks. Frictions preventing interactions between farmers from different social groups offer an explanation for the results. Sub-caste and surname association with suppliers are strong predictors of adoption in networks, but have no effect in door-to-door sales. The main implication from the chapter is that relying on exchanges between farmers to disseminate new seed varieties will not produce an allocation where demand is met.",ucb,,https://escholarship.org/uc/item/17k8m7pt,,,eng,REGULAR,0,0
717,2153,The Invisible Crowd: Individual and Multitude in Roberto BolaÃ±o's 2666,"Brito, Francisco","Masiello, Francine;",2018,"This dissertation argues that Roberto BolaÃ±oâ€™s novel 2666 offers us a new way of thinking about the relationship between the individual and the multitude in the globalized world. I argue that the novel manages to capture the oppressive nature of its structures not by attempting to represent them directly but instead by telling the stories of individuals who feel especially alienated from them. These characters largely fail to connect with one another in any lasting way, but their brief encounters, some of which take place in person, others through reading, have pride of place in a text that, I propose, constitutes a brief on behalf of the marginal and the forgotten in its overall form: it is an example of the novel as an ever-expanding, multitudinous crowd; it strives to preserve the singularity of each of its members while at the same time suggesting that the differences between them are less important than their shared presence within a single narrative whole. I proceed by examining these characters in all their particularity and closely reading the novelâ€™s key scenes, in which they meet one another, while also tracking how these characters and encounters are paradigmatic of different ways of relating. Chapter 1, â€œInsufferable Hierarchies,â€ focuses on 2666â€™s most bookish characters and their rage for â€œgreatness,â€ exploring how literature in the novel both enlarges the moral imagination of certain characters and curdles that of others. My second chapter, â€œWomen in the Shape of Monsters,â€ is centered on the novelâ€™s least bookish stretch, the famous middle section about the murdered women of Santa Teresa. It examines how the fight waged by that sectionâ€™s living women against gendered violence runs up against patriarchal power and the overwhelming burden of having to represent or avenge their murdered sisters. In â€œEverything in Anything,â€ my final chapter, I explore how the novelâ€”itself a storehouse of miscellaneous but true informationâ€”allows its characters to form multi-generational and trans-historical bonds through the sharing of random facts and by unsystematic but assiduous reading in marginal spaces. In a brief coda, I argue that 2666â€™s ethics and poetics require both an openness to the possibility of change and a commitment to oneâ€™s particular way of being.",ucb,,https://escholarship.org/uc/item/1b97k7sz,,,eng,REGULAR,0,0
718,2154,Beyond Choreia: Dance in Ancient Greek Literature and Culture,"Olsen, Sarah Elizabeth","Kurke, Leslie;",2016,"The chorus of Euripidesâ€™ Bacchae heralds the arrival of the god Dionysus by promising that â€œright away, the whole world will dance in a chorusâ€ (Î±á½Ï„Î¯ÎºÎ± Î³á¾¶ Ï€á¾¶ÏƒÎ± Ï‡Î¿ÏÎµÏÏƒÎµÎ¹, 114). Their exuberant claim reflects the enthusiasm for dance generally expressed in early Greek sources. Indeed, it has been well established that dance â€“ specifically choreia (communal song-dance) â€“ played a significant role in archaic and classical Greek social life and was thus accorded a high level of value and esteem in art and literature. My dissertation argues that this esteemed status does not extend to the performance of solo and individualized dance, and demonstrates that Greek literary discourse betrays a deep ambivalence towards dance (orchÄ“sis) when isolated from the multimedia art of choreia.This project thus approaches Greek dance, which has hitherto been studied almost exclusively in the context of the chorus, from a fresh angle. I establish that singular dancing often signifies disruption, violation, and vulnerability within the social and political order. At the same time, I show that the representation of individualized dance constitutes a distinctive mechanism adopted by poets, playwrights, historians and philosophers to foreground and explore the complex relationship between verbal and somatic expression. As a result, the representation of individualized dance in Greek literature offers insight into the place of dance in Greek thought, while also enabling us to identify the particular biases and agendas at work in the literary description of dance performance.My dissertation develops a distinctive methodology for analyzing the relationship between dance and literature. I begin from a basic conviction, grounded in the scholarship of dance studies, that verbal descriptions and literary representations of dance are not neutral reflections of embodied practices, but rather ideological and interpretive forms that work to frame and define our perception of dance. I argue that choreia, as a synthesis of vocal, instrumental, and kinetic expression, becomes an efficient image for poets, philosophers, and historians seeking to harness dance to the power of language. My work thus demonstrates that orchÄ“sis, as individual kinetic expression and kinesthetic experience, not only signifies social and political disruption, but is also imagined as an expressive mode that may resist or re-figure the forces of language and verbal description.My first chapter argues that individual dancers provide a critically engaged alternative to the prevailing model of communal, choral performance, which tends to be logocentric. This chapter lays out a dominant paradigm of choral dance as constructed in early Greek literature, offers a typology of solo and individualized dance forms, and previews the insights to be gained through the consideration of dance â€œbeyond choreia.â€ Chapter Two addresses the descriptions of both choral and individualized dance in Odyssey 8, demonstrating that singular and virtuosic dance is particularly emblematic of Phaeacian culture and that its description operates as a means by which Odysseus and Alcinous competitively negotiate their relative positions of status and authority within the poem. Chapters Three and Four examine individual male and female dancers respectively in epic, lyric, and drama, identifying a complex network of political and artistic concerns that coalesce around literary representations of each type of performer. I argue that solo male dancers tend to be depicted as disruptive and anti-social political actors (e.g., Pericles in Ion of Chios fr. 109 Leurini, Philocleon in Arist. Wasps 1474ff), while individual and outstanding female dancers are marked by their sexual appeal and consequent vulnerability (e.g., the maiden chorÄ“goi of Alcman 1 PMG, Cassandra in Eur. Troades 308ff). These chapters also focus on the performance contexts of specific songs and their ability to frame and define closely related instances of dance. My fifth and final chapter explores how Herodotus, Plato, and Xenophon deploy the various models of individual dance discussed in the preceding chapters in the service of their own historical and philosophical projects. While my primary focus throughout is on literary description, I also discuss the visual and material evidence for solo dance, particularly in cases where it contrasts with the textual tradition.The project as a whole makes two major contributions to the study of Greek literature, culture, and performance. First, it brings together the surviving representations of solo and individualized dance and considers them as evidence for the cultural discourse surrounding both orchÄ“sis and choreia. Second, it develops a theoretical framework for articulating the complex relationship between literary descriptions and historical performance, bringing the scholarly insights of dance studies to bear upon the ancient world.",ucb,,https://escholarship.org/uc/item/1bt36698,,,eng,REGULAR,0,0
719,2155,Enabling More Meaningful Post-Election Investigations,"Cordero, Arel Lee","Wagner, David A.;",2010,"Post-election audits and investigations can produce more transparent, trustworthy, and secure elections. However, such investigations are limited in cases by inadequate tools and methods, an absence of meaningful evidence, and high costs. In this dissertation, I address these concerns in the following three lines of research. First, I describe my research on verifiable and transparent random sample selection for post-election audits. I investigate how counties have typically approached random sample selection, and I analyze the implications and limitations of those approaches. I propose a sampling method that has since found use in counties across the country. Second, I describe a novel approach for logging events in direct recording electronic (DRE) voting systems. My approach gives investigators more meaningful evidence about the behavior of DREs on election day. In particular, I propose to record interactions between the voter and the voting machine such that they can be replayed by investigators while preserving the anonymity of the voter. Last, I describe a novel process for efficiently verifying elections that use optical scan voting systems. My process uses image superposition to let an investigator visualize the content of many ballot images simultaneously while  allowing individual treatment of anomalous ballots. I evaluate this process and demonstrate an order of magnitude improvement in the time it takes to inspect ballot images individually. This approach will let investigators more cost-effectively verify that all ballots have been accurately counted as intended by the voters.",ucb,,https://escholarship.org/uc/item/1d9083h2,,,eng,REGULAR,0,0
720,2156,Nested Narrative:ÃžÃ³rÃ°ar saga hreÃ°u and Material Engagement,"Ward, Elisabeth Ida","Lindow, John;",2012,"Nested Narrative:ÃžÃ³rÃ°ar saga hreÃ°u and Material EngagementÃžÃ³rÃ°ar saga hreÃ°u is an Icelandic saga, of the type usually referred to as Sagas of Icelanders or Family Sagas, that lacks much of the drama of other sagas of a similar type. The dissertation utilizes this genre outlier to test a new method for analyzing the sagas, which combines literary analysis with recent anthropological theory. This method foregrounds the interaction of the material world with the saga narrative as an essential way that meaning is constructed. Chapter 1 looks at the saga's physical manifestation in manuscripts, which offers a new emphasis on the saga as a product of a local community. Chapter 2 turns to the human-made objects referenced within the text, suggesting that the depiction of the material world in this saga is in keeping with the non-modern milieu from which it originated. Chapter 3 focuses on scenes in the saga where characters are described as moving through the landscape, and analyzes these as a way to demonstrate how co-constitutive the real landscape was for the saga narrative. Chapter 4 employs Cultural Memory theory to explain why certain placenames are included in the saga instead of others, noting that placename references are a clear invitation to include the real material world into the meaning-making of the saga. Chapter 5 looks at how the dynamic between the narrative of the saga and the local landscape of SkagafjÃ¶rÃ°ur as place has been disrupted in the modern period. The Conclusion offers a broad assessment of why repositioning the material world back into the interpretation of saga narratives is important.",ucb,,https://escholarship.org/uc/item/1g10j16f,,,eng,REGULAR,0,0
721,2157,"Community Genomic, Proteomic, and Transcriptomic Analyses of Acid Mine Drainage Biofilm Communities","Goltsman, Daniela","Banfield, Jillian F;",2013,"Culturing isolated microorganisms can be challenging, not only because usually the detailed environmental conditions where organisms grow optimally are not known, but also because many of them need to grow in the presence of other organisms. High-throughput sequencing and other `omics' technologies provide important approaches for the study of microorganisms in their natural environments. Specifically metagenomics methods enable culture-independent surveys of organisms and functions in microbial consortia, and can yield near-complete genomes of the most abundant community members, and partial genomes of lower abundance organisms. When coupled to community proteomic and/or transcriptomic analyses it is possible to predict what functions are being expressed within the community. Therefore, `omics' technologies provide a means for the study of community physiology and ecology in natural systems.Acid mine drainage (AMD) is a mining-related problem caused by sulfide mineral dissolution coupled to microbial iron oxidation, which leads to acidification and metal contamination of the environment. The Richmond Mine AMD community is currently the best-studied AMD system. Bacteria of the genus Leptospirillum, of the Nitrospira phylum, generally dominate Richmond Mine AMD microbial communities. Current studies show that Leptospirillum rubarum (group II) tends to dominate early-formed biofilms, and Leptospirillum group II 5way CG (a genotype related to L. rubarum) or L. ferrodiazotrophum (group III) increase in abundance as environmental conditions change. In chapter 1, community genomics was used to reconstruct the near-complete genome of Leptospirillum ferrodiazotrophum, and report the genome annotation and metabolic reconstruction of L. ferrodiazotrophum, Leptospirillum rubarum and an extrachromosomal plasmid associated to these bacteria. In addition, proteomic analyses were used to evaluate protein expression patterns in three AMD biofilms. Results indicate that, despite sharing only 92% identity at the 16S rRNA level, L. rubarum and L. ferrodiazotrophum share more than half of their genes. Both bacteria are motile, acidophilic iron-oxidizers, as evidenced by the presence of cytochrome Cyt572 and an electron transport chain. They are chemoautotrophs, using a reverse tricarboxilic acid (TCA) cycle for carbon fixation. Their metabolic potential indicates that L. rubarum and L. ferrodiazotrophum are capable of amino acid and vitamin biosynthesis, fatty acid biosynthesis, flagella biosynthesis, synthesis of polymers such as cellulose, and the synthesis of compatible solutes for osmotic tolerance. Only L. ferrodiazotrophum is capable of nitrogen fixation, although proteins were not detected by proteomics in the analyzed biofilms. Proteomic analyses indicate that core metabolic proteins are similarly expressed in both bacteria, however high expression of many hypothetical proteins unique to each Leptospirillum might contribute to their differentiation within the biofilms.In chapter 2, the partial genome reconstruction of a new Leptospirillum bacterium, which is closely related to L. ferrodiazotrophum, is reported. The bacterium represents ~ 3% of the sequenced community, and comparison of its 16S rRNA gene sequence with those of other Leptospirilli identifies it as a new group within the Leptospirillum clade: Leptospirillum group IV UBA BS. The bacterium grows in unusually thick, Archaeal-dominated biofilms where other Leptospirillum spp. are found at very low abundance. It shares 98% 16S rRNA sequence identity and 70% amino acid identity between orthologs with L. ferrodiazotrophum. Its metabolic potential indicates that it too is a motile, iron oxidizing chemoautotroph capable of nitrogen fixation, although nitrogen fixation expression was not observed. Leptospirillum group IV UBA BS is distinguished from the other Leptospirilli in that it contains a unique multicopper oxidase likely involved in iron oxidation, and the presence of two clusters of hydrogenase genes. The cytoplasmic hydrogenase is likely used to take up H2 during nitrogen fixation, while the membrane-bound hydrogenase might be involved in anaerobic H2 oxidation for energy generation. Community transcriptomic and proteomic analyses confirm expression of the multicopper oxidase, as well as the expression of many hypothetical proteins and core metabolic genes. Transcription of hydrogenases in the only biofilm in which the nitrogen fixation operon in L. ferrodiazotrophum is transcribed points to potential cooperative interactions between the two bacteria.AMD has long been considered a simple, low-diversity ecosystem. In chapter 3, a new view of the diversity of organisms in AMD was obtained by deep sequencing of the small subunit (SSU) rRNA from 13 biofilm communities. A total of 159 taxa, including Archaea, Bacteria, and Eukaryotes, were identified. Leptospirillum spp. dominate the samples, and members of diverse phyla, such as Actinobacteria, Acidobacteria, Firmicutes, Alpha-, Beta-, Gamma-, and Delta-Proteobacteria, Chloroflexi, and Deferribacter were present at low abundance. Interestingly, members related to Magnetobacterium spp. of the Nitrospira phylum were detected. These bacteria have not been reported present in AMD environments, and they have not been identified in community genomics datasets. However, the presence of magnetosome-like structures observed by cryo-TEM in some AMD biofilms supports the transcriptomics results. The findings indicate that it is dominance by a few taxa, and not lack of complexity of the system that has made AMD environments model systems for the study of microbial physiology and ecology.In Chapter 4, non-ribosomal transcriptomic reads were mapped to several genomes of AMD organisms, including Archaea, Bacteria, and viruses, in order to evaluate the expression profiles of genes and non-coding regions (ncRNAs) in biofilms at increasing stages of development. More than 95% of the genes in the most abundant Leptospirillum group II and group III bacteria were detected by at least one transcriptomic read, indicating that the whole genome is transcribed at some level. More than half of the genes in the Archaea G-plasma and Ferroplasma Type II, and a virus associated to Leptospirillum were also detected. Transposases, cytochromes, and ncRNAs were among the most highly expressed genes in all samples. Gene expression profiles indicate that Leptospirillum group II 5way CG and L. ferrodiazotrophum prefer growth at higher pH and lower temperature, conditions generally present in bioreactors, while the opposite is true for L. rubarum and G-plasma, who prefer conditions found in early to mid-developmental stage environmental biofilms. High levels of expression were observed for a novel ectoine riboswitch predicted in the Leptospirillum group II genome, as well as for other non-coding RNAs. Results provide new insight into understanding functioning and adaptation of acidic ecosystems.",ucb,,https://escholarship.org/uc/item/0zf9f3nx,,,eng,REGULAR,0,0
722,2158,Operationalizing Anticipatory Governance: Steering Emerging Technologies Towards Sustainability,"Philbrick, Mark","Winickoff, David E;",2010,"Technological innovation is a double-edged and contested arena.  On one hand, it has brought us global communications and unprecedented access to information for those connected to the Internet.  The last 100 years have seen the widespread deployment of household electricity, potable tap water, and a host of transportation options in the Global North.   Technologies allow us to manipulate matter at the subatomic level, and to observe the far reaches of the universe.  Humans have been to the moon, discovered life in the deep oceans, and nearly eradicated polio.  Clearly, technologies are a powerful force in the world, and innovation is seen as key to economic prosperity in the 21st century.On the other hand, climate change threatens the survival of many species, and the livelihoods of much of the future human population.  Further, it is far from alone in terms of problems to which large-scale technological deployment has contributed. Asbestos, DES, DDT, and endocrine disruptors are among the many technologies where some, perhaps many, of the negative human and environmental consequences that have ensued could conceivably have been mitigated.  Technological governance is clearly an area for possible improvement, and emerging technologies present a particularly attractive leverage point, as they have yet to develop substantial sociotechnical and institutional momentum.The dominant approach to technological governance in the U.S. is characterized by a combination of market forces, public support for basic science and targeted initiatives, and a ""science-based"" approach to risk assessment and regulation.  In recent years, the EU has emphasized the Precautionary Principle as an alternative governance basis, and there has been much debate about the respective merits of precaution and science.  This dissertation argues that much of that discourse misses a much larger point:  The prevailing approaches to the governance of emerging technologies in both the EU and the U.S. are inadequate, in that they are excessively focused on relatively narrow conceptions of risk, do not provide a coherent framework for considering risks, benefits, and distributional tradeoffs simultaneously, and tend more towards reactivity than proactivity, particularly in terms of the production of public goods.  These failings systematically produce a series of governance gaps in the context of a market economy.  Specifically, the rate of innovation tends to outstrip existing capacities for risk assessment, especially in the case of emerging fields such as nanotechnology.  Second, the capacity lag in oversight tends to undermine public confidence and trust.  Third, markets alone tend to underproduce public goods, a problem that is particularly acute in arenas with substantial environmental externalities.  Finally, relevant existing institutions in the U.S. generally lack a systematic capability to incorporate foresight into current policy-making in a meaningful way.This dissertation proposes a combination of the concepts of anticipatory governance and sustainability as a basis for addressing these governance gaps.  A strong theme of transatlantic translation runs throughout; many of the recent developments in technology assessment have occurred in Europe, and require substantial adaptation to function effectively in the American sociopolitical environment.  Anticipatory governance provides culturally appropriate philosophical underpinning and process; sustainability offers substantive direction.  The goal is not to develop overarching theory, but to operationalize these ideas, to put the combination into practice with respect to the governance gaps articulated above.  The empirical investigations of the first two gaps employ nanoscale technologies as cases to explore specific instances of the general question ""what do we need to anticipate"" with respect to risks and public perceptions, respectively.   The inquiries regarding the third and fourth gaps are more exploratory.  In terms of the production of public goods through innovation, how do the combination of historical patterns and market structures help demark the boundaries of a ""constructive intervention space"" for public investment?  With respect to institutional capacities, how can the combination of anticipatory governance and sustainability assist in evaluating current programs, and designing solutions for the future?Several conclusions with direct relevance to policy, strategy, and governance regarding emerging technologies result.  First, existing decision-making paradigms need improvement in order to consider risk-benefit tradeoffs adequately, and to provide guidance to actors on the ground in the prolonged absence of scientific and regulatory certainty.  Second, effective public engagement programs in the U.S. must complement and feed into existing structures of representative democracy, rather than attempting to circumvent or replace them.  Third, the purported ""Valley of Death"" between invention and market penetration is particularly acute with respect to the production of environmental public goods, as the barriers to entry in these sectors are a poor match for private funding incentives, implying that this is a constructive area for increased levels of public intervention.  Finally, the combination of anticipatory governance and sustainability provides a framework that highlights the fragmented nature of U.S. policy responses to the problem of technological governance, and does indeed provide a solid foundation for the design of future institutions, while recognizing that their implementation will be dynamic, contested, and theoretically impure.",ucb,,https://escholarship.org/uc/item/1053909n,,,eng,REGULAR,0,0
723,2159,Framing the Gap: Education Reform and Conceptions of Racial Equity,"Delgado, Chela Myesha","Nasir, Na'ilah;",2013,"Abstract:Framing the GapbyChela Myesha DelgadoDoctor of Philosophy in EducationUniversity of California, BerkeleyProfessor Na'ilah Nasir, Chair	During her 2006 speech to the American Educational Research Association, educational researcher and then-AERA president Gloria Ladson-Billings called into question ""the wisdom of focusing on the achievement gap as a way of explaining and understanding the persistent inequality that exists (and has always existed) in our nation's schools"" (Ladson Billings 2006). What for so long had been considered a `gap', Ladson-Billings posited, could be more accurately considered a debt owed to children of color by schools. Ladson-Billings' reframing of the gap as a debt insisted on a recognition of the historical and contemporary institutional racism of schooling. Calls to re-frame the gap around the effects of structural racism and examine the culpability of schools and schooling systems as a factor of racial inequity, however, have gone largely unheeded. Instead, a majority of national education reform efforts insist on additional academic and behavioral effort on the part of teachers and students as the most effective means to raise the achievement of students of color (Apple 2001, Leonardo 2007, Darling-Hammond 2010, Tyack 1995, Valencia 1997 and 2010). 	The disconnect between the marketized trend in national education reforms and grounded in a vision of racial justice begged the following research questions:*	How does the public understand the achievement gap as a representation of racial inequity in education?*	How do frames of the achievement gap shape national perceptions of racial inequality?*	What are the possibilities and limitations of these frames for capturing the historical and contemporary roots of racial inequality in schools?*	 Which frames have been most operationalized in education reform and why?*	What are the opportunities and limitations for parents and youth of color to critique and transform dominant frames of the achievement gap?I sought to answer these research questions through a combination of methods: the first was a critical discourse analysis of mainstream representations of the achievement gap. I read and synthesized over 600 articles, reports, policy briefs and speeches to assess the primary frames through which the achievement gap was discussed.  To fully understand the ways in which education reform leaders discuss and understand the racial achievement gap, my analytical method of analyzing frames was key. The second research method was a four year ethnography of the 2008-2012 ""Voices of the Next Generation"" campaign of the San Francisco-based Coleman Advocates, a membership-based community organization of Pacific Islander, Latino and African-American parents and students.  Coleman's local organizing work stands in contrast to both mainstream education reform and the media-generated national picture of parents of color--and particularly Black parents--either supporting conservative trends in education or showing little interest in their children's education (Pedroni 2007).	My findings show that education reforms intended to narrow or close the achievement gap are grounded in a wide spectrum of diagnostic frames (Goffman 1976, Snow & Benford 2000) --attempts to `diagnose' or explain the achievement gap, and prognostic frames--attempts to solve the gap. In my first chapter, I explain how the gap is measured framing theory as a method of discourse analysis I employ in order to understand dissonant approaches of education reformers in regard to the gap. Chapter two examines the history and origins of racialized educations assessment and the phenomenon of the achievement gap, along with its concurrent diagnostic frames. In Chapter three, I write about the dominant frame in educational reforms designed to close the gap--that of the market. Chapter four details the civil rights and racial justice frames, which seek to recenter racism as an active cause of the gap, and chapter five examines Coleman Advocates' on-the-group attempt to implement an education campaign in San Francisco, California, using the racial justice lens.	Finally, I assessed the tensions of these frames. To gain credibility, market-based education reformers have sought to align themselves with parents of color (Hursch 2007, Kumashiro 2008, Pedroni 2007, Stulberg 2008). Underneath the surface, however, it is clear that Black and Latino parents have been more concerned with racial equity in schooling than enamored with standardized tests or charter schools (Mediratta 2001, 2002, 2009; Oakes 2006; Research for Action 2002). As community-based education organization scholar Kavitta Mediratta finds, school reform in the hands of Black and Latino parents often includes demands for culturally relevant curriculum; an end to the system of tracking; mandatory parent communication protocols for teachers; increased teacher quality, improved access for newcomer and special education students to college prep courses; progressive discipline systems; and replacing standardized testing completely with culturally relevant, authentic assessments (field notes, February 7, 2009). Because Coleman is an organized vehicle through which the voices of those most affected by both the achievement gap and education reforms can be heard, the group's work provides a unique opportunity to study and assess an ""on the ground"" effort to redefine the terms of the educational debate and to win concrete reforms. My research overall seeks to interrogate the commonsense term ""achievement gap""; expose the racial framings that shape reform; reexamine the national forces of market-based education; and explore the complexities and challenges of promoting racial justice-based approaches to education reform.",ucb,,https://escholarship.org/uc/item/0tg797bn,,,eng,REGULAR,0,0
724,2160,Public Opinion and Political Representation,"Broockman, David E.","Schickler, Eric;",2015,"This dissertation considers the relationship between the opinions voters have on issues and the positions politicians take on them. The first chapter makes a methodological intervention into existing literature, showing that to understand these relationships we must examine one issue at a time, not boil down the preferences of voters and politicians to summaries of their ideologies. It then considers some implications of this distinction. The second chapter elaborates one of these implications, the implications of polarization for representation. This chapter argues for a different set of implications than is typically drawn. The final chapter then adopts this approach to bring a new perspective to a neglected question: how do politicians see their constituents? By investigating this question in individual issues, the final chapter illustrates the utility of the approach and raises new questions for scholars to consider.",ucb,,https://escholarship.org/uc/item/0vg8049w,,,eng,REGULAR,0,0
725,2161,Variations on Quantum Geometry,"McCurdy, Shannon","Zumino, Bruno;",2013,"This dissertation explores various aspects of quantization and geometry. In particular, we analyze the ground states of a two-dimensional sigma-model whose target space is an elliptically fibered K3, with the sigma-model compactified on S1 with boundary conditions twisted by a duality symmetry. We show that the Witten index receives contributions from two kinds of states: (i) those that can be mapped to cohomology with coefficients in a certain line bundle over the target space, and (ii) states whose wave-functions are localized at singular fibers. We also discuss the orbifold limit and possible connections with geometric quantization of the target space. We also provide a deformation quantization approach for differential forms on symplectic manifolds. After a description of the Z-graded differential Poisson algebra, we introduce a covariant star product for exterior differential forms and give an explicit expression for it up to second order in the deformation parameter, in the case of symplectic manifolds. The graded differential Poisson algebra endows the manifold with a connection, not necessarily torsion-free, and places upon the connection various constraints.",ucb,,https://escholarship.org/uc/item/0x37w7dp,,,eng,REGULAR,0,0
726,2162,Evolutionary constraints on the sequence of Ras,"Bandaru, Pradeep","Kuriyan, John;",2017,"AbstractEvolutionary constraints on the sequence of RasbyPradeep BandaruDoctor of Philosophy in Molecular and Cellular BiologyUniversity of California, BerkeleyProfessor John Kuriyan, ChairRas proteins are highly conserved signaling molecules that exhibit regulated, nucleotide-dependent switching between an active GTP-bound state that transduces signals by binding to effector proteins, and an inactive GDP-bound state that cannot bind to effectors. The high conservation of Ras requires mechanistic explanation, especially given that proteins are generally robust to mutation, a concept that was first established from early structural and phylogenetic analysis of hemoglobin by Max Perutz and John Kendrew.During my thesis research, I adapted a two-hybrid selection system to analyze how mutations affect the functional cycle of human H-Ras, with the ultimate goal of understanding the constraints on the sequence of Ras that give rise to its high evolutionary conservation. My strategy was to isolate just the minimal biochemical network that defines this cycle, comprising Ras, its effector Raf, a GTPase accelerating protein (GAP), and a guanine-nucleotide exchange factor (GEF). Using this selection system, I analyzed the sensitivity of every residue in Ras to mutation in the context of this network, while excluding the effects of the membrane and additional regulatory factors. This approach provided an opportunity, for the first time, to use deep mutational scanning approaches to study how local regulatory networks influence the mutational sensitivity and phenotypic plasticity of key signaling molecules.I found that Ras exhibits global sensitivity to mutation when regulated by a GAP and a GEF, effectively displaying global constraints that result in the majority of mutations leading to a modest decrease in Ras function. In the absence of regulators, Ras shows considerable tolerance to mutation, as seen previously in saturation mutagenesis experiments on other proteins, where the distribution of mutational effects shifted to be largely neutral. Surprisingly, the analysis of Ras in the absence of regulators also revealed allosteric hotspots of activating mutations in residues that restrain Ras dynamics and promote the inactive, GDP-bound state. This showed that structural fold of Ras is intrinsically capable of accommodating sequence changes that, in evolution, could lead to the acquisition of new function, but could also lead to unwanted Ras activation in disease. Indeed, oncogenic mutations that disturb the switching mechanism of Ras result in aberrant signaling and cancer, highlighted by the fact that Ras is one of the most important proto-oncogenes in the human genome.Altogether, my research shows that the local regulatory network places a stringent constraint on the sequence of Ras, and also creates the potential conditions in which it is susceptible to activating mutations. This extended previous observations that mutational sensitivity in proteins is strongly dependent on the selective conditions in which the protein operates. From a practical perspective, though small molecule inhibitors of Ras have yet to achieve clinical relevance despite a concerted effort to obtain such inhibitors, my research also provides a roadmap of allosteric hotspots of Ras activation that can be exploited to design novel cancer therapeutics.",ucb,,https://escholarship.org/uc/item/0sc6g33x,,,eng,REGULAR,0,0
727,2163,Development of complex sound representations in the primary auditory cortex,"Insanally, Michele Nerissa","Bao, Shaowen;",2011,"Development of complex sound representations in the primary auditory cortexbyMichele Nerissa InsanallyDoctor of Philosophy in NeuroscienceUniversity of California, BerkeleyProfessor Shaowen Bao, PhD., ChairThe brain has a tremendous ability to change as a result of experience; this property is known as plasticity. Our mastery of soccer, rhetoric, agriculture and instrumentation are all learned skills that require experience. While the brain is plastic throughout life, during early development, the brain demonstrates a heightened sensitivity to experience. This unique epoch during development in which the brain is particularly susceptible to change is called a critical period. During the critical period, sensory experience results in significant modifications in structure and function. The set of studies described in this dissertation aim to investigate how complex sound representation develops during the critical period in the rat primary auditory cortex. Previous examinations of the critical period in the auditory cortex have typically used simple tonal stimuli. Repeated exposure of rat pups to a tone, for instance, has been shown to selectively enlarge cortical representation of the tone and alter perceptual behaviors. However, probing cortical plasticity with a single-frequency tone might not reveal the full complexity and dynamics of critical period plasticity. After all, natural, biologically important sounds are generally complex with respect to their spectrotemporal properties. Natural sounds often have frequencies that vary in time and amplitude modulation. Psychophysical studies indicate that early experience of complex sounds has a profound impact on auditory perception and perceptual behaviors. Experience with speech, for instance, shapes language-specific phonemic perception, enhancing perceptual contrasts of native speech sounds and reducing perceptual contrasts of some foreign speech sounds. At the electrophysiological level, auditory cortical neurons preferentially respond to certain complex sounds, such as species-specific animal vocalizations. It is unclear how such selectivity for a complex sound emerges, and whether it is innate or shaped by early experience.In order to address this question, we exposed rat pups to a frequency-modulated (FM) sweep in different time windows during early development, and examined the effects of such sensory experience on sound representations in the primary auditory cortex (AI). We found that early exposure to an FM sound resulted in altered characteristic frequency representations and broadened spectral tuning in AI neurons. In contrast, later exposure to the same sound only led to greater selectivity for the sweep rate and direction of the experienced FM sound. These results indicate that cortical representations of different acoustic features are shaped by complex sounds in a series of distinct critical periods.Next, we confirmed this model of brain development in a set of experiments that examine how exposure to noise affects these various critical periods. We examined the influence of pulsed noise experience on the development of sound representations in AI. In naÃ¯ve animals, FM sweep direction selectivity depends on the characteristic frequency (CF) of the neuron--low CF neurons tend to select for upward sweeps and high CF neurons for downward sweeps. Such a CF dependence was not observed in animals that had received weeklong exposure to pulsed noise in periods from postnatal day 8 (P8) to P15 or from P24 to P39. In addition, AI tonotopicity, tuning bandwidth, intensity threshold, tone-responsiveness, and sweep response magnitude were differentially affected by the noise experience depending on the exposure time windows. These results are consistent with previous findings of feature-dependent multiple sensitive periods. The different effects induced here by pulsed noise and previously by FM sweeps further indicate that plasticity in cortical complex sound representations is specific to the sensory input.Identifying how the developing brain processes sensory information provides a foundation for understanding more complex behaviors. These results advance our understanding of the neuronal mechanisms underlying sensory development and language learning. Specifically, they elucidate the age-dependent effects of complex sound exposure on spectral tuning and complex sound representation in the rat primary auditory cortex. In addition, they provide a foundation for subsequent studies investigating the neural basis of language development.",ucb,,https://escholarship.org/uc/item/0nq1b9n8,,,eng,REGULAR,0,0
728,2164,Light-matter Interaction in Deep Sub-wavelength Nano-photonic Structures,"Pholchai, Nitipat","Zhang, Xiang;",2012,"This dissertation focuses on the use of deep sub-wavelength(sub-ï¬) nano-photonic structures to enhance radiation of optical emitters. The deep sub-wavelength designs are based on high permittivity contrast of materials involving either purely dielectric interfaces or metal. The optical properties of metal at infrared and optical frequencies enable optical structures that can confine light to dimension smaller than the wavelength. The building blocks of these nano-photonic system are non-resonant, broadband waveguides with dramatic field confinement in the nano-scale low permittivity region. Strong interaction and enhanced radiation leads to efficient coupling into the primary optical mode of the structures which improves fluorescence brightness, saturation, speed, emission efficiency, single photon fidelity at a single emitter-single photon level and holds promise for solid-state lighting, molecular sensing, and quantum information processing application.             The first part of the dissertation explores deep sub-wavelength waveguiding structures as non-resonant optical component that enhances radiation and collects emitted photon with high fidelity. The last part explores the design of small resonator that is constructed from a subwavelength waveguide for use as addressing optical emitters. The benefits of non-resonant design are highlighted throughout.",ucb,,https://escholarship.org/uc/item/0qp0b1tg,,,eng,REGULAR,0,0
729,2165,Neural Mechanisms of Perceptual Learning,"Rokem, Ariel Shalom","Silver, Michael A;",2010,"Perceptual learning is a pervasive and specific improvement in the performance of a perceptual task with training. This dissertation examines the role of the neurotransmitter acetylcholine(ACh) in perceptual learning in a series of behavioral and pharmacological studies in healthy human subjects.  ACh plays a role in cognitive functions such as attention and in animal models it has been found to play a role in the facilitation of neural plasticity.The work described here focused on the learning of a visual motion direction discrimination task. In the first study described, I provide a theoretical framework for the study of learning of this task. This part examined the ""oblique effect"", an advantage in performing this task when stimuli are presented in cardinal, rather than oblique directions. I present both experimental evidence and a population coding model that indicate the oblique effect in behavior may rely on the unequal representation of oblique and cardinal directions in visual areas in cortex. The model suggests that the oblique effect relies on an interplay of this representation with the decoding of the stimulus in higher cortical regions.In the second part of this thesis, participants were administered the cholinesterase inhibitor donepezil while training on the motion direction discrimination task, performed in oblique directions. As previously described, this training abolishes the behavioral oblique effect. Moreover, donepezil increased the effects of training on performance and the specificity of these effects to the oblique direction and the visual field location in which learning took place, suggesting that ACh directs learning towards cells encoding behaviorally relevant features of the stimulus.The third part presents a study investigating the role of ACh in the allocation of voluntary visual spatial attention (which can be allocated in a goal-oriented manner) and involuntary attention (which is automatically captured by salient events). We used an anti-predictive spatial cueing task to assess the effects of pharmacological enhancement of cholinergic transmission on behavioral measures of voluntary and involuntary attention. We found that cholinergic enhancement with donepezil augments the benefits of voluntary attention but does not affect involuntary attention, suggesting that they rely on different neurochemical mechanisms.Taken together, the results of the second and third parts of this thesis provide converging evidence for a potential mechanism of learning: ACh mediates the allocation of voluntary attention, which in turn provides a necessary substrate for learning to occur.",ucb,,https://escholarship.org/uc/item/0qx1c2mx,,,eng,REGULAR,0,0
730,2166,Neural Representation Learning with Denoising Autoencoder Framework,"Thanapirom, Chayut","DeWeese, Michael R;",2016,"Understanding of how the brain works and how it can solve difficult problems like image recognition is very important, especially for the progress in developing an autonomous intelligent system. Even though we have a lot of experimental data in neuroscience, we are lack of theories which can glue all the observations together. One approach to understand the brain is to investigate the representation of sensory information in the brain at each stage, and try explains it with some computational level principle, for example an efficient coding principle. This thesis follows this approach.In this thesis, I use the denoising autoencoder framework to approach two unsolved problems in Computational Neuroscience. The first problem is learning the group structure in the group sparse coding model. I propose that it is possible to learn the group structure using gradient descent with a data denoising objective function. To verify the method, I train a model on the van Hateren's natural image dataset. The model with the learned group structure shows an improvement in denoising performance 15% (SNR) over the regular sparse coding model. Moreover, the group structure learned groups together sparse coding basis functions with similar location, orientation and scale.The second problem is to understand why we have grid cells. I proposed that a population of place cells and grid cells should be modeled as an attractor network with noisy neurons. Furthermore, this attractor network can be trained with the denoising autoencoder framework to memorized a location in 1D space (simplification of the actual problem where the location is in 2D space). I show that the retrieved location accuracy of the network with both place cells and grid cells is higher than the network with place cells alone. The performance difference is due to the activity of the grid cells acts as an efficient error-correcting code.",ucb,,https://escholarship.org/uc/item/0hm6p6s5,,,eng,REGULAR,0,0
731,2167,Nonzero degree maps between three dimensional manifolds,"Liu, Yi","Agol, Ian;",2012,"The main result of this dissertation shows that every orientable closed 3-manifold admits a nonzero degree map onto at most finitely many homeomorphically distinct non-geometric prime 3-manifolds. Furthermore, for any integer d > 0, every orientable closed 3-manifold admits a map of degree d onto only finitely many homeomorphically distinct 3-manifolds. This answers a question of Yongwu Rong. The finiteness of JSJ piece of the targets under nonzero degree maps was known earlier by the results of Soma and Boileauâ€“Rubinsteinâ€“Wang, and a new proof is provided is this dissertation. We also prove analogous results for dominations rela-tive to boundary. As an application, we describe the degree set of dominations onto integral homology 3-spheres.",ucb,,https://escholarship.org/uc/item/0jj5791w,,,eng,REGULAR,0,0
732,2168,Connecting the Dots. Intelligence and Law Enforcement since 9/11,"Stalcup, Mary Margaret","Rabinow, Paul;",2009,"This work examines how the conceptualization of knowledge as both problem and solution reconfigured intelligence and law enforcement after 9/11. The idea was that more information should be collected, and better analyzed. If the intelligence that resulted was shared, then terrorists could be identified, their acts predicted, and ultimately prevented. Law enforcement entered into this scenario in the United States, and internationally. ""Policing terrorism"" refers to the engagement of state and local law enforcement in intelligence, as well as approaching terrorism as a legal crime, in addition to or as opposed to an act of war. Two venues are explored: fusion centers in the United States and the international organization of police, Interpol. The configuration can be thought of schematically as operating through the set of law, discipline and security. Intelligence is predominantly a security approach. It modulates that within its purview, wielding the techniques and technologies that are here discussed.The dissertation is divided into two sections: Intelligence and Policing Terrorism. In the first, intelligence is taken up as a term, and its changes in referent and concept are examined. The Preface and Chapter One present a general introduction to the contemporary situation and intelligence, via Sherman Kent, as knowledge, organization and activities. Chapter Two traces the development of intelligence in the United States as a craft and profession. Chapter Three discusses some of the issues involving the intersection of intelligence and policy, and how those manifested in the aftermath of 9/11 and the lead up to the 2003 invasion of Iraq. The second section examines the turn to policing terrorism, beginning, in Chapter Four, with how Interpol has dealt with bioterrorism, and an examination of the shifting conceptualization of biological threats in international law. Moving from threats to their consequences, Chapter Five takes up the concept of an event in order to analyze the common comparison of Pearl Harbor and 9/11. Chapters Six and Seven turn to fieldwork done in the United States, with an examination of the suspicious activity reporting system and law enforcement's inclusion in the Information Sharing Environment, focusing on fusion centers and data mining.",ucb,,https://escholarship.org/uc/item/0jz201mh,,,eng,REGULAR,0,0
733,2169,Understanding Magnetism in Multiferroics,"Holcomb, Mikel Barry","Ramesh, Ramamoorthy;",2009,"This dissertation details the study of electric and strain control of antiferromagnetism and ferromagnetism at room temperature using multiferroic BiFeO3 thin films. Piezoelectric force microscopy and photoemission electron micrscopy techniques were used to correlate ferroelectric and antiferromagnetic domains. An angledependent dichroism intensity formula was established and applied to determine magnetic behavior. The effects of thickness and orientation on ferroelectric and magnetic properties in BiFeO3 films were studied. These results were used in the making of a magnetoelectric-ferromagnet heterostructure in which electrically assisted exchange bias was observed. This toolbox of control parameters gives a very strong benefit for the making of new and improved devices, particularly in the computing industry where the traditional magnetic field controlled devices are reaching their limits.",ucb,,https://escholarship.org/uc/item/0kc774qw,,,eng,REGULAR,0,0
734,2170,Three-dimensional Seismic Analysis of Reinforced Concrete Wall Buildings at Near-fault Sites,"Lu, Yuan Jie","Panagiotou, Marios;",2014,"This dissertation approaches the subject of three-dimensional (3D) seismic analysis of reinforced concrete (RC) wall buildings at near-fault sites by first studying two main problems separately: (1) the characterization of base excitation for buildings located at near-fault sites, and (2) modeling the behavior of RC buildings accurately including inelastic behavior and the failure mode. The dissertation culminates with the 3D response history analysis of two 20-story RC core wall buildings models, including the slabs and columns, subject to a strong near-fault ground motion record.First, the presence and characteristics of multiple pulses [with dominant period TP between 0.5 and 12 s] in historical near-fault ground motion records is studied. An iterative method for extracting multiple strong pulses imbedded in a ground motion is presented. The method is used to extract multiple strong velocity pulses from the fault-normal horizontal component of 40 pulse-like ground motion records from 17 historical earthquakes, with magnitudes ranging from MW6.3 to MW7.9, recorded at a distance less than 10 km from the fault rupture with a peak ground velocity greater than 0.6 m / s. The relationships between the dominant period of the extracted pulses, associated amplitudes, and earthquake magnitude are presented, indicating that the amplitude of the strongest pulses with 1.5 s â‰¤ TP â‰¤ 5 s, does not depend significantly on the earthquake magnitude. Next, the effect of soil-foundation-structure interaction (SFSI) for a 20-story core wall building with a caisson foundation subject to single pulse motions is investigated using two-dimensional (2D) nonlinear finite-elements and fiber beam-column elements; nonlinear site effects on the free-field motion and structural response is discussed. The nonlinear site effects for free-field motions result in a de-amplification of peak surface acceleration due to soil yielding, and a maximum of 64% amplification of peak acceleration and velocity of at specific pulse periods for deep soils. SFSI, after removing the nonlinear site effect, has a negligible effect on the maximum value of peak roof acceleration and peak roof drift ratio over the pulse periods considered; however, the effect of the increased flexibility due to SFSI is observed in the peak drift ratio and peak base shear response.A couple of chapters of this dissertation are dedicated to the development and verification of a three-dimensional nonlinear cyclic modelling method for non-planar reinforced concrete walls and slabs. This modeling approached - called the beam-truss model (BTM) - consists of (i) nonlinear Euler-Bernoulli fiber-section beam elements representing the steel and concrete in the vertical and horizontal direction, and (ii) nonlinear trusses representing the concrete in the diagonal directions. The model represents the effects of flexure-shear interaction (FSI) by computing the stress and strains in the horizontal and vertical directions and by considering biaxial effects on the behavior of concrete diagonals. In addition, the BTM explicitly models diagonal compression and tension failures (shear failures) under cyclic or dynamic loading. The BTM is first validated by comparing the experimentally measured and numerically computed response of eight RC walls subjected to static cyclic loading, including two non-planar RC walls under biaxial cyclic loading. Then, the BTM is extended to modeling slabs and validated with a two-bay slab-column specimen. Finally, the BTM is validated by comparing the experimentally measured and numerically computed response and failure mode of a 5-story coupled wall RC building under seismic base excitation.The final chapter presents the 3D response history analysis of two 20-story RC core wall buildings subject to a strong near-fault ground motion record. The 20-story building model includes the RC core wall, post-tensioned slabs, and columns; the core wall and slabs are modeled using the developed BTM while the columns are modeled with fiber-section beam-column elements. The two 20-story RC core wall buildings considered have similar geometry: one is conventionally designed to develop plastic hinging at the base of the core-wall, and the second is designed with a damage resistant structural system that combines two seismic isolation planes. Analysis is conducted using the two horizontal components of the historical TCU52 ground motion recorded 0.7 km from the fault plane of the MW7.6 1999 Chi-chi, Taiwan earthquake. The seismic response and damage of the two buildings is discussed.",ucb,,https://escholarship.org/uc/item/0kq1q3kq,,,eng,REGULAR,0,0
735,2171,Ideologies of Pure Abstraction,"Kim, Amy Chun","Davis, Whitney;",2015,"This dissertation presents a history of the development of abstract art in the 1920s and 1930s, the period of its expansion and consolidation as an identifiable movement and practice of art. I argue that the emergence of the category of abstract art in the 1920s is grounded in a voluntaristic impulse to remake the world. I argue that the consolidation of abstract art as a movement emerged out of the Parisian reception of a new Soviet art practice that contained a political impetus that was subsequently obscured as this moment passed. The occultation of this historical context laid the groundwork for the postwar â€œmultiplicationâ€ of the meanings of abstraction, and the later tendency to associate its early programmatic aspirations with a more apolitical mysticism.Abstraction has a long and varied history as both a conceptual-aesthetic practice and as an ideal. In the first chapter, I provide a conceptual overview of the terms used by abstract artists and their contemporaries, as well as provide a historicization of the meaning of pure abstraction in terms of the relationship of modernism to its own eighteenth century beginnings and antiquity. The second chapter focuses on the â€œSoviet momentâ€ of pure abstraction by looking at the Soviet contributionsâ€”primarily Konstantin Melnikovâ€™s pavilionâ€”to the 1925 Exposition International des Arts DÃ©coratifs et Modernes in Paris and their enthusiastic reception. The third chapter continues the examination of pure abstraction but in the context of the Parisian art world. It begins with an examination of the Lâ€™Art dâ€™Auâ€™jourdâ€™hui exhibit of December 1925 and the two paintings Mondrian contributed to it. I seek to demonstrate that while Mondrianâ€™s practice cannot be assimilated to the revolutionary aesthetics of the previous chapter, it was, nevertheless fundamentally connected to a certain vision of capitalism as a problem of everyday life. I argue that it is within the historical context of a dialectic between a â€œSoviet momentâ€ and a Parisian experience of daily life that the rise and fall of pure abstraction should be understood. In the final chapter, I present the work of Jean HÃ©lion, a young, committed French painter, whose trajectory from geometric to figural abstraction provides an understanding of the aesthetic and political impasses, as well as defeats, of the period, a case that casts an unsettling light on the entire adventure of pure abstraction.",ucb,,https://escholarship.org/uc/item/0m64w57q,,,eng,REGULAR,0,0
736,2172,Virulence Factor Regulation in Listeria monocytogenes,"Portman, Jonathan Lewis","Portnoy, Daniel A;",2017,"Listeria monocytogenes is a Gram-positive intracellular pathogen that is readily amenable to genetic manipulation and for which there are excellent in vitro and in vivo virulence models. These attributes have allowed a thorough examination of the molecular underpinnings of L. monocytogenes pathogenesis, however, there are still a number of major unresolved questions that remain to be answered. For example, it has been known for many years that L. monocytogenes rapidly changes its transcriptional profile upon access to the host cytosol, however the host cues and bacterial components that are involved in driving this change have remained continually unanswered. One large piece of evidence came when the long-sought co-factor for the primary virulence regulator, PrfA, was discovered to be the antioxidant tripeptide, glutathione. Glutathione was demonstrated to play a crucial role in the activation of PrfA in vivoâ€” a finding that has since led to two important discoveries that are described herein. First, the activation of PrfA in vitro requires both exogenous glutathione and a metabolic licensing step that can be recapitulated by a chemically defined synthetic media. Second, glutathione also functions as a post-translational regulator of the pore-forming virulence factor, Listeriolysin O (LLO), by reversibly binding via an S-glutathionylation reaction and preventing membrane association of the LLO monomers. These discoveries elucidate numerous regulatory roles for glutathione during infection and describe how L. monocytogenes is able to sense and respond to critical host compartments to mount a successful infection.Upon entry to the host cell cytosol, the facultative intracellular pathogen Listeria monocytogenes coordinates the expression of numerous essential virulence factors by allosteric binding of glutathione (GSH) to the Crp-Fnr family transcriptional regulator, PrfA. Here we report that robust virulence gene expression can be recapitulated by growing bacteria in a synthetic medium (iLSM) containing GSH or other chemical reducing agents. Bacteria grown under these conditions were 45-fold more virulent in an acute murine infection model and conferred greater immunity to a subsequent lethal challenge compared to bacteria grown in conventional media. During cultivation in vitro, PrfA activation was completely dependent on intracellular levels of GSH, as a glutathione synthase mutant (âˆ†gshF) was activated by exogenous GSH but not reducing agents. PrfA activation was repressed in iLSM supplemented with oligopeptides, but suppression was relieved by stimulation of the stringent response. These data suggest that cytosolic L. monocytogenes interpret a combination of metabolic and redox cues as a signal to initiate robust virulence gene expression in vivo.Cholesterol-dependent cytolysins (CDCs) represent a family of homologous pore-forming proteins secreted by many Gram-positive bacterial pathogens. CDCs mediate membrane binding partly through a conserved C-terminal undecapeptide, which contains a single cysteine residue. While mutational changes to other residues in the undecapeptide typically have severe effects, mutating the cysteine residue to alanine has minor effects on overall protein function. Thus, the function of this highly conserved reactive cysteine residue remains largely unknown.  We report here that the CDC Listeriolysin O (LLO), secreted by the facultative intracellular pathogen Listeria monocytogenes, was post-translationally modified by a S-glutathionylation at this conserved cysteine residue, and that either endogenously synthesized or exogenously added glutathione was sufficient to form this modification. When recapitulated with purified protein in vitro, this modification completely ablated the activity of LLO, and this inhibitory effect was fully reversible by treatment with reducing agents. A cysteine-to-alanine mutation in LLO rendered the protein completely resistant to inactivation by S-glutathionylation and retained full hemolytic activity. A mutant strain of L. monocytogenes expressing the cysteine-to-alanine variant of LLO was able to infect and replicate within bone marrow-derived macrophages indistinguishably from wild-type in vitro, yet was attenuated 4-6 fold in a competitive murine infection model in vivo.  This study suggests that S-glutathionylation may represent a mechanism by which CDC family proteins are post-translationally modified and regulated, and help explain an evolutionary pressure behind the highly conserved undecapeptide cysteine.",ucb,,https://escholarship.org/uc/item/00s2k66d,,,eng,REGULAR,0,0
737,2173,Spatial Data Science for addressing environmental challenges in the 21st century,"Palomino, Jenny Lizbeth","Kelly, Maggi;",2018,"The year 2005 sparked a geographic revolution through the release of Google Maps, arguably the first geographic tool to capture public interest and act as a catalyst for neogeography (i.e. the community of non-geographers who built tools and technologies without formal training in geography). A few years later, in 2008, the scientific community witnessed another major turning point through open access to the Landsat satellite archive, which had been collecting earth observation data since 1972. These moments were critical starting points of an explosion in geographic tools and data that today remains on a rapid upward trajectory. In more recent years, new additions in data and tools have come from the Free and Open Source Software (FOSS), open and volunteered data movements, new data collection methods (such as unmanned aerial vehicles, micro-satellites, real-time sensors), and advances in computational technologies such as cloud and high performance computing (HPC). However, within the broader Data Science community, specific attention was often not given to the unique characteristics (e.g. spatial dependence) and evolutions in geospatial data (e.g. increasing temporal/spatial resolutions and extents). Beginning in 2015, researchers such as Luc Anselin as well as others who had been developing geospatial cyber-infrastructure (CyberGIS) since 2008 began to call for a Spatial Data Science, a field that could leverage the advances from Data Science, such as data mining, machine learning, and other statistical and visualization â€˜bigâ€™ data techniques, for geospatial data. New challenges have emerged from this rapid expansion in data and tool options: how to scale analyses for â€˜bigâ€™ data; deal with uncertainty and quality for data synthesis; evaluate options and choose the right data or tool; integrate options when only one will not suffice; and use emerging tools to effectively collaborate on increasingly more multi-disciplinary and multi-dimensional research that aims to address our current societal and environmental challenges, such as climate change, loss of biodiversity and natural areas, and wildfire management.This dissertation addresses in part these challenges by applying emerging methods and tools in Spatial Data Science (such as cloud-computing, cluster analysis and machine learning) to develop new frameworks for evaluating geospatial tools based on collaborative potential and for evaluating and integrating competing remotely-sensed map products of vegetation change and disturbance.  In Chapter One, I discuss in further detail the historical trajectory toward a Spatial Data Science and provide a new working definition of the field that recognizes its interdisciplinary and collaborative potential and that serves as the guiding conceptual foundation of this dissertation. In Chapter Two, I identify the key components of a collaborative Spatial Data Science workflow to develop a framework for evaluating the various functional aspects of multi-user geospatial tools. Using this framework, I then score thirty-one existing tools and apply a cluster analysis to create a typology of these tools. I present this typology as the first map of the emergent ecosystem and functional niches of collaborative geospatial tools. I identify three primary clusters of tools composed of eight secondary clusters across which divergence is driven by required infrastructure and user involvement. I use my results to highlight how environmental collaborations have benefited from these tools and propose key areas of future tool development for continued support of collaborative geospatial efforts.  In Chapters Three and Four, I apply Spatial Data Science within a case study of California fire to compare the differences as well as explore the synergies between the three remotely-sensed map products of vegetation disturbance for 2001-2010: Hansen Global Forest Change (GFC); North American Forest Dynamics (NAFD); and Landscape Fire and Resource Management Planning Tools (LANDFIRE). Specifically, Chapter Three identifies the implications of the differing creation methods of these products on their representations of disturbance and fire. I identify that LANDFIRE (the traditional created product that integrates field data and public data on disturbance events with remote sensing) reported the highest amount of vegetation disturbance across all years and habitat types, as compared to GFC and NAFD, which are both produced from automated remote sensing analyses. I also find that these differences in reported disturbance are driven by differential inclusion of reference data on fire (rather than differences in environmental conditions) and identify the widest range in reported disturbance (i.e. more uncertainty) in years with more fire incidence and in scrub/shrub habitat. In Chapter Four, I use spatial agreement among the competing products as a measure of uncertainty. I identify low uncertainty in disturbance (i.e. where all products agree) across only 15% of the total area of California that was reported as disturbed by at least one product between 2001 and 2010. Specifically, I find that scrub/shrub habitat had a lower uncertainty of disturbance than forest, particularly for fire, and that uncertainty was universally high across all bioregions. I also identify that LANDFIRE was solely responsible for approximately 50% of the total area reported as disturbed and find large differences between the burned areas reported by the reference data and the areas with low uncertainty of disturbance, indicating potential overestimation of disturbance by both LANDFIRE and the reference data on fire. Last, in Chapter Five, I conclude by highlighting how unresolved key challenges for Spatial Data Science can serve as new opportunities to guide the scaling of methods for â€œbigâ€ data, increased spatial-temporal integration, as well as promote new curriculum to better prepare future Spatial Data Scientists. In all, this dissertation explores the opportunities and challenges posed by Spatial Data Science and serves as a guiding reference for professionals and practitioners to successfully navigate the changing world of geospatial data and tools.",ucb,,https://escholarship.org/uc/item/00s3g6cg,,,eng,REGULAR,0,0
738,2174,Taphonomy and paleoecology of asphaltic Pleistocene vertebrate deposits of the western Neotropics,"Lindsey, Emily Leigh","Barnosky, Anthony D;",2013,"Asphaltic deposits, or ""tar pits,"" present a unique opportunity to investigate the paleobiology and paleoecology of Quaternary mammals due to their tendency to accumulate and preserve remains of numerous taxa, along with associated materials that can aid in paleoenvironmental and chronological analyses.  This role is especially important in areas with low preservation potential or incomplete sampling, such as the Neotropics.  Fossil deposits in the asphaltic sediments of the Santa Elena Peninsula in southwestern Ecuador contain some of the largest and best-preserved assemblages of Pleistocene megafaunal remains known from the neotropics, and thus represent an opportunity to greatly expand our knowledge of Pleistocene paleoecology and the extinction of Quaternary megafuana in this region.  This dissertation reports data from excavations at Tanque Loma, a new late-Pleistocene locality on the Santa Elena Peninsula that preserves a dense assemblage of megafaunal remains in hydrocarbon-saturated sediments along with microfaunal and paleobotanical material.  Chapter 1 details the results of three years of excavations and associated sedimentological, stratigraphic, systematic, taphonomic, and chronological studies at Tanque Loma.  Remains of extinct Pleistocene megafauna are encountered within and up to one meter above a laterally extensive asphalt-saturated sandstone layer along with abundant plant material.  Several meters of presumed-Holocene sediments overlying the megafauna-bearing strata are rich in microvertebrate remains including birds, squamates, and rodents, most likely representing raptor assemblages.  While over 1,000 megafaunal bones have been identified from the Pleistocene strata at Tanque Loma, more than 85% of these remains pertain to a single species, the giant ground sloth Eremotherium laurellardi.  Only five other megafauna taxa have been identified from this site, including Glossotherium tropicorum, Holmesina occidentalis, cf. Notiomastodon platensis, Equus (Amerhippus) santaelenae, and a cervid tentatively assigned to cf. Odocoileus salinae based on body size and geography.   No carnivores have yet been identified from Tanque Loma, and microvertebrate remains are extremely rare in the megafauna-bearing deposits, although terrestrial snail shells and fragmented remains of marine invertebrates are occasionally encountered.  Accelerator Mass Spectrometry radiocarbon dates on Eremotherium and c.f. Notiomaston bones from within and just above the asphaltic layer yielded dates of around 17,000 - 23,500 radiocarbon years BP.  Taken together, the taxonomic composition, taphonomy, geologic context, and sedimentology of Tanque Loma suggest that this site represents a bone bed assemblage in a heavily vegetated, low-energy riparian environment with secondary infiltration of asphalt that helped preserve the bones.The large accumulation of one taxon, Eremotherium laurillardi, at Tanque Loma offers a unique opportunity to investigate the ecology and behavior of this species.  Chapter 2 uses data from this and other paleontological localities as well as modern African ecosystems to investigate the formation of the E. laurillardi assemblage at Tanque Loma and the behavioral ecology and life history of this species.  Multiple lines of evidence, including a monodominant taxonomic composition; a multigenerational age structure with prime adult individuals well-represented; sediments suggestive of a low-energy anoxic aquatic environment; and the presence of abundant plant material that appears to pertain to coprolites of E. laurillardi; suggest that these sloths congregated and died in a protracted mass mortality event in a marshy riparian habitat.  The evidence is consistent with a mass death due to drought and/or disease in a shallow watering hole, paralleling situations observed among large wallowing herbivores in Africa today.  Furthermore, several neonate and fetal individuals are present in the deposit, suggesting that this species may have had a distinct breeding season, which is also common among large herbivores in seasonally dry tropical environments.  Chapter 3 endeavors to offer context for the Tanque Loma locality by combining data from these excavations with analyses of other asphaltic vertebrate localities in the region.  The most well known asphaltic paleontological locality in tropical South America is the Talara tar seeps in northwest Peru, which has yielded a great diversity of microfossils as well as extinct megafauna.  In addition, two other highly productive asphaltic localities have been excavated on the Santa Elena Peninsula -- the La Carolina locality excavated by Robert Hoffstetter in the 1940's, and the Coralito locality excavated by Franz Spillmann in the 1930's and A. Gordon Edmund in the 1960's.  I examined fossils from these excavations currently housed in the collections of the Museo Gustavo Orces in Quito, Ecuador, the Royal Ontario Museum in Toronto, Canada, and the Museum National d'Histoire Naturelle in Paris, France, in order to compare the depositional and environmental contexts of these different sites and to investigate the paleoecology and biogeography of the mammal taxa preserved therein.  In general, the communities of megaherbivores are comparable between these geographically close sites, but Talara and La Carolina present a much more diverse assemblage of birds, micromammals, and carnivores as compared with the other two localities.  Taxonomic, geomorphological, and taphonomic data indicate that these two sites were most likely ""tar pit"" style traps analogous to the famous Rancho La Brea locality in California, USA, while the SEP sites Coralito and Tanque Loma likely represent fossil assemblages in marshy or estuarine settings with secondary infiltration of tar.  In addition, geological and taxonomic differences between the nearby localities Coralito and Tanque Loma suggest differences in local paleoenvironments and lends further support for the hypothesis of gregarious behavior in at least two species of extinct giant ground sloths.Finally, the radiocarbon dates so far obtained on extinct taxa at Tanque Loma and the other asphaltic localities examined here are consistent with a model positing earlier extinctions of megafauna in tropical South America than of related taxa further south on the continent, although this observed pattern may be an artifact of low sampling in the region.",ucb,,https://escholarship.org/uc/item/00s473c2,,,eng,REGULAR,0,0
739,2175,A Grammar of Nevome,"Shaul, David",,1982,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/00s8m4fn,,,eng,REGULAR,0,0
740,2176,The Market for Ethics: Human Subjects Research Oversight in the United States and Canada,"Goldstein, Gabrielle","Keller, Ann;",2018,"An estimated 2.5 million Americans participate in clinical research annually.  Participation exposes human subjects to significant physical risks including death, as it often involves ingesting or introducing drugs and devices to the body that have not yet been proven safe or effective.  In the US, a set of policies around the regulation of biomedical research emerged as a politically and contextually contingent result in the post-war years. The centerpiece for oversight of human subjects research was the local, hospital- or university-based IRB. It emerged at the height of organized medicineâ€™s political and social power and derived from a logic of professional autonomy.Much has changed. Today, 80% of clinical research in the US occurs outside the academic medical context, in community settings such as physiciansâ€™ offices and freestanding research clinics.  These research studies are overseen by thousands of IRBs registered with the federal government, some of which are for-profit businesses. The world for which the oversight regime was built â€“ trained clinical researchers submitting their proposed research to the scrutiny of their trained colleagues in a university setting â€“ no longer exists. In spite of the dramatic changes in who is carrying out research and the associated changes in their motivations for doing so, the regulations governing IRBs and clinical research oversight remain stable. Little empirical scholarship exists regarding how the IRB oversight mechanism is operating now that the research landscape has changed so substantially. Little scholarship empirically charts the entire ecology of IRBs, which now include IRBs in diverse settings such as non-teaching community hospitals, health systems, government facilities, universities and teaching hospitals, as well as independent, central, and commercial IRBs.  There exists no full accounting of how many IRBs there are, of which types, in which locations â€“ and few analytical accounts of variations between IRBs based on organizational or environmental factors. This dissertation reports on the results of an original qualitative research study that involved interviews with IRB members and administrative professionals from commercial IRBs, nonprofit healthcare organization IRBs, academic and government IRBs in the US and Canada. The chapters explore various aspects of the current IRB ecology, in light of drastic changes to the institutional and economic environments in which clinical research occurs over recent decades. After a brief introduction, the second chapter explores IRB professionalsâ€™ experiences with, and responses to, the legal and regulatory environment in which these professionals and their organizations operate.  The third chapter explores variation between types of IRBs, identifying governance gaps and best practices.  The fourth chapter explores the attitudes of IRB professionals toward the commercialization of research ethics review in the United States and Canada. The fifth chapter provides some conclusions and steps for future research.",ucb,,https://escholarship.org/uc/item/00s9w618,,,eng,REGULAR,0,0
741,2177,Managing Intergroup Emotions: How Intergroup Ideologies and Emotion Regulation Can Stifle Positive Emotions and Intergroup Friendships,"O'Connor, Alexander","Mendoza-Denton, Rodolfo;",2012,"In interracial settings, a chief concern among majority group members is whether they appear prejudiced. These concerns often elicit feelings of anxiety and threat, which, ironically, run the risk of being interpreted as prejudice. One of the challenges majority group members face in intergroup interactions is the regulation of these negative emotions. Drawing on Gross's (1998, 2002) emotion regulation framework, I examine individual differences in how people manage negative emotions during intergroup encounters. I investigate whether a particular costly emotion regulation strategy, expressive suppression, is used by majority group members to limit intergroup emotional expressions, and in particular, used by individuals espousing colorblind ideologies that seek to avoid the perception, acknowledgement, and use of race. People endorsing more multicultural ideologies, on the other hand, accept group differences and thus should be less likely to rely on emotional suppression to manage interracial interactions. In Study 1a, I establish intergroup emotion regulation as distinct from global forms of emotion regulation. In Study 1b, I demonstrate the links between ideology and intergroup suppression. In Study 2, I examine of the social consequences of this ideology-suppression link, demonstrating that colorblind ideologies are associated with less positive, and multiculturalism with more positive, intergroup encounters and that these effects are mediated by intergroup suppression. Finally, in Study 3, I test this pathway experimentally, priming participants with multicultural or colorblind ideologies prior to an interracial interaction. Colorblind primes led to more suppression and less positive emotional expression, leading to less positive experiences for interaction partners.",ucb,,https://escholarship.org/uc/item/00t7z3cj,,,eng,REGULAR,0,0
742,2178,Gendered Perspectives in Higher Education: Women in Science and Engineering in Cameroon,"Fielding, Patience","Kramsch, Claire;",2014,"AbstractGendered Perspectives in Higher Education: Women in Science and Engineering in CameroonByPatience Fielding Doctor of Philosophy in EducationUniversity of California, Berkeley Professor Claire Kramsch, ChairWhile women's participation in the national economic growth is seen as critical to sustainable development, women's underrepresentation in Cameroon's higher institutions, such as Ã‰cole Nationale Superieure Polytechnique (ENSP), tasked with training human capacity contradicts such development discourses. Despite calls to provide equal educational opportunities for women to enable their acquisition of the skills needed to compete in the global labor market, females in Cameroon too often lack access to girl-friendly, safe, and supportive spaces in formal schools largely due to patriarchal traditions which restrict women's roles in the society. Most recently, females in Cameroon have sought entrance into ENSP, which has historically prepared male technocrats to serve in the government and private industry. While the institution has opened limited spaces for women, it continues to discursively constitute them as ""outsiders within"" as women venture into traditional environments and participate in activities from which they have been expressly or tacitly excluded.My dissertation thus uses ENSP as a space to examine how the discourses of gendered education come to be defined and practiced. Through analysis of institutional discourses, archival documents, and interviews with staff and students at ENSP, it investigates the conflicting narratives in gendered constructions. Paying attention to institutional texts and individual utterances, the dissertation illustrates the complexities of intimate relationships and highlights the processes of contestation that are so crucial in shaping contemporary, gendered identities. While I underscore the importance of an approach that permits the exploration of the ambiguities of gendered identities, I also present identifications and relationships as imagined and performed in discursive practices. To track the complexity of the process, my analysis includes the ways in which education discourses shape and constrain our understanding and engagement in the world and how gendered beings come to understand themselves and their given situation. While global, national, and local interests shape and structure student efforts at ENSP, the process of becoming an engineer is full of contradictions, tensions, and struggles, thereby leaving open the possibility that female students might take on new roles and behaviors that are deemed contrary to their identities. The dissertation thus situates the current interest in women in Math, Science, and Technology in relation to contemporary and historical definitions and underscores the shifts in education thinking. It lays out the different perspectives held by different social actors to advance a more nuanced understanding of the education practices through which gender is being framed, constructed, contested, and negotiated. It also presents the approaches of ethnography and critical discourse analysis used and employs critical discourse analysis to tease the different subjectivities and power relations at this site. In examining the status quo and patriarchal dominance in the exercise of power against female subservience and determination, I also underscore how female voices challenge the social constructs that define them as 'others' and `outsiders' and how they strategically negotiate their identities using the discourses of schooling. This study thus illuminates the ways in which institutional and individual discourses confirm existing social relationships and behaviors while at the same time introducing new meanings and patterns of being and conflicting enactments of gender within the production of linguistic forms.",ucb,,https://escholarship.org/uc/item/00z4p093,,,eng,REGULAR,0,0
743,2179,A Hodge-theoretic study of augmentation varieties associated to Legendrian knots/tangles,"Su, Tao","Shende, Vivek;Borcherds, Richard;",2018,"In this article, we give a tangle approach in the study of Legendrian knots in the standardcontact three-space. On the one hand, we define and construct Legenrian isotopy invariantsincluding ruling polynomials and Legendrian contact homology differential graded algebras(LCH DGAs) for Legendrian tangles, generalizing those of Legendrian knots. Ruling polynomialsare the Legendrian analogues of Jones polynomials in topological knot theory, in thesense that they satisfy the composition axiom.On the other hand, we study certain aspects of the Hodge theory of the â€œrepresentationvarieties (of rank 1)â€ of the LCH DGAs, called augmentation varieties, associated to Legendriantangles. The augmentation variety (with fixed boundary conditions), hence its mixedHodge structure on the compactly supported cohomology, is a Legendrian isotopy invariantup to a normalization. This gives a generalization of ruling polynomials in the followingsense: the point-counting/weight (or E-) polynomial of the variety, up to a normalized factor,is the ruling polynomial. This tangle approach in particular provides a generalizationand a more natural proof to the previous known results of M.Henry and D.Rutherford. Italso leads naturally to a ruling decomposition of this variety, which then induces a spectralsequence converging to the MHS. As some applications, we show that the variety is of Hodge-Tatetype, show a vanishing result on its cohomology, and provide an example-computationof the MHSs.",ucb,,https://escholarship.org/uc/item/00z6h5x5,,,eng,REGULAR,0,0
744,2180,Political Thugs: Criminal Corporate Raiding and Property Rights in Early-Capitalist Eastern Europe,"Abrams, Neil Andrew","Fish, Michael S;",2014,"Based on evidence from Estonia, Poland, Slovakia, and Ukraine, this dissertation explains variations across post-communist countries in the frequency of criminal corporate raiding and the effectiveness of property-rights institutions.  A criminal corporate raid is the takeover of a private business through the use of force, whether by means of direct violence or the help of the state.  I examine raiding among two segments of the business elite:  the plutocratic stratum, consisting of the twenty richest individuals, and the non-plutocratic elite, which refers to relatively less wealthy businesspeople along with state officials engaged in private business.  Ukraine and Slovakia display extensive raiding among both the plutocratic and non-plutocratic elites.  In Poland, raiding is just as pervasive within the non-plutocratic elite but rarer among the plutocrats.  Estonia exhibits little raiding among both elite groups.  The high incidence of raiding in Ukraine, Slovakia, and Poland means that property-rights institutions are by definition not doing their job.  Only Estonia managed to develop a set of robust property-rights institutions, a fact supported by dozens of interviews with state officials.  These findings fundamentally contradict much of the literature, which regards Slovakia and especially Poland as among the post-communist leaders in developing sound, market-supporting institutions.  Instead, this investigation uncovered shocking and systematic abuses of property rights in both countries of a kind typically seen as restricted to places like Russia and Ukraine.	  The reasons behind these variant outcomes boil down to one key factor:  the extent to which early post-communist governments imposed hard budget constraints on business actors.  Budget constraints are said to be hard when firms cannot access artificial external support in the conduct of their business.  They are soft when firms receive external assistance not justified by economic rationality.  In Poland, Slovakia, and Ukraine, governments did not go far enough in reining in soft budget constraints; emerging business actors benefited from a proliferation of direct transfers from the state budget, illicitly subsidized privatizations, overpriced state contracts, cronyist financing from banks, advantageous price controls, and one-sided transactions with state enterprises.  Soft budget constraints ended up enriching and empowering a class of political thugs whose comparative advantage lay solely in their political connections, not their capacity for productive economic activity.  Having accumulated their initial fortunes by stealing state assets, it was a natural and logical step to begin stealing them from others in the form of raiding.  Poland differed from Slovakia and Ukraine to the extent that its biggest state enterprises were largely sold at market prices.  This accounts for the lower proportion of political thugs among its plutocratic elite and, in turn, the lesser incidence of raiding in this stratum.  Beyond that, however, soft budget constraints were widely available there.  This explains why the non-plutocratic elite in Poland is filled with political thugs.  Estonia avoided these outcomes thanks to the swift and radical imposition of hard budget constraints in the early 1990s.  This cleared away potential opponents to the establishment of effective property rights institutions.The current fashion in much of social science is to look at how institutions shape actors.  This mode of inquiry is obviously important and useful.  But it has led specialists to give what may very well be mistaken advice to policymakers:  change the rules, and the actors will behave in ways better suited to a functioning market economy.  The evidence uncovered here suggests that attempts to build market-supporting institutions, however well-intentioned, will likely fail if powerful economic criminals have a large presence.  If the goal is to construct well-functioning state and market institutions, we would be well-served by examining not only how institutions affect actors but also the ways actors shape institutions.  This study does both.  It first shows how hard budget constraints - which consist of certain key policies and institutions - can help sideline economic predators.  It then goes one step further, examining how the presence or absence of powerful groups of economic criminals determines the prospects that effective property rights institutions can emerge.  Theories of institutional origination generally fail to identify the precise ways that governments can undercut the power of criminal business actors opposed to the status quo.  This study not only corrects for these shortcomings but provides a clear lesson to policymakers in early-capitalist countries who are interested in building property rights institutions; by hardening budget constraints, reformers can fatally undermine the political thugs acting as a constraint on institutional development.  If specialists disagree on whether policies or institutions are more important in creating market economies, hard budget constraints involve some of both.  But they hardly encompass all of both.  What they do offer resource-constrained policymakers is greater bang for their buck; by weakening corrupt actors committed to the status-quo, hard budget constraints can open the way for the construction of a much broader array of market-supporting institutions.",ucb,,https://escholarship.org/uc/item/00z9w1jk,,,eng,REGULAR,0,0
745,2181,Infertile Futures: Sperm and Science in a Chinese Environment,"Lamoreaux, Janelle","Hayden, Cori;",2013,"Based primarily on fieldwork conducted over one year in Beijing and Nanjing, two centers of scientific research and environmental activism in the People's Republic of China, my study explores the ways both scientists and social scientists envision and establish the relationship between exterior environmental problems and interior reproductive health concerns. I argue that during investigations into the quality and quantity of sperm in contemporary China, scientists both examine and produce toxic `environments' of exposure. Toxicologists find that their research subjects embody China's history of industrialism and rapid social change, which become investigable through genetic and epigenetic studies of sperm. Through interviews with faculty and graduate students at multiple universities, participant observation at reproductive toxicology laboratories, and interviews at environmental activist organizations, I explore the way experts and activists bring Chinese environments into being, and the way these environments are found to correlate with changes to reproductive health in China.Reflections from fieldwork have been brought to questions of interest within anthropology more generally, including the definitions and relationships between nature and culture, the universal and particular, the individual and the collective, as well as the body and what stands outside it. Each chapter is concerned with how anthropologists today make sense of interiors and exteriors, content and context. I bring these concerns first and foremost to scientific practice, using the methodology of those I study as a guide for what an anthropology of sperm (and perhaps the body and illness) might look like. In particular, I argue that epigenetic studies of male infertility and birth defects use sperm-environment interaction as a means to understand the biological impact of social processes on the body. The infertility of a toxic Chinese environment, whether brought into being in the laboratory or conceptualized as a devastated national landscape, is understood as correlated with the infertility of male Chinese bodies. Scientists are, then, embracing an understanding of biosocial problems that transcend both biological causation and individual responsibility to enable a form of social critique that takes seriously the epistemological and ontological stakes of thinking environmentally across bodies, generations and domains.Second, I bring these concerns of interiors/exteriors, content and context to questions of scientific translation, asking: how do scientific findings move between transnational, expert and disciplinary domains, even as these contexts (or environments) are brought into being through scientific practice? I argue that scientific practice in China today is effective at translating between domains because of, not in spite of, the toxicity and ambiguity of the environment. Toxicity allows science to proceed, even as reproductive toxicologists work to expose the damage that accompanies toxic exposure. Understood as multiplicity, the ambiguity of the environment benefits scientists, who meet multiple ethical and material research demands. The environment's ambiguity also facilitates an indirect environmental activism, which strives to evoke public attention toward environmental destruction through correlation, not causation. Here, the political stakes of correlative findings prove as if not more powerful than causative evidentiary claims.",ucb,,https://escholarship.org/uc/item/0101q07q,,,eng,REGULAR,0,0
746,2182,How Does Law Matter to Social Movements? A Case Study of Gay Activism in Singapore,"Chua, Lynette Janice","Luker, Kristin;",2011,"This study is aimed at gaining a better understanding of how people fight for change collectively in societies that, unlike the United States, have less of democratic processes, and fundamental civil-political rights, and, of how law matters to their processes of doing so. It focuses on a particular minority group, gay people, in one particular society - Singapore, an Asian country with shades of authoritarianism - and explored how gay activists make sense of their grievances, strategize and take action to achieve their goals, and evaluate their own efforts. Based on systematic collection and analysis of data, including in-depth interviews with 100 activists, the study found: Unlike what sociology of law has learned in the United States, law - in the form of legal rights - is neither a strategic nor symbolic resource for these activists. The role of law in collective fights for social change goes beyond that of rights, which are stymied by the very legal system set up by the powers in control. Gay activists in Singapore regard law as a key source of oppression that obstructs their movement. The ruling party, in control for the past 45 years, has used law's power of sanction and delegitimization not only to deter legally, but also to cultivate cultural norms that discourage its people from coming together to agitate for social change, to use rights, and to ask for change in the form of rights, which are painted as confrontational, and detrimental to their society's stability and economic progress.Hence, these activists focus on achieving social changes outside formal law, such as gaining acceptance from society at large, and the state to come out, speak out, and have their grievances heard, and to organize, and assemble more publicly as a group of people with shared concerns and interests. Rather than turning to the law to aid their cause, they resist it through ""pragmatic resistance,"" a strategy that precariously balances movement survival, and advancement. To ""live to fight another day,"" they abide by the law, and oppressive cultural norms so as to avoid legal sanctions that could lead to the repression of their movement, and demise of small gains already accumulated, thus reversing their hard work; meanwhile, to advance their goals, without changing formal law they imperceptibly push the boundaries of those cultural norms - which are backed by legal sanctions - on what are socially and politically acceptable. They are conscious of, and accept, their strategy as a trade-off between the accumulation of informal gains outside formal law, and the reification and reinforcement of legal power that perpetuates the cultural legitimacy of the existing political order.",ucb,,https://escholarship.org/uc/item/010339d4,,,eng,REGULAR,0,0
747,2183,Performance-Based Seismic Demand Assessment of Concentrically Braced Steel Frame Buildings,"Chen, Chui-Hsin","Mahin, Stephen A;",2010,"The special concentrically steel braced frame (SCBF) system is one of the most effective struc-tural systems to resist lateral forces.  Because of its effectiveness and straightforward design, many SCBFs are incorporated in structures throughout the world.  However, the highly nonlin-ear behavior associated with buckling and non-ductile fracture of braces reduces the ability of the system to dissipate energy resulting in undesirable modes of behavior.  While many studies have investigated the cyclic behavior of individual braces or the behavior of subassemblies, the dynamic demands on the structural system under various seismic hazard levels needs additional study for performance-based earthquake engineering.  Archetype buildings of SCBFs and buckling restrained braced frames (BRBFs) were analyzed using the computer program OpenSees (the Open System for Earthquake Engineering Simulation) to improve the understanding of the seismic behavior of braced frame systems, and to assess seismic demands for performance-based design.  Numerical models were calibrated using test data determined from testing of conventional buckling braces, buckling restrained braces, and the braced frame specimens.  In addition, fiber-based OpenSees models were constructed and compared with results of a sophisticated finite-element model that realistically captured local buckling and local fracture of structural elements.  Because the OpenSees models are reasona-bly accurate and efficient, they were chosen to perform set of parametric computer simulations.  The seismic demands of the system and structural elements were computed and interpreted for 3-, 6-, and 16-story SCBFs and BRBFs under various hazard levels.  The analysis results show large seismic demands for the 3-story SCBF, which may result in unexpected damage of struc-tural and non-structural elements.  The median expected probability of a brace buckling at one or more levels in a 3-story SCBF is more than 50% for an earthquake having a 50% probability of exceedance in 50 years (the service-level event).  The possible need to replace braces fol-lowing such frequent events due to brace buckling should be considered in performance-based earthquake engineering assessments.  In addition, brace fracture in SCBFs is likely for an earthquake having a 2% probability of exceedance in 50 years (the MCE-level event).  Analy-ses show that in general, BRBF models had larger drift demands and residual drifts compared to SCBF systems, because of the BRBF's longer fundamental period.  However, the tendency to form a weak story in BRBFs is less than that in SCBFs.  Evaluation of seismic demand parameters were performed for 2-, 3-, 6-, 12-, and 16-story SCBFs and BRBFs, which demonstrated that short-period braced frame systems, especially SCBFs, had higher probability of collapse than longer-period braced frame systems.  Substantially improved response was observed by lowering the response reduction factor of the 2-story SCBF building; this reduced the collapse risk at the hazard level of 2% probability of exceedance in 50 years.  For long-period (taller) structures, although the collapse probability was lower compared to the short-period structures, weak story behavior was commonly observed in conventionally designed SCBF.  A design parameter related to the ratios of story shear demand and capacity under a pushover analysis is proposed to modify member sizes to reduce weak story behavior efficiently.  This is demonstrated for a 16-story SCBF building.  Regarding local deformation and force demands, simple methods to estimate out-of-plane buck-ling deformation of braces and column axial force demands are proposed.  The investigation of system performance and member behavior provides seismic demands to more accurately assess the socio-economic losses of SCBFs and BRBFs for performance-based earthquake engineering.",ucb,,https://escholarship.org/uc/item/0103g2t3,,,eng,REGULAR,0,0
748,2184,Fundamental Interactions in Gasoline Compression Ignition Engines with Fuel Stratification,"Wolk, Benjamin Matthew","Chen, Jyh-Yuan;",2014,"Transportation accounted for 28% of the total U.S. energy demand in 2011, with 93% of U.S. transportation energy coming from petroleum. The large impact of the transportation sector on global climate change necessitates more-efficient, cleaner-burning internal combustion engine operating strategies. One such strategy that has received substantial research attention in the last decade is Homogeneous Charge Compression Ignition (HCCI). Although the efficiency and emissions benefits of HCCI are well established, practical limits on the operating range of HCCI engines have inhibited their application in consumer vehicles. One such limit is at high load, where the pressure rise rate in the combustion chamber becomes excessively large.Fuel stratification is a potential strategy for reducing the maximum pressure rise rate in HCCI engines. The aim is to introduce reactivity gradients through fuel stratification to promote sequential auto-ignition rather than a bulk-ignition, as in the homogeneous case. A gasoline-fueled compression ignition engine with fuel stratification is termed a Gasoline Compression Ignition (GCI) engine. Although a reasonable amount of experimental research has been performed for fuel stratification in GCI engines, a clear understanding of how the fundamental in-cylinder processes of fuel spray evaporation, mixing, and heat release contribute to the observed phenomena is lacking. Of particular interest is gasoline's pressure sensitive low-temperature chemistry and how it impacts the sequential auto-ignition of the stratified charge. In order to computationally study GCI with fuel stratification using three-dimensional computational fluid dynamics (CFD) and chemical kinetics, two reduced mechanisms have been developed. The reduced mechanisms were developed from a large, detailed mechanism with about 1400 species for a 4-component gasoline surrogate. The two versions of the reduced mechanism developed in this work are: (1) a 96-species version and (2) a 98-species version including nitric oxide formation reactions. Development of reduced mechanisms is necessary because the detailed mechanism is computationally prohibitive in three-dimensional CFD and chemical kinetics simulations.Simulations of Partial Fuel Stratification (PFS), a GCI strategy, have been performed using CONVERGE with the 96-species reduced mechanism developed in this work for a 4-component gasoline surrogate. Comparison is made to experimental data from the Sandia HCCI/GCI engine at a compression ratio 14:1 at intake pressures of 1 bar and 2 bar. Analysis of the heat release and temperature in the different equivalence ratio regions reveals that sequential auto-ignition of the stratified charge occurs in order of increasing equivalence ratio for 1 bar intake pressure and in order of decreasing equivalence ratio for 2 bar intake pressure. Increased low- and intermediate-temperature heat release with increasing equivalence ratio at 2 bar intake pressure compensates for decreased temperatures in higher-equivalence ratio regions due to evaporative cooling from the liquid fuel spray and decreased compression heating from lower values of the ratio of specific heats. The presence of low- and intermediate-temperature heat release at 2 bar intake pressure alters the temperature distribution of the mixture stratification before hot-ignition, promoting the desired sequential auto-ignition. At 1 bar intake pressure, the sequential auto-ignition occurs in the reverse order compared to 2 bar intake pressure and too fast for useful reduction of the maximum pressure rise rate compared to HCCI. Additionally, the premixed portion of the charge auto-ignites before the highest-equivalence ratio regions. Conversely, at 2 bar intake pressure, the premixed portion of the charge auto-ignites last, after the higher-equivalence ratio regions. More importantly, the sequential auto-ignition occurs over a longer time period for 2 bar intake pressure than at 1 bar intake pressure such that a sizable reduction in the maximum pressure rise rate compared to HCCI can be achieved.",ucb,,https://escholarship.org/uc/item/015933zs,,,eng,REGULAR,0,0
749,2185,Character in the Age of Adam Smith,"Chamberlain, Shannon Frances","Sorensen, Janet;",2017,"What does Adam Smithâ€™s moral philosophy owe to the literary discourse of his own time? Many recent studies of Smith have focused on finding his fingerprints on later imaginative literature, particularly in the nineteenth-century novels of free indirect discourse. The argument of this dissertation is that we gain both a better understanding of Smith and the eighteenth-century evolution of novels by attempting to place Smith in his original literary context, as a well-informed participant in the debates around the moral and didactic purpose of literature, especially as they concerned â€œcharacter.â€The use and purpose of literary character was undergoing profound philosophical changes during Smithâ€™s career (1748-1790). From the scandalous and barely disguised society figures who occupied the pages of proto-novels and romances in the early part of the century, to Hugh Blairâ€™s late-century assertion that â€œfictitious historiesâ€¦furnish one of the best channels for conveying instruction, for painting human life and manners, for showing the errors into which we are betrayed, for rendering virtue amiable and vice odious,â€ literary character in novels became the crux of a larger debate on the relationship between rhetoricâ€”previously a somewhat suspect and corrupt artâ€”and morality. Smithâ€™s method of instruction in the Lectures on Rhetoric and Belles Lettres has long been understood as revolutionary, but relatively less attention has been paid to how his description of the â€œcharacter of the authorâ€ and this figureâ€™s careful deployment of readersâ€™ sympathies engages with the relatively new notion that fictional characters were easier to sympathize with, and therefore better figures for the teaching of ethics, than â€œrealâ€ people. Notions of charactersâ€™ fictionality evolved, I argue, into The Theory of Moral Sentimentsâ€™ assertion that all other human beings are essentially fictional to us, products of their rhetoric and our imagination. I examine the evolution of moral and literary â€œcharacterâ€ throughout Smithâ€™s careerâ€”from his praise for epistolary novels in The Theory of Moral Sentiments to his engagement with Edinburgh literary circles in the later eighteenth century and especially the novels of his close friend, Henry Mackenzieâ€”to offer a fuller portrait of how Smithâ€™s theories came to play such an outsized role in nineteenth-century novels. But part of the purpose of this project is to revise our nineteenth- and post-nineteenth-century understandings of Smith as they have been inflected by J.S. Mill and later thinkers in the liberal tradition, and reinvigorate Smith as the product of a moment that was just beginning to theorize a moral role for imaginative literature. Gulliverâ€™s Travels, Clarissa, and Julia de RoubignÃ© are stories about how we represent ourselves as moral beings to others, and provided Smith with practical examples about rhetoric as a means of moral inquiry and formation. Most fundamentally, I argue that Smithâ€™s conception of the â€œmoral sentimentsâ€ evolved from formulating a relationship between readers and writers through characters, a subject that was also a particular interest of the eighteenth-century novel.",ucb,,https://escholarship.org/uc/item/0159k8cz,,,eng,REGULAR,0,0
750,2186,Predictive and Programmable Testing of Concurrent and Cloud Systems,"Joshi, Pallavi","Sen, Koushik;",2012,"Today's software systems often have poor reliability. In addition to losses of billions, software defects are responsible for a number of serious injuries and deaths in transportation accidents, medical treatments, and defense operations. The situation is getting worse with concurrency and distributed computing becoming integral parts of many real-world software systems. The non-determinism in concurrent and distributed systems and the unreliability of the hardware environment in which they operate can result in defects that are hard to find and understand. In this thesis, we have developed tools and techniques to augment testing to enable it to quickly find and reproduce important bugs in concurrent and distributed systems. Our techniques are based on the following two key ideas: (i) use program analysis to increase coverage by predicting bugs that could have occurred in ""nearby"" program executions, and (ii) provide programming abstractions to enable testers to easily express their insights to guide testing towards those executions that are more likely to exhibit bugs or help achieve testing objectives without having any knowledge about the underlying testing process. The tools that we have built have found many serious bugs in large real-world software systems (e.g. Jigsaw web server, JDK, JGroups, and Hadoop File System).In the first part of the thesis, we describe how we can predict and confirm bugs in the executions of concurrent systems that did not show up during testing but that could have shown up had the program under consideration executed under different thread schedules.This improves the coverage of testing, and helps find corner-case bugs that are unlikely to be discovered during traditional testing. We have built predictive testing tools to find different classes of serious bugs like deadlocks, hangs, and typestate errors in concurrent systems.In the second part of the thesis, we investigate how we can improve the efficiency of testing of distributed cloud systems by letting testers guide testing towards the executions that are interesting to them. For example, a tester might want to test those executions that are more likely to be erroneous or that are more likely to help her achieve her testing objectives. We have built tools and frameworks that enable testers to easily express their knowledge and intuition to guide testing without having any knowledge about the underlying testing process. We have investigated programmable testing tools in the context of testing of large-scale distributed systems.",ucb,,https://escholarship.org/uc/item/0161k90g,,,eng,REGULAR,0,0
751,2187,Fast Image Filters for Depth-of-Field Postprocessing,"Kosloff, Todd Jerome","Barsky, Brian A.;",2010,"The original and primary motivation for the work described in this thesis is depth of field post-processing.    Previous methods for simulating depth of field were either too slow, or of low quality.    Depth of field post-processing is a critical component of high quality rendering.    Without depth of field, everything is in perfectly sharp focus, lending an unnatural, overly crisp look.    In fact, lack of depth of field is an important tip-off that an image is computer generated.    Depth of field is challenging to achieve when both high quality and high speed are desired simultaneously.    This is because high quality methods are traditionally based on brute force, and fast methods make too many approximations.    Intuitively, it seems odd that depth of field is so computationally expensive.    Depth of field is a type of blur, and blur inherently removes information from an image, producing an output lacking high frequencies.    Producing a simpler image should not be so expensive, and this thesis shows that, indeed, high quality depth of field blurring can be achieved    in real time.    Central to this thesis are the concepts of gathering and spreading.  Gathering is the process of forming an output pixel by taking a linear combination    of input pixels.  Spreading is the process of expanding each input pixel into a point spread function (PSF) of some kind, and accumulating those PSFs into the output image.  Image filtering is traditionally thought of as gathering, but a central idea of this thesis is that depth of field    (and a variety of other applications) is much better suited to spreading.      In the course of developing the mathematical theory of filter spreading, we happened upon a new type of image    filter that is equally well-described as gathering and spreading.  This new method, which we refer to as the tensor filter, is conceptually simple and can leverage the benefits of any combination of gathering and spreading algorithms.    We have developed a variety of new fast, high quality image filtering algorithms.  Except for the tensor method, all of these are spreading methods.    The reason why we have a variety of techniques is because there are a variety of PSFs that one may wish to use.  Our methods achieve speed    by exploiting structure in the PSFs.  As such, truly arbitrary PSFs are problematic, but high quality results can be achieved with both polynomial and Gaussian PSFs, as well as PSFs that have an arbitrary outline but a constant-intensity interior.    While the original motivation for this work was depth of field, filter spreading actually has a variety of other applications.    We have developed proof-of-concept applications for motion blur, implicit curves, and image warping.  Filter spreading may also be useful    for radial basis function evaluation and volume rendering.",ucb,,https://escholarship.org/uc/item/0161q94f,,,eng,REGULAR,0,0
752,2188,Essays on the Economics of Organization,"Bennett, Victor Manuel","Tadelis, Steven;",2010,"This dissertation is comprised of three studies that investigate the implications and determinants of firms' choice of organizational form.In the first study, I present a model of a negotiated sales production process with two variations, whether the firm has a parallel or serial allocation of tasks, and whether they have a process for accounting for customers' return to the system.  I predict, first, that a hierarchal sales process allows firms to capture profitable low valuation sales that are ignored by firms with a parallel process.  Second, I predict that an information tracking process will allow firms to capture additional value from transactions that would have been completed anyway.  I find support for these predictions in a dataset combining data on organizational details collected from a survey I conducted of 500 US auto dealers and transaction-level data on auto sales at those dealerships.The second study investigates the allocation of control rights by firms.  I present a model of a multidivisional firm faced with a choice relating to its divisions.  The managers of those divisions have more information about the most productive choice for their division, but there is value to coordinating the choices.  I predict that tasks with high coordination values will be more likely to be centralized and that for tasks with lower coordination values, delegation is more likely when the manager has a greater information advantage, which manifests in the volatility of the environment.The third study proposes that vertical integration between manufacturers and lessors can generate externalities that improve the competitiveness of competing independent lessors.  Often, manufacturers provide warranties in the sales market to inspire confidence in their customers.  Because they are unable to observe the identity of customers, however, these same warranties can be used to recondition cars returned to independent lessors from leases shorter than the warranty.  The ability to free-ride on maintenance cost in this segment of the market allows independent lessors to overcome some of the captives' informational advantage.  I find support for this proposal in a dataset of 200,000 leases from 1997-2002.",ucb,,https://escholarship.org/uc/item/0168m8t1,,,eng,REGULAR,0,0
753,2189,Understanding the Gap Between Fertility Intentions and Outcomes,"Vohra, Divya","Abrams, Barbara;",2014,"The ability of women to safely and effectively control their fertility is a critical health and human rights issue. Family planning provides a range of health, social, and economic benefits for women and their families, yet more than 200 million women worldwide who wish to delay or limit their childbearing are not using any method of contraception. The global health community has recently seen renewed interest in promoting family planning use worldwide. This increased focus on family planning raises questions about how best to deliver reproductive health services to the women who need them, which requires a clearer understanding of how women make decisions about their fertility and how such decisions are carried out.This dissertation explores the reasons why women's stated fertility intentions do not always align with their fertility behaviors or outcomes. The first paper explores the concept of unmet need for family planning by using both quantitative and qualitative methods to better understand why women who want to prevent a pregnancy choose not to use contraception in Luanda Province, Angola. The second paper considers the social context in which women make decisions about their pregnancies in a post-abortion care context in Zanzibar, Tanzania.  The third paper examines how women's experiences in early childhood shape their risk of unintended pregnancy later in life in the United States.These dissertation papers provide three distinct examples of how researchers' and policy makers' perceptions of women's reproductive decision-making may fail to recognize or address crucial determinants of fertility intentions, behaviors, and outcomes. In particular, they highlight the importance of considering women's fertility decisions within the context of their full reproductive lives, and point to the need for further research to better understand how such decisions shift over the course of women's lives. Policies and programs that can accommodate how these decisions are formed and changed over time are necessary for ensuring that all women have the right to make decisions about their reproduction and to enact these decisions in safe and effective ways.",ucb,,https://escholarship.org/uc/item/01b9z3j5,,,eng,REGULAR,0,0
754,2190,Advances in Zero-Field Nuclear Magnetic Resonance Spectroscopy,"Theis, Thomas","Pines, Alexander;",2012,"In the course of the last century, Nuclear magnetic resonance (NMR) has become a powerful and ubiquitous analytical tool for the determination of molecular identity, structure, and function. Traditionally, the great analytical power of NMR comes at the cost of mobility and large expenses for cryogenic cooling. This thesis presents how zero-field NMR detected with an atomic magnetometer is emerging as a new, potentially portable and cost-effective modality of NMR with the ability of providing information-rich and high-resolution spectra. A detailed description of the zero-field NMR spectrometer and its operation is provided. The thesis details how the acquired zero-field NMR spectra result from the electron mediated scalar interaction (J-coupling) of nuclear spins in an analyte. Simple rules of addition of angular momenta are introduced for the prediction of the observed spectral lines overcoming the need for numerical simulations and enabling unambiguous assignment of peaks to different molecules. Additional information can be obtained in the near zero field regime, where the Zeeman interaction can be treated as a perturbation to the J-coupling. The presence of small magnetic fields results in splitting of the zero-field NMR lines, imparting additional information to the pure zero-field spectra. In addition to the utilization of the atomic magnetometers for enhanced sensitivity, hyperpolarization schemes can be implemented. This thesis shows that chemically specific zero-field NMR spectra can be recorded using hydrogenative and non-hydrogenative parahydrogen induced polarization (PHIP, NH-PHIP), enabling high-resolution NMR. The increased sensitivity enables detection of compounds with 13C or 15N in natural abundance. Since PHIP and NH-PHIP operate in situ, and eliminate the need for a prepolarizing magnet, they broaden the analytical capabilities of zero-field NMR. Lastly, this thesis gives insight into the PHIP and NH-PHIP mechanism by developing an appropriate theoretical framework.",ucb,,https://escholarship.org/uc/item/01d528kh,,,eng,REGULAR,0,0
755,2191,Flammability of Combustible Solids in Spacecraft Environments,"Thomsen Solis, Maria","Fernandez-Pello, Carlos;",2018,"The flammability of combustible materials inside a spacecraft environment is of importance for fire safety applications because the conditions in this type of environments can greatly differ from those on earth, and a fire could have catastrophic consequences. Moreover, experimental testing in space can be difficult and expensive, and ground-based microgravity facilities such as drop towers or parabolic flights are limited to a few second duration. Additionally, future space missions may require spacecraft cabin environments different than those currently used in the International Space Station, 21% O2 by volume and 101.3 kPa, moving to reduced cabin pressure and increased oxygen concentration. These new conditions may result in an increased fire risk of the materials used due to the higher flame temperatures and the reduction in the convective losses from the heated solid surfaces. In particular, the influence of low pressure on material flammability and flame spread behavior is emphasized here because of the similarities existing in between the flow field produced in reduced pressure environments and microgravity.Chapter 1 presents an introduction to material flammability and flammability testing, focusing particularly on flame spread over solid fuels and previous related research. Chapter 2 presents a description of the experimental setups used for the normal gravity and the microgravity tests presented as part of this work. Chapter 3 is an experimental study covering flammability boundaries for flame spread of fire resistant fabrics exposed to different environmental conditions. In Chapter 4 and 5, an experimental study is presented along with a phenomenological analysis to simulate the burning behavior of thin and thick solids in microgravity conditions by using reduced pressure environments. Then, in Chapter 6 a similar approach is presented to study the flame spread behavior under the effect of external radiant heating, making comparisons with low pressure and microgravity environments.",ucb,,https://escholarship.org/uc/item/01d5j8mc,,,eng,REGULAR,0,0
756,2192,The Use of Discretionary Expenditures as an Earnings Management Tool: Evidence from Financial Misstatement Firms,"Sun, Yuan","Dechow, Patricia;",2013,"This study examines the use of real earnings management in a setting where earnings manipulation is likely to have occurred. Using firms subject to SEC Accounting and Auditing Enforcement Releases, I find that misstating firms show lower discretionary SG&A but higher discretionary R&D than the control sample in the years in which they overstate earnings. I then investigate whether this result is explained by heightened management incentives to support stock prices. I find evidence consistent with investors overvaluing high discretionary R&D and low discretionary SG&A during misstatement years. Overall, these results suggest that while cutting SG&A is considered a feasible earnings management tool to inflate earnings and stock prices, cutting R&D is not a viable option in a setting where managers desire to signal growth and maintain high stock market valuations.",ucb,,https://escholarship.org/uc/item/01d6814q,,,eng,REGULAR,0,0
757,2193,Fast and Effective Approximations for Summarization and Categorization of Very Large Text Corpora,"Godbehere, Andrew B.","El Ghaoui, Laurent;",2015,"Given the overwhelming quantities of data generated every day, there is a pressing need for tools that can extract valuable and timely information. Vast reams of text data are now published daily, containing information of interest to those in social science, marketing, finance, and public policy, to name a few. Consider the case of the micro-blogging website Twitter, which in May 2013 was estimated to contain 58 million messages per day: in a single day, Twitter generates a greater volume of words than the Encyclopedia Brittanica. The magnitude of the data being analyzed, even over short time-spans, is out of reach of unassisted human comprehension.    This thesis explores scalable computational methodologies that can assist human analysts and researchers in understanding very large text corpora. Existing methods for sparse and interpretable text classification, regression, and topic modeling, such as the Lasso, Sparse PCA, and probabilistic Latent Semantic Indexing, provide the foundation for this work. While these methods are either linear algebraic or probabilistic in nature, this thesis contributes a hybrid approach wherein simple probability models provide dramatic dimensionality reduction to linear algebraic problems, resulting in computationally efficient solutions suitable for real-time human interaction. 	Specifically, minimizing the probability of large deviations of a linear regression model while assuming a $k$-class probabilistic text model yields a $k$-dimensional optimization problem, where $k$ can be much smaller than either the number of documents or features. Further, a simple non-negativity constraint on the problem yields a sparse result without the need of an $\ell_1$ regularization. The problem is also considered and analyzed in the case of uncertainty in the model parameters. Towards the problem of estimating such probabilistic text models, a fast implementation of Sparse Principal Component Analysis is investigated and compared with Latent Dirichlet Allocation. Methods of fitting topic models to a dataset 	 are discussed. Specific examples on a variety of text datasets are provided to demonstrate the efficacy of the proposed methods.",ucb,,https://escholarship.org/uc/item/01d8j64m,,,eng,REGULAR,0,0
758,2194,"States, Movements and the New Politics of Blackness in Colombia and Brazil","Paschel, Tianna Shonta","Evans, Peter B.;",2011,"The 1990s marked a dramatic shift throughout Latin America from constitutions and state policies that hinged on ideas of colorblindness and mestizaje to targeted policies for black and indigenous peoples. This study analyzes the role black social movements played in this shift in Colombia and Brazil, two countries where the state adopted the most comprehensive reforms for black populations in the region. It also analyzes the impact of achieving such reforms on black movements' trajectories in the two countries. In so doing, I not only examine how black movements are shaped by the political context in which they emerge, but how they are able to reconfigure that political context in ways that ultimately reshape black movements themselves. Drawing on 18 months of fieldwork including in-depth interviews, archival analysis, and ethnographic methods, this study reveals new ways of understanding ethno-racial politics in these countries and offers insights about the relationship between movements and the state, as well as contestation within movements. Further, in examining how black movements seize upon changes in the global political field, appropriate global discourses into local struggles, and build transnational alliances, this work also challenges us to integrate the constant interplay between global and local processes into our analyses, especially when our aim is to understand social movement dynamics in the Global South.  In the first part of the dissertation, I show how the rise of global policy norms around multiculturalism, and the Durban World Conference against Racism, provided political openings for black movements in Colombia and Brazil, respectively. However, I maintain that it was the interplay between such global factors and national political developments paired with strategic action by black movements that best explains states' adoption of these historic reforms. Even so, while both countries adopted policies for black populations beginning in the 1990s, the dominant discourse around black rights in Brazil centers on notions of ""the right to equality"" and inclusion, whereas black issues in Colombia are largely framed in terms of the ""right to difference"", culture, territory and autonomy. I suggest that these discursive differences have as much to do with how black populations were historically imagined by the state in the two cases, as they do with the different discursive tactics used by black movements when making demands on the state.  The second part examines the consequences of the shift to ethno-racial legislation on internal black movement dynamics in the two countries. More specifically, I analyze the nature of formal structures of political participation set up for black populations in response to movement pressure. I do this by examining how movement actors negotiate, inhabit and contest such spaces, revealing a reality of social movement institutionalization that is much more complex than the literature suggests. Whereas black movements in Brazil have been absorbed into mainstream politics within a relatively democratic state, black movements in Colombia have either been repressed violently or institutionalized into precarious alternative political structures leading to unique internal movement dynamics. In order to understand the relationship between structure and agency as well as ntional and international political processes in these two cases, I propose the conceptual framework of national and global political fields which I argue contributes both to the literature on race in Latin America and social movements.",ucb,,https://escholarship.org/uc/item/01f2j9dx,,,eng,REGULAR,0,0
759,2195,Provably Efficient Algorithms for Numerical Tensor Algebra,"Solomonik, Edgar","Demmel, James;",2014,"This thesis targets the design of parallelizable algorithms and communication-efficient parallel schedules for numerical linear algebra as well as computations with higher-order tensors.  Communication is a growing bottleneck in the execution of most algorithms on parallel computers, which manifests itself as data movement both through the network connecting different processors and through the memory hierarchy of each processor as well as synchronization between processors. We provide a rigorous theoretical model of communication and derive lower bounds as well as algorithms in this model. Our analysis concerns two broad areas of linear algebra and of tensor contractions. We demonstrate the practical quality of the new theoretically-improved algorithms by presenting results which show that our implementations outperform standard libraries and traditional algorithms. We model the costs associated with local computation, interprocessor communication and synchronization, as well as memory to cache data transfers of a parallel schedule based on the most expensive execution path in the schedule. We introduce a new technique for deriving lower bounds on tradeoffs between these costs and apply them to algorithms in both dense and sparse linear algebra as well as graph algorithms. These lower bounds are attained by what we refer to as 2.5D algorithms, which we give for matrix multiplication, Gaussian elimination, QR factorization, the symmetric eigenvalue problem, and the Floyd-Warshall all-pairs shortest-paths algorithm. 2.5D algorithms achieve lower interprocessor bandwidth cost by exploiting auxiliary memory. Algorithms employing this technique are well known for matrix multiplication, and have been derived in the BSP model for LU and QR factorization, as well as the Floyd-Warshall algorithm. We introduce alternate versions of LU and QR algorithms which have measurable performance improvements over their BSP counterparts, and we give the first evaluations of their performance. We also explore network-topology-aware mapping on torus networks for matrix multiplication and LU, showing how 2.5D algorithms can efficiently exploit collective communication, as well as introducing an adaptation of Cannon's matrix multiplication algorithm that is better suited for torus networks with dimension larger than two. For the symmetric eigenvalue problem, we give the first 2.5D algorithms, additionally solving challenges with memory-bandwidth efficiency that arise for this problem. We also give a new memory-bandwidth efficient algorithm for Krylov  subspace methods (repeated multiplication of a vector by a sparse-matrix), which is motivated by the application of our lower bound techniques to this problem. The latter half of the thesis contains algorithms for higher-order tensors, in particular tensor contractions. The motivating application for this work is the family of coupled-cluster methods, which solve the many-body SchrÃ¶dinger equation to provide a chemically-accurate model of the electronic structure of molecules and chemical reactions where electron correlation plays a significant role. The numerical computation of these methods is dominated in cost by contraction of antisymmetric tensors. We introduce Cyclops Tensor Framework, which provides an automated mechanism for network-topology-aware decomposition and redistribution of tensor data. It leverages 2.5D matrix multiplication to perform tensor contractions communication-efficiently. The framework is capable of exploiting symmetry and antisymmetry in tensors and utilizes a distributed packed-symmetric storage format. Finally, we consider a theoretically novel technique for exploiting tensor symmetry to lower the number of multiplications necessary to perform a contraction via computing some redundant terms that allow preservation of symmetry and then cancelling them out with low-order cost. We analyze the numerical stability and communication efficiency of this technique and give adaptations to antisymmetric and Hermitian matrices. This technique has promising potential for accelerating coupled-cluster methods both in terms of computation and communication cost, and additionally provides a potential improvement for BLAS routines on complex matrices.",ucb,,https://escholarship.org/uc/item/00b4r0mv,,,eng,REGULAR,0,0
760,2196,John Rawls: the Path to A Theory of Justice,"Galisanka, Andrius","Bevir, Mark;",2013,"This dissertation is an intellectual biography of American political philosopher John Rawls [1921-2002] from his early years to the publication of his classic work, A Theory of Justice [1971]. I focus the historical narrative on Rawls's changing conceptions of philosophy: his ways of raising ethical and political questions and justifying answers to them. I pay particular attention to two aspects of the conception of philosophy found in A Theory of Justice: its claim that ethical and political positions are defended by showing that all reasonable persons endorse them in their political judgments, and its aspiration to explicate all of these political judgments in terms of principles of justice.This conception of philosophy was very influential for Anglophone political thought, contributing to the resurgence of analytic political theory in the 1950s and 1960s. I aim to understand the intellectual origins of this influential philosophical approach and thereby shed light on A Theory of Justice and contemporary political thought. Taking this historical approach, I follow the development of Rawls's thought, contextualizing him in contemporary traditions and analyzing his numerous private papers recently deposited in the Harvard University Archives.I argue that, much to our surprise, Rawls's conception of philosophy originated in logical positivism, the very tradition that is thought to have foreclosed the possibility of political thought in the 1940s. Inspired by logical positivists, Rawls modeled ethics on the ""method of science,"" and, taking ethical judgment as ""data,"" tried to formulate principles, or laws, to explicate them. This analogy between reasoning in ethics and reasoning in science provided Rawls with a conception of objectivity: principles of justice were objective if they explicated the considered political judgments of all reasonable persons. This notion of objectivity made possible reasoned discussion on ethical and political issues and required attention to actual political questions. Yet it also committed Rawls to a contestable view that all reasonable persons agree on a sufficient number of political judgments to yield a conception of justice.This conception of philosophy changed over the following two decades, but, I argue, it remained positivist. In the early 1950s, Rawls drew on linguistic philosophy's conception of ethical reasoning as a practice, and in the late 1950s he was led on the Wittgensteinian path of considering political questions against the background of seeing morality as a form of life. Nevertheless, the influence of his Harvard colleague W.V.O. Quine in the 1960s brought to light Rawls's positivist conception of philosophy. Rawls continued to justify political principles by the fact that all reasonable persons endorse them in their political judgments.My historical narrative contests and supplements the traditional interpretations of Rawls as a Kantian or a theorist in the social contract and rational choice theory traditions. It therefore paints a different picture of 20th century Anglophone political thought. But, as I argue in Epilogue, my narrative also helps to illuminate Rawls's shift to Political Liberalism. Doing so, I hope it opens new questions about contemporary attempts to define shared political reasons.",ucb,,https://escholarship.org/uc/item/00b9s5vw,,,eng,REGULAR,0,0
761,2197,Single Molecule and Synthetic Biology Studies of Transcription,"Zamft, Bradley Michael","Bustamante, Carlos J;",2011,"The horizons of biology are ever expanding, from the discernment of the detailed mechanisms of enzyme function, to the manipulation of the physiological processes of whole organisms and ecosystems. Single molecule studies allow for the characterization of the individual processes that comprise an enzyme's mechanochemical cycle. Through standardization and generalization of biological techniques, components, and knowledge, synthetic biology seeks to expand the scale of biological experiments and to usher in an age of biology as a true engineering science, in which those studying different hierarchical levels of sophistication need not start from the fundamental biochemical principles underlying all biological experiments. Here we report our findings on the processes governing transcription and its role in gene expression through the use of both single molecule and synthetic biology methods.We have established a promoter-free, factor-free method of initiation of transcription by the mitochondrial RNA polymerase in Saccharomyces cerevisiae, Rpo41 through the use of synthetic oligonucleotides to imitate the hybridization geometry of Rpo41 during active transcription. Using this system, we have established that a sub-micromolar NTP concentration is appropriate for non-saturating transcriptional runoff assays. We have optimized the transcription buffer and found that 10 mM MgCl2, 40 mM KCl, and 10 mM DTT are sufficient for robust transcription. Stability studies show that Rpo41 loses approximately 30% of its activity during each freeze-thaw cycle, and that the pre-formed elongation complex loses transcriptional activity with a half-life of 7.4Â±1.5 hr.Through the use of optical trapping techniques, we have established a method to monitor the transcription of individual Rpo41 molecules in real time. This has allowed us to measure the kinetic rates of nucleotide incorporation by the enzyme: Km = 22Â±13 ÂµM-1 and vmax = 25Â±2.5 bp/s. Both of these rates are more similar to those of the main nuclear RNA polymerase in the same organism, RNA Polymerase II (Pol II) than to that of the T7 RNA polymerase, despite the fact that Rpo41 is a single-subunit RNA polymerase with homology to those of the T-odd bacteriophage and no discernable homology to Pol II. Furthermore, like Pol II and the E. coli RNA polymerase, transcription by Rpo41 consists of periods of processive transcription interspersed with periods of pausing. We have also observed retrograde motion of Rpo41 during pauses, termed backtracking, a process that has not been reported in phage-like RNA polymerases.We have performed single molecule assays of transcription by both Pol II and Rpo41 on templates of differing base pair composition and found that, in general, the characteristics of pausing are attenuated in templates of higher GC content. Specifically, the frequency of pausing is decreased in GC-rich templates, as is the average pause duration. The distribution of pause durations is correspondingly shifted to shorter pauses on GC-rich templates.We discuss two mechanisms by which template composition may affect pausing: (1) movement of the backtracked transcription bubble is affected by differences in the base stacking energies from the disrupted/created DNA/DNA and RNA/DNA base pairs at the ends of the bubble, and (2) secondary structure of the nascent RNA upstream of the backtracked transcription bubble imposes an energetic barrier to its backward movement. We give in silico evidence that it is the latter mechanism. Incorporation of this secondary structure energy barrier (an ""energy penalty"") into a model of transcriptional pausing by backtracking allows for statistical fits of the mean pause densities, mean pause durations, and the distribution of pause durations for each enzyme on each template. Furthermore, incorporation of the energy penalty allows for fitting of the pause characteristics for a given enzyme using a single, enzyme specific hopping rate, k0, that is independent of template, and a single, template dependent energy penalty term, Î”GRNA, which is enzyme independent. For Rpo41, we find that k0, the hopping rate of the backtracked enzyme along DNA without RNA secondary structure, is 5.4Â±1.8 s-1, while it is 2.9Â±0.3 s-1 for Pol II. Furthermore, the average energy penalty due to the nascent RNA, Î”GRNA, on the AT-rich template used in this study is 0.7Â±0.1 kT, while it is 0.8Â±0.1 kT for random DNA and 1.0Â±0.1 kT for GC-rich DNA.In order to confirm that it is the secondary structure of the RNA that is the cause of the energy penalty, we performed the same single-molecule transcription assays in the presence of RNase A, an enzyme that digests unprotected RNA in both single-stranded and double-stranded form. The pausing characteristics of all traces on all templates in the presence of RNase A are statistically indistinguishable from those on AT-rich DNA without RNase, indicating that the RNase digested enough of the nascent RNA to disrupt any secondary structure. Protection of the 5' region of the nascent RNA by steric interactions between the polymerase and the RNase prevented full degradation of the RNA, and thus allowed for some backtracking. This strongly supports the new model, presented here, of modulation of transcriptional pausing by secondary structure of the nascent RNA.In contrast to the detailed and isolated nature of single-molecule transcription, we also performed a synthetic biology project involving Rpo41. The intent of this project was to investigate the plausibility of the creation of a transcriptionally independent mitochondrion, and by extension a minimal cell, by movement of the mitochondrial transcriptional machinery from the nuclear to the mitochondrial genome. Thus we performed in vivo mitochondrial transformation of yeast cells with a synthetic construct containing the gene encoding for Rpo41. We report that we have successfully integrated said synthetic gene into the mitochondrial genome, and have seen its expression to the transcriptional level. Furthermore, we are fairly confident that the full, intact mRNA of the synthetic gene is being created within the mitochondrial matrix.We have not been able to detect expression of the protein product of the integrated synthetic construct, nor have we been able to isolate a strain that exhibits its expression in the absence of the wild-type, nuclear copy. Because the length of Rpo41 is longer than any other protein synthesized within the mitochondrial organelle, we have begun experiments to determine the maximal polypeptide length able to be translated by the mitochondrial ribosome and associated cofactors.",ucb,,https://escholarship.org/uc/item/00c6w2pc,,,eng,REGULAR,0,0
762,2198,Pronouns and Pronominal Morphology in Tibeto-Burman,"Bauman, James",,1975,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/00h8574v,,,eng,REGULAR,0,0
763,2199,The Aeroacoustics of Nasalized Fricatives,"Shosted, Ryan K",,2006,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/00h9g9gg,,,eng,REGULAR,0,0
764,2200,Data-Driven Approaches for Sensing and Control of Robot Manipulators,"Wang, Cong","Tomizuka, Masayoshi;",2014,"In a sensing rich system, a large amount of data can be obtained over time and utilized to improve the performance and functionality of a robotic system.  Data-driven approaches emphasize on the utilization of auxiliary sensors, sensor fusion, and data learning. Real-time control systems of robotic systems often run at kilo-Hertz sampling frequencies. New data is obtained from a variety of feedback sources every one or a few milliseconds.  Auxiliary sensors provide additional feedbacks and enable sensor fusion.  This dissertation presents a series of data-driven approaches to improve the sensing and control of robot manipulators from several aspects, including sensor fusion for motion sensing, statistical learning for feedback compensation, nonparametric learning control, and intelligent modeling and identification.In regard to the limited sensing capability of conventional indirect drive-trains of industrial robots, a sensor fusion approach based on auxiliary optical and inertial sensors is introduced for direct motion sensing of robot end-effectors.  The approach is especially useful to applications where high accuracy is required for end-effector performance in real-time.  Meanwhile, for the scenarios where auxiliary sensor are not allowed, a statistical learning algorithms is developed for sensing compensation so that control of systems with limited feedback capability can be significantly improved.  A major application of the approach is vision guidance of industrial robots. The proposed learning approach can significantly increase the visual tracking bandwidth without requiring high-speed cameras.  Besides improving the sensing capability of robots, nonparametric learning control is developed to control systems with complex dynamics.  A major motivation of the approach is robotic laser and plasma cutting.  Furthermore, to obtain high-fidelity models more efficiently, planning and learning algorithms are discussed for intelligent system modeling and identification. The applications of the proposed approaches range from vision guided robotic material handling to precision robotic machining.  Various tests are designed to validate the proposed approaches.",ucb,,https://escholarship.org/uc/item/00j7068w,,,eng,REGULAR,0,0
765,2201,Structural and evolutionary studies on CaMKII oligomerization,"McSpadden, Ethan DeNardo","Kuriyan, John;",2018,"Calcium-calmodulin-dependent protein kinase II (CaMKII) plays a critical role in animal learning and memory (Giese et al. 1998). CaMKII operates as a multi-subunit holoenzyme and can detect the frequency of incident Ca2+ pulse trains entering the cell (Chao et al. 2011; De Koninck and Schulman 1998). CaMKII holoenzymes become destabilized and appear to exchange subunits following catalytic activation, which results in the spread of kinase activity to inactive holoenzymes (Stratton et al. 2014; Bhattacharyya et al. 2016).  This phenomenon is not fully understood and is under active investigation.This work focuses on the structural and biochemical characterization of CaMKII hub domain assemblies, which are essential to holoenzyme formation (Shen and Meyer 1998; Kolb et al. 1998). The crystal structure of a dodecameric human CaMKII-Î± hub domain assembly shows how the protein may be able to sense the activation state of the CaMKII kinase domain. This helps explain why kinase activation destabilizes CaMKII holoenzymes and promotes subunit exchange.The characterization of CaMKII hub domains from evolutionarily divergent organisms is also presented.  Hub domains encoded by three related green algae assemble into 16-, 18-, and 20-subunit oligomers. These are the largest known CaMKII hub domain assemblies. A crystal structure of one, the Chlamydomonas reinhardtii 18-mer, revealed multiple intra-chain hydrogen bonds not present in the human isoform. When these hydrogen bonds were engineered into the human CaMKII hub domain by mutation the protein oligomerized into larger assemblies. A larger CaMKII holoenzyme is predicted to be more resistant to deactivation by protein phosphatases and thus may be useful in studying the consequences of aberrant CaMKII activity. These hydrogen bonds are also predicted to inhibit subunit exchange in human CaMKII by dampening structural fluctuations in the hub domain.Work is also described on an interfacial mutation in the hub domain that weakens the integrity of the CaMKII holoenzyme. CaMKII dimers are present in solution as a result of this mutation. Catalytic activation increases the dimer population, demonstrating that the hub domain interface weakened by the mutation is further destabilized when CaMKII is active. This suggests that dimer release is a central component of activation-triggered subunit exchange between CaMKII holoenzymes.",ucb,,https://escholarship.org/uc/item/00k0r2pt,,,eng,REGULAR,0,0
766,2202,"Flooded by Progress: Law, Natural Resources, and Native Rights in the Postwar Pacific Northwest","Dougherty, John J.","Biolsi, Thomas;",2014,"This dissertation examines the politics of federal Indian law and the changing economic and environmental landscape of the postwar Pacific Northwest. In particular, it contends that the changing legal status of Native lands and resources was instrumental in both the massive industrial expansion, and subsequent environmental transformation, of the postwar Pacific Northwest. It traces the relations between economic and environmental changes and their connections to the dramatic policy shift in Indian affairs from the early 1950s, when the federal government unilaterally terminated tribal status of 109 Native communities, most of them in the Pacific Northwest, to the Native sovereignty movement, which precipitated new national policies of self-determination in the 1970s. Not only does the dissertation illuminate how Native communities in the Pacific Northwest were inequitably burdened by the region's environmental and economic transformations in the second half of the 20th century, it also demonstrates how these transformations fueled the national economy in the postwar years as well as the emergence of Native activism, and how Native communities actively navigated and influenced seminal directions in federal Indian policy. Lastly, it illustrates how Native communities in the Pacific Northwest responded to the economic and environmental struggles of the early sovereignty era. This dissertation remaps the field of Native American history, by foregrounding its critical intersections with 20th-century environmental and economic histories. It relies heavily on materials from the National Archives & Records Administration Regional Office in Seattle, Washington, and agency archives from the Bonneville Power Administration, US Army Corps of Engineers, Bureau of Indian Affairs, and Columbia River Inter-Tribal Fish Commission, all in Portland, Oregon. In addition, the dissertation utilizes a variety of additional evidentiary support, and records from tribes in the Pacific Northwest.",ucb,,https://escholarship.org/uc/item/0hm6r1mz,,,eng,REGULAR,0,0
767,2203,Development of Superconducting High-Resolution Gamma-Ray Spectrometers for Nuclear Safeguards,"Dreyer, Jonathan","Prussin, Stanley G;",2012,"Superconducting high-resolution gamma-ray spectrometers based on molybdenum/ copper transition edge sensors (TES) with tin absorbers have been developed for nuclear safeguard applications. This dissertation focuses on plutonium analysis, specifically the direct measurement of the 242Pu gamma-ray signature at 44.915keV. As existing nondestructive analysis methods cannot directly measure this or any other 242Pu signature, the feasibility of making such a measurement using a TES based system is presented. Analysis from of Monte Carlo simulations and analytical noise models shows that the direct detection of this gamma-ray line of is possible and can be quantified in the presence of a 240Pu gamma-ray line with a line separation of 324eV, even if the emission from the 240Pu is several orders of magnitude stronger. Spectroscopic measurements conducted in a liquid cryogen system offered an energy resolution of 180eV, adequate for the measurement of 242Pu; however, TES operation in a liquid-cryogen-free pulse tube refrigerator degraded sensor performance such that this measurement was no longer possible. The numerical noise model indicates that the energy resolution of this device is adequate to demonstrate a direct measurement of 242Pu if the noise pickup from the mechanical cooler can be suppressed. This work shows that the precise measurement of low-intensity gamma-ray signatures, such as the 44.915keV gamma ray from 242Pu, will require arrays of low-noise TES sensors and that such a system would offer invaluable information in the analysis of plutonium bearing materials.",ucb,,https://escholarship.org/uc/item/0wf1p1mr,,,eng,REGULAR,0,0
768,2204,Three Essays on Development Economics and Behavioral Economics,"Song, Changcheng","DellaVigna, Stefano;Miguel, Edward;",2012,"This dissertation studies retirement savings, weather insurance take-up and reference-dependent theory in the literature of development economics and behavioral economics. It consists of two field experiments and one laboratory experiment.In Chapter one, I uses a field experiment to study the relationship between financial literacy and retirement savings in China. When the Chinese government launched a highly subsidized pension system in rural areas in 2009, 73% of households chose to save at a level that is lower than that implied by a benchmark life-cycle model. We test to what extent the low contribution level is due to a fundamental misunderstanding of the nature of compound interest. In a field experiment with more than 1000 Chinese households, we randomly assigned some households to a financial education treatment, emphasizing the concept of compound interest. This treatment increased the pension contribution by roughly 40%. The increase accounts for 51% of the gap between contribution levels in the Control group and those implied by the benchmark model. To pinpoint mechanisms, we elicited financial literacy after the intervention, and added a third group in which we explain the pension benefit in general. We find that the neglect of compound interest is correlated with low contributions to the pension plans in the control group, and that financial education about compound interest does help households partially correct their erroneous understanding of compound interest. Moreover, explaining compound interest increases their ability to translate benefits into their own situation. Welfare analysis suggests that financial education increases total welfare, although the fact that the treatment effects are heterogeneous implies that some households end up saving more than the level implied by the benchmark model. In Chapter two (coauthored with Jing Cai), we use a novel experimental design to test the role of experience and information in insurance take-up in rural China, where weather insurance is a new and highly subsidized product. We randomly selected a group of poor households to play insurance games and find that it increases the actual insurance take-up by roughly 48%. To pinpoint mechanisms, we test whether the result is due to: (1) changes in risk attitudes, (2) changes in the perceived probability of future disasters, (3) learning the objective benefits of insurance, or (4) the experience of hypothetical disaster. We show that the overall effect is unlikely to be fully explained by mechanisms (1) to (3), and that the experience acquired in playing the insurance game matters. To explain these findings, we develop a descriptive model in which agents give less weight to disasters and benefits which they experienced infrequently. Our estimation also suggests that experience acquired in the recent insurance game has a stronger effect on the actual insurance take-up than that of real disasters in the previous year, implying that learning from experience displays a strong recency effect.In Chapter three, I conducted a controlled lab experiment to test to what extent expectations and the status quo determine the reference point. In the experiment, I explicitly manipulated stochastic expectations and exogenously varied expectations in different groups. In addition, I exogenously varied the time of receiving new information and tested whether individuals adjust their reference points to new information, and the speed of the adjustment. With this design, I jointly estimated the reference points and the preferences based on the reference points. I find that both expectations and the status quo influence the reference point but that expectations play a more important role. Structural estimation suggests that the model of the stochastic reference point fits my data better than that with expected utility certainty equivalent as the reference point. The result also suggests that subjects adjust reference points quickly, which further confirms the role of expectation as reference point",ucb,,https://escholarship.org/uc/item/0wn1772g,,,eng,REGULAR,0,0
769,2205,Experiencing the Past: The Virtual (Re)Construction of Places,"El Antably, Ahmed Hamed","Kalay, Yehuda E;",2011,"Place is characteristically imbued with a multiplicity of meanings contingent on the specificities of the society, time and space in which place is perceived. It is essentially subjective, relational and differential. As such, virtual multiplicity is the main characteristic of place where the virtual is understood as the sum of the real, a subjective mental act, and a motive or confusion. Space, conversely, is that which has no virtuality in it; it is objective. Accordingly, at least in theory, it is hard to distinguish between virtual and actual places. Outside of theory, virtual place typically suggests a system of representations that aims to convey through the senses an imagined or illusionary reality. Virtual place is mostly associated with places constructed using computational technology; most popular among them are massive multiuser online games (MMOGs). Lately, scholars are increasingly using such MMOGs to virtually construct or reconstruct historic places. The interpretation of such historic places depends on the affordances of the medium through which place is perceived and the ways in which such a medium is socially deployed and interpreted. This dissertation explores (1) the ways in which MMOGs are perceived and (2) the formative effect virtual reconstructions of historic places using gaming technology exercise on the interpretation of such places.Using two popular MMOGs as a case study, this dissertation shows that the perception of such places is conditioned by the suggestive technical affordances of the medium in addition to complex socio-historical forces. These conditions predispose expert users of MMOGs to perceive these virtual places in ways different than new users. New users perceive virtual environments in ways similar to the ways they perceive the actual environment: They assume an isomorphic relationship between the virtual reconstruction of a given place and its actuality. They value the formal and multi-sensory aspects of the environment. Expert users, on the other hand, perceive the virtual environment from a structural and functional perspective and pay less attention to its formal and sensory qualities. They seek novel and interesting social activities, increased technical knowledge and improved social status.This dissertation also uses the ancient settlement of Sirkap, located in modern-day Pakistan, as a case study, to demonstrate that the use of gaming technology to virtually reconstruct a historical place may entail a change in the interpretation of archaeological records. Most conventional historical accounts of Sirkap use two-dimensional site maps and city plans as the primary media to represent the urban fabric of the ancient settlement. The medium lends itself to interpret the Block D Apsidal Temple complex as the dominant socio-religious structure in the affluent northern parts of the settlement. When the author developed an interactive three-dimensional reconstruction of Sirkap using gaming technology--a medium that allows users, through their avatars, to explore the settlement from the standpoint of a pedestrian--it was immediately obvious that the aforementioned Block D Apsidal Temple complex did not demand such an interpretation. Instead, this study argues that, at least in the affluent northern parts of the settlement, the northern gate, its adjacent fortifications, and the Block A stupa court were the dominant structures. Such an interpretation leads the authors to question the canonical understanding of the role of the state and its military apparatus in the socio-religious life of Sirkap.",ucb,,https://escholarship.org/uc/item/0x37z9m1,,,eng,REGULAR,0,0
770,2206,"Behavioral and chemical ecology of ants (Hymenoptera, Formicidae) and their natural enemies in dynamic coffee agroecosystems.","Mathis, Kaitlyn A.","Tsutsui, Neil D;",2015,"Social insects rank among the most ubiquitous and ecologically dominant terrestrial animals on Earth. Complex communication and social organization are two defining features of social insect societies and ants, in particular, have evolved extensive systems of chemical communication. In both natural and agricultural systems, including coffee agroecosystems, ants are important predators and often have strong and complex effects on pest species.  In this dissertation, I explore how chemical communication plays a role in dynamics between ants and their natural enemies within these coffee agroecosystems, to gain a better understanding of both how ants utilize their own chemical communication systems and how natural enemies of ants take advantage of these systems to exploit ants.In the first dissertation chapter, I review the literature on chemical and visual cues that one natural enemy of the ant, the phorid fly, uses to successfully parasitized its host ants. Phorid fly parasitoids that use ants as hosts often require the use of multiple cues, ranging from general to highly specific, to home in on an ideal host. Here I outline the five common steps in which phorid flies use cues: (a) host habitat location, (b) host location, (c) host acceptance, (d) host discrimination and (e) host regulation. I then discuss our current understanding of how phorid flies use each of these steps to successfully parasitize ant hosts. Finally, I examine the wide variety of strategies and cues used by a multiple species of phorid flies within three separate genera (Apocephalus, Pseudacteon, and Neodohrniphora) and discuss future directions within this field of study. In the second dissertation chapter, I investigate the role of Azteca ant cuticular hydrocarbon cues as a short-range cue for a suite of Pseudacteon spp. phorid fly parasitoids commonly found within coffee agroecosystems. Here I describe the hierarchy of cues Pseudacteon spp. phorid flies use to successfully identify Azteca host ants. I use behavioral observations in the field to show phorid flies are attracted to two cryptic Azteca taxa, but will only attack Azteca sericeasur (Hymenoptera: Formicidae: Dolichoderinae). To test whether the phorid flies are able to distinguish between the two Azteca taxa using their cuticular hydrocarbons, I first document and compare the cuticular hydrocarbons of two cryptic Azteca taxa using gas chromatography-mass spectrometry (GC/MS). Using cuticular hydrocarbon transfer experiments with live ants from both Azteca taxa, I identify the cuticular hydrocarbons of Azteca sericeasur as a short-range host location cue used by Pseudacteon lasciniosus (Diptera: Phoridae) to locate the ants.  In the third chapter, I describe two new species of Myrmedonota (Coleoptera: Staphylinidae) that are also natural enemies of Azteca sericeasur ants.  Here I record this genus of beetle in Mexico for the first time, and describe M. xipe and M. shimmerale for the first time.  This chapter also documents that both Myrmedonota species will aggregate towards agitated ants, to prey on Azteca sericeasur and these species will form mating swarms, either with no apparent landmark or in the vicinity of ants.My fourth chapter examines the role of the beetle, Myrmedonota xipe, in Azteca ant-phorid fly interactions. Here I use pheromone bioassays to show that M. xipe is attracted to a component of A. sericeasur alarm pheromone. In the field experiments, I determine that the beetles are able to locate parasitized ants as prey items but not healthy ants.   In choice tests in the lab, I also show that beetles will preferentially attack parasitized ants over healthy ants.  Analysis of the choice tests also indicates that the aggression in parasitized ants is so reduced that beetles are essentially able to eat these ants alive without interruption. These results suggest that, although beetles are predators of the ants, by preying primarily on ants harboring phorid fly eggs, the beetles may also provide indirect positive effects for the ant colonies as a whole. In my final dissertation chapter I examine the variation in cuticular hydrocarbon blends of three species of arboreal twig nesting ants, Pseudomyrmex simplex, Pseudomyrmex ejectus and another Pseudomyrmex sp. (referred to as PSW-53) commonly found within the coffee agroecosystems of Southern Mexico.  In this study, I examined whether variation in the ecology and social structure of these species is reflected in the variation in their cuticular hydrocarbon profiles. I tested the hypothesis that the more abundant species with higher nest densities exhibit lower cuticular hydrocarbon variation. The results showed that worker ants of abundant P. ejectus and P. simplex from the study site exhibit significantly lower variation in cuticular hydrocarbon profiles than workers of the rare ant species P. PSW-53. Our study reinforces the idea that examining cuticular hydrocarbon profiles can provide insight into the colony structure of social insects.	Overall, the results from my dissertation provide insight into the complex interactions between ants and their natural enemies, particularly shedding light on how these interactions are facilitated through the use of ant pheromones as cues and signals.  The ants and their natural enemies that I focus on in my dissertation are also important players within the greater coffee agroecosystems in which they inhabit, and this body of work is thus critical for understanding how their network of interactions involving ants and their natural enemies may impact coffee pest control.",ucb,,https://escholarship.org/uc/item/0xr8p7ps,,,eng,REGULAR,0,0
771,2207,"Our Breaths We Take: Outdoor Air Quality, Health, and Climate Change Consequences of Household Heating and Cooking with Solid Fuels","Chafe, Zoe Anna","Smith, Kirk R;",2016,"Worldwide, nearly 3 billion peopleâ€”40% of the global populationâ€”burn wood, coal, and other solid fuels every day to cook their food; this number is even larger when including those who heat their homes with solid fuels as well. Exposure to pollution from heating and cooking fires causes about 3 million deaths each year, making it one of the biggest environmental health problems the world faces. The harm from this smoke is not restricted to those who breathe it, however: it contains gases and particles that contribute to global climate change as well. Chapter 2 shows that household cooking with solid fuels caused an estimated 12% of population-weighted ambient PM2.5 worldwide in 2010. Exposure to this air pollution caused the loss of 370,000 lives and 9.9 million disability-adjusted life years (DALYs) globally in the same year. In Chapter 3 I demonstrate that household heating with solid fuels caused an estimated 21% of population-weighted ambient PM2.5 in 2010 in Central Europe, 13% in Eastern Europe, 12% in Western Europe, and 8% in North America. Exposure to this air pollution results caused approximately 60,000 premature deaths in Europe, and nearly 10,000 deaths in North America, as well as an estimated 1.0 million disability-adjusted life years (DALYs) in Europe and 160,000 DALYs in North America. Chapter 4 addresses drivers of household wood combustion pollution in the San Francisco Bay Area, where the sector is the largest source of PM2.5 and regulators recently introduced amendments to wood burning rules for the airshed. Fireplaces are the source of the vast majority (84%) of PM2.5 from residential wood combustion in the San Francisco Bay Area, despite their use primarily as an aesthetic or recreational combustion activity. By evaluating hypothetical fuel and combustion device changeouts, I find that replacing fireplaces with gas would yield significant health and economic benefits. Specifically, retrofitting frequently used fireplaces (300,000 units) to gas inserts in the Bay Areaâ€™s nine counties would reduce sector emissions by about 90%, avoiding approximately 140-310 premature deaths and 19,000 lost days of work each year, and creating upwards of $1 billion in annual financial benefits from improved public health. Chapter 5 explains methodological overlaps and differences between the previous chapters. In Chapter 6, I explore the current regulatory and policy mechanisms specific to household heating with solid fuels, and relate these to the climate change implications associated with the sector. In Chapter 7, I highlight the relative dearth of data on household heating with biomass and its nuanced climate implications. This leads to a series of recommendations for future research, including collection of better household heating data in China and further work to understand how household combustion of biomass interfaces with both local air quality policy and climate change mitigation, outlining areas where this topic is currently visible in California.",ucb,,https://escholarship.org/uc/item/0z63m5v3,,,eng,REGULAR,0,0
772,2208,Progress Toward the Total Synthesis of Terpenoid Natural Products: the Neomangicols and the Yohimbine Alkaloids,"Wood, Jessica Louise","Sarpong, Richmond;",2011,"Progress has been made toward the total synthesis of a diverse array of natural products. Chapter 1 begins by introducing the isolation, bioactivity, and biosynthesis of the neomangicol and mangicol sesterterpenoids. Subsequent to that introduction, a summary of previous synthetic approaches to these natural products is presented. In the third section, our synthetic approaches are detailed, beginning with a first generation synthesis of the ABD tricycle, followed by a description of our revised route to the neomangicol tetracyclic core and our work toward the rearrangement of that core to the mangicol spirocyclic core. This chapter concludes with a summary of our accomplishments in this natural product area and outlines several strategies to achieve the desired rearrangement. The last section also includes our initial studies into the formation of the mangicol core. Preliminary work toward the synthesis of the ABD tricycle was performed by Dr. Brian Pujanauski.Chapter 2 details our work in the area of the yohimbine alkaloids. It begins with an introduction to these pentacyclic indole-containing natural products, discussing their isolation, proposed biosynthesis and giving a brief overview of the rich bioactivity that has been ascertained for these molecules. Closely related synthetic approaches are also described. Finally, our divergent approach to the synthesis of several yohimbine alkaloids, as well as strategic analogs, is delineated. Our proposal for rendering this route enantioselective is also detailed. This work was done in collaboration with Dr. Terry Lebold and Josh Deitch.",ucb,,https://escholarship.org/uc/item/11w011kh,,,eng,REGULAR,0,0
773,2209,"Urban Form, Wind, Comfort, and Sustainability: The San Francisco Experience","Kim, Hyungkyoo","Macdonald, Elizabeth;",2014,"In 1985, spurred by the residents' strong interest in the quality of the built environment and in securing comfort in public open spaces, San Francisco became the first city in North America to adopt a downtown plan, supplemented by a planning code, on ground-level wind currents to mitigate the effects of adverse wind. Since then, the plan has mandated that new developments in the downtown and four additional areas in the Rincon Hill, South of Market, Van Ness, and South Beach neighborhoods, all associated with high density or development potential and substantial outdoor activities, be designed or adopt wind-baffling measures so as to not cause ground-level wind current in excess of 7 mph in places for seating and 11 mph in those for walking for no more than ten percent of the time year round, between 7 am and 6 pm, to minimize potential discomfort generated by excessive ground-level wind currents; and 26 mph for no more than one hour per year to secure pedestrian safety. This research examines whether San Francisco's plan on ground-level wind currents made the city's public open spaces more comfortable and what is the impact on use of sustainable transportation modes. More specifically, it studies (1) whether the plan changed San Francisco's urban form so as to provide a more wind-comfortable environment; (2) whether the wind speed criteria stipulated in the plan effective determinants of outdoor comfort in San Francisco; and (3) whether the plan achieves a wind comfort level that would increase the residents' willingness to use sustainable transportation modes.Two types of methods were adopted in this research: wind tunnel tests and field studies. The wind tunnel tests, carried out in 2013 at the Center for Environmental Design Research (CEDR), use a boundary layer wind tunnel in which the wind movement in a selected urban area is simulated through use of a scale model of the area's built form. The field study, carried out from July 2012 to December 2012, consisted of pedestrian survey combined with on-site collection of microclimate data, such as wind speed, temperature, relative humidity, and solar radiation. The two methods are effective in addressing the relationships that the sub-research questions seek to examine and the nature of the variables that need to be measured. They also successfully incorporate a mixed-method approach that amalgamates qualitative methods such as observation, interview, and mapping with quantitative statistical analyses.This research presents the following findings. First, San Francisco's wind planning has changed the city's urban form so as to provide a more wind comfortable environment. Through a series of simulations using the boundary layer wind tunnel and comparing the wind speed ratios at 318 locations in the selected sites of Yerba Buena, Van Ness, Civic Center, and Mission Bay North in the 1985 and 2013 urban form conditions, it was discovered that the overall mean wind speed ratio dropped by 22 percent from 0.279 in 1985 to 0.218 in 2013. It means that the urban forms of the four sites have been changed so that the expected actual ground-level wind speeds have decreased by the same rate. However, there still exist a number of excessively windy places in San Francisco that are associated with specific urban form conditions, including direct exposure of street orientation to the west wind, high-rise building faÃ§ades that directly meet the ground, and continuous street walls. Second, through on-site surveys and microclimate measurements, it was discovered that wind speed significantly affects people's perceived outdoor comfort and that 11 mph is an effective criterion that determines outdoor thermal comfort in San Francisco. Significant differences are found in the frequency distributions of people's responses to all of the four comfort measures, which are thermal sensation, wind sensation, wind preference, and overall comfort. Also, the net effects of equivalent wind speed on the comfort measures are strong when the speed is less than 11 mph but become weaker when the speed is 11 mph or higher, meaning that there exists a difference in how much wind determines comfort between the two wind conditions. However, a wide range of dimensions on how people perceive wind and comfort exists, including adaptation, surrender, and avoid, which makes it difficult to judge the effectiveness easily.Third, the research findings suggest that San Francisco's wind planning does not achieve a wind comfort level that would increase people's willingness to use sustainable transportation modes. It was found that higher wind levels discourage people to wait at transit stop with no shelter, to bike, to walk outside, or to sit outside. Also, significant differences with regard to people's willingness to use sustainable transportation modes exist between when the equivalent wind speed is less than 11 mph and when it is 11 mph or higher. However, the net effects of equivalent wind speed in both wind conditions were not statistically significant, indicating that the criterion does not successfully determine whether people are comfortable enough to be willing to use sustainable transportation modes. Although the criterion was not originally developed to consider the use of sustainable transportation modes, it can be suggested that the criterion can be revised.A wide range of solutions must be studies for cities in varied climate regions. Cities and regions should not only study and develop their own climate-based ways to make a more climate-responsive city but also vigorously evaluate their effectiveness. Collaboration and cooperation between urban design, urban climatology, and many other relevant fields of expertise is crucial in future research and practice.",ucb,doctor of philosophy,https://escholarship.org/uc/item/1231t897,city and regional planning,,eng,REGULAR,0,0
774,2210,"The Work of Diaspora: Engaging Origins, Tradition and Sovereignty Claims of Jamaican Maroon Communities","Nisbett, Mario","Hintzen, Percy C;",2015,"This dissertation examines the concept of the African Diaspora by focusing on four post-colonial Maroon communities of Jamaica, the oldest autonomous Black polities in the Caribbean, which were established by escapees from slave-holding authorities during the seventeenth and eighteenth centuries. In exploring the Maroons as a Black community, the work looks at how they employ diaspora in making linkages to other communities of African descent and for what purposes.Maroons are being positioned in relation to the amorphous concept of diaspora, which is normally used to refer to people who have been dispersed from their place of origins but maintain tradition and connections with kin in other countries. However, I complicate the definition, arguing that diaspora, specifically the African Diaspora, is the condition that produces the collective consciousness of sameness rooted in the idea of common African origins based on a common experience of Black abjection. This understanding of diaspora opens the way to see that the uses of the concept are varied. This approach to diaspora challenges conventional debates in the humanities and social sciences on whether the concept is either a grouping of peoples, a process, or a method, making it possible to simultaneously engage all three modes along with their conceptual and theoretical contributions to the field of Diaspora Studies. Most importantly, the study permits us to see how the critical practice of diaspora is articulated in communities of African descent. Here, â€œcritical practiceâ€ refers to acts or utterances that critique, challenge, and re-position distorted understandings of particular peoples and communities. The African Diaspora seen as a critical practice ultimately challenges Western understandings of Black people. Another important concept is â€œarticulation,â€ as in enunciation and making linkages, which highlights the significance, aim, and utility of the critical practice of diaspora for different Black communities. This approach to diaspora as a critical practice that explores articulation is crucial for understanding the varying responses of Black peoples to global inequality and exclusion. It creates a nuanced approach to diaspora and shows how different Black communities may engage it in their own way.  In addition, this study demonstrates how diaspora, not race, as a unit of analysis for understanding the connection of peoples who are considered Black. I view race here to be a social construct that has no biological basis. Thus, in its articulations, diaspora is not a matter of subscribing to an essentialist racial agenda, but incorporates significant differences across diverse Black peoples to fully understand their lived realities and experiences. Furthermore, this view of diaspora permits an interdisciplinary approach to engage the fields of history, anthropology, literature, and political philosophy in the study. Such a comparative and interdisciplinary approach helps to explore systematically the significance of diaspora to Black peoples in general and at site-specific locations. In this case, it de-centers Americo-centric analysis by focusing on the Caribbean. Overall, the dissertation, through an innovative approach, explores how Black communities, particularly Jamaican Maroons, engage diaspora. Undeniably, diaspora, as a critical practice, has contributed much from its earliest articulation and will undoubtedly continue to contribute to an enhanced understanding of Black peoples. Arguably, exploration of the critical practice and articulation of diaspora demonstrates the significance of communities of African descent engaging in endeavors for Black autonomy and sovereignty against the discourse of Black inhumanity.",ucb,,https://escholarship.org/uc/item/15m4j459,,,eng,REGULAR,0,0
775,2211,The Molecular Mechanism of Host Responses to Viral Infection,"Schock, Suruchi Nandu","Winoto, Astar;",2014,"All living organisms, including humans, are constantly under attack by various pathogens such as viruses. Activation of appropriate host innate immune pathways like interferon and/or cell death is often critical for efficient viral clearance. Using biochemical and immunological methods, I have studied these two aspects of the antiviral response. I initially examined the role and regulation of the virus-induced host protein Tripartite Motif Containing Protein 21 (TRIM21) and the adaptor molecule Fas-Associated Death Domain (FADD) in the context of RNA virus infection. I found that TRIM21 functions in concert with FADD to negatively regulate ubiquitination of the transcription factor IRF7, thereby functioning in a negative feedback loop for the viral-induced interferon response. I have also identified the presence of a novel complex consisting of FADD, TRIM21 and RIP1 where TRIM21 and RIP1 regulate each other's ubiquitination status. FADD and RIP1 have been recently implicated in an alternative form of programmed cell death: necroptosis, whose physiological function is not completely clear but it has been suggested to serve as a backup host pathway to fight viral infection. To investigate this possibility, I have screened seven viruses for their ability to induce necroptosis. I found two of them, Sendai virus and MHV68, are capable of inducing necroptosis, particularly in conditions when apoptosis is blocked. I found that MHV68 activates the cytoplasmic sensor molecule STING, leading to production of tumor necrosis factor (TNF) and subsequently causing necroptotic death. In contrast, Sendai virus induced death occurs independently of TNF or the adaptor STING. Instead, Sendai virus-mediated necroptosis requires the RNA sensor RIG-I in conjunction with the deubiquitin protein CYLD and several Sendai virus proteins, leading to de-ubiquitination of RIP1 and formation of RIP1/3 necrosome to promote necroptosis. These data are consistent with the notion that necroptosis may be an additional antiviral mechanism that hosts can employ when apoptosis is blocked. Necrotic cells can then release inflammatory contents which may help alert the immune system. These findings offer insight into the complex host-pathogen relationship, and with continued study may help guide the judicious development of antiviral drugs and vaccines.",ucb,,https://escholarship.org/uc/item/1644r8bt,,,eng,REGULAR,0,0
776,2212,Quantum Coherence and Energy Landscapes in Photosynthetic Systems Investigated with Two-Dimensional Electronic Spectroscopy,"Calhoun, Tessa Rae","Fleming, Graham R;",2010,"Two-dimensional (2D) electronic spectroscopy has recently emerged as a powerful technique for the study of complex photodynamics in a variety of condensed phase systems. The application of this technique to both photosynthetic pigment-protein complexes and chromophore solutions has provided insight into their intricate excitation energy transfer mechanisms and landscapes. Analysis of beating peak amplitudes in 2D spectra of the Fenna-Matthews-Olson bacteriochlorophyll complex combined with changing lineshapes has revealed signals consistent with excitonic coherence. In addition, the long lifetime of the coherence indicates a reversible, wavelike motion of energy through the complex as opposed to the classical hopping picture. This quantum-mechanical behavior may explain the near unity quantum efficiency of excitation energy transfer observed in networks of photosynthetic complexes. The inclusion of a noncollinear optical parametric amplifier producing broad bandwidth pulses enables the exploration of excitonic coherence in Light Harvesting Complex II, the most abundant antenna complex in higher plants. Long-lived quantum coherence is again observed suggesting this to be a universal phenomenon in natural photosynthetic systems. To expand upon these findings, a coherence power spectrum is produced. This novel technique allows the first direct experimental determination of the excitonic energy levels. In another set of experiments, the building an adaptive, pulse-shaping apparatus allows optimal compression of the broad bandwidth pulses for use in probing the debated electronic structure of Î²-carotene. Oscillating lineshapes reveal coupling of electronic states to high energy vibrational modes while the short pulses allow the initial dynamics of this system to be studied in unprecedented detail revealing several new features whose origins are still under investigation.",ucb,,https://escholarship.org/uc/item/17w0d9v2,,,eng,REGULAR,0,0
777,2213,Wildlife Monitoring and Conservation in a West African Protected Area,"Burton, Andrew Cole","Brashares, Justin S;",2010,"Global declines in biological diversity are increasingly well documented and threaten the welfare and resilience of ecological and human communities. Despite international commitments to better assess and protect biodiversity, current monitoring effort is insufficient and conservation targets are not being met (e.g., Convention on Biological Diversity 2010 Target). Protected areas are a cornerstone of attempts to shield wildlife from anthropogenic impact, yet their effectiveness is uncertain. In this dissertation, I investigated the monitoring and conservation of wildlife (specifically carnivores and other larger mammals) within the context of a poorly studied savanna reserve in a tropical developing region: Mole National Park (MNP) in the West African nation of Ghana.  I first evaluated the efficacy of the park's long-term, patrol-based wildlife monitoring system through comparison with a camera-trap survey and an assessment of sampling error. I found that park patrol observations underrepresented MNP's mammal community, recording only two-thirds as many species as camera traps over a common sampling period. Agreement between methods was reasonable for larger, diurnal and social species (such as many larger ungulates and primates), but camera traps were more effective at detecting smaller, solitary and nocturnal species (particularly carnivores). Long-term patrol data were also subject to considerable sampling variation that could make interpretation of wildlife trends unreliable, and I suggest ways in which this locally based monitoring program may be improved.Given the ecological and cultural importance of carnivore species, their propensity for human conflict, and the difficulty with which they are monitored, I assessed their status and vulnerability to extinction in MNP. Only 9 of 16 historically occurring carnivore species were detected in the camera-trap survey (covering 253 stations deployed for 5,469 trap days between October 2006 and January 2009). A hierarchical multi-species occupancy model applied to camera-trap data indicated a low overall likelihood of the presence of undetected species. Results from concurrent sign, call-in, and village surveys, as well as patrol records, provided more equivocal evidence of carnivore occurrence but supported the conclusion that many carnivores have declined and are likely functionally or fully extirpated from the park, including the top predator, lion (Panthera leo). Evidence of local human-carnivore conflict was also documented, including hunting of carnivores for traditional use and in retaliation for livestock depredation. Contrary to expectation, variation in carnivore persistence was not explained by ecological or life-history traits such as body size, home range size or fecundity, thus raising doubt as to the predictability of carnivore community disassembly. I extended the multi-species occupancy model to test hypotheses about extrinsic influences on carnivore community dynamics in MNP. I derived spatially explicit GIS descriptors of heterogeneity in illegal hunting pressure, law enforcement patrol effort, prey biomass, and habitat productivity, and used a Bayesian modeling framework to assess support for their effects on carnivore occurrence. The framework explicitly accounted for spatial autocorrelation and variation in species- and site-specific detection probabilities. Contrary to my expectation, there was no indication of a consistent, negative effect of illegal hunting activity on spatial patterns of carnivore occurrence. By contrast, occurrence patterns of most species were positively associated with prey biomass, and several species had either positive or negative associations with riverine forest (but not with other indicators of habitat heterogeneity). I conclude that pressure from hunting and other anthropogenic impacts remains high for West African wildlife, even within protected areas, but that human-wildlife relations are complex and their consequences inadequately predicted by simple models of extinction risk. Existing monitoring programs may generate data unsuitable for strong inference on wildlife community dynamics, and careful attention to objectives and methodology is needed. More attention to the protection and recovery of carnivore populations is also needed, as are further focused and interdisciplinary efforts to inform and improve wildlife conservation in West Africa.",ucb,,https://escholarship.org/uc/item/1d90h6xw,,,eng,REGULAR,0,0
778,2214,Nature As Discourse: A Co-Evolutionary Systems Approach to Art and Environmental Design,"Hays, Susannah","Cranz, Galen;",2016,"ABSTRACTNature As Discourse:A Co-Evolutionary Systems Approach to Art and Environmental Designby Susannah HaysDoctor of Philosophy in Interdisciplinary Studies University of California, BerkeleyProfessor Galen Cranz, ChairTransdisciplinarity, an international education movement that explores pathways to a coherent epistemology beyond all disciplines, seeks to become a sustaining vital force in human development. To do so, it needs to be complemented by a branch of epistemology called epistemics or self-knowledge. Only if co-evolutionary phylogenetic principles of human-brain and autonomic nervous system functioning are included in transdisciplinarityâ€™s model can individuals experientially evolve to the levels of reality the model entails. An actual, â€œtrue to life,â€ transdisciplinary education teaches isomorphic qualities intrinsic to perception, pattern mapping, language, and aesthetic (non-directive) skills. Curricula utilizing these educational tools will result in indispensable, creative learning environments. A trajectory not yet explored in other literature on Transdisciplinarity is an emphasis on cross-cultural research in human- brain and autonomic nervous system dynamics. Three key understandings that guide human biological evolutionary processes toward higher levels of consciousness are Paul MacLeanâ€™s triune-brain neuroethology, Stephen Porgesâ€™ Polyvagal Theory of emotions, and G. I. Gurdjieffâ€™s three-centered self-study practice. Each chapter describes a non-profit organization whose goal is to raise humanityâ€™s normative level of participation in environmental sustainability. These organizations demonstrate how Transdisciplinarity can recalibrate human evolution, if the educational movement synthesizes the autonomic/cognitive forces within Homo sapiensâ€™ biological organization.",ucb,,https://escholarship.org/uc/item/1dj8x8hb,,,eng,REGULAR,0,0
779,2215,Host-Factor Enhancement of Therapy for Tuberculosis,"Schump, Michael","Riley, Lee W;",2015,"Host-Factor Enhancement of Therapy for TuberculosisbyMichael David SchumpDoctor of Philosophy in Infectious Diseases and ImmunityUniversity of California, BerkeleyProfessor Lee W. Riley, ChairTuberculosis (TB) is a disease of major public health importance and improvements to its treatment could greatly benefit efforts aimed at eliminating the disease. Current treatment options for TB are limited in effectiveness and have numerous fundamental failings due to the necessarily lengthy duration of therapy and toxicity of the antimicrobial drugs deployed, among other issues. The studies described herein where undertaken with the goal of developing adjunctive treatments or modifications of existing treatments which could improve the treatment course, outcome, or both for standard TB antimicrobial chemotherapy.Three areas of research are discussed beginning with adaptive immune augmentation through therapeutic vaccination, proceeding to investigations of innate immune adjuvant therapy and concluding with host environment mediated improvement of selectivity index of TB antimicrobial compounds.A post-treatment, therapeutic vaccine was studied with the goal of developing a tool which could prevent relapse or reactivation disease. Though the project was a follow up to a study which demonstrated exceptional protection, the vaccine candidate did not demonstrate any detectable efficacy in three parallel murine infection experiments. Possible reasons and implications of this failure are discussed.Because correlates of protection for adaptive immunity to TB are poorly understood and have not proven to be tractable for intervention, innate immune enhancement was investigated. Autophagy, a cell-intrinsic process with antimicrobial capabilities, was selected due to its well described tuberculocidal activity and pharmacologic manipulability. However, despite the apparent capacity of some test compounds to increase autophagic flux, none demonstrated robust restriction of mycobacterial growth in murine or human macrophages. That study did, however, lead to the serendipitous discovery that pH based drug partitioning can increase the selectivity index of antimicrobial drugs against M. tuberculosis inside cultured macrophages.",ucb,,https://escholarship.org/uc/item/1g64c7zf,,,eng,REGULAR,0,0
780,2216,"The Linguistic Ecology of Northwestern California: Contact, Functional Convergence and Dialectology","Conathan, Lisa",,2004,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0jj5h838,,,eng,REGULAR,0,0
781,2217,Electronic Properties of Low-Dimensional Materials Under Periodic Potential,"Jamei, Mehdi","Zettl, Alex;Javey, Ali;",2015,"In the quest for the further miniaturization of electronic devices, numerous fabrication techniques have been developed. The semiconductor industry has been able to manifest miniaturization in highly complex and ultra low-power integrated circuits and devices, transforming almost every aspect of our lives. However, we may have come very close to the end of this trend. While advanced machines and techniques may be able to overcome technological barriers, theoretical and fundamental barriers are inherent to the top-down miniaturization approach and cannot be circumvented.As a result, the need for novel and natural alternatives to replace old materials is valued now more than ever. Fortunately, there exists a large group of materials that essentially has low-dimensional (quasi-one- or quasi-two-dimensional) structures. Graphene, a two-dimensional form of carbon, which has attracted a lot of attention in recent years, is a perfect example of a prime material from this group. Niobium tri-selenide (NbSe3), from a family of trichalcogenides, has a highly anisotropic structure and electrical conductivity. At sufficiently low temperatures, NbSe3 also exhibits two independent â€œsliding charge density wavesâ€â€“ an exciting phenomenon, which could be altered by changing the overall size of the material.In NbSe3 (and Blue Bronze K0.3MoO3 which has a similar structure and electrical behavior), the effect of a periodic potential could be seen in creating a charge density wave (CDW) that is incommensurate to the underlying lattice. The required periodic potential is provided by the crystal ions when ordered in a particular way. The consequence is a peculiar non-linear conductivity behavior, as well as a unique narrow-band noise spectrum. Theoretical and experimental studies have concluded that the dynamic properties of resulting CDW are directly related to the crystal impurity density, and other pinning potentials. Therefore, reducing the overall size of the crystal could potentially alter the CDW behavior in a significant way.Theoretical studies, as well as preliminary experimental results, suggest exceptionally interesting charge carrier behavior, including an energy gap opening and an anisotropic modulation of carrier mobility, in graphene when it is under a periodic potential.  The fabrication process to achieve the desired periodic structure, with the required length scale on graphene is a challenging one. Therefore, in this manuscript, the fabrication process and its challenges are discussed.The arrangement of the manuscript is as follows: In Chapter 1, first, I study the theory of charge density waves and their dynamics. Next, I describe the fabrication process for thin NbSe3 and Blue Bronze crystals and devices. Finally, I discuss the device measurement results, and compare them with bulk crystals. In Chapter 2, I focus on the fabrication of periodic potentials on graphene layers. I begin by providing the theoretical background and motivations of the project. Then, the fabrication process is discussed in details. And lastly, I present the fabrication and preliminary electrical measurement results.  Chapter 3 is a summary of additional experiments that I performed during the course of my PhD.",ucb,,https://escholarship.org/uc/item/0k5728t9,,,eng,REGULAR,0,0
782,2218,UV Second-Harmonic Studies of Concentrated Aqueous Electrolyte Interfaces,"Otten, Dale Edward","Saykally, Richard J;",2010,"The nature of liquid-vapor interfaces is a rapidly developing field of research, aimed at ascertaining the properties and structure of this unique microscopic environment.  The mechanism of aqueous electrolyte partitioning and chemistry in the aqueous-vapor interface region is explored herein, using the surface selective technique of second-harmonic generation to probe these systems via strong electronic resonances in the ultraviolet.In Chapter 1, the current descriptions of the neat water- and electrolyte solution-vapor interfaces are reviewed.  Mechanistic explanations for anion adsorption to such interfaces are highlighted as an object of research.  Previous applications of the surface second-harmonic technique to aqueous electrolyte systems are also described, and the results of these studies are built upon herein.In Chapter 2 the principles of applying second harmonic generation as a spectroscopic probe of liquid-vapor interfaces are discussed, followed by a consideration of the adapted Langmuir model used to interpret such studies.  This model is then developed to include both cations and anions as a consequence of the requirement of electroneutrality of the solution-vapor interface.  The resulting expressions are then identified as diagnostics for mechanisms of ionic adsorption to the solution interface.  A discussion of the technical requirements and developments necessary to apply femtosecond ultraviolet second-harmonic spectroscopy to these systems in a reliable and reproducible manner is also presented in Chapter 2.The models developed in Chapter 2 are used to interpret the interfacial adsorption processes of aqueous sodium nitrite and sodium nitrate solutions in Chapter 3.  Nitrite surface activity is found to exhibit second-order bulk concentration dependence which is then interpreted to indicate the adsorption of nitrate and sodium into the interface region as ion-pairs.  These ion-pairs are found to adsorb with a standard Gibbs free energy of -37Â±1 kJ*mol-1.  The related sodium nitrate electrolyte is not found to be strongly surface active.The temperature dependence of the surface activity of aqueous potassium thiocyanate is explored in Chapter 4, and in terms of an adapted Langmuir adsorption model is found to be an exothermic process (-5.9Â±0.2 kJ*mol-1) with a weakly unfavorable entropic component (-8Â±1 J*mol-1).  The process is also found to exhibit first-order dependence on bulk electrolyte concentration, indicating that the cation and anion translations are not highly correlated.  These results are discussed in the framework of current theories of anionic interface adsorption mechanisms and alternative explanations are considered.The UV photochemical products of aqueous potassium and sodium thiocyanate are also observed to obscure the second-harmonic response of thiocyanate in the interface, and elemental sulfur is proposed to be excluded from the bulk solution into the interface in this system.  In Chapter 5 the effect of oxidation on the interface of aqueous sodium iodide solutions is also investigated, determining that much like thiocyanate the second-harmonic signal is obscured by the products that are generated.  The potential for exploring rate-law behavior at aqueous electrolyte interfaces by this method is also established.  Lastly, the ultraviolet second-harmonic spectrum of sodium iodide at molar and millimolar concentrations is found to be qualitatively different to previously reported spectra.  Two postulates are made to explain this variance, one being an experimental configuration difference, the other being due to the oxidation products found in these systems.",ucb,,https://escholarship.org/uc/item/0kq3w6pk,,,eng,REGULAR,0,0
783,2219,Essays on Deception and Lying Aversion,"Gawn, Glynis","Perloff, Jeffrey M;",2015,"AbstractEssays on Deception and Lying AversionByGlynis Margaret E. GawnDoctor of Philosophy in Agricultural and Resource EconomicsUniversity of California, BerkeleyProfessor Jeffrey M. Perloff, ChairThis dissertation consists of three experimental essays on deception and lying aversion.  Chapter 2, â€œDo Lies Erode Trust?â€ studies the interaction between honesty and trust and trustworthiness.  Specifically, the chapter investigates the effect of being lied to or told the truth in a Gneezy (2005) deception game on behavior in a subsequent trust game with different players.  Treatment effects are decomposed between the impacts of being â€œburnedâ€ by a low payoff in the deception game, mood change, and the specific experience of a lie.  The specific experience of being lied to significantly erodes trust, trustworthiness, and the use of communication to promote trust.  However, the experience effect on trustworthiness occurs only for subjects who are burned. 	Chapter 3, â€œPure Lying Aversionâ€, studies several factors affecting the propensity to tell the truth when no one would be directly negatively impacted by the lie.  Utilizing a simple experiment, the effect of the strength of the message one is using to convey information is examined, while the economic incentive to lie is also varied.  The effect of being lied to in a prior interaction on oneâ€™s subsequent truthfulness is also studied in a separate set of experiments.  The strength of the message has a strong effect on truthfulness regardless of the incentive to lie, while the effect of the size of the economic gain from lying has a non-monotonic effect on truthfulness.  Additionally, the effect of knowledge about whether one has been lied to before interacts with the payoff outcome received in the prior interaction to reduce truthfulness in some cases and increase it in others.	Chapter 4, â€œLying Through Othersâ€, considers the question of how agency relationships, ubiquitous in economic interactions, affect an individualâ€™s willingness to lie for monetary advantage?  Does individual lying aversion tend to decline if the lie (or truth) is sent through an agent, rather than sent directly by the individual?  In three experiments that control for the effects of delegation on preferences over payoffs and probabilities of actions, it is found that delegation reduces â€“ but does not eliminate â€“ lying aversion.",ucb,,https://escholarship.org/uc/item/0n1899r5,,,eng,REGULAR,0,0
784,2220,Essays on Technology and the Environment from an International Perspective,"Barrows, Geoffrey","Zilberman, David;",2015,"In this dissertation, I present three essays that consider the environmental consequences of technological change, from an international perspective. The first two chapters use firm-level production data to estimate the response of CO2 emission intensity to changes in competition in foreign markets. The first chapter estimates this response with respect to foreign demand shocks, i.e., a positive shock to exports. The second chapter exploits a specific liberalization episode to estimate the impact with respect to foreign competition shocks, i.e., a negative shock to exports. Both papers are co-authored with Helene Ollivier. The final chapter analyzes the decision to adopt genetically engineered seeds in different countries around the world, and the attendant impacts on supply and land-use. This last chapter is co-authored with David Zilberman and Steven Sexton and was previously published in Environment and Development Economics.The first chapter investigates the impact of exporting on the CO2 emission intensity of manufacturing firms in India.  Recent papers have argued that export market access encourages firms to upgrade technology, which lowers the emission intensity of production; however, data limitations confound previous attempts to separately identify productivity impacts from simultaneous changes in prices and product-mix.  We present a model of how these alternative channels could also explain the results documented in the literature.  Then, using a highly detailed production dataset of large Indian manufacturing firms that contains information on physical units of inputs and outputs by product, we are able to decompose the overall firm impact into three components -- prices, product-mix, and technology.  Export impacts at the firm level are identified from import demand shocks of foreign trading partners.  We find that prices systematically bias down estimates of emission intensity in value, that firms adjust emission intensity in quantity through changing output shares across products, but that firms do not lower emission intensity within products over time (technology).  The results imply that the productivity benefits from market integration alone are not enough to induce clean technology adoption. The second chapter investigates the ``third-party'' impact of trade liberalization on the environmental performance of firms in countries that lose market share as a result of the liberalization. If competition matters for exporting (as previous research indicates), and exporting matters for emission intensity, then emission intensity reductions in liberalized markets may be offset by emission intensity increases in countries peripheral to the liberalization. To test for this indirect effect, we exploit quasi-natural variation arising from the elimination of quota constraints on textile and apparel exports to the US between 1994 and 2007. Using a detailed panel of production and emission data at the firm-product level, we find that Indian exporters in Prowess lost on average 14% export sales as a result of liberalized trade between the US and India's competitors. This loss of export sales was accompanied by an increase in CO2 intensity of 9%. The results do not appear to be due to fuel-switching, but there is suggestive evidence that capital investments and switching to higher emission intensity varieties may have played a role. Overall, the results support the importance of international competition for production and pollution decisions of firms around the world.The final chapter uses aggregate data to estimate supply, price, land-use, and greenhouse gas impacts of genetically engineered (GE) seed adoption due both to increased yield per hectare (intensive margin) and increased planted area (extensive margin).  An adoption model with profitability and risk considerations distinguishes between the two margins, where the intensive margin results from direct ``gene"" impacts and higher complementary input use, and the extensive margin reflects the growing range of lands that become profitable with the GE technology.  We identify yield increases from cross-country time series variation in GE adoption share within the main GE crops- cotton, corn, and soybeans.  We find that GE increased yields 34% for cotton, 12% for corn and 3% for soybeans. We then estimate quantity of extensive margin lands from year-to-year changes in traditional and GE planted area.  If all production on the extensive margin is attributed to GE technology, the supply effect of GE increases from 5% to 12% for corn, 15% to 20% for cotton, and 2% to 40% for soybeans, generating significant downward pressure on prices. Finally, we compute ``saved"" lands and greenhouse gases as the difference between observed hectarage per crop and counterfactual hectarage needed to generate the same output without the yield boost from GE. We find that all together, GE saved 13 million hectares of land from conversion to agriculture in 2010, and averted emissions are equivalent to roughly 1/8 the annual emissions from automobiles in the US.",ucb,,https://escholarship.org/uc/item/0nj2q50r,,,eng,REGULAR,0,0
785,2221,Naturally Occurring Melanin Synthesis Regulators and Their Modes of Action,"Satooka, Hiroki","Kubo, Isao;",2011,"The effect of various naturally occurring and structurally related chemical compounds on mushroom tyrosinase and B16-F10 melanoma cells were examined. For each natural product or related compound, the detailed mechanism of regulatory effect on tyrosinase-catalyzed melanin synthesis was elucidated, and occasionally, the optimal mechanism of cytotoxicity on B16 melanoma cells exhibited by chemical compounds are evaluated. Arbutin (hydroquinone-o-Î²-D-glucopyranoside), a well-known depigmenting reagent, was oxidized by tyrosinase to the corresponding o-quinone with an extremely slow rate, and this reactive metabolite caused melanocytotoxicity, resulting in the antimelanogenic effect. Another monophenol derivative, thymol (5-methyl-2-isopropylphenol), on the other hand, did not act as either a substrate or an inhibitor, but it acted as a redox inhibitor, due to its prooxidative property, to disrupt the melanin formation. This prooxidant effect also triggered the prooxidative-related toxicity on melanoma cells. In the case of cardols (5-alkylresorcinol), naturally occurring resorcinolic lipid, hydrophilic head, and hydrophobic tail concept was applied to the mechanism of both tyrosinase inhibition and cytotoxicity. More specifically, resorcinol moiety quickly and reversibly bound to binuclear copper of tyrosinase, and then, hydrophobic tail portion slowly and irreversibly interacted with the hydrophobic portions proximate to the active site of the enzyme. However, this inhibitory mechanism was only observed when the hydrophobic alkyl chain was long enough to interact with the hydrophobic portion of the enzyme. In the case of cellular effect, cardol (C15:3) preferentially acted as a surfactant to disrupt the function of cellular membrane while cardol (C5:0) produced prooxidant-related toxicity. Alkyl-3,5-dihydroxybenzoate and 3,5-dihydroxyphenyl alkanoate with various lengths of alkyl chain were examined to clarify the effect of the lengths of alkyl chain on inhibitory and toxic effect. In the cases of both alkyl-3,5-dihydroxybenzoate and 3,5-dihydroxyphenyl alkanoate, basically, both compounds with alkyl chain longer than C9 caused a two-step inactivation on mushroom tyrosinase. In addition, alkyl-3,5-dihydroxybenzoate acted as a surfactant to cause cytotoxicity while 3,5-dihydroxyphenyl alkanoate were decomposed in the growth medium. Alkyl-3,5-dihydroxybenzoate, interestingly and importantly, inhibited melanogenesis without affecting any cell growth, which is due to the combined effect of the two-step tyrosinase inactivation and surfactant activity. Finally, the effect of polyphenolic compounds, resveratrol and luteolin, were subjects to elucidate their effect on tyrosinase and B16 melanoma cells. In both cases, tyrosinase oxidized them to the corresponding o-quinone. However, in the case of resveratrol (trans-3,5,4'-trihydroxystilbene), the corresponding o-quinone irreversibly interacted with tyrosinase, indicating that kcat type (suicide) inhibition was the mechanism of inhibition. Resveratrol did not show any toxicity up to 200 Î¼M, and at 200 Î¼M, melanogenesis was suppressed with the addition of resveratrol. Luteolin (2-(3,4-Dihydroxyphenyl)-5,7-dihydroxy-4-chromenone), on the other hand, the corresponding o-quinone did not inhibit tyrosinase but did act as a redox cycler, which oxidized leukodopachrome to dopachrome when luteolin was coexisted L-DOPA and tyrosinase. In the case of luteolin, luteolin o-quinone was also an active principle for the toxicity. Through the investigation, the biological significance of each molecule was observed. Based on the dynamic perspective and the biological significance, the possible utilization of these chemicals was also discussed.",ucb,,https://escholarship.org/uc/item/0pc7b3f7,,,eng,REGULAR,0,0
786,2222,Characterization of the Essential Role of the miR-200 family of microRNAs in Mucociliogenesis,"Cisson, Jennifer Lauren","He, Lin;",2016,"MicroRNAs are small, noncoding RNAs that play essential roles in regulation of a variety of processes including development, cellular feedback mechanisms, and reproductive biology.  The studies that follow focus on a particularly pleiotropic set of miRNAs: the miR-200 family.  For example, the miR-200 family is known to be crucial for differentiation and maintenance of specialized epithelial cell types, which corresponds to its role in regulation of Epithelial-Mesenchymal Transition (EMT).1â€“4 miR-200 consists of five homologous members, divided between two distinct genomic loci and can be further segregated into two subfamilies based on seed sequence homology: miR-200a/141 and miR-200b/200c/429.   In an effort to discern the importance of the miR-200 family of miRNAs, we acquired and intercrossed mir-200b/a/429 (â€œ200abâ€) and mir-200c/141 (â€œ200câ€) heterozygotes to breed single and double knockout mice (a generous gift from the Bradley Lab at the Sanger Institute).5  I discovered a striking postnatal lethality in mir-200ab/c double knockout (DKO) progeny, whereby neonates survive less than 16 hours post parturition.  In an effort to characterize the lethal effect of losing miR-200, I performed behavioral studies, expression profiling, histopathological analysis, and electron microscopy.  Interestingly, DKO pups exhibit no milk spot, and develop labored breathing, along with pale, sticky skin within hours after birth.  As expected, epithelial tissues such as the lungs and gastrointestinal tract exhibit the highest expression level of miR-200 between E18.5 and P0.  Although tissue architecture appears relatively normal in DKO epithelial tissues immediately after birth, I observed rapid accumulation of debris and mucus in the large and small airways that correlates with cyanosis.  Intriguingly, the phenotypic changes in the lungs, gut, and skin of DKO pups are comparable to human symptoms of cystic fibrosis and asthma, which may provide novel insights into these diseases, where current mouse models fail.6,7  We have acquired a mir-200c/141: LacZ reporter (a generous gift from the McManus Lab, UCSF), and used double immunofluorescence (IF), FACS, and single cell qPCR to determine the cell population expressing miR-200 is largely motile ciliated cells (MCCs).  In addition, we have found that miR-200 is highly enriched in MCCs during frog development in published datasets.8  Finally, in single (200ab) knockout males, I observed an infertility defect that correlates with mislocalized flagella â€“ a structure related to the cilia in MCCs.  Future studies include further characterizing the mucociliary defect, and identifying miR-200 targets.  Overall, I have shown that miR-200 is absolutely essential for the proper function and maintenance of the mucociliary epithelium lining the airways.  Loss of miR-200 leads to excessive mucus secretion into the airway lumen, and correspondingly, respiratory distress culminating in death.",ucb,,https://escholarship.org/uc/item/0qx3k429,,,eng,REGULAR,0,0
787,2223,"Advancing Health Impact Assessment: A Study of Training, Practice and New Approaches in the United States","Schuchter, Joseph","Seto, Edmund YW;Satariano, William A;",2013,"In an era of growing interest in transdisciplinary collaboration, evidence-based decision-making, open government, and social impact strategies responding to political and economic challenges, Health Impact Assessment (HIA) is increasingly relevant. HIA sits at the juncture of a number of paradigms for democratic processes for dealing with uncertainty and adding value in decision-making. It draws from a rich history of impact assessment that has accounted for multiple bottom lines. While HIA has gained attention as a specific tool, it is also recognized as part of a suite of more ecological and equitable approaches to health. HIA developers are asking both how to make it work better, so that ultimately government will work better.This research examines the state of HIA in the United States. It examines the earliest efforts to train a variety practitioners across the country, acknowledging multiple opportunities for capacity-building and many influences on effective HIA practice. More importantly, it identifies a broad definition of effectiveness. Research on HIA practice builds on this, finding that practice is not fully aligned with standards but not necessarily deficient. While objectives should guide HIA processes, the research on training and practice highlights resources as a key driver. The third component of this research considers the resource constraints of public health in general and the opportunities to leverage outside resources using the paradigm of HIA.In moving the field forward, frameworks for community-based prevention and transdisciplinary education can inform HIA capacity-building. Evaluation of both processes and outcomes will be useful. While methodological challenges remain, the institutionalization of partnerships, processes, and indicators will support public health goals. The definition and standardization of HIA practice must be balanced with efforts to expand its utility in new areas such as community development. In such cases the HIA process and paradigm can leverage investments by estimating returns in health and social denominations. HIA also helps solve the ""wrong pocket"" problem by accounting for outcomes across sectors and institutions. If used wisely, HIA will be a critical component of health in all policy, sustainability agendas, and social impact strategies.",ucb,,https://escholarship.org/uc/item/0r336861,,,eng,REGULAR,0,0
788,2224,"Securing the Internet of Things via Locally Centralized, Globally Distributed Authentication and Authorization","Kim, Hokeun","Lee, Edward A;",2017,"The Internet of Things (IoT) brings about benefits through interaction with humans and the physical world using a variety of technologies including sensors, actuators, controls, mobile devices and cloud computing. However, these benefits can be hampered by malicious interventions of attackers when the IoT is not protected properly. Hence, authentication and authorization comprise critical parts of basic security processes and are sorely needed in the IoT. Characteristics of the IoT render existing security measures such as SSL/TLS (Secure Socket Layer/Transport Layer Security) and network architectures ineffective against emerging networks and devices. Heterogeneity, scalability, and operation in open environments are serious challenges that need to be addressed to make the IoT secure. Moreover, many existing cloud-based solutions for the security of the IoT rely too much on remote servers over possibly vulnerable Internet connections.This dissertation presents locally centralized, globally distributed authentication and authorization to address the IoT security challenges. Centralized security solutions make system management simpler and enable agile responses to failures or threats, while having a single point of failure and making it challenging to scale. Solutions based on distributed trust are more resilient and scalable, but they increase each entity's overhead and are more difficult to manage. The proposed approach leverages an emerging network architecture based on edge computers by using them as locally centralized points for authentication and authorization of the IoT. This allows heterogeneity and an agile access control to be handled locally, without having to depend on remote servers. Meanwhile, the proposed approach has a globally distributed architecture throughout the Internet for robustness and scalability.The proposed approach is realized as SST (Secure Swarm Toolkit), an open-source toolkit for construction and deployment of an authentication and authorization service infrastructure for the IoT, for validation of locally centralized, globally distributed trust management. SST includes a local authorization entity called Auth to be deployed on edge computers which are used as a gateway for authorization as well as for the Internet. Software building blocks provided by SST, called accessors, enable IoT developers to readily integrate their IoT applications with the SST infrastructure, by encapsulating cryptographic operations and key management. In addition to protection against network-based intruders, SST supports a secure migration mechanism for enhancing availability in the case of failures or threats of denial-of-service attacks, based on globally distributed and trusted Auths.For evaluation, I provide a formal security analysis using an automated verification tool to rigorously show that SST provides necessary security guarantees. I also demonstrate the scalability of the proposed approach with a mathematical analysis, as well as experiments to evaluate security overhead of network entities under different security profiles supported by SST. The effectiveness of the secure migration technique is shown through a case study and simulation based on a concrete IoT application.",ucb,,https://escholarship.org/uc/item/01k1q7cg,,,eng,REGULAR,0,0
789,2225,An Evaluation of Market Based Policy Instruments for Clean Energy in the Global South,"Tran, Jimmy H.","O'Rourke, Dara J.;",2015,"The growth of carbon markets over the past decade has emerged as a powerful form of pro-poor financing that has quickly increased the number of rural household energy efficiency programs implemented in the global south. This dissertation explores the role that market-based policy instruments can have in advancing the dual goals of rural energy access and sustainable development in the global south. Historic low carbon prices combined with contentious international climate negotiations and an uncertain future for emission trading systems serve as the context for this study, which offers insights for policymakers in structuring future market mechanisms for increasing energy access for the global poor. My findings highlight the important role of nonstate actors in creating predominantly private and voluntary systems of market-based policy initiatives that are only now emerging in the face of faltering international action for climate change in a post-Kyoto Protocol era. Through grounded case studies I examine key assumptions that underlie carbon accounting rules and metrics to understand the consequences for practical monitoring, reporting, and verification (MRV) under market-based policies. I also present empirical data from household air pollution and fuel consumption measurements from village homes in China to highlight the importance of developing robust monitoring protocols for new technologies prior to inclusion into market-based programs.",ucb,,https://escholarship.org/uc/item/01k4z0mg,,,eng,REGULAR,0,0
790,2226,Information Markets and Aggregation,"Phatak, Narahari Mohan","Parlour, Christine;",2012,"Markets serve a price discovery function. In commodity markets, this supports efficient trade between agents with different preferences and endowments. Prices equilibrate so that aggregate supply of a good matches aggregate demand for that good. In financial markets, agents form demands on the basis of both preferences and information about cash flows and discount rates. Here, prices take on additional significance. In the chapters that follow, I explore how markets aggregate private information in experimental and natural settings. Each chapter considers information and aggregation from a different perspective.Chapter one considers bounded rationality and information aggregation in an experimental forecasting game. I examine the effects of different types of complexity on decision making and information transmission. I focus on two types of complexity: the number of alternatives and the organization of information about risks and rewards. Each of these can prevent subjects from making appropriate choices. My results suggest that simplification of financial decisions, within limits, may improve information transmission while helping individuals make better choices. Though individuals make poor choices in very complex environments, constraining their choices too much also makes it difficult for them to choose well. My experimental setup also enables me to construct forecasts by aggregating information from agents' choices. I show how these complex choice environments can lead to inefficient forecasts.Chapter two explores the use of markets to elicit and aggregate information in places where these mechanisms do not normally exist. I assemble a novel data set gathered from a corporate prediction market in which managers at a software firm allowed employees to place bets on key variables. I use these data to examine the static and dynamic properties of information within the firm. I find that employees are privately informed about project outcomes. However, information is not evenly distributed across the firm - some groups appear to know more than other about products and sales. To examine the flow of information within the firm, I focus on a subset of bets that were later revised by employees. Revised bets preform well relative to bets pre-revision, suggesting that employees acquire private information over time. Again, the quality of information flow appears related to job function. This study lends insight into the source of managers' private information in a corporate finance setting.Finally, in chapter three, I focus on the motivations of market participants. I examine a betting market which experienced an exogenous shock to incentives. I use data on bets placed by participants to assess whether tournament prizes elicit exaggerated forecasts. I also study whether changing the value of monetary incentives has bearing on the participants' willingness to make forecasts. I find that, in line with predictions, reducing the proportion of prize winners, appears to increase the riskiness of their bets with no measurable increase in information content. However, contrary to expectations, I do not find that smaller prize values lead to lower participation. This study complements the theoretical literature on forecasting contests that suggests professional analysts have incentives to exaggerate their claims.",ucb,,https://escholarship.org/uc/item/01n4w2hm,,,eng,REGULAR,0,0
791,2227,Quadratic and linear optimization with analog circuits,"Vichik, Sergey","Borrelli, Francesco;",2015,"In this work we propose and investigate a new method of solving quadratic and linear optimization problems using analog electrical circuits instead of digital computation. We present the design of an analog circuit which solves Quadratic Programming (QP) or Linear Programming (LP) problems.In particular, the steady-state circuit voltages are the components of the QP (LP) optimal solution.The thesis shows how to construct the circuit and provides a proof of equivalence between the circuitand the QP (LP) problem. We study the stability of the analog optimization circuit. The circuit dynamics are modeled as a switched affine system. A piece-wise quadratic Lyapunov function and the KYP lemma are used to derive the stability criterion. The stability criterion characterizes the range of critical circuit parameters for which the QP circuit is globally asymptotically stable.The proposed method is used to build a printed circuit board (PCB) using programmable components to allow solution of various QP problems. The board supports implementation of an MPC controller for buck DC-DC converter. We conduct an experimental study to evaluate the performance of the analog optimization circuit.We study the feasibility of very high speed implementation of the optimization circuit using Analog Very Large Scale Integration (AVLSI) technology. In AVLSI, all the required circuit components are built on top of a silicon substrate using advanced photo-lithographic technologies. AVLSI circuits are fast, small and cheap. Thus, AVLSI implementation is paramount to make the proposed technology commercially competitive.We discuss the possible usage of the proposed method to make fast MPC controllers, image processors, communication decoders and analog co-processors. In fact, any application that requires a repeating solution of related optimization problems can benefit from this technology. Besides being faster than the digital computers, analog computers are more power efficient, may occupy smaller area on silicon and may be more resilient in harsh environments.",ucb,,https://escholarship.org/uc/item/01q7h2ng,,,eng,REGULAR,0,0
792,2228,Multichroic Bolometric Detector Architecture for Cosmic Microwave Background Polarimetry Experiments,"Suzuki, Aritoki","Lee, Adrian T;",2013,"Characterization of the Cosmic Microwave Background (CMB) B-mode polarization signal will test models of inflationary cosmology, as well as constrain the sum of the neutrino masses and other cosmological parameters. The low intensity of the B-mode signal combined with the need to remove polarized galactic foregrounds requires a sensitive millimeter receiver and effective methods of foreground removal. Current bolometric detector technology is reaching the sensitivity limit set by the CMB photon noise. Thus, we need to increase the optical throughput to increase an experiment's sensitivity. To increase the throughput without increasing the focal plane size, we can increase the frequency coverage of each pixel. Increased frequency coverage per pixel has additional advantage that we can split the signal into frequency bands to obtain spectral information. The detection of multiple frequency bands allows for removal of the polarized foreground emission from synchrotron radiation and thermal dust emission, by utilizing its spectral dependence. Traditionally, spectral information has been captured with a multi-chroic focal plane consisting of a heterogeneous mix of single-color pixels. To maximize the efficiency of the focal plane area, we developed a multi-chroic pixel. This increases the number of pixels per frequency with same focal plane area.We developed multi-chroic antenna-coupled transition edge sensor (TES) detector array for the CMB polarimetry. In each pixel, a silicon lens-coupled dual polarized sinuous antenna collects light over a two-octave frequency band. The antenna couples the broadband millimeter wave signal into microstrip transmission lines, and on-chip filter banks split the broadband signal into several frequency bands. Separate TES bolometers detect the power in each frequency band and linear polarization. We will describe the design and performance of these devices and present optical data taken with prototype pixels and detector arrays. Our measurements show beams with percent level ellipticity, percent level cross-polarization leakage, and partitioned bands using banks of two and three filters. We will also describe the development of broadband anti-reflection coatings for the high dielectric constant lens. The broadband anti-reflection coating has approximately 100% bandwidth and no detectable loss at cryogenic temperature. We will describe a next generation CMB polarimetry experiment, the POLARBEAR-2, in detail. The POLARBEAR-2 would have focal planes with kilo-pixel of these detectors to achieve high sensitivity. We'll also introduce proposed experiments that would use multi-chroic detector array we developed in this work. We'll conclude by listing out suggestions for future multichroic detector development.",ucb,,https://escholarship.org/uc/item/00m6b8rn,,,eng,REGULAR,0,0
793,2229,Removing Unwanted Variation from Microarray Data with Negative Controls,"Gagnon-Bartsch, Johann Andreas","Speed, Terence P;Stark, Philip B;",2012,"Microarray expression studies suffer from the problem of batch effects and other unwanted variation.  Unwanted variation complicates the analysis of microarray data, leading to high rates of false discoveries, high rates of missed discoveries, or both.  Many methods have been proposed to adjust microarray data to mitigate the problems of unwanted variation.  Because the factors causing the unwanted variation are frequently unknown, several of these methods rely on factor analysis to infer the unwanted factors from the data.  A central problem with this approach is the difficulty in discerning the unwanted variation from the biological variation that is of interest to the researcher.  To overcome this problem, we present novel methods that use negative controls to help identify the unwanted factors and separate the unwanted variation from the variation that is of interest.  Negative control genes are genes known a priori not to be differentially expressed with respect to the biological factor of interest.  The first method we present is a simple two-step procedure that we name RUV-2.  In the first step RUV-2 estimates the unwanted factors by performing factor analysis on the negative control genes.  Here, RUV-2 exploits the fact that any variation in the expression levels of negative control genes can be assumed to be unwanted variation.  In the second step, RUV-2 regresses the expression data on the factor of interest, including the estimated unwanted factors as covariates in the regression model.  The principal difficulty with RUV-2 is choosing the number of unwanted factors to include in the model.The second method we present is a more complicated four-step procedure that we name RUV-4.  Compared to RUV-2, RUV-4 is relatively insensitive to the number of unwanted factors included in the model; this makes estimating the number of factors less critical.  We also present a novel method for estimating the genes' variances that may be used even when a large number of unwanted factors are included in the model and the design matrix is full rank.  We name this method the ""inverse method for estimating variances.""  By combining RUV-4 with the inverse method, it is no longer necessary to estimate the number of unwanted factors at all.We discuss various techniques for assessing the performance of an adjustment method, and compare the performance of RUV-2, RUV-4, and their variants with the performance of other commonly used adjustment methods such as Combat, SVA, LEAPP, and ICE.  We present several example studies, each concerning genes differentially expressed with respect to gender in the brain.  We find that our methods performs as well or better than other methods.",ucb,,https://escholarship.org/uc/item/01j8t3qn,,,eng,REGULAR,0,0
794,2230,Automated Registration of Image Pairs with Dramatically Inconsistent Appearance,"Kwon, Youngwook Paul","McMains, Sara;",2017,"The objective of this research is to advance the state of the art in image matching algorithms, especially with regard to input image pairs that include dramatically inconsistent appearance (e.g., different sensor modalities, significant intensity/color changes, different times such as day/night and years apart, etc.). We denote this range of input as disparate input. To handle disparate input, one should be able to capture the underlying aspects not affected by superficial changes to appearance.To this end, we present a novel image descriptor based on the distribution of line segments in an image; we call it DUDE (DUality Descriptor). By exploiting line-point duality, DUDE descriptors are computationally efficient and robust to unstable line segment detection. Our experiments show that DUDE can provide more true-positive correspondences for challenging disparate datasets.Beyond traditional image matching, we have designed an effective autograding system for multiview engineering drawings that also uses DUDE to improve its performance. The autograding system needs to be able to compare drawings that may include appearance changes due to students' mistakes, but also needs to differentiate between allowable and erroneous translation and/or scale changes.In the addition to hand-crafted descriptors, this research also investigates data-driven descriptors generated by new deep learning based approaches. Due to the lack of labeled disparate imagery datasets, it is still challenging to effectively target disparate input using deep learning approaches. Therefore we introduce an aggressive data augmentation strategy called Artificial Intensity Remapping (AIR). By applying AIR to standard datasets, one can obtain models that are more effective for registration of disparate data. Finally, we compare the DUDE descriptor to a deep learning based descriptor powered by AIR.",ucb,,https://escholarship.org/uc/item/0nz6r315,,,eng,REGULAR,0,0
795,2231,The Effect of Nutritional Status on the Post-treatment Prophylactic Effect of Two Artemisinin-based Combination Therapies (ACTs) in Ugandan Children Treated for Malaria,"Verret, Wendy Joy","Reingold, Arthur;",2010,"Malaria and malnutrition are major causes of morbidity and mortality in children worldwide. Malnourished children may be at higher risk of malaria due to impaired immune response.  Malnutrition and young age may alter the pharmacokinetics (PK) of antimalarial treatment thus potentially impacting treatment efficacy.  Though malaria and malnutrition frequently coexist, results from previous studies that have investigated the association between these two co-morbidities are conflicting.  No previous studies have evaluated the effect of malnutrition on response to treatment with artemisinin-based combination therapies (ACTs).  Moreover, there are no other studies that have evaluated the effect of malnutrition on the PK of ACT regimens.  This dissertation examines the following:  1) the magnitude of the difference between efficacy estimates derived from 3 analytical methods and discusses the optimal statistical approach for monitoring in vivo efficacy (chapter 2); 2) the effect of nutritional status on the response to treatment with artemisinin-combination therapy (ACT) in young Ugandan children with malaria (Chapter 3); and 3) the effect of nutritional status on the pharmacokinetics (PK) of two ACT treatment regimens (chapter 4).Chapter 2 utilizes data from 29 clinical trials conducted in Africa and Thailand to compare the risk estimates of treatment failure, adjusted and unadjusted by genotyping, derived by 3 analytical methods; intention to treat (ITT), modified intention to treat (mITT) and per protocol (PP) analysis.  Estimates of treatment failure were consistently higher when derived from the ITT or PP analyses compared to the mITT approach in both unadjusted and adjusted analyses.  Poor patient adherence to follow-up, higher incidence of P. vivax relapse and high incidence of P. falciparum new infections were all factors contributing to differences in failure estimates.  Because estimates of antimalarial clinical efficacy vary significantly depending on the analytical methodology from which they are derived, standardized analytical tools should be used to monitor temporal and spatial trends in antimalarial efficacy.  Survival analysis is the preferred approach to monitor in vivo efficacy of malaria treatment. Chapter 3 and 4 utilize data from the Tororo Child Cohort (TCC) Study conducted in Tororo, Uganda.  In chapter 3, children aged 4 to 12 months diagnosed with uncomplicated malaria were randomized to either dihydroartemisinin-piperaquine (DP) or artemether-lumefantrine (AL) and followed for up to 2 years for repeated episodes of malaria.  The primary exposure variables of interest were height-for-age (HAZ score) and weight-for-age (WAZ score) z-scores and outcomes included parasite clearance at day 2 and 3 and risk of recurrent parasitemia after 42 days of follow-up.  HAZ and WAZ scores were not associated with a positive blood smear two days following treatment with DP or AL.  In children treated with DP not on trimethoprim-sulfamethoxazole (TS) prophylaxis, a decreasing HAZ score was independently associated with a higher risk of recurrent parasitemia. However, statistical significance was reached only when comparing HAZ scores <-1 with those > 0 (HAZ Â¡Ã -2 - <-1: HR=2.89, p=0.039; HAZ <-2: HR=3.18, p=0.022).  Overall, DP and AL are effective antimalarial therapies in chronically malnourished children in a high transmission setting however, children taking DP with signs of mild to moderate chronic malnutrition not taking TS prophylaxis are at higher risk of recurrent parasitemia.  In chapter 4, PK samples were collected from a subset of patients ages 6 months to 2 years who were randomized to DP or AL and followed prospectively for multiple episodes of malaria providing a total of 214 treatments for DP and 243 treatments for AL for PK analysis.  Primary exposure variables included stunting and underweight, (HAZ score of <-2 and WAZ score of <-2, respectively).  Chronic malnutrition appeared to be associated with day 3 piperaquine concentrations in adjusted analyses with stunted children having lower concentrations than non-stunted children (OR=0.78, p=0.007).  Stunting was associated with apparent clearance (CL/fpip) (OR=1.32, p=0.001) with stunted children having higher CL/fpip   than non-stunted children which may be the consequence of a lower overall exposure to drug and is consistent with the lower piperaquine concentrations measured on day 3.  Chronic malnutrition does not have an effect of piperaquine or lumefantrine concentrations at day 7 Â¨C an important determinant for treatment response.Overall, our results indicate that DP and AL are effective antimalarial treatments in very young chronically malnourished children.  This is supported by the finding that chronic malnutrition does not have an effect on of piperaquine or lumefantrine day 7 concentrations, an indicator for treatment response.  However, children taking DP not on TS prophylaxis may be at higher risk of recurrent parasitemia.  Further studies should be conducted to justify these results and provide a definitive understanding of the causal relationship between malnutrition and malaria.",ucb,,https://escholarship.org/uc/item/0pd0s82d,,,eng,REGULAR,0,0
796,2232,The CRISPR endoribonuclease Csy4 utilizes unusual sequence- and structure-specific mechanisms to recognize and process crRNAs,"Haurwitz, Rachel Elizabeth","Doudna, Jennifer A;",2012,"Many prokaryotes contain clustered regularly interspaced short palindromic repeats (CRISPRs) that together with CRISPR-associated (cas) genes confer resistance to invasive genetic elements. Central to this immune system is the production of CRISPR-derived RNAs (crRNAs) via enzymatic cleavage of CRISPR locus transcripts. These crRNAs serve as guides for foreign nucleic acid targeting and degradation. Here we identify Csy4 as the endoribonuclease responsible for CRISPR transcript processing in Pseudomonas aeruginosa UCBPP-PA14. Biochemical assays and six co-crystal structures of Csy4 bound to substrate and product crRNAs reveal the complex mechanisms Csy4 utilizes to recognize, position, and cleave its cognate RNA substrate in order to generate mature crRNAs. Csy4 makes sequence-specific contacts to the major groove of its cognate RNA stem-loop and makes extensive electrostatic interactions with the phosphate backbone that are highly sensitive to the helical geometry of the substrate, resulting in an extremely high affinity binding interaction (Kd â‰ˆ 50 pM). Csy4 has equally tight affinity for both its substrate and product RNAs and therefore functions in vivo as a single turnover catalyst. Phylogenetically conserved serine and histidine residues constitute a catalytic dyad in which the serine pins the ribosyl 2â€²-hydroxyl nucleophile in place, allowing the histidine to deprotonate the active site 2â€²-hydroxyl, leading to nucleophilic attack on the scissile phosphate. The Csy4 active site lacks a general acid to protonate the leaving group and positively charged residues to stabilize the transition state, explaining why the observed catalytic rate constant is ~104-fold slower than that of RNase A. The RNA cleavage step carried out by Csy4 is essential for assembly of the Csy protein-crRNA complex that facilitates target recognition. Considering that Csy4 recognizes a single cellular substrate and subsequently sequesters the cleavage product, evolutionary pressure has likely selected for substrate specificity and high-affinity crRNA interactions at the expense of rapid cleavage kinetics.A major goal of synthetic biology is to construct reliable and predictable genetic circuits. However, synthetic genetic systems often perform unpredictably due to structural interactions between DNA, RNA, and protein components. Here we present a novel synthetic RNA processing platform utilizing Csy4 and its cognate target RNA to physically separate otherwise linked genetic elements such as promoters, ribosome binding sites, cis regulatory elements, and riboregulators. Implementation of this platform provides a general approach for creating context-free standard genetic elements that can be readily applied to the bottom-up construction of increasingly complex biological systems in a plug-and-play manner.",ucb,,https://escholarship.org/uc/item/0rh5940p,,,eng,REGULAR,0,0
797,2233,Of Ghosts and Survivors: The History and Memory of 1968 in Italy,"Carbotti, Rosaria","Spackman, Barbara;",2015,"The year 1968 saw the rise of a manifold protest movement among Italian university students that evolved well into the late 1970s and spread to all segments of society. Today, the memory of this collective experience represents one of the most haunting episodes of the 20th century. Torn between celebrating national events and giving in to cultural amnesia, the Italian cultural discourse around â€™68 appears deliberately opaque. In recent historiography, fiction, and film this momentous year appears to be condensed in a puzzle of contrasting snapshots that do not fit well together. On one hand, it is remembered as a watershed event that altered the course of the nationâ€™s history and the lives of individuals in radical ways. On the other hand, many â€™68 storytellers mourn the complete erasure of their experience from contemporary culture and criticize the moral wasteland that is associated with the current political arena when contrasted with the vanished hopes of the past. From Luisa Passerini to Guido Viale, from Erri De Luca to Giovanni Moro, a generation of protagonists and witnesses of those times raise through their work a number of urgent questions that deal with issues of periodization, selective perception, genealogical inheritance, and a paralyzing feeling of melancholia. Such questions do not seem to find answers within traditional historical or sociological approaches. At the same time, these accounts pose problems of their own, as they reflect a desire to simultaneously preserve and shatter the memory of 1968 as it has come to be celebrated in popular culture. My dissertation is in dialogue with a vast intellectual constellation that encompasses historical, theoretical, literary, and cinematic readings of 1968. As I investigate the existing narrative production around the â€™68 phenomenon, I question what lies at the heart of the possessive forms of memory that have come to characterize our present approach to that time. In doing so, I challenge the current widespread view that sees possessive memory as an obstacle to a proper understanding of the past. On the contrary, I argue that it is precisely by looking closer at the stumbling blocks that seem to hinder the flow of historical narrative surrounding â€™68 that we might get at the core of our collective attachment to that time, a bond that is shaped by the labor of forces and emotions that cannot yet be put to rest. My research pays particular attention to the ways in which the memory of â€™68 has been defiantly organized at the narrative level as an attempt to resist periodization. Thus, I interpret such resistance as a way to protract the presence of â€™68 beyond its temporal confines and ultimately deny the symbolic death of a groundbreaking collective experience. Through the analysis of the narrative figuration of the ghost, I explore the ways in which melancholia â€” the feeling of painful attachment to a lost ideal â€” can be taken to be an affirmative disposition that originates a form of critical agency on the part of the â€™68 storytellers. The imperative need â€œto begin with oneselfâ€ â€” to impose the primacy of subjective affect, understanding oneâ€™s involvement in social phenomena as the merging of individual and collective expression â€” becomes a melancholic act of rebellion against the objectifying discourse of history. From this perspective, I take 1968 to be reactivated as a temporal category against which the present needs to be questioned in order for a future to be imagined anew.",ucb,,https://escholarship.org/uc/item/1843v368,,,eng,REGULAR,0,0
798,2234,Development and Validation of New Approaches for the Detection of Sight-Threatening Diabetic Macular Edema,"Litvin, Taras","Roorda, Austin;",2016,"Clinically significant macular edema (CSME) results from the microvascular complications of diabetes in the central retina of the eye. Diabetes-induced disregulation of the intra- and extra-vascular fluid homeostasis leads to the accumulation of fluid in the retinal tissue. When left untreated, CSME can result in vision loss by disrupting normal metabolic processes, ischemia and by mechanical disruption of the intricate microstructure of the retina. CSME can be succesfully treated when detected in time. Vision loss caused by CSME is largely irreversible and, therefore, the emphasis is on early detection. Challenges in CSME detection result in a significant number of untreated patients with devastating visual consequences. Tele-medicine screening which relies on either monoscopic or stereoscopic fundus photographs is widely implemented and has good sensitivity and specificity for the detection of retinopathy levels. Contrary to that, CSME detection in fundus photographs faces substantial challenges. Stereoscopic photographs require dilation which increases the risk of vision loss due to an elevated intraocular pressure which may lead to angle closre glaucoma and is generally time consuming and incovenient for patients which further decreases compliance with screening. Non-dilated stereophotography suffers from unacceptably high number of stereophotographs of poor quality. Monoscopic images have low sensitivity for CSME detection because of the lack of stereopsis. Programs which use monoscopic images must rely on surrogate markers of CSME. There is currently no consensus on how to detect CSME in monoscopic images. Different programs use different surrogate markers primarily because there is lack of evidence in scientific literature regarding the unequivocal validity of a particular CSME detection method. This dissertation includes three studies that share a common goal of improving the accuracy of CSME detection. The anticipated impact of this research is that we will be able to improve the efficiency with which we identify patients requiring treatment, thereby increasing their chance for reducing the risk of vision loss due to CSME. The first study presented in Chapter 3 of this dissertation evaluates the ability of hard exudates, a surrogate marker of DME, located within one-disc diameter from the center of the macula to detect CSME. This study tests the â€œreal worldâ€ implementation of this approach in a diabetic retinopathy screening program and attempts to resolve the discrepancies regarding the accuracy of this method that exists in the literature. The second study presented in Chapter 4 of this dissertation tests whether the proximity of hard exudates to the fovea is associated with more severe cases of CSME and thus warrants a more expedited referral and intervention. In addition, we proposed an OCT-based adaptation of CSME severity scale based on the DME severity scale derived from the Early Treatment Diabetic Retinopathy Study (ETDRS) data.  This is a step forward towards developing a more repeatable and objective structural measure of sight-threatening diabetic macular edema which correlates with functional measures of retinal health, such as visual acuity and electroretinographic recordings. Finally, the third study presented in Chapter 5 builds on the findings of the first two studies and utilizes a novel approach for the detection of CSME. This approach is based on the combined measure of the proximity of hard exudates to the fovea and the areal extent of exudation in the central macula. We have also evaluated the association of the photopic 30 Hz flicker ERG measurements with CSME. Finally, the model of CSME detection which combines relevant clinical variables was evaluated as the CSME detection tool.The results of the studies presented in this dissertation support the use of a new approach â€“ radially arranged sectors in the central macula - for the detection of CSME. This approach was shown to accurately classify patients with and without CSME. Moreover, the increase in the number of sectors affected by hard exudates is also associated with the increase in probability of having severe CSME, justifying a more expedited referral for treatment. Finally, while significantly associated with CSME, the latency of the 30 Hz photopic flicker ERG response does not seem to offer additional power to discriminate between patients with and without CSME.",ucb,,https://escholarship.org/uc/item/18c438gw,,,eng,REGULAR,0,0
799,2235,Wildfire Effects on the Ecohydrology of a Sierra Nevada Watershed,"Boisrame, Gabrielle Boisrame","Thompson, Sally E;",2016,"The mountain watersheds of the Sierra Nevada supply the majority of California's water, but this supply has always been highly variable. The 2012-2016 drought in California has demonstrated that this water supply is also highly vulnerable to increasing temperatures and/or reduced precipitation. Not only did the 2012-2016 drought reduce water supply for human use, but it also led to unprecedented forest mortality and fire damage. Unfortunately, the fire suppression strategy that was nearly uniformly applied to mountain forests during the 20$^{th}$ century may have exacerbated the effects of drought by increasing vegetation density and thus increasing evapotranspiration and precipitation interception. Could restoring fire regimes to their pre-European settlement condition increase water yield from these forested catchments? Such a policy would also have the potential to restore the ecological function of landscapes and reduce the risk of catastrophic fires (such as the 2013 Rim Fire) by reducing fuel loads. This dissertation studies the hydrological and landscape-level ecological effects of restoring a frequent, mixed severity fire regime to the Illilouette Creek Basin in Yosemite National Park.  A combination of field measurements, historical data analysis, remote sensing, and modeling approaches are employed to strengthen the argument by providing multiple lines of evidence. There is limited data available for Illilouette Creek Basin during much of the four decades in which the new fire regime became established, inhibiting direct evaluation of the fire regime's effects. Nevertheless, a variety of different metrics and analyses indicate a number of important changes that can be attributed to the restored fire regime: increased landscape diversity (including reduced forest cover), increased soil moisture and streamflow (both according to measurements and hydrological modeling), and decreased drought stress (both according to observations and from hydrological modeling).",ucb,,https://escholarship.org/uc/item/19b6f3q9,,,eng,REGULAR,0,0
800,2236,"Republican Monsters: The Cultural Construction of American Positivist Criminology, 1767-1920","Burton, Chase Smith","Tomlins, Christopher;",2019,"This dissertation examines the history of and cultural influences on positivist criminology in the United States. From Benjamin Rush to the present day, the U.S. has produced an extensive corpus of empirical and theoretical studies that seeks to discern an objective, scientifically-grounded basis for criminal behavior. American positivist criminology has drawn on numerous subfields and theories, including rational choice / economic theory, biology, and psychology, but in all cases, maintains that a purely scientific explanation of offending is possible. This study proceeds from the perspective that divisions between scientific and non-scientific thought are untenable. Drawing on scholarship in literary criticism and sociology, I argue that positivist criminology confronts an inherent contradiction in purporting to develop a purely scientific account of phenomena that are defined by the moral and cultural sentiments of a society. I thus hypothesize that positivist criminology is in fact reliant on the irrational and fictive cultural tropes and images of crime that it claims to exorcize. The dissertation proceeds by reviewing the literature on the history of criminology, developing a set of functional types or tropes for character analysis, and then examining four separate periods in the development of scientific criminology: eighteenth century studies of rational action, nineteenth century studies of defective reasoning, early twentieth century studies of race and crime, and the development of scientifically informed criminalistics programs. Each of these cases captures a different period and focus in the development of scientific criminology. In threading continuity between these cases, I show how criminological positivism is consistently reliant on culturally informed tropes and characters to render itself sensible and coherent.",ucb,,https://escholarship.org/uc/item/1bb046x0,,,eng,REGULAR,0,0
801,2237,Integrated Nanoscale Antenna-LED for On-Chip Optical Communication,"Fortuna, Seth Andrew","Wu, Ming C;",2017,"Traditional semiconductor light emitting diodes (LEDs) have low modulation speed because of long spontaneous emission lifetime. Spontaneous emission in semiconductors (and indeed most light emitters) is an inherently slow process owing to the size mismatch between the dipole length of the optical dipole oscillators responsible for light emission and the wavelength of the emitted light. More simply stated: semiconductors behave as a poor antenna for its own light emission. By coupling a semiconductor at the nanoscale to an external antenna, the spontaneous emission rate can be dramatically increased alluding to the exciting possibility of an antenna-LED that can be directly modulated faster than the laser. Such an antenna-LED is well-suited as a light source for on-chip optical communication where small size, fast speed, and high efficiency are needed to achieve the promised benefit of reduced power consumption of on-chip optical interconnect links compared with less efficient electrical interconnect links. Despite the promise of the antenna-LED, significant challenges remain to implement an antenna-coupled device in a monolithically integrated manner. Notably, most demonstrations of antenna-enhanced spontaneous emission have relied upon optical pumping of the light emitting material which is useful for fundamental studies; however, an electrical injection scheme is required for practical implementation of an antenna-LED.In this dissertation, demonstration of an electrically-injected III--V antenna-LED is reported: an important milestone toward on-chip optical interconnects. In the first part of this dissertation, the general design principles of enhancing the spontaneous emission rate of a semiconductor with an optical antenna is discussed. The cavity-backed slot antenna is shown to be uniquely suited for an electrically-injected antenna-LED because of large spontaneous emission enhancement, simple fabrication, and directional emission of light. The design, fabrication, and experimental results of the electrically-injected III--V antenna-LED is then presented. Clear evidence of antenna-enhanced electroluminescence is demonstrated including a large increase in the emitted light intensity with respect to an LED without antenna. Furthermore, it is shown that the active region emission wavelength is influenced by the antenna resonance and the emitted light is polarized; consistent with the expected behavior of the cavity-backed slot antenna. An antenna-LED consisting of a InGaAs quantum well active region is shown to have a large 200-fold enhancement of the spontaneous emission rate.In the last half of this dissertation, the performance of the antenna-LED is discussed. Remarkably, despite the high III--V surface recombination velocity, it is shown that an efficient antenna-LED consisting of an InGaAs active region is possible with an antenna-enhanced spontaneous emission rate. This is true provided the active region surface quality is preserved through the entire device process. A novel technique to preserve and clean InGaAs surfaces is reported. Finally, a rate-equation analysis shows that the optimized antenna-LED with cavity-backed slot antenna is fundamentally capable of achieving greater than 100 GHz direct modulation rate at high efficiency thus showing that an antenna-LED faster than the laser is achievable with this device architecture.",ucb,,https://escholarship.org/uc/item/1d31s4p6,,,eng,REGULAR,0,0
802,2238,Model Predictive Control for Energy Efficient Buildings,"Ma, Yudong","Borrelli, Francesco;",2012,"The building sector consumes about 40% of energy used in the United States and is responsible for nearly 40% of greenhouse gas emissions. Energy reduction in this sector by means of cost-effective and scalable approaches will have an enormous economic, social, and environmental impact. Achieving substantial energy reduction in buildings may require to rethink the entire processes of design, construction, and operation of buildings. This thesis focuses on advanced control system design for energy efficient commercial buildings.Commercial buildings are plants that process air in order to provide comfort for their occupants. The components used are similar to those employed in the process industry: chillers, boilers, heat exchangers, pumps, and fans. The control design complexity resides in adapting to time-varying user loads as well as occupant requirements, and quickly responding to weather changes. Today this is easily achievable by over sizing the building components and using simple control strategies.Building controls design becomes challenging when predictions of weather, occupancy, re- newable energy availability, and energy price are used for feedback control. Green buildings are expected to maintain occupants comfort while minimizing energy consumption, being ro- bust to intermittency in the renewable energy generation and responsive to signals from the smart grid. Achieving all these features in a systematic and cost-effective way is challenging. The challenge is even greater when conventional systems are replaced by innovative heat- ing and cooling systems that use active storage of thermal energy with critical operational constraints.Model predictive control (MPC) is the only control methodology that can systematically take into account future predictions during the control design stage while satisfying the system operating constraints. This thesis focuses on the design and implementation of MPC for building cooling and heating systems. The objective is to develop a control methodology that can 1) reduce building energy consumption while maintaining indoor thermal comfort by using predictive knowledge of occupancy loads and weather information, (2) easily and systematically take into account the presence of storage devices, demand response signals from the grid, and occupants feedback, (3) be implemented on existing inexpensive and distributed building control platform in real-time, and (4) handle model uncertainties and prediction errors both at the design and implementation stage.The thesis is organized into six chapters. Chapter 1 motivates our research and reviews existing control approaches for building cooling and heating systems.Chapter 2 presents our approach to developing low-complexity control oriented models learned from historical data. Details on models for building components and spaces thermal response are provided. The thesis focuses on the dynamics of both the energy conversion and storage as well as energy distribution by means of heating ventilation and air conditioning (HVAC) systems.In Chapter 3, deterministic model predictive control problems are formulated for the en- ergy conversion systems and energy distribution systems to minimize the energy consumption while maintaining comfort requirement and operational constraints. Experimental and simu- lative results demonstrate the effectiveness of the MPC scheme, and reveal significant energy reduction without compromising indoor comfort requirement.As the size and complexity of buildings grow, the MPC problem quickly becomes com- putationally intractable to be solved in a centralized fashion. This limitation is addressed in Chapter 4. We propose a distributed algorithm to decompose the MPC problem into a set of small problems using dual decomposition and fast gradient projection. Simulation results show good performance and computational tractability of the resulting scheme.The MPC formulation in Chapter 3 and 4 assumes prefect knowledge of system model, load disturbance, and weather. However, the predictions in practice are different from actual realizations. In order to take into account the prediction uncertainties at control design stage, stochastic MPC (SMPC) is introduced in Chapter 5 to minimize expected costs and satisfy constraints with a given probability. In particular, the proposed novel SMPC method applies feedback linearization to handle system nonlinearity, propagates the state statistics of linear systems subject to finite-support (non Gaussian) disturbances, and solves the resulting optimization problem by using large-scale nonlinear optimization solvers.",ucb,,https://escholarship.org/uc/item/1dj908bs,,,eng,REGULAR,0,0
803,2239,Structural Behavior of Bent Cap Beams in As-built and Retrofitted Reinforced Concrete Box-Girder Bridges,"Moustafa, Mohamed Aly Abdel-Razik","Mosalam, Khalid M;",2014,"Research on resilient infrastructure systems is expanding. As we experience more infrastructure deterioration in the US, numerous efforts are ongoing for building the nation's new infrastructure and maintaining the existing one. Bridges are key components of infrastructure that are vulnerable to earthquakes and are undergoing retrofit or complete replacement. Thus, optimized seismic design of new bridges and informed retrofit decisions are indispensable. A specific design issue that is concerned with the structural response of bent cap beams in as-built and retrofitted box-girder bridges under gravity and seismic loads is tackled in this dissertation. The lack of proper account of box-girder slabs contribution to the integral bent cap can lead to an uneconomical seismic design of new bridges or unfavorable mode of failure in retrofitting existing ones. A combined experimental and computational research was undertaken in this study to investigate the structural behavior and seismic response of bent cap beams in as-built and retrofitted reinforced concrete box-girder bridges under the combined effect of vertical and lateral loading. In particular, the contribution of the box-girder slabs to the stiffness and strength of the integral bent caps was evaluated for optimized design and enhanced capacity estimation. The computational part of the study consisted of two phases: pre-test and post-test analyses. The experimental program involved testing two 1/4-scale column-bent cap beam-box girder subassembly using quasi-static and Hybrid Simulation (HS) testing methods. The test specimens were adopted from a typical California bridge that is modified from the Caltrans Academy Bridge, and were designed in light of the most recent AASHTO and Caltrans provisions. The pre-test analysis phase of the computational research utilized one-, two-, and three-dimensional finite element models to carry out different linear and nonlinear static and time history analyses for both of the full prototype bridge and the test specimen. The pre-test analysis successfully verified the expected subassembly behavior and provided beneficial input for the experimental program. The first stage of the experimental program involved quasi-static cyclic loading tests of the first specimen in as-built and repaired conditions. Bidirectional cyclic loading tests in both transverse and longitudinal directions were conducted under constant gravity load. A rapid repair scheme was adopted for the tested specimen using a Carbon Fiber Reinforced Polymer (CFRP) column jacket. A similar quasi-static cyclic test to the as-built specimen was carried out for the repaired specimen for comparison purposes and to verify the essentially elastic status of the bent cap beam. The second stage of the experimental study embraced the HS testing technique for providing the lateral earthquake loading to the test specimens. A new practical approach that utilized readily available laboratory data acquisition systems as a middleware for feasible HS communication was achieved as part of this study. The proper communication among the HS components and the verification of the HS system were first performed using tests conducted on standalone hydraulic actuators. A full specimen HS trial test was conducted using the previously tested repaired specimen to validate the whole HS system. The last phase in the experimental program involved retrofitting the column of the second specimen using CFRP jacketing before any testing to increase the demands on the bent cap beam for further investigation into its inelastic range of structural response. The retrofitted second specimen was then tested using multi-degree of freedom HS under constant gravity load using several scales of unidirectional and bidirectional near-fault ground motions.  The post-test analysis was the final stage of this study. The results from the as-built first specimen cyclic tests were used to calibrate the most detailed three-dimensional finite element model, which was previously developed as part of the pre-test analysis stage. The calibrated model was used to explore the effect of reducing the bent cap reinforcement on the overall system behavior and to investigate the box-girder contribution at higher levels of bent cap seismic demand. Based on the computational and experimental results obtained in this study, the effective slab width for integral bent caps was revisited. The study concluded that the slab reinforcement within an effective width, especially in tension, should be included for accurate bent cap capacity estimation. The study was finalized with an illustrative design example to investigate the design implications of the revised effective slab width and bent cap capacity estimation on the optimization of the bent cap design for a full-scale bridge.",ucb,,https://escholarship.org/uc/item/1f02v32f,,,eng,REGULAR,0,0
804,2240,A cyber-infrastructure for the measurement and estimation of large-scale hydrologic processes,"Kerkez, Branko","Glaser, Steven D;",2012,"Water shortages, particularly evident in the state of California, emphasize the need for a better hydrologic understanding, and improved water management techniques. The majority of the state's water originates in the Sierra Nevada as snow, melting throughout the year to meet the needs of various stakeholders. Current measurement techniques are unable to resolve variability of the snowpack at the basin scale, and snowmelt processes are not well captured by existing hydrologic models. A system-level solution is introduced to facilitate scientific understanding and water management decisions in basins of the Sierra Nevada. The core of this thesis focuses on expanding current sensing methods at the hydrologic catchment (1-2km2) scale to develop an improved understanding of the mountain water balance. It is shown that Wireless Sensor Networks (WSN) offer an ultra low-power, cost-effective solution to instrument catchment-scale regions. An explicit WSN deployment strategy is derived, leveraging in-situ network statistics of the Packet Delivery Ratio, and Received Signal Strength Indicator to optimize network performance. A hydrologic variability analysis is carried out on the dataset collected by the network, showing temporal stability in the variability of snowdepth at the catchment scale, and validating the use of a stratified sampling approach based on the even instrumentation of physiographic variables. To derive estimates of Snow Water Equivalent (SWE) at the larger basin scale, an estimation framework based on Gaussian Processes, and an optimal sampling strategy, which maximizes Mutual Information, is described. The sensor placement method outperforms a number of other sampling designs, reducing estimation error by up to 100mm. A computationally tractable, Hybrid System model of snow dynamics is introduced and shown to accurately reflect the various stages of snowmelt when compared to observations in the Sierra Nevada. The need for improved sensing and estimation procedures is highlighted by an analysis of the effects of improved SWE estimates on basin-scale streamflow forecasting methods. The use of historically reconstructed SWE data shows that streamflow forecasting error could be reduced by nearly 10% through improved SWE estimates.",ucb,,https://escholarship.org/uc/item/1f83k6nf,,,eng,REGULAR,0,0
805,2241,A Liberal Space: A History of the Illegalized Working-Class Extensions of Lisbon,"Castela, Tiago Luis Lavandeira","AlSayyad, Nezar;",2011,"This dissertation discusses the history of the so-called ""clandestine"" suburban subdivisions of the Lisbon metropolitan area of Portugal, focusing on the period between their emergence in 1958, as Salazar's dictatorship adopted policies of economic liberalization, and the beginning of political democratization in 1974. After the Second World War, a significant part of the new extensions of the city in Southern Europe--in urbs such as Barcelona, Rome, Belgrade, Athens or Istanbul--were produced informally, i.e. land subdivision and the construction of new housing were often unlicensed by municipal governments. Portugal's capital Lisbon was not an exception. The informal production of suburban subdivisions from the late 1950s onwards corresponded to a new form of working-class extension of the city, distinct from the older spaces of unlicensed self-building which took place in occupied or informally rented land. Those earlier ""shack"" neighborhoods, of which Quinta da Serra is a present-day example, had become part of Lisbon's expansion since the end of the First World War, but were always subject to periodic demolitions undertaken by the city's municipal government. In contrast, the creation of ""clandestine"" subdivisions by private developers in the late 1950s was done through legally registered sales of plots, in farms such as Brandoa or Casal de Cambra outside Lisbon's municipal limits. Since there were no provisions for subdivision by private developers in national planning law, the new subdivisions were informally created through successive lot splits. For Lisbon's low-income households, the process provided access to the ownership of land. Even though building was often unlicensed, this was initially not necessarily illegal, and in practice the local governments of the suburban municipalities around Lisbon rarely demonstrated a willingness to subject unlicensed housing to demolition practices. On the contrary, during the 1960s local municipalities started surveying the new de facto subdivisions and creating limited public infrastructure networks. However, at the same time the central state gradually changed national planning laws in an explicit reaction to fears about the ""clandestine,"" illegalizing informal modes of suburban subdivision and building. By the early 1970s, the coexistence of illegalization by the dictatorial central state and transformation through tentative municipal planning practices fostered a state of expectancy for re-legalization and for the provision of full public infrastructure. This state of expectancy stimulated local community organization and participation in the creation of public infrastructure. In addition, the state of expectancy promoted the formation of municipal planners as entrepreneurial bureaucrats, employing elements of planning to manage the growth of spaces purportedly outside the domain of formal state planning. When political democratization started in 1974, the fundamental ideas of a national planning framework formed through the illegalization of informality were not challenged, and a dual planning regime became consolidated in the Lisbon suburbs. Today, this dual planning regime has not been fully dismantled. Even though full public infrastructure was provided to ""clandestine"" subdivisions during the 1990s, most informal land subdivision has yet to be licensed, as the existence of a thriving legalization industry shows. As for propertyless informality in present-day ""shack"" neighborhoods such as Quinta da Serra, demolition practices have recently been attuned to selecting households according to one of the ideas that has supported the management of the ""clandestine"" since the beginning: the need for the state to foster homeownership by low-income households.",ucb,,https://escholarship.org/uc/item/1fj0856h,,,eng,REGULAR,0,0
806,2242,Ecology in the laboratory: the molecules that shape the ecological relationship between D. melanogaster and S. cerevisiae,"Schiabor, Kelly Marie","Eisen, Michael B;",2015,"Drosophila melanogaster and Saccharomyces cerevisiae, two model systems of molecular biology, interact in the wild, but the chemical basis of this interaction is largely unknown. This thesis details an effort to understand the molecular basis of this ecological co-localization. Results: 	While screening a collection of wild and laboratory yeast strains for their ability to attract adult D. melanogaster (Raleigh 437), I noticed a large difference in fly preference for two nearly isogenic strains of S. cerevisiae, BY4741 and BY4742. Using standard genetic analyses, I tracked the difference in preference to lack of mitochondria in BY4742. I used gas chromatography coupled with mass spectrometry (GC-MS) to examine the volatile compounds produced by BY4741 and the mitochondria-deficient BY4742, and found that they differed in their production of many known fly attractants, including ethyl hexanoate and ethyl octanoate, which were produced at much higher levels in yeast strains with mitochondria during aerobic fermentation, a metabolic strategy that distinguishes S. cerevisiae from most other microbes. Through a detailed investigation of the volatile profiles and RNA expression patterns produced by yeast under multiple nutritional scenarios, I determined that the production of ethyl hexanoate and ethyl octanoate during aerobic fermentation depends on the level and type of nitrogen present in the substrate. Fermentative growth on nitrogen-limited substrates requires mitochondrial engagement and the metabolic configuration employed by S. cerevisiae to contend with this nutritional scenario results in the production of these ester attractants. This nutritional scenario â€“ high sugar but limited nitrogen â€“ matches the composition of fruit, the natural co-localization environment for flies and yeast, suggesting that these volatiles are ecologically relevant cues for D. melanogaster.  D. melanogaster sense ethyl hexanoate and ethyl octanoate via the odorant receptor genes Or22a and Or22b. A single gene version of this locus, which is a chimera of the two ancestral copies, is segregating in natural D. melanogaster populations. I found that D. melanogaster lines harboring the chimeric locus (including Raleigh 437) show more robust preference for ethyl hexanoate and ethyl octanoate-producing S. cerevisiae cultures.  This provides an ecological basis, and possibly selective advantage, for the maintenance of this allele in D. melanogaster.",ucb,,https://escholarship.org/uc/item/1g1435mc,,,eng,REGULAR,0,0
807,2243,"Behavior, Ecology and Genetics of Geoffroy's Tamarin (Saguinus geoffroyi)","Diaz-Munoz, Samuel Luis","Lacey, Eileen A;",2010,"Cooperative behavior in reproductive contexts is rare among animals, especially males. Tamarins exhibit a rare breeding system called cooperative polyandry, in which a single breeding female mates with two or more males to produce fraternal twins and the males cooperate in caring for the infants by carrying young for the first 10 weeks of their lives. This peculiar breeding system raises questions about the adaptive consequences of male behavior. The nature of the breeding system also prompts questions about the ecological context and genetic consequences of this social organization. My research attempted address fundamental questions in behavior, ecology and evolutionary biology through the lens of individual behavior. Tamarins oftentimes inhabit disturbed habitats, however detailed space use within fragmented habitats is not well characterized. I used fine scale spatial and behavioral data in order to quantify habitat preference of S. geoffroyi in a heterogenous urban-forest landscape in central Panama. Using home range- based analyses and a novel method, first passage time analysis, I showed that tamarins spend significantly more time in secondary forest habitat and are more likely to forage and engage in social behavior in forest as compared to human-modified habitats.	I examined the role of two aquatic barriers of varying age in creating population genetic structure in S. geoffroyi. I found that there was significant population differentiation across the Chagres River, an older, established riverine barrier and smaller, but detectable population structure across the Panama Canal, a recent anthropogenic riverine barrier. Finally, I examined the possible adaptive benefits of cooperative male parental care using genetic analyses of paternity and relatedness. I found that males in a group are often related and that they share paternity over multi-year associations. My results suggest that indirect and direct fitness benefits may play a role in maintaining male-male cooperation in tamarins.",ucb,,https://escholarship.org/uc/item/0th0r680,,,eng,REGULAR,0,0
808,2244,Laser Assisted Nanomanufacturing with Solution Processed Nanoparticles for Low-cost Electronics and Photovoltaics,"Pan, Heng","Grigoropoulos, Costas;",2009,"Nanomanufacturing is a term used to describe either the production of nanoscale materials, or to describe the manufacturing of parts `bottom up' from nanoscale material or `top down' in smallest steps for high precision. In recent years, nanomanufacturing is facing a new and broadened definition the essential themes lie on low cost, scalability, reliability and sustainability, besides the conventional requirements. The emphasis on this technology is towards realizing the low cost and large area manufacturing with control and benefits from nanoscale structures and nano enabled properties. The newly defined nanomanufacturing is expected to find extensive new applications in the large area electronics and renewable energy industries.Nanomanufacturing based on solution processed nanoparticles is a promising approach towards this goal and is currently under intense research. This process significantly simplifies the manufacturing process and enables potential applications that are low temperature and extremely low cost. The solution deposition eliminates the need of multiple steps and vacuum deposition. It is capable of being integrated into the simple process, i.e. roll-to-roll manufacturing, to increase yield and process area. The suppressed melting temperature of nanoparticles also eliminates the need of high temperature processing. In solution processing, in general, nanoparticles prepared chemically with controlled stability and compositions are deposited in solution phase (e.g. printing) followed by annealing and/or nanopatterning steps for achieving the required spatial resolution and reasonable electrical and optical properties.Laser processing, including annealing/sintering and ablation, of solution deposited nanoparticles is a new and interesting process that has barely been explored. Not only this process can take advantage of laser processing for low cost, low temperature device fabrication, but also a large amount of physics can be explored including laser-nanomaterial interaction, nanoscale heat/mass transport, defects / grain boundaries evolution, semiconductor device physics and photonics, etc. The present study examined various components in the low cost nanomanufacturing process, including laser annealing of semiconductor and metallic nanoparticles, optical characterization of sintering process and scalable nanopatterning techniques. The emphasis is on both demonstration of novel processes and working devices as well as fundamental experimental and numerical studies of transport phenomena involved in laser induced solid state and melt-mediated nanoparticle coalescence.Firstly, to provide insight into the transport phenomena involved in laser induced non-melt and melt-mediated nanoparticle coalescence, experimental and numerical studies were performed. The Molecular Dynamics simulation was adopted to study the solid-state nanoparticle sintering by laser heating. The comparison between MD and macroscopic model reveals the unique mechanisms in sub-10nm nanoparticles sintering suggesting enhanced neck growth rate can be expected for nanoparticle with size below 10nm. The melt-mediated nanoparticle coalescence induced by ns laser is probed in-situ by a pump-and-probe technique. The probing results reveal several important characteristic times during laser annealing that will determine the morphology and crystalline structures of the laser processed film: characteristic coalescence time, cooling and resolidification times. The coalescence time compares favorably with MD simulation. Secondly, the laser annealing of metal oxide nanoparticles for electronics and photovoltaics are demonstrated. Solution deposited ZnO nanoparticles can be transformed into active channel layer of thin film transistors (TFTs) by Excimer laser annealing (ELA). The device shows mobility ~0.1 cm2/V-s and current on/off ratios of more than 104. The properties of the laser annealed films were characterized by scanning electron microscopy, high-resolution TEM, DC conductance, and photoluminescence measurements. Furthermore, ELA is combined with spray deposition of TiO2 nanoparticle to fabricate dye sensitized solar cells (DSSCs) on glass and plastic substrates. All-laser-annealed TiO2 photoelectrodes achieve efficiency at ~3.8% with Ruthenium dyes.Optical characterization of optical properties and film thickness by ellipsometry is also demonstrated for silver nanoparticle film sintering. The ellipsometric measurement is found to be robust in capturing the percolation transition, dielectric constant and film thickness evolutions. The non-contact optical measurement is also found to correlate well with DC measurement. Finally, large-area optical near-field nanoprocessing, i.e. nanosintering and nanoablation, of solution deposited nanoparticle is demonstrated. The feature size down to ~150nm is obtained and annealing in furnace further reduces the feature size down to ~50nm. Nanoscale organic transistors are fabricated by nanosintered electrodes and solution deposited air-stable semiconductor polymer. Molecular Dynamics simulation is also employed to calculate the effective thermal conductivity of the nanoparticle film to facilitate thermal modeling of the nanoscale laser processing.",ucb,,https://escholarship.org/uc/item/0ts3p44r,,,eng,REGULAR,0,0
809,2245,"Ion Energy Distribution in Collisionless and Collisional, Capacitive RF Sheath","Wang, Ying","Lieberman, Michael A;Verboncoeur, John P;",2012,"Capacitively driven radio frequency (rf) discharges are commonly used for plasma-assisted material processing. Because of the significant difference in the mobility of electrons and ions, a thin layer of sheath is always established at the boundary, which separates the discharge into two regions: quasi-neutral bulk plasma and positively charged sheath. Within the sheath, ions are accelerated by electric fields and therefore, bombard the electrode with significant energies. The ion energy distribution (IED) on the substrate is essentially important in the optimization of discharge operations. For plasmas sources typically operated at higher densities and lower pressures, the ion motion in the rf sheath is mainly collisionless since the ion mean free path is much larger than the sheath width. At high operating pressures and large sheath voltage drops, the sheaths are typically collisional.A fast and simple model consisting of a series of computational steps is of great value to predict the plasma parameters and IEDs, given the control parameters of the discharge. Respective models for IEDs in collisionless and collisional capacitive rf sheaths are developed in this dissertation, based on the sheath models developed in late 1980s. Both models do not rely on any intermediate parameters from simulation or experimental results and only take a few seconds (collisionless) or minutes (collisional) to get the final IEDs. Ion-neutral charge exchange reactions are considered for collisional rf sheaths. Energy dependent ion mean free path is taken into account. Particle-in-cell (PIC) simulations are used to verify the previous sheath models and the IED models.The PIC code OOPD1, being developed with a powerful capability as the development of the collisional IED model goes on, is introduced in this dissertation. Comparisons with XPDP1 are presented, which confirms OOPD1 as a trustable, friendly, and extensible simulation tool to observe the rf discharges.",ucb,,https://escholarship.org/uc/item/0v9249t8,,,eng,REGULAR,0,0
810,2246,On-chip Benchmarking and Calibration without External References,"Lee, Cheol-Woong","Niknejad, Ali M.;",2011,"The strong market demand for mobile applications such as iPhone makes value on the form factor of the mobile devices. The form factor means how much we can integrate many functions in a given size of the mobile devices. The external component size is almost comparable to a chip size so that elimination of external component is crucial to the success of mobile devices in addition to the cost issues of the external components. External reference resistors are often used as the standard for calibrating voltage sources, current sources, and other component values within a circuit. Often these calibrations occur at a factory, but may also occur on an electronic device as it is used. However, external reference resistors consume area and cost and it is desirable to eliminate them. This work introduces a new way to calibrate on-chip resistance and capacitance without the external reference resistors. An integrated circuit includes a benchmarking circuitry and a tunable circuitry. The benchmarking circuit includes a target component and an internal reference component. The internal reference component exhibits a lower sensitivity to the changes in test conditions than the target component. Benchmarking Metric Measurement Module (BMMM) measures benchmarking metrics for the internal reference component and the target component. A benchmark value is calculated based on the benchmarking metrics. The novelty of this work is the powerful way to cancel the parasitic and systemic errors caused by operational amplifiers in RC tuner circuitry. This technique is broadly applicable to any RF and analog circuits that need the calibration of tunable circuit elements without external references.",ucb,,https://escholarship.org/uc/item/0ww5b96x,,,eng,REGULAR,0,0
811,2247,Gathering Kinds: Radical Faerie Practices of Sexuality and Kinship,"Sanford, Jesse Oliver","Cohen, Lawrence;",2013,"Though they began as a gay male counterpart to the lesbian separatisms of the 1970s, the radical faeries are now a growing movement with thousands of participants in the United States and Europe. This dissertation argues that they are also a kinship phenomenon, a form of extended family. Faeries' polymorphous sexual ethics and economic collaboration entail consequences and constitutive exclu-sions divergent in some ways (but not others) from those of the United States' dominant homonormativity. While some faerie practices are also reminiscent of a new religious movement, others are not, and a careful consideration of the phenomenon in fact destabilizes the distinctions between sexuality, religion and kinship. The case of the radical faeries suggests that religiosity be seen as an erotic orientation and the coming out experience as a religious conversion. Although faeries articulate in many ways the anarchist and ecological themes of modern Euroamerican homosexuality, their own normative philosophies can be rooted in deistic and ultimately authoritarian totalizing ethics, and might be productively re-thought via greater attention to issues of difference and mindfulness. Moreover, while some faeries have achieved remarkable success in establishing rural collective property and new pat-terns of migration, their consensus-based governance processes can be rife with conflicts that might bely claims of filiality. In the conclusion, I argue that same-sex marriage does not go far enough and that advocates for progressive family law reform would be wise to take the example of the faeries into account.",ucb,,https://escholarship.org/uc/item/0xs1770q,,,eng,REGULAR,0,0
812,2248,Consistent population estimates: an application to Brazil,"Borges, Gabriel Mendes","Wachter, Kenneth W;Feehan, Dennis M;",2018,"Demographers have long been aware that the practical application of basic demographic identities often leads to inconsistent estimates due to data quality limitations. Despite the considerable effort to produce demographic estimates and reconcile inconsistent demographic data in different contexts, this issue remains unsolved. This dissertation proposes a Bayesian probabilistic approach that allows for a concurrent estimation of consistent counts of population, mortality, fertility and migration. Old and new methods are combined, thus building upon well-established demographic techniques and statistical methods.This dissertation addresses these issues in three chapters. Chapter 3 describes a set of methods to estimate and reconcile past demographic data, including measures of uncertainty. These methods are highly flexible and applicable to different contexts, both at national and subnational levels, with varying availability and quality of data. In addition to reconciling past population estimates, the methods detailed in this dissertation are concerned with estimating fertility, mortality and migration based on a set of raw observed data. Chapter 4 presents novel estimates and analysis of population, fertility, mortality and migration for Brazil and its 27 states for the period 1980-2010, based on several methods and data sources. Chapter 5 applies the data obtained in Chapter 4 to the methods presented in Chapter 3 to produce demographic estimates for Brazil and its three selected states.This dissertation offers several important methodological contributions to the fields of formal demography and statistical demography. The application of the proposed methods for Brazil and its states unveils new perspectives for generating higher quality demographic estimates in the country. Among other findings, the results reveal higher internal migration flows than those estimated with census data, and fertility estimates that differ from previous work.This dissertation also offers substantial methodological contributions specifically toward techniques of fertility and mortality estimation. Chapter 4 presents a new sensitivity analysis for the Brass P/F ratio method to evaluate the magnitude of bias in the results of its application when one or more conditions of the method are not met. This leads to a refinement in the original method to correct for these biases. This chapter also introduces a method to adjust for bias on recent deaths in the household reported in censuses. The application of the method for Brazil shows that such adjustment greatly improves the efficiency of mortality estimation, particularly at old ages, resolving one of the main limitations of these data.",ucb,,https://escholarship.org/uc/item/0z00s2xq,,,eng,REGULAR,0,0
813,2249,Physicochemical Modeling of Copper Chemical Mechanical Planarization (CMP) Considering Synergies in Removal Materials,"Choi, Seungchoun","Dornfeld, David A;Doyle, Fiona M;",2013,"With stringent requirements of copper chemical mechanical planarization (CMP), such as minimized step heights, enhanced uniformity and minimal defects, the CMP process needs to be improved based on a fundamental understanding of the material removal mechanisms. Also, with the stringent requirements, the problems in copper CMP process cannot be resolved solely improving the process itself; rather, systemic understanding of the entire manufacturing processes is necessary, demanding a robust copper CMP model to be implemented to design for manufacturing (DfM) tools. Previous models heavily relied on Preston's equation ( ), which needs to be calibrated for every new set of processing parameters, slowing down the process development. Previous models focused on limited interactions of the consumables and the workpiece during copper CMP, being insufficient at capturing the synergies between chemical and mechanical aspects of copper CMP. Therefore, a quantitative and physicochemical model of copper CMP that predicts material removal rate (MRR) was proposed while focusing on the interplay of consumables and copper and the synergies between chemical and mechanical aspects of the process. While considering the synergies, two mechanisms of the material removal during copper CMP were suggested: chemically dominant and mechanically dominant mechanisms. The total MRR during copper CMP was determined by summing those two contributions.The chemically dominant mechanism attributed the material removal during copper CMP to the removal of the protective material formed on the surface of copper and to the chemical dissolution of copper from the surface both at regions occupied and not occupied by the protective material with different rates. The kinetics of the formation of the protective material at the millisecond scale were studied through electrochemical experiments and theoretical analysis where a governing equation for the adsorption of benzotriazole (BTA) was constructed and solved. It was found that the grown protective material (CuBTA) during copper CMP was only a fraction of a monolayer partly occupying the surface of a wafer. This was because the time allowed for the adsorption of BTA on the surface of copper was limited by the time between consecutive asperity and copper interactions, which was only of the order of one millisecond. The formation and the removal of the protective material were assumed to be balanced during CMP, yielding a constant chemically dominant MRR. The removal of the protective material by abrasion with abrasive particles was investigated by in situ electrochemical measurement during polishing. The removal efficiency of a pad asperity where abrasive particles are embedded was evaluated from the measurements and was compared with the theoretical analysis. It showed a good agreement and suggested that the copper during CMP is mostly deformed elastically by the abrasive particles. The influence of the concentration of copper ions on the kinetics of the formation of the protective material was also investigated using potential-step chronoamperometry using two types of copper microelectrode, namely a three dimensional and a planar electrode. The amount of copper ion may easily build up to a level that exceeds the solubility product of Cu(II)BTA2. Under these conditions, Cu(II)BTA2 can nucleate, consuming the protective material formed on the surface of copper. This phenomenon is highly undesirable as it increases the dissolution rates at the regions where the protective material is removed, worsening the topography after copper CMP.The mechanically dominant MRR was determined from the volume of a wafer that is plastically deformed by indentation of abrasives that are squeezed between pad asperities and the wafer. The shear stress induced in copper by the force applied on an abrasive is lower than the ideal shear strength of copper, which is the relevant property for plasticity at this length scale. However, the crystallographic defects in the copper crystal may reduce the hardness of the material, allowing the material to be plastically deformed. Especially the roughness of the surface induced by chemical additives in the slurry greatly reduces the resistance to plastic deformation of copper. Because of the localized spatial distribution of those crystallographic defects the plastic deformation occurs only locally. Also, only a part of the plastically deformed material will be detached from the surface, contributing to the MRR. While applying this mechanism, the discrepancy of the MRR behavior with varying size and concentration of abrasives between the prediction and the experimental observations was resolved by proposing a new mechanism that determines the number of abrasives participating in the abrasion of the material. The transport mechanisms of abrasive particles toward a wafer and the electrostatic interactions between abrasives were considered to affect the number of abrasive particles deposited on the surface of a wafer. If the deposition of abrasives on the surface of a wafer is limited by the diffusion of abrasives, the MRR decreases with the size of the abrasives. In contrast, the MRR increases with the size of abrasives if the deposition of the abrasives is limited by the jamming limit of the deposited abrasives at the surface of the wafer. Also, micrometer sized abrasives increases the MRR when the size is increased because the deposition of abrasives is limited by the interception mechanism of the abrasives. The proposed model successfully captured the synergies between chemical and mechanical aspects and quantitatively predicted the MRR during copper CMP. In the future, the model will be applied to predict the pattern dependent variability of topography of a wafer after CMP. The proposed model quantitatively predicts the local MRR of copper. Along with a robust model for dielectric and barrier materials, the model can predict the topography after CMP, contributing to the optimization of the CMP process.",ucb,,https://escholarship.org/uc/item/0zg1b4gm,,,eng,REGULAR,0,0
814,2250,Connections beyond the margins of the power grid: Information technology and the evolution of off-grid solar electricity in the developing world,"Alstone, Peter","Kammen, Daniel M;",2015,"This work explores the intersections of information technology and off-grid electricity deployment in the developing world with focus on a key instance: the emergence of pay-as-you-go (PAYG) solar household-scale energy systems. It is grounded in detailed field study by my research team in Kenya between 2013-2014 that included primary data collection across the solar supply chain from global businesses through national and local distribution and to the end-users.  We supplement the information with business process and national survey data to develop a detailed view of the markets, technology systems, and individuals who interact within those frameworks. The findings are presented in this dissertation as a series of four chapters with introductory, bridging, and synthesis material between them. The first chapter, Decentralized Energy Systems for Clean Electricity Access, presents a global view of the emerging off-grid power sector. Long-run trends in technology create â€œa unique moment in historyâ€ for closing the gap between global population and access to electricity, which has stubbornly held at 1-2 billion people without power since the initiation of the electric utility business model in the late 1800â€™s. We show the potential for widespread near-term adoption of off-grid solar, which could lead to ten times less inequality in access and also ten times lower household-level climate impacts. Decentralized power systems that replace fuel-based incumbent lighting can advance the causes of climate stabilization, economic and social freedom and human health.Chapters two and three are focused on market and institutional dynamics present circa 2014 in for off-grid solar with a focus on the Kenya market.  Chapter 2, â€œOff-grid Power and Connectivityâ€, presents our findings related to the widespread influence of information technology across the supply chain for solar and in PAYG approaches.  Using digital financing and embedded payment verification technology, PAYG businesses can help overcome key barriers to adoption of off-grid energy systems.  The framework provides financing (or energy service payment structures) for users of off-grid solar, and we show is also instrumental for building trust in off-grid solar technology, facilitating supply chain coordination, and creating mechanisms and incentives for after-sales service. Similar models are also being tested and launched for on-grid electricity (pre-pay energy meters) and agricultural water pumping among others. While there is a clear potential to extend the reach of critical infrastructure networks, there are also important concerns for achieving equitable and sustained access.  Some are at the business network level, where telecommunications firms have a unique role as gatekeepers and enablers of mobile communication systems and (sometimes) also competing participants in the emerging PAYG market. Another is the importance of balancing privacy and the value of data-driven technology systems like PAYG. We talked with users who both recognized the value in their personal data and were concerned about widespread sharing beyond the boundary of the retail-facing firms that they interact with.  Overall the work highlights how information and energy systems are co-evolving at the edge of the grid. Chapter 3, Quality Communication, delves into detail on the information channels (both incumbent and ICT-based) that link retailers with regional and global markets for solar goods.  In it we uncover the linked structure of physical distribution networks and the pathway for information about product characteristics (including, critically, the quality of products). The work shows that a few key decisions about product purchasing at the wholesale level, in places like Nairobi (the capital city for Kenya) create the bulk of the choice set for retail buyers, and show how targeting those wholesale purchasers is critically important for ensuring good-quality products are available.  Chapter 4, the last in this dissertation, is titled Off-grid solar energy services enabled and evaluated through information technology and presents an analytic framework for using remote monitoring data from PAYG systems to assess the joint technological and behavioral drivers for energy access through solar home systems. Using large-scale (n ~ 1,000) data from a large PAYG business in Kenya (M-KOPA), we show that people tend to co-optimize between the quantity and reliability of service, using 55% of the energy technically possible but with only 5% system down time. Half of the users move their solar panel frequently (in response to concerns about theft, for the most part) and these users experienced 20% lower energy service quantities. The findings illustrate the implications of key trends for off-grid power: evolving system component technology architectures, opportunities for improved support to markets, and the use of background data from business and technology systems. Overall the work reveals both opportunities and pitfalls in a combined information-energy system. With increased visibility and control of the system there are opportunities to better support the market, but there are often disincentives to share certain data for private sector actors that operate the decentralized power system and frictions at the interface of mismatched information systems. If barriers to interoperability and scale are addressed, basic human needs for energy can be met by solar at an accelerated pace through connections to emerging information technology and business networks that extend beyond the margins of the grid.",ucb,,https://escholarship.org/uc/item/10z3p2jt,,,eng,REGULAR,0,0
815,2251,Classical Nonparametric Hypothesis Tests with Applications in Social Good,"Ottoboni, Kellie","Stark, Philip B;",2019,"Hypothesis testing has come under fire in the past decade as misuses have become increas- ingly visible. It is common to use tests whose assumptions donâ€™t reflect how the data were collected, and editorial policies of many journals reward â€œp-hackingâ€ by setting the arbitrary threshold of 0.05 to determine whether a result merits publication. In fact, properly designed hypothesis tests are an invaluable tool for inference and decision-making. Classical nonparametric tests, once reserved for problems that could be worked out with pencil and paper or approximated asymptotically, can now be applied to complex datasets with the help of modern computing power. This dissertation tailors some nonparametric tests to modern applications for social good.Permutation tests are a class of hypothesis tests for data that involve random (or plausibly random) assignment. The parametric assumptions for common tests, like the t-test and linear regression, may not hold for randomized experiments; in contrast, the assumptions of permutation tests are implied by the experimental design. But off-the-shelf permutation tests are not a panacea: tests must be tailored to fit the experimental design, and there are subtle numerical issues with implementing the tests in software. We construct permutation tests and software to address particular questions in randomized and natural experiments, including identifying what, if anything, student evaluations of teaching measure, and whether voting machines malfunctioned in Georgiaâ€™s November 2018 election.Risk-limiting post-election audits (RLAs) have existed for a decade, but have not been adopted widely, in part due to logistical hurdles. This thesis uses classical nonparametric techniques, including Fisherâ€™s combination method and Waldâ€™s sequential probability ratio test, to build new RLA methods that accommodate the idiosyncratic logistics of statewide elections. A new, more flexible method for using stratified samples in RLAs makes it easier and more efficient to audit elections conducted on heterogeneous voting equipment. This thesis also develops an RLA method based on Bernoulli sampling, which allows ballots to be audited â€œin parallelâ€ across precincts on Election Day. The RLA method for stratified samples of ballots was piloted in Michigan to study its performance in the face of real-world constraints.",ucb,,https://escholarship.org/uc/item/115807vf,,,eng,REGULAR,0,0
816,2252,"The Sober Revolution: The Political and Moral Economy of Alcohol in Modern France, 1954-1976","Bohling, Joseph Estle","Laqueur, Thomas;",2012,"This dissertation examines how, after World War Two, the French state and powerful interest groups shifted the debate over drink from an issue of personal morality into a battle of political economy.  Contrary to the widely held Tocquevillian assumption that France has had weak and fragmented interest groups with little capacity to influence state policy, this dissertation argues that a relatively weak public health movement became influential when it struck alliances with powerful state and economic interests.  Working together, the different and sometimes antagonistic interests of doctors, French and European technocrats, luxury winegrowers, and automobile and insurance groups combined to issue alarms about France's allegedly rising alcoholism and mobilize public opinion against the country's alcohol producers and industrial, mono-cropping winegrowers.  This movement was abetted by important structural transformations:  the fall of the Fourth Republic (1946-1958) and the foundation of the Fifth (1958-), where a strong executive branch circumvented the industrial wine lobby and Parliament; the end of empire, which meant the eventual termination of cheap Algerian wine imports; and the creation of the European Community, which adopted France's luxury Appellation d'origine contrÃ´lÃ©e (AOC) labeling system and discouraged industrial wine production and consumption.  In short, I maintain that this anti-alcohol campaign helped prepare appellation wine producers and the state for competition in the world economy. This dissertation uses drink as a prism through which to understand France's dramatic economic modernization after World War Two.  It contributes to our understanding of France and Europe's so-called ""Economic Miracle,"" particularly the role of the state and the wine industry in shaping European market integration.  Against the common view that the wine industry has been a conservative force in French society, this dissertation argues that it played an active role in its own modernization in order to compete internationally in the context of European integration and, by the 1970s, globalization.",ucb,,https://escholarship.org/uc/item/11w2j6nt,,,eng,REGULAR,0,0
817,2253,What People Say They Do With Words,"Verschueren, Jozef",,1979,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/12z3489j,,,eng,REGULAR,0,0
818,2254,Biomaterials for Cell Engineering and Regenerative Medicine,"Downing, Timothy","Li, Song;",2013,"The promise of regenerative medicine relies on the ability to tightly control cell behavior. Given the broad influence of epigenetics in cell behavior and phenotype determination, it is critical to better understand how interactions with the physiological microenvironment and/or implanted materials affect cell's epigenetic state and, thus, identity. In this dissertation we demonstrate, for the first time, that biophysical cues (e.g., matrix topography and stiffness) can significantly improve the efficiency of cell reprogramming, the process of reverting somatic cells back to pluripotency, by inducing key epigenetic changes in adult fibroblasts. To help elucidate the role of biophysical factors in cell reprogramming we have utilized induced pluripotent stem cell (iPSC) technology in conjunction with various bioengineered substrates. These substrates include poly(dimethyl siloxane) (PDMS) microgrooves, aligned nanofibrous membranes, and polyacrylamide hydrogels. Our data largely suggest that cytoskeletal proteins play an important role in the process of cell reprogramming and that manipulation of the biophysical microenvironment can induce dramatic changes in histone acetylation and methylation patterns. These epigenetic changes significantly improve iPSC generation efficiency and replace the effects of potent small molecule epigenetic modifiers valproic acid (VPA) and tranylcypromine (TCP). Furthermore, we identify specific mediators of this epigenetic mechanomodulation, while distinguishing the role of cell shape in the observed histone modifications. This novel biophysical regulation of epigenetics has important implications in cell biology and in the optimization of materials for broad biological application. Finally, this dissertation work describes how drug-eluting microfibrous patches and nanofibrous scaffolds, in conjunction with iPSC-derived stem cells, can be utilized to combine the effects of biophysical guidance cues, therapeutic drug delivery, and cell engineering for the study and repair of spinal cord injury after both traumatic insult and neural tube defect.",ucb,,https://escholarship.org/uc/item/13m6r4sr,,,eng,REGULAR,0,0
819,2255,Metalâ€“Oxo and Dioxygen Chemistry in Metalâ€“Organic Frameworks: Applications in Catalysis and Gas Separations,"Xiao, Dianne Jing","Long, Jeffrey R;",2016,"The work herein describes progress towards using metalâ€“organic frameworks as scaffolds for stabilizing metalâ€“oxo and dioxygen species, and their application in hydrocarbon oxidation catalysis and O2/N2 separations. Metalâ€“organic frameworks are a class of highly porous and functionally versatile crystalline solids consisting of inorganic cations or clusters bridged by organic linkers. They are attractive as solid supports for metalâ€“oxo and dioxygen chemistry for many reasons, including the presence of well-defined, site-isolated metal centers with highly tunable local and outer coordination spheres.Chapter 1 provides an introduction to the electronic structure, reactivity, and biological relevance of metalâ€“oxo and dioxygen species, with a particular emphasis on iron and cobalt. In addition, a brief historical overview of the development of biomimetic ironâ€“oxo, ironâ€“dioxygen, and cobaltâ€“dioxygen chemistry, with selected molecular and heterogenous examples, is provided. The chapter concludes with a summary of the methods currently used to install coordinatively-unsaturated redox-active metal sites into metalâ€“organic frameworks. A perspective on the potential of metalâ€“organic frameworks in metalâ€“oxo and dioxygen chemistry is given.Chapter 2 describes an initial foray into metalâ€“organic framework-supported ironâ€“oxo chemistry. Specifically, the nitrous oxide activation and hydrocarbon oxidation reactivity of the coordinatively-unsaturated iron(II) sites in the metal-organic frameworks Fe2(dobdc) and Fe0.1Mg1.9(dobdc) (dobdc4â€“ = 2,5-dioxido-1,4-benzenedicarboxylate) is detailed. In the presence of N2O, the latter framework is able to selectively and catalytically convert ethane to ethanol upon mild heating. Structural and spectroscopic characterization of the initial iron(II)â€“N2O adduct and an iron(III)-hydroxide decay product, reactivity studies, and detailed electronic structure calculations strongly suggest that the active oxidant in this system is a high-spin, S = 2 iron(IV)â€“oxo. This rare electronic structure is a direct result of the weak ligand field imparted by the dobdc4â€“ ligand.In addition to primary coordination sphere properties such as a weak ligand field, longer-range pore-environment effects could also become a powerful parameter in metalâ€“organic framework catalyst design. Chapter 3 explores this idea in the context of solution-phase cyclohexane oxidation in the biphenyl and terphenyl expanded Fe2(dobdc) derivatives. A three-fold enhancement of the alcohol:ketone (A:K) ratio and an order of magnitude increase in turnover number is observed by simply altering the framework pore diameter and installing nonpolar, hydrophobic functional groups near the iron center. The increase in A:K selectivity is attributed to an increased affinity of the pore walls for cyclohexane, which may help increase its local concentration near the iron site.Chapter 4 departs from ironâ€“oxo chemistry and oxidation catalysis, focusing instead on cobaltâ€“dioxygen binding for O2/N2 separation applications. Specifically, this chapter details the reversible O2 binding properties of Co-BTTri (H3BTTri = 1,3,5-tri(1H-1,2,3-triazol-5-yl)benzene), a sodalite-type framework containing coordinatively-unsaturated cobalt(II). It was found that the O2 binding affinity could be tuned by altering the local ligand field. Electronic structure calculations reveal the extent of electron transfer from cobalt to O2 in these systems is highly dependent on the local environment, and can vary between 0.2 to 0.7 electrons.Chapter 5 combines aspects of both Chapter 1 and 4, focusing on the development of new iron(II)-based frameworks with tunable primary coordination spheres, and the effect of ligand field on reactivity and gas adsorption properties. The synthesis and characterization of two new sodalite-type frameworks, Fe-BTTri and Fe-BTP (H3BTP = 1,3,5-tri(1H-pyrazol-4-yl)benzene), is reported. Interesting O2 and CO gas adsorption properties are displayed by bulk Fe-BTTri, and are briefly described in this chapter. In addition, initial N2O/ethane oxidation studies performed on Fe-BTT suggest defect sites are responsible for the reactivity seen in this material, highlighting the inhomogeneity present even in highly crystalline metalâ€“organic frameworks.",ucb,,https://escholarship.org/uc/item/14f2b0xj,,,eng,REGULAR,0,0
820,2256,"Peacebuilding, Political Order, and Post-War Risks","Willcoxon, George Frederick","Weber, Steven;",2015,"Since 1945, violent conflict has occurred primarily within sovereign states rather than among them. These internal conflicts have far surpassed international conflicts in lethality, economic destruction, and social upheaval. This phenomenon is diverse: no region has avoided civil wars, while the stated aims of rebel groups have ranged widely. Prominent examples include anti-colonial nationalists in Algeria, Mozambique, and Kenya; ethnic separatists in Eritrea and Bosnia; leftists in Latin America and Southeastern Asia; Islamic fundamentalists in Afghanistan, Iraq, and Syria; and income seeking warlords in Liberia and Sierra Leone. Internal conflicts have emerged in rich European countries such as the United Kingdom and Spain, and in the context of state collapse and extreme poverty in the Democratic Republic of Congo and Somalia. Some civil wars have lasted only weeks, while the longest-- in Sudan-- lasted over 40 years.Intense violent conflicts often leave core state institutions debilitated, fragmented, or, in some cases, totally destroyed. For these societies, the central tasks for ending conflict and beginning post-war recovery involve reinvigorating or reestablishing legitimate state authority. These post-war states must both win the acquiescence of the governed and develop the infrastructural power to implement state policy. The risks of conflict relapse are significant: since 1970, 44 of 111 post-war cases (40 percent) relapse into a full-fledged civil war, while 68 of 111 (61 percent) experience at least a low-level conflict. The time for policymakers to mitigate this risk is short: of post-war countries that fall back into civil war, the median time to relapse is just 35.5 months. The immediate post-war environment is therefore particularly critical for determining the political, economic, and social trajectories of conflict-affected countries. The right combination of policies can help determine whether a country recovers quickly and secures any available peace dividend, or whether it relapses and slides into a conflict trap. This dissertation explains how societies that have managed to end their civil wars are able or unable to rebuild political order in the their post-war period.This dissertation focuses on one key policy arena-- perhaps the most critical policy arena-- for post-war societies to address: the security sector. It may sound simplistic or even tautological to claim that the organization, disposition, control, and reform of armed groups are the most important task for a post-war society to undertake. It may seem obvious to stress the importance of the size, competencies, oversight, social embeddedness, and other qualities of the military, the police, the intelligence services, and any remaining armed non-state actors. But such qualities resist easy quantification, and most scholars and practitioners over the past decade have focused on economic performance, political democratization, communal reconciliation, post-conflict justice, and other â€œsoft-powerâ€ variables to explain patterns of post-war successes and failures. The following chapters attempt to shift the conversation back to the formation and reformation of security sector actors in war-affected countries.",ucb,,https://escholarship.org/uc/item/14p455v4,,,eng,REGULAR,0,0
821,2257,Emotional Response Coherence and Interoceptive Awareness: Development and Validation of a Novel Assessment Method,"Muhtadie, Luma","Levenson, Robert W;",2017,"Interoceptive awareness (IA), the conscious perception of signals originating in the body, is a fundamental component of our subjective experience of emotion and may be its proximate cause. IA is integral to attention, motivation, emotion regulation, and decision-makingâ€”processes that are essential to our survival, sense of agency, and wellbeing. A clear understanding of individual differences in IA is currently hampered by the limitations of prevailing assessmentsâ€”namely, self-report questionnaires and heartbeat perception tasksâ€”which have questionable reliability and validity, fail to capture the full spectrum of individual variability, and disregard the emotional contexts in which interoceptive processes naturally unfold. This dissertation proposes a novel method for assessing IA that capitalizes on emotional response coherence. Specifically, the method assesses variability in the extent to which physiology and subjective experience track together within individuals while they are experiencing strong emotions. Theoretical and empirical rationales for considering the â€œCoherence Taskâ€ to be a proxy measure of IA are elucidated and its psychometric properties are systematically evaluated.  Fifty-six men and women aged 18 to 50 completed the coherence task on two occasions spaced one week apart. While watching evocative film montages that captured a range of emotions, subjects provided momentary ratings of their subjective experience on valence and arousal dimensions (2 separate trials per session) and their physiology was continuously recorded. Cross-correlation coefficients of the coherence between subjective ratings and heart period (â€œCoherence Scoresâ€) were then computed for each individual. Coherence Scores derived from valence-based ratings of subjective experience and heart period demonstrated significant 1-week test-retest reliability (i.e., temporal stability); were positively associated with self-reported body awareness (i.e., convergent validity); and were negatively associated with a composite measure of distress and positively associated with empathy (i.e., predictive validity). Moreover, these findings showed specificity for the coherence between subjective experience and visceral over somatic signals (i.e., for interoceptive over proprioceptive awareness; discriminant validity).The Coherence Task shows early promise as an empirically grounded assessment of individual differences in IA. This task would also enable us to evaluate the efficacy of interventions that target interoceptive awareness for health and wellbeing (e.g., mindfulness meditation).",ucb,,https://escholarship.org/uc/item/14z3k333,,,eng,REGULAR,0,0
822,2258,Resolving Soft Material Crystallinity through Scanning Transmission Electron Nanodiffraction,"Panova, Ouliana","Minor, Andrew;",2018,"A method for imaging the semi-crystalline structure of organic, electrically conduct- ing molecular thin films using scanning nanodiffraction transmission electron microscopy (4D-STEM) is developed, with a maximum achieved resolution of 5-10 nm, depending on the material analyzed. The changes in local nanocrystalline structure of the polymer thin films under study - regioregular Poly(3-hexyl-thiophene-2,5-diyl), the small molecule 7,7â€™- (4,4 - bis(2 - ethylhexyl) - 4H - silolo[3,2-b:4, 5-bâ€™] dithiophene - 2,6 - diyl) bis(6 - flu- oro - 4 - (5â€™- hexyl[2,2â€™-bithiophen] - 5 - yl)benzo[c][1,2,5] - thiadiazole), abbreviated as p- DTS(FBTTh2 )2 or T1, and Poly[2,5-bis(3-tetradecylthiophen-2-yl)thieno[3,2-b ]thiophene], abbreviated as PBTTT - with processing conditions, such as solvent addition and annealing, are characterized and visualized. The methods presented show spatially resolved features such as overlapping grains and nematic liquid crystal character that have not been directly imaged before, and help remove ambiguities in X-ray and other techniques that have been thus far used to characterize these materials.The method is first applied to polyethylene and P3HT samples to demonstrate the via- bility of the electron transmission technique on soft materials and determine its limitations. A resolution of 5 nm is achieved on P3HT. The method is subsequently used to visualize the changes in crystal structure of T1 when treated with 1,8-diiodooctane (DIO), and then on PBTTT to characterize morphological changes upon annealing. It is found that for T1, while the untreated samples exhibited a liquid-crystal-like structure with crystalline orientations varying smoothly over all possible rotations, the addition of a co-solvent induces partial segmentation of the structure characterized by the emergence of sharp grain boundaries and overlapping domains with unrelated orientations. In the case of PBTTT, the crystalline character of the nematic liquid crystal phase increases upon annealing. These results demon- strate how scanning electron nanobeam diffraction can provide a new level of insight into the structure of functional organic solids, and show how structure-property relationships can be visualized in organic systems using nanoscale electron microscopy techniques previously only available for hard materials such as metals and ceramics.",ucb,,https://escholarship.org/uc/item/15d1x9g0,,,eng,REGULAR,0,0
823,2259,Time-Course Analysis and Clustering of Gene Expression Data,"DeGraaf, Stephanie","Purdom, Elizabeth;",2020,"High-throughput time-course studies collect measurements from samples across time. Inparticular, longer-duration high-throughput time-course studies are becoming more common, such as in the case of 16S sequencing of bacterial communities or single-cell mRNA sequencing of developmental lineages. A common focus of these studies is on significancetesting per gene. However, in many settings, particularly those studying developmental processes, large numbers of genes show temporal changes, and the relevant question is instead to classify genes into different types of temporal changes. We propose a mixture-model clustering method that estimates a functional spline model for the mean of the cluster, inorder to cluster the temporal patterns of genes independent of scale. The model allows fora wide range of likelihood models to suit a variety of data types. In addition, this clusteringstrategy accounts for time-course data under different experimental conditions or developmental lineages, and it provides a method for evaluating, per cluster, significant differences in temporal patterns between conditions. This allows for an integrated analysis of differential expression analysis and clustering. We demonstrate the benefits of our method using simulated data. In addition, we explore several real data sets to illustrate both the context for and the application of the mixture model.",ucb,,https://escholarship.org/uc/item/16k3r09c,,,eng,REGULAR,0,0
824,2260,Hot Beats and Tune Outs: Atom Interferometry with Laser-cooled Lithium,"Cassella, Kayleigh","MÃ¼ller, Holger;",2018,"Ushered forth by advances in time and frequency metrology, atom interferometry remains an indispensable measurement tool in atomic physics due to its precision and versatility. A sequence of four $\pi/2$ beam splitter pulses can create either an interferometer sensitive to the atom's recoil frequency when the momentum imparted by the light reverses direction between pulse pairs or, when constructed from pulses without such reversal, sensitive to the perturbing potential from an external optical field. Here, we demonstrate the first atom interferometer with laser-cooled lithium, advantageous for its low mass and simple atomic structure. We study both a recoil-sensitive Ramsey-Bord
  


",ucb,,https://escholarship.org/uc/item/00m7m6np,,,eng,REGULAR,0,0
825,2261,The Role of Folklore Study in the Rise of Russian Formalist and Czech Structuralist Literary Theory,"Merrill, Jessica Evans","Paperno, Irina;",2012,"Russian formalism and Czech structuralism are understood to have initiated the study of literature as a self-sufficient discipline by applying linguistic concepts to the analysis of literary texts. This dissertation seeks to enrich our understanding of this development by examining the transition from linguistics to literary theory from an intellectual-historical perspective. My thesis is that folklore study played a crucial role in the rise of formalist and structuralist literary theory by serving as a mediating field between language and <&ldquo>high literature. Folkloristics, which traditionally approached its subject matter through linguistic theory, understood verbal art to behave like language<&mdash>--as an impersonal repertoire of poetic forms which adhere to regular laws governing their usage and evolution. This body of scholarly work provided early literary theorists with a model for theorizing literature or art as a law-abiding, <&ldquo>scientific object of study akin to language. The transfer of ideas from the field of folkloristics to literary theory was the product of scholarly training, personal intellectual exchange, and institutional affiliations. In the first chapter I focus on Victor Shklovsky's use of A. N. Veselovsky's writings to develop a universalist theory of narrative structure in his Theory of Prose . Drawing on Roman Jakobson's The Newest Russian Poetry and his work on the Cyrillo-Methodian legacy, the second chapter illustrates parallels between Jakobson's conceptions of literary value and literary evolution and the work of his teacher V. F. Miller. The last chapter argues that Jan MukaÅ™ovskÃ½'s Aesthetic Function, Norm and Value as Social Facts drew on P. G. Bogatyrev's functional structural ethnography and compares their respective conceptions of the semiotic collective. By tracing these intersections, we can see how the emergence of theory intended to explain <&ldquo>high literature was galvanized by moments of contact with folklore studies. Highlighting the role that folkloristics played in the work of these three pioneering literary theorists (Shklovsky, Jakobson and MukaÅ™ovskÃ½) allows us to better understand the emergence of twentieth-century literary theory as an autonomous discipline.",ucb,,https://escholarship.org/uc/item/00v0958b,,,eng,REGULAR,0,0
826,2262,Global Analysis of Murine Cytomegalovirus Open Reading Frames Using Yeast Two-Hybrid and Growth Phenotype Analysis,"Umamoto, Sean","Liu, Fenyong;",2011,"Human cytomegalovirus (HCMV), a beta-herpesvirus, is an important opportunistic pathogen that primarily affects individuals with compromised or immature immune systems.  It is of great significance in AIDS patients where it can cause serious morbidity through retinitis-associated blindness, and other complications, such as pneumonia and enteritis.  In developed nations, it is a leading viral cause of congenital disease, where in-utero infection manifests in mental and behavioral disorders.  In order to control infection and HCMV associated disease, new compounds and novel strategies must be developed.  Understanding the role viral proteins play during the course of infection will help elucidate the mechanisms of HCMV pathogenesis and provide important information on potential targets for new treatments.     However, the strict species specificity of HCMV prevents any studies into the pathogenesis of the virus in an animal host.  This limitation can be overcome through the use of murine cytomegalovirus (MCMV).  MCMV, like HCMV, is a betaherpesvirus that exhibits similar pathogenesis in mice to HCMV infection in the human host.  The genetic structure of MCMV contains significant sequence homology to HCMV AD169 in at least 78 ORFs and can thereby be used as an important tool in elucidating the functions of these ORFs in a complete in vivo system.       In our study, we have conducted a comprehensive YTH screen to identify potential interactions between approximately 170 MCMV ORFs.  Growth phenotype analysis were also conducted using five different cell lines potentially involved in various aspects of CMV infection.  Between these 170 predicted proteins we have identified 94 potential interactions that exhibit varying levels of essentiality depending on the type of cell infected.      We aim to understand the nature of the interactions between the viral particle and proteins encoded by the virus in order to elucidate potential mechanisms by which these proteins help to assemble and create new progeny viruses.  The interactions that we have identified in this study provide a framework to predict the functions of uncharacterized viral proteins.  And understanding the importance of each protein in the context of infection can further help to determine the nature of these unknown viral proteins.  Together using information about known viral proteins that interact with these unknown elements, we can develop a better understanding of how all of these components contribute to viral infection which can be used to determine more effective methods to treat or prevent CMV associated diseases.",ucb,,https://escholarship.org/uc/item/0177z7vw,,,eng,REGULAR,0,0
827,2263,Bayesian Network Methods for Modeling and Reliability Assessment of Infrastructure Systems,"Tien, Iris","Der Kiureghian, Armen;",2014,"Bayesian Network Methods for Modeling and Reliability Assessment of Infrastructure Systemsby Iris TienDoctor of Philosophy in Civil and Environmental EngineeringUniversity of California, BerkeleyInfrastructure systems are essential for a functioning society. As these systems age, however, system reliability analyses are required to identify the critical components and make decisions regarding inspection, repair, and replacement to minimize the risk of system failure. In this study, we present novel Bayesian network (BN) methodologies for the modeling and reliability assessment of infrastructure systems. In an environment where information about a system is evolving and is oftentimes uncertain, BNs are able to both update the network when new information becomes available, and handle information probabilistically to support engineering decision making under conditions of uncertainty. One of the major limitations of the BN framework, however, is the size and complexity of the system that can be tractably modeled as a BN.In this study, we propose a novel compression algorithm that significantly reduces the memory storage requirements for a BN model, along with an inference algorithm that performs both forward and backward inference on the compressed matrices. We also present several heuristics to improve the computational efficiency of the algorithms. Through the application of these algorithms and heuristics to example systems, we show the proposed methodology to achieve significant gains in both memory storage and computation time. Together, these algorithms enable larger systems to be modeled as BNs for system reliability analysis.In addition, we propose a methodology based on the dynamic BN (DBN) to assess the response of a structure as it evolves through time under an excitation that is stochastic, e.g., an earthquake ground motion, based on sensor measurements that are uncertain. We look at the maximum response in particular, and derive an analytical solution for estimating the distribution of the peak response. In applying the proposed DBN framework to a multi-story shear-type building, we show the method to be robust relative to uncertainties in the structural characteristics, ground characteristics, and input motion parameters. This work informs decision making in the management of structures subject to seismic hazard and for the development and design of smart structural health monitoring systems.",ucb,,https://escholarship.org/uc/item/01s3t6pq,,,eng,REGULAR,0,0
828,2264,Item cluster-based assessment: Modeling and design,"Arneson, Amy E.","Wilson, Mark;",2019,"This three-paper dissertation explores item cluster-based assessments, first in general as it relates to modeling, and then, specific issues surrounding a particular item cluster-based assessment designed There should be a reasonable analogy between the structure of a psychometric model and the cognitive theory that the assessment is based upon. Specifically, for item response theory (IRT) models in educational assessment scores, the structure of dependencies among items that are designed as item clusters (groups of items that share common stimulus material, etc) should be reflected in the model. This type of designed local item dependence (LID) can be modeled in many different ways. The literature on the existence of LID and models developed to account for this LID is somewhat extensive, though there is little work to unify and organize these different approaches. The first paper presents a general framework to guide modeling decisions for item cluster-based assessments by first formalizing some of the terminology used  in the context of LID, providing an overview of methods for detecting LID, and discussing general modeling approaches for response data that is theorized to exhibit LID.Recent pushes for increased rigor and focus on complex constructs (such as critical thinking) in K-16 education highlight a need to develop assessments that measure these complex constructs. The second paper explores these issues in the context of a particular complex constructs in statistics education, that of Linking Data to a Claim (LDC), Meta-Representation Competence (MRC), and Formal Inference (FoI). We present a multidimensional treatment and analysis of field test data for the Critical Reasoning for College-Readiness (CR4CR) Assessment, an item cluster-based assessment. We found that the LDC and FoI items as written can provide a mapping of student ability estimates to the construct map levels as defined, but that the MRC items do not. Further, as expected, we found moderately strong correlations among the three constructs.The third paper describes the design of selected response items based on open-ended counterparts for the CR4CR Assessment, and the empirical comparisons of these different formats. It is commonly thought that multiple choice (or selected response) items on tests do not provide useful information to educators regarding higher level thinking skills such as argumentation or critical thinking. However, there is also a need for diagnostic assessments to provide educators with timely feedback on student performance so that instruction can be adapted or interventions administered based upon student needs. We found that though existing literature suggests that selected response item types are easier, in general, than constructed response item types, this may not be the case for all constructs. We found that, for the LDC and FoI constructs, multi-select multiple choice items behaved similarly to their constructed response counterparts.",ucb,,https://escholarship.org/uc/item/01t619d1,,,eng,REGULAR,0,0
829,2265,"""Odds and Sods"": Minorities in the British Empire's Campaign for Palestine, 1916-1919","Saltman, Julian","Adamthwaite, Anthony;",2013,"This dissertation examines the role of minority soldiers in Britain's Army during the campaign for Palestine in the First World War. It compares the experiences of two distinct, yet parallel, groups--three battalions of black, British West Indians (the British West Indies Regiment) and three battalions of Jewish soldiers (the ""Jewish Legion""). Past scholarship has mostly ignored the history of these men, and what does exist has tended to conflate or subsume the specific experiences of the men in Egypt and Palestine within the broader histories of their specific minority groups, generally those that occurred on the Western Front. This work diverges from these past understandings, arguing that a comparative assessment of minority soldiers within the Palestine theater of war yields a new understanding of how Britain fought the First World War, as well as how wartime experience differed significantly amongst various minority groups.The first main part of this project assesses the specific military experiences of West Indian and Jewish soldiers in Palestine, tracing their recruitment, training, and military roles. By outlining how the British government and military maintained hierarchies of ethno-racial identity, as well as how minority soldiers conceived their own identities, these chapters are able to dispute narratives of homogenous military service. Specifically, West Indians in Palestine viewed themselves as elevated in class and culture not only from other ""non-European"" colonial soldiers, but also from other West Indian military units, including units of their own regiment stationed in Europe.   Similarly, the identity of the Jewish battalions--often viewed historically as distinctly Zionist--was heavily contested by assimilated British Jews, leading to a more diverse military experience than often assumed. Both chapters demonstrate that West Indians and Jews played key roles in the front lines, and suggest that they represented a distinct tier of imperial soldiering, one precariously situated above explicitly colonial units.The second part of this dissertation explores frameworks of imperial conditioning, offering detailed examinations of military justice and forced athletic training inside the West Indian and Jewish battalions. First, it examines how West Indian and Jewish soldiers encountered military justice, with a specific focus on how their minority identities influenced the application of military law. These chapters conclude that military law was applied in both a punitive and nuanced manner--allowing prejudice and stereotype to affect the sentencing of minority soldiers, but also providing an effective counter through a mechanical system of appeals, remission, and commutation.  The final portion of this section argues that frequent athletic competition amongst soldiers in the EEF was more than a leisure activity or form of military training, but was an indirect means of inculcating potentially disruptive soldiers with a set of British values and norms that would make them amenable to postwar imperial governance. This was a direct reaction to the political radicalism unleashed by the war--namely Bolshevism and the rise of pan-nationalisms inside the British Empire.   This dissertation uses military service records, the application of military justice, and a set of wartime and postwar conditioning policies to reveal the ways in which the British Empire was forced to broaden its definitions of who in its empire could serve, what expectations their service would create, where they could bear arms, and how this would affect the postwar empire.",ucb,,https://escholarship.org/uc/item/01t9w60m,,,eng,REGULAR,0,0
830,2266,"The Borders of Friendship: Transnational Travel and Tourism in the East Bloc, 1972-1989","Keck-Szajbel, Mark Aaron","Connelly, John;",2013,"The ""borders of friendship"" was an open border travel project between Czechoslovakia, East Germany, and Poland starting in 1972. The project allowed ordinary citizens to cross borders with a police-issued personal identification card, and citizens of member countries were initially allowed to exchange unlimited amounts of foreign currency. In this  episode of liberalized travel - still largely unknown in the West -  the number of border-crossings between member states grew from the tens of thousands to the tens of millions within a very brief period.     This dissertation analyses the political, economic, social and cultural effects of this open border policy. It first clarifies what motivated authorities in Poland, East Germany, and Czechoslovakia to promote unorganized foreign tourism in the 1970s and 1980s. Then, it explores how authorities encouraged citizens to become tourists. Governments wanted the ""borders of friendship"" to be successful, but they were unsure how to define success. Each government had different understandings about what the project was supposed to entail. Whatever the case, officials worked to ensure that their population reaped the greatest rewards from the open border.     For ordinary citizens, the ""borders of friendship"" were popular, but were fraught with problems. They liked being able to go abroad, but felt uneasy about foreigners entering their own lands, often plagued by shortages. Additionally, border guards and custom officials harassed people going abroad. Furthermore, people had not forgotten unpleasant chapters of World War II, including forced population movements and genocide. Finally, even if people gained a greater sense of ""freedom"" through open borders, few forgot the looming presence of the totalitarian state.      Yet the open border project (like the travel it was meant to encourage) was not organized by the state. Contrary to commonplace views of the East bloc, officials did not act in unison, but rather struggled unsuccessfully to control undesirable travel and to gain reliable information to disseminate to socialist neighbors. Additionally complicating matters was the fact that everyone had different understandings as to what the open border project was meant to entail. Nevertheless, even if locals were chagrinned by shortages in their supermarkets, the open border project provided everyday individuals with a new social environment. By 1989, travel had become engrained in the habitus not only of citizens in the West, but of East Central Europe, as well.      In sum, I paint a picture of late state socialism which, on the one hand, alters our commonplace perceptions of life behind the ""Iron Curtain,"" but on the other, which also confirms views of governments hyper-sensitive to change.",ucb,,https://escholarship.org/uc/item/01z3n1qx,,,eng,REGULAR,0,0
831,2267,Through the Second Looking Glass: Inventing the Minority Bildungsroman,"Henry, Alvin","JanMohamed, Abdul;",2012,"My dissertation argues for the importance of what I term the minority Bildungsroman, a genre that twentieth-century writers adopted in order to represent racial anxiety as well as to imagine a way for the minority subject to move beyond it. By looking at the minority Bildungsroman as a literary form that exposes the process of Bildung not as self-formation but as self-dissolution, I aim to offer an important new perspective into how minority literature uses genre and literary history: only close attention to plot, character, and narrative reveals how these texts create a new genre to depict the minority subject's escape from the complex of socially-imposed identities originating from the dead mother complex.  Unlike the subject of the traditional Bildungsroman, who achieves social integration and a stable ego, the minority subject in this new genre fails to successfully internalize the social roles that he is assigned.  The instability and suffering imposed by double consciousness and racial anxiety cause him to throw off his prescribed identities.  The narrator of Invisible Man, for instance, pursues experiences aimed at achieving social integration.  Yet these paths result only in failure.  He excels at college and glimpses a future of affluence and prominence, for instance, but only to be summarily expelled.  Such experiences fail to produce what they promise, eventually thwarting his desire for normality and success.  Seeking to be more than the poor, rural blacks that haunt his memory, yet unable to assimilate, Invisible Man progressively casts off elements of his social identity, and, in the novel's climax, reaches a state of social formlessness, or invisibility.  The structures of white society, Ellison implies, cannot but deform those minorities who attempt to live in accordance with them.  This movement towards self-disintegration, however, opens the space for the radical conclusion of the minority Bildungsroman. As a subject without subjectivities he begins what I call a ""second mirror stage."" By combining Lacan's notion of subject formation with Du Bois's conception of the end of double consciousness as a ""longing to attain self-conscious manhood, to merge his double self into a better and truer self,"" I argue that the second mirror stage allows the subject to reconstitute his ego and identity. This process terminates racial anxiety and the double consciousness that engenders it. Reworking the form of the traditional Bildungsroman, these authors use formal innovation as the means of reimagining the self and, I argue, show that literary analysis is capable of recovering otherwise hard-to-access originary psychic traumas.",ucb,,https://escholarship.org/uc/item/020614nk,,,eng,REGULAR,0,0
832,2268,Essays in Labor Economics,"Freeman, Donald Eric","Reich, Michael;",2010,"We first investigate properties of employee replacement costs, using a panel survey of California businesses in 2003 and 2008. We establish that replacement costs are substantial relative to annual wages and that they are associated negatively with the use of seniority in promotion. We also find some evidence, albeit not under all specifications, that replacement costs are positively associated with establishment size, which is consistent with monopsony. Bivariate scatterplots, pooled regressions and panel-based estimates suggest a positive (although not robust) relationship between replacement costs and the wage. This result constitutes an anomaly for hiring and separation models, such as Manning (2003), in which the negative wage elasticity of replacement costs is a key assumption. We also examine particular occupational groups, and find that although there is not necessarily a clear incongruity for blue collar workers, there is one for professional and managerial workers. To resolve the anomaly, we propose several possible solutions including for one the role of heterogeneity, and on the other hand expanding Manning's model to incorporate the effects of wages on productivity.We also study the relationship of wages with High Performance Workplace Organization (HPWO) practices, a broadly-defined set of changes in establishment policies, often involving flexible policies, that have been adopted by many firms in the U.S. in the past few decades. HPWO practices include, for example, employee meetings to discuss workplace problems, self-managed teams, and job rotation. There is no empirical consensus in the literature on the relationship between wages and these practices, and the data required for its study is somewhat uncommon. We use the same panel survey of California businesses to investigate the issue. We find some evidence that adoption of these practices is correlated with increased entry wages for blue collar workers, and correlated with increased entry and highest tier wages for professionals and managers. There is also evidence that HPWO incidence is associated with increased wages for workers at manufacturing establishments, establishments with few professional or managerial workers, and workplaces with a union presence. Regarding individual practices, we find support for a positive correlation between higher wages and increased worker participation in self-managed teams, especially among blue collar workers and at establishments with few professional or managerial workers. But there is a negative relationship between team adoption and wages for professionals and managers. Finally, the adoption of formal quality management programs is associated with higher wages for professionals and managers. We conclude that the relationship between HPWO intensity and wages is complex and heterogeneous, varying for different groups of workers and different practices, and in fact some of these correlations may lead to cancelling effects when one looks only at aggregate relationships. Thus any analysis of high performance practices should be examined with a focus on fine levels of interaction.We also study the interplay among tenure, firm size and firm age. The positive association between firm size and the average tenure of employees at a firm is well-known. We present evidence, at the establishment level, that a large portion, though likely not all, of this association can be explained by the following observation: there is a positive association between an establishment's size and its age so that when combined with the mechanical relationship between establishment age and average employee tenure, this fact leads to a size-tenure correlation. We investigate this issue by making use of the panel survey of California establishments; it is very useful for studying the topic because it is unusual in having data on all three of these quantities.We also study the relationship of certain employer policies with voluntary quit rates, using the panel survey of California businesses. We particularly focus on two policies, namely (i) how much employers give preference to current employees when hiring for a position above entry level, and (ii) how much employers take into account seniority when promoting to a position they have already decided to fill internally. This question has been studied before by Fairris (2004), but the data required is scarce; moreover, with this survey of California establishments, we can answer new related questions. We find some evidence from pooled regressions that (i) is associated positively with quit rates. Balanced panel regressions do not confirm these results; however, the standard errors are quite large due to small sample sizes. We find no clear evidence of associations between (ii) and the overall voluntary quit rate. Moreover, the data set allows us to investigate questions that have not yet been studied, by breaking down quits into finer categories. We find, albeit with non-robust results, that (i) is positively associated in particular with quits to accept another job. We find as well that (i) and (ii) are both negatively associated with employee turnover rates due to unsatisfactory performance.We also examine a controlled randomized health care experiment known as the RAND Health Insurance Experiment, which tested the effect of free insurance on use of medical services and on health outcomes, relative to control groups which paid a larger share of health costs out-of-pocket. We focus on using the experiment to test econometric techniques, by comparing experimental estimates of the benefit of free insurance on certain health outcomes to estimates one might obtain using nonexperimental econometric techniques if an analogous ``natural experiment'' were all that was available, following Lalonde (1986). Like Lalonde, our results cast doubt on the use of these nonexperimental methods. We then present a closer inspection of both the setup of the experiment and some findings of previous researchers. This work yields a slight caveat in the form of one piece of evidence that the result we have used in the test may not be robust. We also consider other health outcomes, and identify one beneficial impact of free health insurance that does not appear to have been noticed previously by RAND researchers, as well as finding several apparently unnoticed beneficial effects that vary by income category. Like the RAND researchers, we do not find very many beneficial impacts of the free plans, but when interpreting the experimental results, one should bear in mind the low maximums on total out-of-pocket expenditures under the control group insurance plans.",ucb,,https://escholarship.org/uc/item/0210k3c9,,,eng,REGULAR,0,0
833,2269,Interface Engineering of Garnet Solid Electrolytes,"Cheng, Lei","De Jonghe, Lutgard C;Doeff, Marca M;",2015,"Solid lithium ion conductors represent a promising class of materials for next generation high energy density batteries, with the potential for enabling use of high capacity Li metal anodes and providing opportunities for novel lithium-free cathode materials. However, highly resistive interfaces stymie their practical use. This urgent scientific challenge requires mechanistic understanding of ion transport at interfaces, as well as development of novel processes to achieve low interfacial resistances. The goal of this PhD dissertation was to generate fundamental understandings of garnet-structured Al substituted Li7La3Zr2O12 (LLZO) electrolyte surfaces and interfaces with lithium metal electrodes. Specifically in this research, the topmost surface microstructure, local chemical environment, and surface chemistry were carefully studied.  The ceramic processing of garnet is discussed and ways to control the sintering behavior and microstructures were explored and successfully demonstrated. Factors contributing to high interfacial resistance were systematically studied. The source of the high interfacial impedance has been traced to the presence of Li2CO3 on pellet surfaces resulting from air exposure after processing. In addition, it was discovered that surface grain boundaries are surprisingly fast ion transport pathways and surface microstructure is critically important to lithium ion transport at interfaces. Complex homo- and heterostructured LLZO solid electrolytes with controllable surface and bulk microstructures were successfully fabricated, which allowed the comparison and separation of the contribution from the surface and the bulk. Engineered pellet surfaces allowed us to achieve the lowest interfacial resistance ever reported for this composition, resulting in significantly improved cycling behavior. Lastly, it was found that LLZO surfaces can be effectively stabilized under air exposure conditions, preventing Li2CO3 formation and maintaining low interfacial resistances. This opens new opportunities for garnet solid electrolyte in practical applications.",ucb,,https://escholarship.org/uc/item/0210t6jj,,,eng,REGULAR,0,0
834,2270,Partially Coherent Quantitative Phase Retrieval with Applications to Extreme Ultraviolet Lithography,"Claus, Rene Andre","Waller, Laura;",2015,"This dissertation presents a new quantitative phase retrieval algorithm that fully models partially coherent imaging in microscopes. Unlike existing algorithms, our algorithm fully considers the pupil function and illumination by using the Weak Object Transfer Function (WOTF). Using an iterative approach, we extend the applicability of the WOTF beyond weakly scattering objects. This allows almost any measurement to be used during phase retrieval. As an example of how this feature can be used to invent practical new measurement schemes, we present the illumination switched pupil. This measurement uses a phase contrast objective and varied illumination to maximize the sensitivity of the microscope to both the phase and amplitude of the sample. Using only two images, the complex field can be recovered with high sensitivity at almost all spatial frequencies.A complete model of imaging in the microscope enables self-calibration of the measurements and improved phase retrieval. Since all important characteristics of the microscope can be incorporated, an optimization over critical parameters, such as the best focus position and image alignment, can be performed after the images have been captured. This allows errors in the calibration to be corrected after the measurements have been performed, improving the accuracy of the recovered field while simplifying the experiments.To verify and apply the algorithm experimentally, we have performed phase retrieval measurements of Extreme Ultraviolet (EUV) photomasks on the zone plate microscope, SHARP, at Lawrence Berkeley National Laboratory (LBNL). Phase retrieval has enabled the quantitative analysis of multilayer roughness and defects. Experiments, comparing the size of defects measured using phase retrieval to measurements performed by AFM, indicate that AFM consistently underestimates the effective height of the buried multilayer defects by 1 nm. Other measurements of defects, comparing the recovered field extracted from standard and from phase contrast images as well as measurements taken under varying illumination, showed consistent results and provide experimental evidence that the algorithm handles the pupil function and partial coherence correctly.",ucb,,https://escholarship.org/uc/item/0227g8sw,,,eng,REGULAR,0,0
835,2271,Negative Capacitance for Ultra-low Power Computing,"Khan, Asif Islam","Salahuddin, Sayeef;",2015,"Owing to the fundamental physics of the Boltzmann distribution, the ever-increasing power dissipation in nanoscale transistors threatens an end to the almost-four-decade-old cadence  of continued performance improvement in  complementary metal-oxide-semiconductor (CMOS) technology. It is now agreed that the introduction of new physics into the operation of field-effect transistors--in other words, ``reinventing the transistor''-- is required to avert such a bottleneck.  In this dissertation, we present the experimental demonstration of a novel physical phenomenon, called the negative capacitance effect in ferroelectric oxides, which could dramatically reduce  power dissipation in nanoscale transistors. It was theoretically proposed in 2008 that by introducing a ferroelectric negative capacitance material into the gate oxide of a metal-oxide-semiconductor field-effect transistor (MOSFET), the subthreshold slope could be  reduced below the fundamental Boltzmann limit of 60 mV/dec, which, in turn, could arbitrarily lower the power supply  voltage and the power dissipation. The research presented in this dissertation establishes the theoretical concept of  ferroelectric negative capacitance as an experimentally verified fact. \The main results presented in this dissertation are threefold. To start, we present the first direct measurement of  negative capacitance in  isolated, single crystalline, epitaxially grown thin film capacitors of ferroelectric Pb(Zr$_{0.2}$Ti$_{0.8}$)O$_3$. By constructing a simple resistor-ferroelectric capacitor series circuit, we show that, during ferroelectric switching, the ferroelectric voltage decreases, while  the stored charge in it increases, which directly shows a negative slope in the charge-voltage characteristics of a ferroelectric capacitor. Such a situation is completely opposite to what would be observed in a regular resistor-positive capacitor series circuit. This measurement could serve as a canonical test for negative capacitance in any novel material system.  Secondly, in epitaxially grown ferroelectric Pb(Zr$_{0.2}$Ti$_{0.8}$)O$_3$-dielectric SrTiO$_3$ heterostructure capacitors, we show that negative capacitance effect from the ferroelectric Pb(Zr$_{0.2}$Ti$_{0.8}$)O$_3$ layer could result in an enhancement of the  capacitance of bilayer heterostructure over that of the constituent dielectric SrTiO$_3$ layer. This observation apparently violates the fundamental law of circuit theory which states that the equivalent capacitance of two  capacitors connected in series is smaller than that of each of the constituent capacitors. Finally, we present a design framework for negative capacitance field-effect-transistors and project  performance  for such devices.",ucb,,https://escholarship.org/uc/item/0283855m,,,eng,REGULAR,0,0
836,2272,Microscale devices for quantitative characterizations of human biology,"Jeeawoody, Shaheen","Herr, Amy E;",2019,"To understand the function and dysfunction of cells in biological organisms, it is important to characterize one of the key functional actors of the cell, proteins. With indications of poor correlation between mRNA expression and proteomic expression at the single cell level, in vitro assays directly quantifying protein expression, in both the spatial and temporal context, are needed. To span the extensive cellular heterogeneity in gene and protein expression and activity observed in cells and tissues, proteomic assays should interrogate single- and low-cell resolution with sufficiently high throughput to identify cellular sub-populations. These proteomic assays would also require sufficiently high selectivity and analytical sensitivity with which to interrogate protein isoforms and post-translational modifications. To address this measurement gap, we introduce and further develop proteomic assays towards these specifications.We enhanced the analytical sensitivity of the ultrathin isoelectric focusing assay (IEF) with subsequent immunoblot, by developing a highly-porous hydrogel matrix as a new substrate for the assay. We characterized the effect of this 10-fold increase in gel porosity on the IEF separation performance, paired with a reagent modification that directly impacts separation performance. Additionally, we assessed the benefits of the increased porosity on the in-gel immunoblot. Furthermore, we investigated the compartmentalization of protein lysate from single cells within the microwells embedded in the hydrogel substrate in our proteomic assays. We characterized the height of the fluid film between multi-material interfaces. We used numerical modeling and experimental validation to assess the contribution of the fluid film to the diffusive losses that reduce analytical sensitivity in our assays.We then re-imagined the ultrathin IEF assay for a 100-fold increase in throughput by developing 3D projection electrophoresis. We interrogated the IEF separation performance of this proof-of-concept high-throughput IEF assay with several optimizations. We conclude this section of this dissertation with an in-depth discussion of the potential further developments for this platform, towards a high-sensitivity, high-throughput proteomic assay with multiplexing capabilities.In a parallel line of inquiry in this dissertation, to further understand and characterize cellular functions at a larger scale, in vitro biological models mimicking human physiology are needed. Due to inter-species differences in ion channels, biological pathways, and pharmacokinetic properties, animal models do not faithfully predict human cardiotoxicity. Human in vitro tissue models, with similar three-dimensional microenvironments to those found in in vitro human organs, that are predictive of human drug responses would be a significant advancement for understanding, studying, and developing new drugs and strategies for treating diseases. To assess the measurement needs in this space, we surveyed the breadth of in vitro cardiac devices mimicking human cardiac physiology.The lipid storage and processing within adipose tissue strongly affects drug concentrations in vivo, and adipose tissue interacts with other organs via paracrine signals and fatty acid release, affecting the safety profiles of a large number of drug-like molecules. To address this measurement gap, we developed a microfluidic device with adipose tissue. We used numerical modeling and an analytical model to characterize the convective and diffusive transport within the device. We confirmed the maintenance of adipose cell viability and growth, extracellular matrix deposition, and adipose tissue functionality over two weeks.We anticipate that the developments of analytical proteomic assays and in vitro biological models discussed in this dissertation will support quantitative characterizations of human biology, leading towards future development of targeted clinical therapies for improved length and quality of life.",ucb,,https://escholarship.org/uc/item/04c203pg,,,eng,REGULAR,0,0
837,2273,Algorithms for Human Genetics,"KIRKPATRICK, BONNIE","Karp, Richard M;",2011,"Whereas Mendel used breeding experiments and painstakingly counted peas, modern biology increasingly requires computational tools. In the late 1800's probability and experimental genetics were the critical tools for discovering the gene. Today, the combined use of statistical and computational methods to make genetic and genomic discoveries has increased after the discovery of the DNA double-helix and the development of sequencing methods.  By examining relationships among individuals using computational tools, geneticists have been able to understand the biological mechanisms that produce genetic diversity, map ancestral movements of populations, reconstruct ancestral genomes, and identify relatives.  Furthermore, models in genetics have inspired advances in computer science, notably the model for inheritance in families is an early example of a graphical model and helped inspire the sum-product algorithm.The genetic data of interest is single-nucleotide polymorphism (SNP) data, which are positions in the genome known to have nucleotide variation across the population.  Humans are diploid individuals having two copies of each chromosome.  Data for an individual can come in two forms, either haplotypes or genotypes.  The haplotypes are two strings, each giving the sequence of nucleotides that appear together on the same chromosome.  The genotypes, for each position in the genome, give an unordered set of nucleotides that appear.  In particular the genotype is said to be `unphased' due to the lack of information about which nucleotide appears on which chromosome.In human genetics there are two main ways to model relatedness: evolutionary relationships between people and closer, family relationships.  Evolutionary relationships, from the domain of population genetics, occur through a distant relative and leave small traces of the relationship in the genome.  Family relationships are typically much closer and leave much larger traces in the genome. This thesis examines algorithms for both types of relationships.For evolutionarily related individuals, this thesis presents the perfect phylogeny and coalescent and then examines two related questions.  The first is related to privacy of genetic data used for research purposes.  In order to share data from studies while hopefully maintaining the privacy of study participants, geneticists have released the summary statistics of the data.  A natural question, whether individuals can be detected in the summary data, is answered in the affirmative by using a perfect phylogeny model.  The second question is how to construct perfect phylogenies from haplotypes where there is missing data.  We introduce a polynomial-time algorithm for enumerating such phylogenies.  This algorithm can be used to compute the probability of the data as an expectation over possible coalescent genealogies.Recent relationships are modeled using a family tree, or pedigree graph.  Traditionally, geneticists construct these graphs from genealogical records in a very tedious process of examining birth, death, and marriage records.  Invariably mistakes are made due to poor record keeping or incorrect paternity information.  As an alternative to manual methods, this thesis addresses the problem of automatically constructing pedigree graphs from genetic data.The most obvious way to reconstruct pedigrees from genetic data is to use a structured machine learning approach, similar to phylogenetic reconstruction.  That method would involve a search over the space of pedigree graphs where the objective is to find the pedigree graph with the highest likelihood of generating the observed data. Unfortunately, this is not a good way to proceed for two reasons: the space of pedigree graphs is exponential, and the likelihood calculation has exponential running time.  The likelihood calculation given genotype data is known to be NP-hard.  In an attempt to make use of the likelihood in complex pedigrees, the method PhyloPed uses a Gibbs sampler to infer haplotypes from genotype data.  In a second attempt to use likelihood methods, this time for haplotype data, an NP-hardness result is presented.  A third attempt to find an efficient algorithm for the likelihood problem results in a state-space reduction method for the pedigree hidden Markov model.Since likelihood-based approaches seem completely infeasible, a completely different approach is introduced.  We focus on the problem of inferring relationships between a set of living individuals with available identity-by-descent data.  For convenience, we assume that the inferred pedigree is monogamous without inter-generational mating. Two heuristic and practical pedigree reconstruction methods are introduced, one for inbred pedigrees and the other for outbred pedigrees.  This work immediately reveals another important problem, that of evaluating the resulting inferred pedigree against a ground-truth pedigree.  This can be done either by determining whether the two pedigrees are isomorphic or by finding the edit distance between the two pedigrees.",ucb,,https://escholarship.org/uc/item/04c284r3,,,eng,REGULAR,0,0
838,2274,"The Effects of Neurosteroids, such as Pregnenolone Sulfate and its receptor, TrpM3 in the Retina.","Webster, Corey Michael","Feller, Marla B.;",2019,"Pregnenolone sulfate (PregS) is the precursor to all steroid hormones and is produced in neurons in an activity dependent manner. Studies have shown that PregS production is upregulated during certain critical periods of development, such as in the first year of life in humans, during adolescence, and during pregnancy. Conversely, PregS is decreased during aging, as well as in several neurodevelopmental and neurodegenerative conditions. There are several known targets of PregS, such as a positive allosteric modulator NMDA receptors, sigma1 receptor, and as a negative allosteric modulator of GABA-A receptors. Recently a transient receptor potential channel, TrpM3 has been shown to be activated by PregS. TrpM3 is a heat sensitive (between 33-40oC), non-selective cation channel that is outwardly rectifying. PregS has been shown to increase the frequency of post-synaptic currents in the hippocampus and developing cerebellum, induce calcium transients in a subset of retinal ganglion cells, and enhance memory formation in rodents. Furthermore, PregS mediated TrpM3 activation induces calcium dependent transcription of early immediate genes, suggesting that activation of this channel may produce lasting effects on cells and systems in which it is activated. Because PregS is abundant during critical periods of development, we hypothesized that it may play a significant role during development. Furthermore, the role of PregS or its receptor TrpM3, has not previously been well characterized in the retina. To address this question, in this dissertation, we examine the role of the neurosteroid PregS and its receptor, TrpM3, on retinal waves, which are characteristic of specific stages of synaptic development and connectivity. Briefly, we show that PregS induces a TrpM3 dependent prolonged calcium transient, which is absent in the TrpM3-/- animals and increases the correlation of cell participation in waves. We also show that TrpM3 increases the frequency of post-synaptic currents, indicating a mechanism of action presynaptic to retinal ganglion cells, but that TrpM3 is expressed primarily in RGCs and MÃ¼ller glia. Taken together, our results indicate that both PregS and TrpM3 are important in modulating spontaneous synaptic activity during development.",ucb,,https://escholarship.org/uc/item/04d8607f,,,eng,REGULAR,0,0
839,2275,Molecular Epidemiology of Human Immunodeficiency Virus Type 1 (HIV-1) in Southern Africa and Northern California,"Dalai, Sudeb C.","Reingold, Arthur;",2017,"Human immunodeficiency virus type 1 (HIV-1) exhibits extraordinary genetic diversity that is driven by high rates of mutation and recombination, coupled with elevated rates of viral turnover and the persistent nature of infection. Through these mechanisms, HIV-1 group M, the group largely responsible for the global pandemic, diversified into nine distinct subtypes and additional circulating recombinant forms. Subtype C HIV-1, which accounts for approximately 50% of the estimated 37 million individuals living with HIV/AIDS worldwide, predominates the epidemics in southern Africa and South / Southeast Asia. In contrast, subtype B HIV-1, predominant in the United States and Europe, comprises 12% of the global HIV-1 prevalence.Current US and, increasingly, international guidelines recommend that HIV genotypic sequencing be performed for newly-identified HIV-positive patients, in order to identify genetic mutations that may confer drug resistance and pose a barrier to effective antiretroviral therapy (ART). This practice has resulted in the generation of enormous amounts of genotypic data often linked with clinical, demographic, and geospatial information. Analyses of these large datasets, utilizing sophisticated statistical and computational methods, has enabled identification of important and previously unrecognized trends in epidemiologic and vertical transmission, persistence of HIV within and among distinct risk groups, and the accumulation and propagation of ART resistance. Results of these analyses, in turn, advance treatment and care for persons living with HIV.This thesis takes advantage of large, de-identified and anonymized datasets of HIV-1 genotypic and demographic information available through clinical testing and treatment programs in two distinct and epidemiologically important regions in the global HIV pandemic, northern California and southern Africa. Employing innovative and computationally intensive tools which combine experience from molecular biology, evolutionary biology and biostatistics, in-depth analyses were undertaken to explore and define risk factors, underlying epidemiologic features and clinical characteristics relevant to transmission and ART resistance. The final chapter provides a comprehensive literature review of the relevance of molecular epidemiologic principles and innovative biology to the complex and continued phenomenon of mother-to-child transmission of HIV, as a case study exemplifying the need for continued molecular studies grounded in solid epidemiologic principles. The findings presented here are ultimately intended to help guide domestic and global efforts to scale up ART treatment while considering well-known and lesser-known factors that may influence eventual clinical and epidemiologic outcomes.",ucb,,https://escholarship.org/uc/item/04f200gq,,,eng,REGULAR,0,0
840,2276,Performance Characterization of Beams with High-Strength Reinforcement,"To, Duy Vu","Moehle, Jack P;",2018,"A laboratory test and analytical research program was undertaken to characterize the performance of reinforced concrete beams with high-strength reinforcement subjected to reversed cyclic lateral loading simulating earthquake effects. The beams are representative of beams used in special moment frames. Four beams were tested in the laboratory test investigation, one with A706 Grade 60 reinforcement, one with Grade 100 reinforcement having tensile-to-yield strength ratio (T/Y) of 1.17, one with Grade 100 reinforcement with T/Y = 1.26, and one with A1035 Grade 100 reinforcement. In each beam, the noted reinforcement grade was used for both longitudinal and transverse reinforcement, except for beam with Grade 100 T/Y = 1.17 that had transverse reinforcement of Grade 100 with T/Y = 1.26. Overall, all beams achieved rotation capacity of at least 0.045 radians. The beams with A706 Grade 60 and Grade 100 (T/Y = 1.26) reinforcement failed by buckling of longitudinal bars over several hoop spacings. The other two beams with Grade 100 reinforcement failed by fracture of longitudinal bars at the maximum moment section. Strain gauges installed on longitudinal bars indicated that beams with higher T/Y achieved greater spread of plasticity compared to beams with lower T/Y. In the analytical study, the seismic performance of tall reinforced concrete special moment resisting frames with high-strength reinforcement was investigated through nonlinear dynamic analyses. Four 20-story reinforced concrete moment frames, three reinforced with Grade 100 steel and one with Grade 60 steel were designed in accordance with ASCE 7-16 and ACI 318-14 at a hypothetical site in San Francisco, California. All four frames had the same dimensions and concrete properties, resulting in identical design drifts. Frames with Grade 100 reinforcement were designed to have reduced amount of longitudinal reinforcement to provide equivalent nominal strength as was provided in the Grade 60 reinforcement model. Tests had demonstrated that frames with higher-grade reinforcement had greater strain penetration into beam-column joints, resulting in greater slip of reinforcement from connections. This effect combined with reduced reinforcement ratios caused the frames with Grade 100 reinforcement to be more flexible than the frame with Grade 60 reinforcement. In addition, some currently available types of Grade 100 reinforcement have lower tensile-to-yield strength ratio and lower uniform elongation compared with Grade 60 reinforcement. The reduced T/Y results in reduced strain-hardening, increased strain localization, and increased P-Delta effects. The effects of these local behaviors on overall frame performance are studied through the nonlinear dynamic analyses. The various types of reinforcement were found to result in minor differences in overall frame seismic performance.",ucb,,https://escholarship.org/uc/item/04f3s8zw,,,eng,REGULAR,0,0
841,2277,"The Politics of Polarization: Legitimacy Crises, Left Political Mobilization, and Party System Divergence in South America","Handlin, Samuel Paltiel","Collier, Ruth;Collier, David;",2011,"The rise of the left across much of South America in the aftermath of market reforms catalyzed a major divergence in regional party systems.  In some countries, polarizing party systems emerged, marked by conflictual patterns of contestation between major political parties and the politicization of class cleavages in party competition.  In other countries, integrative party systems consolidated, characterized by largely consensual patterns of competition and class cleavages that remain unexpressed in party competition.  This variation in party systems represents a critical macropolitical legacy to emerge from the tumultuous recent decades and offers fruitful ground for developing and testing theory regarding the causes of political and social polarization in the younger democracies of the highly unequal developing world.Examining the cases of Venezuela, Brazil, and Chile, this dissertation develops an argument that centers on the avoidance or occurrence of ""legitimacy crises"": anti-systemic episodes involving protracted failures of governance and steep erosions of public confidence in state institutions.  The presence or absence of legitimacy crisis decisively shaped factional contestation within the partisan left in each country, leading to party system divergence along two dimensions.  Whether radical or moderate left coalitions consolidated entailed the establishment of conflictual or consensual patterns of contestation within party systems.  Once in office, the radical and moderate left also pursued different strategies of political mobilization - mass-organizational in Venezuela and catchall in Brazil and Chile - that subsequently drove variation in the translation of class cleavages into party competition.The study relies on a variety of qualitative and quantitative data sources, including those gathered during 11 months of fieldwork in Venezuela, and utilizes both process-tracing and statistical methods (primarily genetic matching) to draw causal inferences.  A concluding chapter shows that the argument can also explain variation in party systems in two other countries where the left has taken power (Bolivia and Uruguay), suggesting that the framework in the study might provide a broader model of macropolitical divergence in the region during the last decades.",ucb,,https://escholarship.org/uc/item/04f5b060,,,eng,REGULAR,0,0
842,2278,Causes and Impacts of Rainfall Variability In Central Mexico on Multiple Timescales,"Bhattacharya, Tripti","Byrne, Anthony R.;",2016,"The eastern sector of Mexicoâ€™s Trans-Mexican Volcanic Belt is a semi-arid region, where interannual rainfall variability is significantly stresses regional water resources and the livelihood of millions. The region is linked to a broader summertime rainfall regime known as the North American Monsoon (NAM). My dissertation uses multiple lines of evidence, from geochemistry to climate model output, to understand the causes of long-term droughts in this region.My first dissertation chapter uses instrumental data to diagnose the causes of El NiÃ±o-induced droughts in Mexico. This work explores the mechanisms responsible for rainfall changes over Central Mexico during the developing versus decaying phase of an El NiÃ±o event. This study was the first to demonstrate the importance of moisture transport anomalies in reducing rainfall in highland Mexico during the decay phase of an El NiÃ±o.  My second dissertation chapter uses the oxygen stable isotope ratios (Î´18O) of lacustrine carbonates as well as elemental geochemistry to reconstruct late Holocene drought in Central Mexico. This sub-centennially resolved record is the first to identify a significant dry interval in central Mexico from 1300-1100 cal yr. B.P., which may be temporally coherent with increased drought frequencies recorded on the Yucatan Peninsula. These results also hint at a role for climate change in regional prehistoric cultural changes at the nearby site of Cantona. 	My third dissertation chapter explores impacts of late Holocene droughts on the terrestrial ecosystems in Central Mexico. I reconstructed past vegetation and fire dynamics from pollen and microscopic charcoal, and compared these data to our stable-isotope based climate reconstruction and regional archaeological records. My fourth chapter is an exploration of the causes of centennial-scale across Mexico and Central America in the late Holocene. This work identifies the spatiotemporal patterns of late Holocene drought by synthesizing Mesoamerican proxy records. It presents a new hypothesis pointing to the role of changes in Atlantic circulation in causing droughts in Mesoamerica. Finally, I synthesize the knowledge in each of the dissertation chapters and point to avenues of future research.",ucb,,https://escholarship.org/uc/item/04j0w63x,,,eng,REGULAR,0,0
843,2279,"Tunneling in low-power device-design: A bottom-up view of issues, challenges, and opportunities","Ganapathi, Kartik","Salahuddin, Sayeef;",2013,"Simulation of electronic transport in nanoscale devices plays a pivotal role in shedding light on underlying physics, and in guiding device-design and optimization. The length scale of the problem and the physical mechanism of device operation guide the choice of formalism. In the sub-20 nanometer regime, semi-classical approaches start breaking down, thus necessitating a quantum-mechanical treatment of the electronic transport problem. Non-equilibrium Green's function (NEGF) is a theoretical framework for investigating quantum-mechanical systems - interacting with surroundings through exchange of quasiparticles - far from equilibrium. Although hugely computation-intensive with a realistic device-representation, it provides a rigorous way to include particle-particle interactions and to model phenomena that are inherently quantum-mechanical.We build the Berkeley Quantum Transport Simulator (BQTS) - a massively parallel, generic, NEGF-based numerical simulator - to explore low-power device-design opportunities. Demonstrating scalability and benchmarking results with experimental tunnel diode data, we set out to understand tunneling in devices and to leverage it for both digital and analog applications.    Investigating InAs short-channel band-to-band tunneling transistors (TFETs), we show that direct source-to-drain tunneling sets the leakage-floor in such devices, thereby limiting the minimum subthreshold swing (SS) in spite of excellent electrostatics. A heterojunction TFET with a halo doping in the source-channel overlap region is proposed and is shown to achieve steep SS as well as large ON current. We discover that by band-offset engineering, the steepness therein could be controlled primarily by the modulation of heterojunction-barrier. Subsequently, exploring layered materials for analog applications, we demonstrate that doping the drain underlap region in graphene FETs prolongs the onset of tunneling in their output characteristics, and hence significantly increases their output resistance (r0) and intrinsic gain (gmr0). Due to large bandgap, and consequently, large r0, monolayer-MoS2 FETs exhibit a significant enhancement in maximum oscillation frequency (fmax) over their graphene counterparts.",ucb,,https://escholarship.org/uc/item/04n5p798,,,eng,REGULAR,0,0
844,2280,Material and Optical Design Rules for High Performance Luminescent Solar Concentrators,"Bronstein, Noah","Alivisatos, A. Paul;",2015,"This dissertation will highlight a path to achieve high photovoltaic conversion efficiency in luminescent solar concentrators, devices which absorb sunlight with a luminescent dye and then re-emit it into a waveguide where it is ultimately collected by a photovoltaic cell. Luminescent concentrators have been studied for more than three decades as potential low-cost but not high efficiency photovoltaics. Astute application of the blackbody radiation law indicates that photonic design is necessary to achieve high efficiency: a reflective filter must be used to trap luminescence at all angles while allowing higher energy photons to pass through. In addition, recent advances in the synthesis of colloidal nanomaterials have created the possibility for lumophores with broad absorption spectra, narrow-bandwidth emission, high luminescence quantum yield, tunable Stokes shifts and tunable Stokes ratios. Together, these factors allow luminescent solar concentrators to achieve the optical characteristics necessary for high efficiency.We have fabricated and tested the first generation of these devices. Our experiments demonstrate that the application of carefully matched photonic mirrors and luminescent quantum dots can allow luminescent concentration factors to reach record values while maintaining high photon collection efficiency. Finally, the photonic mirror dramatically mitigates the negative impact of scattering in the waveguide, allowing efficient photon collection over distances much longer than the scattering length of the waveguide.After demonstrating the possibility for high performance, we theoretically explore the efficacy of luminescent concentrators with dielectric reflectors as the high-bandgap top-junctions in two-junction devices. Simple thermodynamic calculations indicate that this approach can be nearly as good as a traditional vertically stacked tandem. The major barriers to such a device are the optical design of narrow-bandwidth, angle-insensitive reflectors with near-unity reflectivity in the reflection band and near unity transmissivity in the pass-band. Additionally, lumophores with narrow emission line widths and carefully controlled Stokes shifts are required. If new lumophores and optical designs can be created that meet the demanding needs of this application, high performance two-junction photovoltaics that collect both the direct and diffuse light could be achieved.",ucb,,https://escholarship.org/uc/item/04p1p3zj,,,eng,REGULAR,0,0
845,2281,"Moving On Up? U.S. Military Service, Education and Labor Market Mobility among Children of Immigrants","Barry, Catherine N.","Bloemraad, Irene;",2013,"This dissertation examines U.S. military service enlistment intentions and post-military education and labor market outcomes among young adult children of immigrants in the United States in the early 21st century. I assert that the military is perceived as an alternative pathway to incorporation among disadvantaged children of immigrants. Rather than relying on ethnic social ties and networks to get ahead as segmented assimilation asserts is necessary for success among disadvantaged children of immigrants, they actively avoid downward mobility by choosing military service as a pathway to mobility. I argue that disadvantaged children of immigrants who aspire to upward mobility are pushed toward military service by structural disadvantages but only those who perceive the military as best option to get ahead make the decision to enlist.An empirical investigation of post-military educational and labor market outcomes reveal that military service does not uniformly benefit veteran children of immigrants as they may have hoped. Analyses of nationally representative data, the National Longitudinal Survey of Adolescent Youth (ADD Health) and the Current Population Surveys (CPS) reveal that veteran children of immigrants are less likely to acquire bachelor's degrees and are less likely to be enrolled in post-secondary school than non-veterans by the ages of 24 to 32 but unemployment and earnings are similar between veteran and non-veteran children of immigrants. However, these findings hide the variation underlying these outcomes that are revealed by in-depth interviews with thirty-three veteran children of immigrants. While some individuals seamlessly transition from military service to stable, middle-income occupations in the civilian labor market, others face unemployment, underemployment and obstacles to post-secondary degrees because of non-transferable military job skills and the challenges of transitioning from military to civilian life. Members of a third group were pursuing four-year degrees at the time of their interviews and post-college trajectories have yet to be determined.",ucb,,https://escholarship.org/uc/item/04s1n7sq,,,eng,REGULAR,0,0
846,2282,Modeling and optimization of transients in water distribution networks with intermittent supply,"Lieb, Anna Marie","Wilkening, Jon;Rycroft, Chris;",2016,"Much of the world's rapidly growing urban population relies upon water distribution systems to provide treated water through networks of pipes. Rather than continuously supplying water to users, many of these distribution systems operate intermittently, with parts of the network frequently losing pressure or emptying altogether. Such intermittent water supply deleteriously impacts water availability, infrastructure, and water quality for hundreds of millions of people around the world. In this work I introduce the problem of intermittent water supply through the lens of applied mathematics. I first introduce a simple descriptive mathematical model that captures some hydraulic features of intermittency not accounted for by existing water distribution system software packages. I then consider the potential uses of such a model in a variety of optimization examples motivated by real-world applications. In simple test networks, I show how to reduce pressure gradients while the network fills by changing either the inflow patterns or the elevation profile. I also show test examples of using measured data to potentially recover unknown information such as initial conditions or boundary outflows. I then use sensitivity analysis to better understand how various parameters control model output, with an eye to figuring out which parameters are worth measuring most carefully in field applications, and also which parameters may be useful in an optimization setting. I lastly demonstrate some progress in descriptively modeling a real network, both through the introduced mathematical model and through a fluid-mechanics-based method for identifying data where the model is most useful.",ucb,,https://escholarship.org/uc/item/04s697wk,,,eng,REGULAR,0,0
847,2283,Mobility in Wireless Sensor Networks,"Mehta, Ankur Mukesh","Pister, Kristofer S. J.;",2012,"The combination of mobility with wireless networks greatly expands the application space of both robots and distributed sensor networks; such a pervasive system can enable seamless integration between the digital and physical worlds.  However, there are a number of issues in both robotic and wireless sensor network (WSN) fields that demand research, and their integration generates further challenges.  A fundamental open problem in robotic systems is the issue of self-contained localization.  Especially difficult when considering small scale flying robots, the ability to determine one's position using only on-board sensing is necessary for autonomous robots.  GINA, a small wireless inertial measurement unit weighing only 1.6~g was designed to calculate the 6 degree of freedom position of a rigid body.  Together with necessary software and hardware, the resulting WARPWING platform served as a highly capable and versatile flight controller for micro air vehicles (MAVs).  As an open source hardware project, WARPWING further enabled other unrelated research projects by abstracting away the electronic system design.As designed, the WARPWING platform was used to control small flying robots.  Rocket systems can be used to deliver microelectronic sensor nodes into low earth orbit (LEO) as tiny satellites; analysis of the mechanical parameters demonstrates the feasibility of using a small scale multistage solid fuel guided chemical rocket to deliver a small payload into an orbital trajectory given a suitable controller.  Helicopters, similar to rockets, employ attitude control to effect stability and guidance, and so share similar control requirements.  Off the shelf toy helicopters can be used as a mechanical airframe; replacing the control electronics with the GINA board enables the design of autonomous MAVs.  Purely inertial operation of the GINA board provided stability control, but accumulated drift inhibited guidance control.  To calculate position, the state estimator was augmented with additional vision-based sensors such as the VICON motion capture system or an on-board smart camera aimed at an infrared beacon.The GINA board, containing a wireless enabled processor, was also a platform for WSN research.  The key design parameter in WSN systems is power consumption; minimizing energy requirements extends node and system lifetime or lowers required battery mass.  A time synchronized, channel hopping (TSCH) medium access control (MAC) protocol, standardized as the IEEE 802.15.4e specification, combines time division multiple access with frequency diversity to ensure reliable, robust low power communication across environmental conditions.  This TSCH protocol can be augmented with a variable data rate coding scheme at the physical (PHY) layer to further improve power saving and scalability.  The environmental conditions that enable higher data rates also allow wireless communication with imprecise frequency references.  A modified PHY layer with frequency offset compensation can be used to implement crystal-free radios with on-board LC oscillators.Enabling multi-hop networking to mobile MAVs required combining the previous two research thrusts.  A helicopter augmented with a payload bay could deploy GINA nodes as wireless repeaters along a flight path, and communicate along them to a base station acting as its controller.  The base station can be further connected to the internet; a mobile phone application was used to interface to a remote helicopter over a hybrid multi-hop path, passing downstream control commands and receiving upstream video images.  To maintain the performance and reliability benefits of TSCH mesh networks in the presence of such MAV elements, the protocols designed for stationary networks were redesigned with extensions optimized for mobile nodes. This work on an integrated system as well as the separate subsystems paves the path towards networked robots.  Future work can focus on system-level solutions to fully implement the vision of smart pervasive mobile swarms.",ucb,,https://escholarship.org/uc/item/04t4p3c4,,,eng,REGULAR,0,0
848,2284,Probing the mechanism of 3D-domain swapping,"Miller, Katherine Helen","Marqusee, Susan;",2011,"Domain swapping, the process in which a structural unit is exchanged between monomers to create a dimer containing two versions of the monomeric fold, is believed to be an important mechanism for oligomerization and the formation of amyloid fibrils. Very little is known, however, about the structural determinants and mechanisms of domain swapping. The goal of the work in this thesis is to elucidate these features in the C-terminal domain swapping of bovine pancreatic ribonuclease A (RNase A).RNase A is a well studied protein that domain swaps under extreme conditions, such as lyophilization from acetic acid. The major domain-swapped dimer form of RNase A exchanges a Î²-strand at its C-terminus to form a C-terminal domain-swapped dimer. The hinge region between the exchanged beta-strand and the rest of the protein contains an amide bond between N113 and P114 that adopts a cis conformation in the monomer, and a trans conformation in the domain-swapped dimer. In this thesis, we show that this proline acts as a conformational gatekeeper to control domain swapping. Substitution of P114 with an amino acid that favors the trans conformation, such as glycine or alanine, results in significant population of the domain-swapped dimer under physiological conditions. This variant then allowed us to investigate the mechanism of domain swapping under experimentally accessible conditions. NMR and hydrogen-deuterium exchange demonstrated that compared to variants that do not readily domain swap, P114G shows decreased hydrogen-exchange protection near the C-terminal hinge region, indicating increased local protein flexibility. These results suggest that local conformational fluctuations play a role in the mechanism of C-terminal domain swapping. To further investigate this potential pathway for C-terminal domain swapping, we used a fragment approach - studying fragments of the two regions, or `domains', in isolation and in combination. A fragment of RNase A containing residues 1-115 represents the large region of the protein that remains after the swapped arm has exchanged. In isolation, this fragment lacks significant secondary and tertiary structure, suggesting that significant unfolding must take place during the swapping event. Upon noncovalent association with a peptide containing residues 112-124, native structure and function can be restored.   These data are consistent with a mechanism in which the protein adopts a partially unfolded structure with the domain -swapped arm undocked and exposed.",ucb,,https://escholarship.org/uc/item/02b290rq,,,eng,REGULAR,0,0
849,2285,Power to the Powerless: Interpersonal Influence through Sympathy Appeals,"Shirako, Aiwa","Kray, Laura J.;",2011,"In this dissertation I examine the elicitation of sympathy as an influence strategy to overcome weak positioning in mixed-motive interactions.  I show that by making appeals to sympathy, low power individuals can mitigate their disadvantage, and can claim more value in mixed-motive situations.  This dissertation makes two contributions to the literature.  First, attempts to elicit emotions in others is as of yet, an underexplored area of the emotions literature.  The field has examined how experiencing emotions affect our judgment and behavior, and how our emotional expressions affect others' judgment and behavior, but with a few notable exceptions (e.g. Fulmer & Barry, 2004; Kilduff, Chiaburu & Menges, 2010; Mayer & Salovey, 1997), researchers have barely scratched the surface on the idea that individuals can elicit and manage the emotional experiences of others.  Thus, this dissertation explores the idea that individuals can elicit sympathy in others for their own instrumental gain.  Sympathy is particularly interesting to examine, because the experience of sympathy can motivate the sympathizer to help the disadvantaged--thus showing potential as a valuable emotion to elicit in others.  Second, I explore the connection between the psychology of power, and the effectiveness of appeals to sympathy.  Thus far the literature on power has focused primarily on individuals in positions of power, and while notable exceptions exist (e.g. Simpson, Markovsky, & Steketee, 2011; Whitson & Galinsky, 2008), I seek to contribute to the literature on power by examining a heretofore unexplored low power influence strategy.  I explore these topics in eight studies which vary in methodology and participant population, and conclude by discussing the theoretical and practical implications of my findings, and by presenting a number of potential avenues for future research.",ucb,,https://escholarship.org/uc/item/02b5h960,,,eng,REGULAR,0,0
850,2286,Expression analysis of the CLE signaling gene family in Arabidopsis thaliana and functional characterization of CLE8 in seed development,"Fiume, Elisa","Fletcher, Jennifer C;",2010,"ABSTRACTExpression analysis of the CLE signaling gene family in Arabidopsis thaliana and functionalcharacterization of CLE8 in seed developmentby Elisa FiumeDoctor of Philosophy in Plant BiologyUniversity of California, BerkeleyProfessor Jennifer C. Fletcher, Chair	In plants, as in all multicellular organisms, cell-to-cell communication is fundamental  for coordinating growth and differentiation. Although non-peptide hormones have been long known to act as signaling molecules in plants, the sequencing of the Arabidopsis genome revealed the presence of many potential ligand-encoding genes and receptors suggesting that plants widely use them in different signaling pathways. In the past decade, signaling mediated by peptides in plants has become an emerging area of research and has been found to be utilized in a broad range of developmental processes. However, very few receptors and ligands have been functionally characterized. The Arabidopsis CLAVATA3/EMBRYO SURROUNDING REGION (ESR)-related (CLE) small polypeptide family is currently the best studied, nonetheless only few members have been assigned a function. To unveil possible developmental processes regulated by these putative signaling molecules, I undertook a systematic expression analysis of its members. These studies showed that all Arabidopsis tissues express one or more CLE gene, suggesting that CLE-mediated signaling regulates a wide range of developmental processes. Furthermore, no two CLE genes showed the same expression pattern, indicating that specificity of function is in part achieved at the regulatory level. On the other hand, many CLE genes have overlapping expression patterns, suggesting that there could be a a high degree of redundancy among the family members.	 The putative functional redundancy of the CLE genes has been used to explain the  lack of cle single mutant phenotypes, however, many other factors could also contribute. For example, CLE genes encode  small proteins and generally lack introns making it difficult to obtain knock-out mutants using T-DNA insertional mutagenesis. Alternatively, the CLE genes might be regulating processes that when impaired, produce phenotypes that are overlooked in conventional screens because they are subtle or conditional to environmental stimuli. In recent years, it has also been discovered that the Arabidopsis genome contains hundreds of essential genes, and because null mutations in these genes cause seed lethality, their functions are largely unknown. A number of signaling molecules has been implicated in regulating seed development, but no putative peptide ligand has been involved thus far. 	In my research, I have isolated and studied a hypomorphic mutation in CLE8, the only member of the CLE family whose expression is limited to the young embryo and the endopserm. My phenotypic characterization of the mutant, together with molecular studies, have led me to discover that CLE8 is necessary for proper seed development, indicating that most likely it is an essential gene. I demonstrate that CLE8 is involved in patterning of the embryo basal domain, in elongation and patterning of suspensor cells, in endosperm maturation and thus ultimately regulates seed size. I found CLE8 to be expressed in the embryo-proper and the endosperm and to have a non-cell-autonomous effect on suspensor cells. I showed that CLE8 positively regulates the expression of the putative transcription factor WOX8 both in the suspensor cells and in the endosperm, and that it likely evolved to specifically fulfill its role during seed development and diverged from other members of the family to solely interact with its receptor/s. 	Furthermore, I have analyzed a partial loss-of-function allele of another essential gene: EMB1611. EMB1611 encodes a large, novel protein with an N-terminal coiled-coil domain and two trans-membrane domains, but whose molecular function still remains elusive. From expression, phenotypic and molecular analysis I discovered that EMB1611 functions to maintain cells in growing tissues in a proliferative or uncommitted state through maintaining  the cellular organization of the shoot apical meristem and directly promoting stem cell fate. EMB1611 null alleles are lethal at early stages of embryogenesis; thus, as for CLE8, the availability of a hypomorphic allele has been essential for studying the role of EMB1611 in plant development.",ucb,,https://escholarship.org/uc/item/02g3670h,,,eng,REGULAR,0,0
851,2287,"Parallelism, Patterns, and Performance in Iterative MRI Reconstruction","Murphy, Mark","Lustig, Michael;Keutzer, Kurt;",2011,"Magnetic Resonance Imaging (MRI) is a non-invasive and highly flexiblemedical imaging modality that does not expose patients ionizing radiation.MR Image acquisitions can be designed by varying a large number ofcontrast-generation parameters, and many clinical diagnosticapplications exist.However, imaging speed is a fundamental limitation to many potentialapplications.Traditionally, MRI data have been collected at Nyquist sampling rates to producealias-free images.However, many recent scan acceleration techniques produce sub-Nyquistsamplings.For example, Parallel Imaging is a well-established acceleration techniquethat receives the MR signal simultaneously from multiple receive channels.Compressed sensing leverages randomized undersampling and the compressibility(e.g. via Wavelet transforms or Total-Variation) of medical imagesto allow more aggressive undersampling.Reconstruction of clinically viable images from these highly acceleratedacquisitions requires powerful, usually iterative algorithms.Non-Cartesian pulse sequences that perform non-equispaced sampling of k-spacefurther increase computational intensity of reconstruction, as theypreclude direct use of the Fast Fourier Transform (FFT).Most iterative algorithms can be understood by considering the MRIreconstruction as an inverse problem, where measurements of un-observableparameters are made via an observation function that models the acquisitionprocess.Traditional direct reconstruction methods attempt to invert thisobservation function, whereas iterative methods require its repeated computationand computation of its adjoint.As a result, na\""ive sequential implementations of iterative reconstructionsproduce unfeasibly long runtimes. Their computational intensity is asubstantial barrier to their adoption in clinical MRI practice.A powerful new family of massively parallel microprocessor architectures hasemerged simultaneously with the development of these new reconstructiontechniques.Due to fundamental limitations in silicon fabrication technology,sequential microprocessors reached the power-dissipation limits ofcommodity cooling systems in the early 2000's.The techniques used by processor architects to extract instruction-levelparallelism from sequential programs face ever-diminishing returns, and furtherperformance improvement of sequential processors via increasing clock-frequencyhas become impractical.However, circuit density and process feature sizes still improve atMoore's Law rates. With every generation of silicon fabrication technology,a larger number of transistors are available to system architects.Consequently, all microprocessor vendors now exclusively produce multi-coreparallel processors.Additionally, the move towards on-chip parallelism has allowed processorarchitects a larger degree of freedom in the design of multi-threaded pipelinesand memory hierarchies.Many of the inefficiencies inherent in superscalar out-of-order design arebeing replaced by the high efficiency afforded by throughput-oriented designs.The move towards on-chip parallelism has resulted in a vast increasein the amount of computational power available in commodity systems.However, this move has also shifted the burden of computational performancetowards software developers.In particular, the highly efficient implementation of MRI reconstructionson these systems requires manual parallelization and optimization.Thus, while ubiquitous parallelism provides a solution to the computationalintensity of iterative MRI reconstructions, it alsoposes a substantial software productivity challenge.In this thesis, we propose that a principled approach to the design andimplementation of reconstruction algorithms can ameliorate this softwareproductivity issue.We draw much inspiration from developments in thefield of computational science, which has faced similar parallelizationand software development challenges for several decades.We propose a Software Architecture for the implementation of reconstructionalgorithms, which composes two Design Patterns that originated inthe domain of massively parallel scientific computing.This architecture allows for the most computationally intense operationsperformed by MRI reconstructions to be implemented as re-usable libraries.Thus the software development effort required to produce highly efficientand heavily optimized implementations of these operations can be amortizedover many different reconstruction systems.Additionally, the architecture prescribes several different strategies formapping reconstruction algorithms onto parallel processors, easing theburden of parallelization.We describe the implementation of a complete reconstruction, $\ell_1$-SPIRiT,according to these strategies.$\ell_1$-SPIRiT is a general reconstruction framework that seamlessly integratesall three of the scan acceleration techniques mentioned above.Our implementation achieves substantial performance improvement over baseline,and has enabled substantial clinical evaluation of its approach to combiningParallel Imaging and Compressive Sensing.Additionally, we include an in-depth description of the performance optimizationof the non-uniform Fast Fourier Transform (nuFFT),  an operation used in allnon-Cartesian reconstructions.This discussion complements well our description of $\ell_1$-SPIRiT, which wehave only implemented for Cartesian samplings.",ucb,,https://escholarship.org/uc/item/02h6z66w,,,eng,REGULAR,0,0
852,2288,Explaining Infinite Series: An Exploration of Students' Images,"Champney, Danielle Dawn","Schoenfeld, Alan H.;",2013,"This study uses self-generated representations (SGR) - images produced in the act of explaining - as a means of uncovering what university calculus students understand about infinite series convergence. It makes use of student teaching episodes, in which students were asked to explain to a peer what that student might have missed had they been absent from class on the day(s) when infinite series were introduced and discussed. These student teaching episodes typically resulted in the spontaneous generation of several SGR, which provided physical referents with which both the student and an interviewer were able to interact. Students' explanations, via their SGR, included many more aspects of what they found important about that content than did the standard research technique of asking students to answer specific mathematics tasks.This study was specifically designed to address how students construct an understanding of infinite series. It also speaks to the broader goal of examining how students use SGR as a tool for explaining concepts, rather than simply as tools for solving specific problems. The main analysis indicates that both students and their professors/textbook, when introducing the topic of infinite series, make use of the following five different image types: plots of terms, plots of partial sums, areas under curves, geometric shapes, and number lines. However, the aspects of the mathematical concepts that the students and the professors/textbooks highlight in their explanations and modes of use for those image types are different, and at times conflicting. In particular, differences emerged along three dimensions of competence - limiting processes (Tall, 1980), language, and connections. While students using SGR generated many of the images that had been used by their professors, the limiting processes that they discussed via those images contrasted sharply. The professors and textbook chapter prioritized the limiting processes represented in particular image types to support mathematically sound conclusions. In contrast, many student explanations focused on limiting processes that did not lead to valid arguments about series convergence. There were also differences in use of language, in that students often assumed much more meaning than was intended in their professors' language choices, leading to problems with their explanations. Finally, while the experts connected their representations in meaningful ways, using other images to clarify or exemplify those that were used to define, students connected their understanding in different ways that were not always supportive of the convergence arguments that they were trying to make. This study expands the literature on students' understanding of infinite series topics, pointing to gaps in student understanding and ways in which students mis-applied what teachers had presented. In doing so, it suggests many avenues for improving infinite series instruction. In addition, the methods employed in this study are general, and open up ways of looking at student thinking that can be applied to many problematic areas in the curriculum. Typical studies ask students to address tasks and issues framed by a researcher. This study instead asked students to explain the content, thereby providing a much larger window into what counts, from the student perspective.",ucb,,https://escholarship.org/uc/item/02h9g7dw,,,eng,REGULAR,0,0
853,2289,Chinese Letters and Intellectual Life in Medieval Japan: The Poetry and Political Philosophy of ChÅ«gan Engetsu,"Morley, Brendan Arkell","Horton, H. Mack;",2019,"This dissertation explores the writings of the fourteenth-century poet and intellectual ChÅ«gan Engetsu ä¸­å·Œå††æœˆ, a leading figure in the literary movement known to history as Gozan (â€œFive Mountainsâ€) literature. In terms of modern disciplinary divisions, Gozan literature straddles the interstices of several distinct areas of study, including classical Chinese poetry and poetics, Chinese philosophy and intellectual history, Buddhology, and the broader tradition of â€œSiniticâ€ poetry and prose (kanshibun) in Japan.Among the central contentions of this dissertation are the following: (1) that ChÅ«gan was the most original Confucian thinker in pre-Tokugawa Japanese history, the significance of his contributions matched only by those of early-modern figures such as OgyÅ« Sorai, and (2) that kanshi and kanbun were creative media, not merely displays of erudition or scholastic mimicry. ChÅ«ganâ€™s expository writing demonstrates that the enormous multiplicity of terms and concepts animating the Chinese philosophical tradition were very much alive to premodern Japanese intellectuals, and that they were subject to thoughtful reinterpretation and application to specifically Japanese sociohistorical phenomena. No less intrepid in the realm of poetry, ChÅ«gan candidly addressed themes such as illness, war, and poverty, and experimented with unusual Sinitic forms such as hexasyllabic quatrains and the vernacular â€œsong lyricâ€ or ci è©ž, which though popular in China was very seldom seen in Japan.The thematic and stylistic breadth of ChÅ«ganâ€™s oeuvre reveals the catholicity of Gozan literary culture and suggests directions for further research into Japanese intellectual history and Sinitic poetry during the medieval era.",ucb,,https://escholarship.org/uc/item/02q068mw,,,eng,REGULAR,0,0
854,2290,Manganese Oxide-Coated Sand Geomedia for Treatment of Contaminated Stormwater,"Charbonnet, Joseph","Sedlak, David;",2018,"Managed aquifer recharge is an increasingly popular means of replenishing groundwater supplies by intentionally infiltrating stormwater or tertiary wastewater effluent into the subsurface. Although this practice can recharge depleted aquifers, it poses potential risks to water quality, especially when practiced with stormwater from city streets which may contain contaminants derived from automobiles, asphalt, biocides and consumer products. To provide a means of protecting water quality during managed aquifer recharge, engineered geomediaï€­materials designed to passively remove contaminantsï€­can be introduced to recharge basins. Manganese oxide-coated sand is a promising engineered geomedia because manganese oxides can oxidize certain organic contaminants and adsorb metal cations. This dissertation describes the use of a manganese oxide-coated sand produced using a novel, room-temperature synthesis for the passive treatment of urban stormwater.The useful lifetime of manganese oxide-containing geomedia may be limited. After oxidizing organic contaminants and exposure to other solutes typically present in stormwater (e.g., Ca2+, CO32-, natural organic matter), the initially high reactivity of the manganese oxide-containing geomedia decreased. The passivation appeared to be related to the reduction of Mn(IV) sites and subsequent accumulation of Mn(II/III). To address the loss of reactivity, methods for regenerating the reactivity of manganese oxide-coated sand were evaluated (Chapter 2). Hypohalites produced the most reactive regenerated phase when dosed in sufficient abundance to fully oxidize all Mn in the failed geomedia to the +4.0 oxidation state. Permanganate also oxidized Mn(III)-rich manganese oxides, but the resultant mineral phase was less reactive with bisphenol A than that produced by HOCl or HOBr. These results imply that mineralogy, in addition to oxidation state, is important to manganese oxide reactivity.Chlorine was evaluated as a means of in situ regeneration by oxidizing reduced Mn without removing the geomedia from columns (Chapter 3). Chemical analyses indicated that the average manganese oxidation state of the geomedia coating increased after exposure to hypochlorous acid (HOCl). The regenerated geomedia demonstrated similar reactivity and longevity to virgin geomedia. X-ray absorption spectroscopy and X-ray diffraction indicated that the virgin and regenerated geomedia coatings had similar manganese oxide structures. These more reactive minerals were nanocrystalline and lacked long-range mineral order, whereas the failed geomedia coating exhibited greater crystallinity and resembled the manganese oxide cryptomelane. These results suggest that manganese oxide-coated sand may be capable of oxidizing organic contaminants urban stormwater for many years and that it is possible to regenerate the oxidative capacity of manganese oxide-coated sands without excavating stormwater recharge systems.Manganese oxide-coated sand was also assessed for its ability to remove metal contaminants from stormwater. The presence of Cu, Zn, Cd and Pb in urban runoff pose potential risks to humans and aquatic organisms. Batch and column experiments were conducted to evaluate the capacity of manganese oxide-coated sand to adsorb metals in a stormwater matrix containing background ions and natural organic matter (Chapter 4). In batch tests the geomedia adsorbed over 95% of Cu, Cd and Pb in conditions typical of natural organic matter-free stormwater. In the presence of natural organic matter, however, the geomedia had a lower capacity for metal removal. Results from column tests indicate that, even in the presence of natural organic matter, a typical stormwater infiltration system containing manganese oxide-coated sand would have a lifetime of over 20 years before the complete breakthrough of Cu and Zn and of over 800 years before the complete breakthrough of Cd or Pb. In field applications, the geomedia could become saturated or release adsorbed metals because of changes in chemical conditions. Therefore, chemical treatments were evaluated for their ability to recover adsorbed metals from the geomedia and restore the adsorption capacity of the manganese oxide-coated sand. A mild acid wash (i.e., pH 3 HCl) restored the adsorptive capacity of the geomedia with minimal degradation of the Mn coating. The regenerated geomedia demonstrated similar performance to the virgin geomedia.The research described in this dissertation indicates that manganese oxide-coated sand may be effective for treating urban stormwater during managed aquifer recharge. High levels of removal of organic contaminants and toxic metals could be sustained for decades with regenerative treatments. In managed aquifer recharge systems, other geomedia will likely be used in conjunction with manganese oxides, and their contributions to pollutant remediation may be complementary or inhibitory. For instance, biochar could protect manganese oxides from fouling by natural organic matter, but anoxic conditions induced by upstream carbonaceous geomedia could reduce manganese oxides. Further research is needed to determine the optimal configurations of geomedia in unit process treatment trains in order to target contaminants across a range of stormwater qualities.",ucb,,https://escholarship.org/uc/item/02r135pn,,,eng,REGULAR,0,0
855,2291,Gesture as a Dialogic Resource in STEM Instructional Interactions,"Flood, Virginia Jane","Abrahamson, Dor;",2020,"When science, technology, engineering, and mathematics (STEM) students and educators interact, they draw on a variety of multimodal communicative resources including gesture, prosody, facial expression, gaze, and body positioning. This dissertation examines how educators and learners take up and interact with each otherâ€™s multimodal utterances in mathematics and computer science learning settings.Using multimodal microanalysis of video, I identify and characterize new forms and functions of gesture as a dialogic resource in these settings, building on previous work on dialogic gesture in different contexts. My analyses are informed by ethnomethodology and conversation analysis (EMCA) and the Co-Operative Action Framework, which provide key theoretical tools for understanding how participants reuse and transform components of each otherâ€™s multimodal utterances to make meaning together.This dissertation contains three papers. The first paper investigates different ways educators can repeat and reformulate learnersâ€™ gestures and speech in order to help learners make connections between their ideas and more formal ways of describing those ideas mathematically. My second paper explores how dialogic gesture functions to organize a common pattern of classroom dialogueâ€”the Initiation-Response-Evaluation/Follow-up (IRE/F) sequenceâ€”and supports the co-construction of public knowledge in programming classrooms. Finally, my third paper builds on the previous two by illustrating additional ways educators attend to and engage with studentsâ€™ multimodally-expressed ideas to help lead children towards new mathematical discoveries.Overall, this dissertation contributes to learning theory by broadening our understanding of multimodal communication in STEM education. More specifically, it demonstrates the important roles dialogic gesture can play in STEM instruction, and it shows how responsiveness to studentsâ€™ gestures is beneficial for learning mathematics and computer science. Numerous strategies for using dialogic gesture are provided that educators can adopt.",ucb,,https://escholarship.org/uc/item/02r6s470,,,eng,REGULAR,0,0
856,2292,Prescribed fire and tanoak (Notholithocarpus densiflorus) associated cultural plant resources of the Karuk and Yurok Peoples of California,"Halpern, Arielle","Sousa, Wayne P;Carlson, Thomas J;",2016,"The targeted application of prescribed fire has long been used by Native Californian peoples to manage plant resources of cultural value. Their ability to employ this management tool has been increasingly restricted by local, state and federal agencies in response to recent drought conditions and the highly flammable state of most western U.S. forests, where, for decades, fires of any magnitude have been suppressed as a matter of policy. This diminished access to cultural prescribed fire has impacted tribal access to many of the plant resources and cultural activities upon which Karuk and Yurok cultures are based.  The research presented in this dissertation: 1) uses historical and modern references to describe fire management practices of tribes throughout California, specifically targeting acorns as food resource, 2) investigates the effect of non-traditional spring burns on rates of tanoak (Notholithocarpus densiflorus, Fagaceae) acorn infestation by frugivorous Filbertworms (Cydia latiferreana, Tortricidae) and Filbert weevil larvae (Curculio occidentalis, Curculionidae), 3) evaluates the effectiveness of indigenous acorn collection criteria in correctly assessing the consumption quality of acorns, and  4) examines the responses of culturally significant plant species found in tanoak gathering areas to prescribed fire applied non-traditionally in spring. Assessed metrics were species diversity, species richness, and vegetative cover.  Traditionally, fires used to maintain productive heritage tanoak stands were set in late summer and fall.  However, prescribed burns at this time of year have become very difficult to implement, so this study focused on the responses of the assemblages to a more easily executed, spring burn.  During the two year study of acorn infestation rates in response to fire, I found that spring prescribed fire reduced acorn infestation in both the year of the fire and one year post-fire. Indigenous acorn collection criteria were very effective in distinguishing edible acorns from both inedible and insect-infested acorns. However, the traditional method of acorn evaluation did misclassify a sizeable proportion of the acorns as inedible, when they were actually of good food quality.  Intended or not, this conservative misidentification provides added insurance that insect-infected acorns are not added to those stored within a household for future use. Over the three years that the responses of culturally valuable understory taxa to fire were monitored, all three metrics (species diversity, species richness, and vegetative cover) were sharply reduced by fire.  Species diversity and richness showed substantial recovery by the second post-fire year, but neither had fully recovered.  Understory plant cover showed very little, in any, recovery two years after the experimental fires. In particular, the percent cover of tanoak seedlings and lignotuber sprouts were greatly reduced, which may increase visibility of abscised acorns to tribal gatherers. As a single cultural fire targets multiple resource goals, these results indicate that some, but not all, resource goals traditionally achieved with fall fire were met with spring prescribed fire.",ucb,,https://escholarship.org/uc/item/02r7x8r6,,,eng,REGULAR,0,0
857,2293,Individual heterogeneity in life history processes: Estimation and applications of demographic models to stage-structured arthropod populations,"Scranton, Katherine","de Valpine, Perry;",2012,"Life history variation is a general feature of natural populations. Most studies assume that local processes occur identically across individuals, ignoring any genetic or phenotypic variation in life history traits. In part, this is because a realistic treatment of individual heterogeneity results in very complex population models. Fitting models with individual heterogeneity to real data is further complicated by random effects in groups of the data, observations set at specific intervals, and the non-independence of data following a cohort of individuals through time. In this dissertation, I assume that individuals differ in the duration they spend in each developmental stage and also in the amount of time they live. Stage durations and survival times follow probability distributions with parameters specific to populations and stages. Parameters of these distributions may also include random effects when considering a subset of sampled populations and covariates such as temperature. In the first chapter I formulate a model and likelihood for variable development, using the time-to-event model framework. In the second chapter I use this model to ask whether field populations of herbivorous arthropods (Tetranychus pacificus) form host-associations on different cultivars of the same host species. In the third chapter I incorporate variable development with variable survival and ongoing reproduction in a stage-structured population model. I explore the ability of the approximate Bayesian computation framework to fit such a complex model to data, evaluating posterior distributions and model performance.",ucb,,https://escholarship.org/uc/item/02x0q09x,,,eng,REGULAR,0,0
858,2294,"Movement, Abundance Patterns, and Foraging Ecology of the California Two Spot Octopus, Octopus bimaculatus","Hofmeister, Jennifer Krista Kaulalani","Caldwell, Roy L;",2015,"Understanding the reciprocal interactions between animal behavior and other inter-related systems such as physiology, morphology, ecology, and evolution has been called a â€œGrand Challengeâ€ for organismal biology. Behavior offers two unique contributions to our understanding of organism response to environmental change: 1) behavior is a rapid and reversible response, and 2) organisms can directly influence their surrounding environment, and thus the stimuli their physiological and morphological systems are exposed to, by â€œchoosingâ€ their environment. My dissertation utilizes the temperate octopus Octopus bimaculatus to understand the reciprocal interactions between behavior and ecology in a human-altered landscape. I use a combination of animal population surveys, dietary analysis, mathematical models, and acoustic telemetry to understand the feedback loops been behavior and ecology. Octopuses play a key predatory role in shaping communities that is unmatched by any other invertebrate. Additionally, their â€œlive fast, die youngâ€ life history strategy means their populations respond quickly and dramatically to changes in the environment, which suggests they could be an indicator of ecosystem change. Specifically, I addressed three fundamental questions about octopus behavior and ecology on Santa Catalina Island, CA: 1) What are key environmental variables influence octopus environmental choice?; 2) Where and when do octopuses move?; and 3) Does octopus diet reflect differences in predatory behavior within a human-altered habitat?The first question addressed a key gap in current knowledge of the environmental variables that influence octopus abundance and distribution in a rocky reef ecosystem by combining intensive population surveys with imperfect detection modeling. Binomial mixture modeling is used when the likelihood of false non-detection is high, and is ideal for studying highly cryptic species like octopuses. Abundance and detection probability was modeled with site and survey-level covariates such as abiotic variables, octopus predator and prey abundances, and habitat structural characteristics. No single abundance covariate explained the data for either year, but detection in 2013 was best modeled with Julian date. Model-averaged estimates of abundance had high ranges of possibility, and all correlations with estimated octopus abundance and environmental variables had weak support. These results highlight the high variability in patterns of octopus abundance and the necessity for integration of multiple environmental factors to elucidate drivers of octopus abundance and small-scale distribution. The second question quantified movement and habitat use of individual octopuses. This study pioneered acoustic telemetry research on small octopus species and was the first of its kind in the continental United States. I collected and tagged O. bimaculatus and recorded their position over the course of approximate 2 weeks and measured movement continuously over one day for each animal. I found that O. bimaculatus is highly mobile and, combined with my survey data, concluded that they do not stay in a single den for more than a few days. Additionally, movement distances and diurnal movement patterns are highly variable, suggesting a behavioral response to avoid predation.The third question aimed to identify the diet of O. bimaculatus using Stable Isotope Analysis (SIA) and assess if octopus diet differed between Marine Protected Area (MPA) sites and non-protected sites. SIA allows for the indirect assessment of diet because the isotopic composition of prey is incorporated with reliable fidelity into the predatorsâ€™ tissues. I analyzed the carbon and nitrogen stable isotope ratios in octopuses and their prey collected inside and outside the Blue Cavern MPA. I found that octopus diet differs between MPA and non-MPA sites and octopuses located within the MPA have a more varied diet than those located outside. This difference indicates a change in the predator-prey relationships with the establishment of the MPA and suggests larger changes in the community structure.Together, these findings yield novel insights into what influences the populations and behavior of octopuses in a rocky reef environment, and lay the groundwork for directly testing how octopuses will respond to and influence the changes in their surrounding community.",ucb,,https://escholarship.org/uc/item/02x5h47b,,,eng,REGULAR,0,0
859,2295,A Historical-comparative Study of the Tani (Mirish) Branch in Tibeto-Burman,"Sun, Tianshin",,1993,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/02z2h2fw,,,eng,REGULAR,0,0
860,2296,Reading at the Opera: Music and Literary Culture in Early Nineteenth-Century Italy,"Jacobson, Edward Lee","Smart, Mary Ann;",2020,"This dissertation emerged out of an archival study of Italian opera libretti published between 1800 and 1835. Many of these libretti, in contrast to their eighteenth-century counterparts, contain lengthy historical introductions, extended scenic descriptions, anthropological footnotes, and even bibliographies, all of which suggest that many operas depended on the absorption of a printed text to inflect or supplement the spectacle onstage. This dissertation thus explores how literatureâ€”and, specifically, the act of readingâ€”shaped the composition and early reception of works by Gioachino Rossini, Vincenzo Bellini, Gaetano Donizetti, and their contemporaries. Rather than offering a straightforward comparative study between literary and musical texts, the various chapters track the often elusive ways that literature and music commingle in the consumption of opera by exploring a series of modes through which Italians engaged with their national past. In doing so, the dissertation follows recent, anthropologically inspired studies that have focused on spectatorship, embodiment, and attention. But while these chapters attempt to reconstruct the perceptive filters that educated classes would have brought to the opera, they also reject the historicist fantasy that spectator experience can ever be recovered, arguing instead that great rewards can be found in a sympathetic hearing of music as it appears to us today.",ucb,,https://escholarship.org/uc/item/02z6w65s,,,eng,REGULAR,0,0
861,2297,"Integrated regulation at the levels of mRNA, translation, and protein in Saccharomyces cerevisiae","Cheng, Ze","Brar, Gloria A.;",2019,"Gene expression in eukaryotes is tightly controlled to determine cell identity and function, and to drive complex differentiation and developmental programs. Each step in the gene expression process can be regulated to influence the final outcome and thus, a comprehensive view of gene expression changes at multiple levels is required for full understanding of the regulation of gene expression. To investigate how the cell integrates regulatory mechanisms at multiple levels to achieve precise and coordinated regulation of gene expression, we combined genome-wide techniques and traditional methodologies to obtain key measurements of gene expression and to dissect the contribution of each step in determining gene expression levels. In chapter 2, we focus on how ribosomal deficiencies alter gene expression. We probed the changes of mRNA abundances, translation rates, and protein abundances in a panel of ribosomal protein mutants and observed changes that were both specific to individual ribosomal proteins and dose-dependent with degree of translation deficiency. We also observed strong evidence of compensatory regulation, including at the transcriptional and protein degradation levels, and distinct cellular responses to loss of small and large ribosomal subunit components. Three current models have been proposed to explain the specific phenotypes associated with loss of ribosomal proteins in diseases of ribosome deficiency, or ribosomopathies, and our data revealed evidence to support all of these models, albeit to differing extents. In chapter 3, we describe a novel regulatory mechanism in which the protein levels are determined by identity, instead of levels, of the transcripts. We collected parallel measurements of mRNA, translation, and protein through a timecourse in yeast meiosis and discovered that the toggling between canonical transcript isoforms and long undecoded transcript isoforms regulates the outcome of gene expression for 380 genes. In chapter 4, we study the regulation of ribosome biogenesis (RiBi) genes and show that global inhibition of translation de-represses RiBi mRNA transcription in a condition-dependent manner. In summary, in three different contexts, we report a surprisingly high degree of integration between different levels of gene expression, highlighting the importance of parallel measurements of different gene expression intermediates to interpret the link between gene expression and cellular outcomes.",ucb,,https://escholarship.org/uc/item/04v4w6bc,,,eng,REGULAR,0,0
862,2298,An Argument for Knowledge Variety in Evidence-Based Management,"Martelli, Peter Fabrizio","Shortell, Stephen M;",2012,"Evidence-based management has been proposed as a method to impose rationality on decision-making, and thereby link research more closely with practice. Yet, knowledge, not evidence per se, is the means by which organizations map the uncertainty of their environments and develop strategies accordingly. This essay develops a theoretical argument to support the construct of knowledge variety, and argues that knowledge variety is a preferable operationalization of evidence-based management for organizations facing the epistemic uncertainty of complex environments. The first section sets a foundation drawing upon diversity in socio-behavioral regulation, requisite variety, the social structure of knowledge, epistemic knowledge categories, rhetorical persuasion, sensemaking, and organizational attention. The second section describes a mixed-methods project to examine evidence-based management in terms of knowledge variety across 42 hospitals associated with a common knowledge intermediary. An original proposition intending to establish a relationship between the maintenance of variety and an organization's ability to customize diffusing best practices was unable to confirm or disconfirm a relationship. Nevertheless, a series of qualitative and quantitative findings on evidence-based management and best practices are presented, including a relationship between knowledge variety and the number of years since last academic degree, suggesting the importance of revisiting the Aristotelian notion of practical wisdom (or, phronesis). Reflections, limitations, and suggestions for future research are described throughout.",ucb,,https://escholarship.org/uc/item/0514w6wx,,,eng,REGULAR,0,0
863,2299,"Do ""Accidents"" Happen? An Examination of Injury Mortality Among Maltreated Children","Hornstein, Emily Putnam","Berrick, Jill D;",2010,"This dissertation is based on a unique dataset constructed by probabilistically linking records across three independent sources of data from California: 1) vital birth records, 2) administrative child protective service records, and 3) vital death records. The final dataset captures 4.3 million children born in California between 1999 - 2006 and includes maltreatment allegation information for over 500,000 children who were reported to child protective services (CPS), as well as death reports on 2,000 children who were fatally injured before age five. Three research questions were examined in the context of a prospective birth cohort analysis: 1) Is a referral to child protective services an independent risk factor for injury mortality? 2) Is allegation disposition associated with injury fatality risk? 3) Does injury fatality risk vary across maltreatment allegation types? To answer these three questions, a series of multivariate survival models were specified. Separate models were estimated for overall risk of injury death, risk of unintentional injury death, and risk of intentional injury death. Findings indicate that an earlier report to CPS is the strongest predictor of a child's injury death during the first five years of life. Children previously reported for maltreatment died from accidental injuries at twice the rate of their unreported, demographically similar peers, and from intentional injuries at five times the rate. After adjusting for other characteristics, children whose report of maltreatment was evaluated out without an in-person investigation by CPS died of injuries at significantly higher rates than children who had never been reported. Children with a substantiated allegation of maltreatment and no foster care placement died of intentional injuries at over 10 times the rate of children who had not been reported. Placement in foster care for even a single day was protective. Children with a prior allegation of physical abuse died from injuries at rates that were notably higher than not only unreported children, but also children reported for reasons of sexual abuse, neglect, or other forms of maltreatment. When only intentional injury fatalities were modeled, a prior allegation of physical abuse was associated with a rate of death that was 38 times that of children who had not been reported.This study represents the most rigorous longitudinal analysis of mortality outcomes following a report to CPS to date, with several key implications for practice and policy emerging. First, these data underscore that a child's report to CPS is not random, nor is it simply a function of poverty. Rather, a report to CPS signals a level of risk, including a risk of death, that is greater than sociodemographic factors would alone predict. A second and related point is that children evaluated out after a CPS hotline call reflect a group whose risk of injury death is far greater than their unreported sociodemographic peers. The decision to screen these children out without an investigation, under the logic that these children were assessed to be at no greater risk of harm than other demographically similar children, is not supported by the empirical evidence generated from this study. Third, these data highlight that although there has been a recent emphasis on the unmet service needs of children reported for neglect, it is young children reported for physical abuse who face the greatest risk of death. Given that physical abuse allegations represent a minority of reports received by CPS, these data suggest that a different protocol for investigating and intervening in cases in which physical abuse is alleged may be justified. Finally, the finding that a prior allegation of maltreatment is the single greatest predictor of not just intentional injury death, but also unintentional injury death, lends support to calls that have been made for child welfare services to be pursued under a broader, public health-oriented agenda, focused on the prevention of all manners of injury death.",ucb,,https://escholarship.org/uc/item/0522n5pp,,,eng,REGULAR,0,0
864,2300,Essays in Financial Economics,"Sohn, Sung Bin","Malmendier, Ulrike;",2012,"This dissertation consists of three essays in financial economics. The first two essays explore how initial public offerings are affected by various stock market conditions. In the third essay, I study the meaning of innovations in investor sentiment.In the first essay, I use cointegration techniques to decompose stock market shocks into permanent and transitory shocks, building on the idea that transitory shocks should not have long-run effects on dividends and stock prices. The decomposed shocks improve on existing valuation measures by indicating the extent to which market value is driven by permanent or transitory fluctuations. I then examine the effects of these shocks on several aspects of IPOs, and find that (1) despite the lack of long-run effects on firms' value, more firms go public in response to stronger transitory shocks; (2) firms that go public after stronger transitory shocks underperform their benchmark more severely in the long run; (3) during the book-building period, managers are more likely to limit secondary share sales after stronger transitory shocks; and (4) managers who limit secondary share sales further during the book-building period exhibit more severe long-run underperformance. These findings are consistent with the hypothesis that transitory shocks induce more IPOs that opportunistically exploit temporarily higher market valuation than IPOs that finance profitable projects in better market conditions. The findings are also consistent with the hypothesis that managers are more prone to become overconfident after stronger transitory shocks and that the resulting overconfidence leads to long-run underperformance. The decomposition methodology can also be applied to other corporate finance decisions such as SEOs, mergers and investments.The second essay establishes a model that incorporates both uncertainty and dispersion of opinion to examine how these two factors affect IPO stock performance. The model predicts that, unlike uncertainty, dispersion of opinion has nonlinear effects. There is a threshold of dispersion of opinion below which the dispersion does not affect IPO stock performance. Above the threshold, on the other hand, larger dispersion of opinion bids up the stock price higher and consequently yields the lower long-run return. The level of the threshold is increasing in the amount of free-floating shares in the market. Since IPO firms tend to have relatively small free-floating shares than other listed firms, IPO stocks are more subject to the dispersion of opinion. Thus, empirical researches that do not control the dispersion of opinion can produce misleading results on IPO performance. The model also predicts IPOs observations are subject to self-selection bias. Private firms would decide not to go public under the combination of high uncertainty and small dispersion of opinion, which could actually yield high long-run returns. This prediction helps explain the time variation of IPO volume and the general pattern of IPO long-run underperformance.The third essay tries to understand the meaning of innovations in investor sentiment. The role of investor sentiment in the stock market has attracted attentions of economists. Previous papers show that investor sentiment has return predictability and it is more pronounced among stocks that are more difficult to value and to arbitrage, and emphasize the behavioral role of investor sentiment. However, it still remains unclear whether this predictability is due to a causal effect of autonomous animal spirits or not. Alternatively, investor sentiment may reflect systematic risks and the predictability could be mere coincidence, not causation. For a structural interpretation, I introduce a modified version of the long-run risks model in which sentiment innovations arise from both animal spirit shocks and several risk shocks, and animal spirit shocks could affect stock returns. By matching impulse responses from data simulated according to the theoretical model to those from actual data, I estimate parameters in the model. The estimated model moderately replicates the historical data in the actual stock market. The estimation results show that a substantial amount of variation in investor sentiment is explained by systematic risk shocks as well as by animal spirit shocks, and that animal spirit shocks can have significant effects on stock returns. The findings suggest that investor sentiment is a noisy proxy of animal spirits and that autonomous animal spirits are at least in part responsible for the apparent return predictability of investor sentiment.",ucb,,https://escholarship.org/uc/item/0523t2kb,,,eng,REGULAR,0,0
865,2301,"Kinship, Social Structure, and the Ecological Bases for Sociality in Torch-Tail Spiny Rats, Trinomys yonenagae (Rodentia: Echimyidae): Evidence from Field and Molecular Data","Alves dos Santos, Jose Wellington","Lacey, Eileen A;",2010,"Sociality (i.e., group-living) is a multi-dimensional aspect of behavior that occurs in many vertebrate species. Because living in spatially and behaviorally cohesive groups provides the foundation for most forms of complex, cooperative interactions, understanding the reasons for group-living is a fundamental goal for behavioral biologists. In mammals, ecological factors are hypothesized to play a major role in the formation of social groups; the ecological correlates of sociality in individual mammalian species, however, are often poorly understood. The torch-tail spiny rat (Trinomys yonenagae) is a South American hystricognath rodent endemic to semiarid sand dunes in northeastern Brazil. T. yonenagae is divergent from its congeners in that it is group-living, semi-fossorial, and desert-dwelling; other Trinomys species inhabit forests where individuals live aboveground and are solitary. To explore the adaptive bases for these distinctive attributes of T. yonenagae, I combined field studies with molecular genetic analyses to (1) characterize the social organization and kin structure of torch-tail spiny rats and (2) identify the primary ecological factors influencing sociality in this species. Most (76.2%) burrow systems monitored were occupied by more than one adult, including same-sex pairs, male-female pairs, and multiple adults of both sexes. Spatial overlap among burrow mates was extensive (72.0 Â± 27.0%) and included the use of the same nest site. Kinship among adults decreased as the distance between the burrow systems in which individuals were resident increased. Burrow mates - particularly females - were typically close kin, although unrelated individuals (apparent immigrants from other burrow systems) were also detected within groups. Among adults captured in two successive field seasons, nearly half remained in the same burrow system; among the remaining animals, dispersal was male-biased. Individuals that dispersed to new burrow systems were more related to opposite-sex burrow mates than were individuals that remained in the same burrow system in consecutive years. At the same time, relatedness between dispersers and opposite-sex adults was lower in the new (as compared to the original) burrow system, suggesting that dispersal is related to inbreeding avoidance. Data regarding the distribution of vegetation on the study site revealed that proximity of food resources to a burrow system was significantly associated with group size. Protective vegetation and number of burrow openings, however, were better predictors of burrow sharing, suggesting that predation is the primary factor shaping social structure in this species. Comparisons of these findings with data from both other echimyids and other desert-dwelling rodent species yield intriguing new insights into the factors favoring sociality among burrow-dwelling rodents.",ucb,,https://escholarship.org/uc/item/030595ck,,,eng,REGULAR,0,0
866,2302,Working memory and choice encoding in medial prefrontal cortex of rats performing a spatial double alternation task.,"Leonard, Mathew","DeWeese, Michael;",2014,"Neural correlates of working memory, errors, and response choice were identified in the medial prefrontal cortex (mPFC) of rats performing a spatial delayed double alternation task. The mPFC of rats has been associated with attention, working memory, emotional control, and other executive functions. Lesioning or inactivation of mPFC, the prelim- bic (PL) and infralimbic (IL) areas in particular, have been shown to produce significance decreases in performance of delayed response tasks. Previous recording experiments have found examples of neurons in mPFC that have elevated and selective activity during the delay period. However, these experiments have been quite simple compared to experiments in monkeys. For instance, many experiments in rats have used spatial alternation tasks but none have been able to successfully separate coding for past spatial goals from coding for future spatial goals. The aim of the experiment reported here was to develop a task with complexity similar to what is found in primate research and to record from mPFC as the rats performed the task. We developed a delayed double alternation, in which rats must respond in a left-left-right-right pattern. This task requires the rats to store and recall at least two items of information to receive rewards. After training rats to perform the task, we recorded from mPFC of awake behaving rats using chronically implanted tetrodes. We found groups of neurons encoding past goals, future goals, switch-stay strategy, and response errors. Encoding for past goals and future goals are performed by two separate, but over- lapping, ensembles of neurons. Anatomically, the mPFC is involved in neural circuits which serve to integrate internal and external stimuli into behavioral actions. Our results reveal a functional integration of the past, present, and future into action.",ucb,,https://escholarship.org/uc/item/0340949z,,,eng,REGULAR,0,0
867,2303,Discourse connectedness: The syntax--discourse structure interface,"Baclawski Jr, Kenneth Paul","Jenks, Peter;",2020,"This dissertation argues for the existence of a new A'-feature, discourse connected (DC), which grammatically encodes a constraint on the relation between the constituent it attaches to and discourse relations to previous sentences. Connectives like That's because and For example encode the rhetorical relations of explanation and elaboration. DC encodes these relations as well, but by A'-movement of a phrase to the left edge of a clause or noun phrase, specifically the movement of a phrase that is previously mentioned in the sentence that the current one is explaining or elaborating upon. I argue that there must be a DC-feature in the lexicon on par with other A'-features, such as wh. Given that DC encodes a discourse structural constraint, there must be a syntax--discourse structure interface.One consequence of this dissertation is descriptive: a range of phenomena in the Eastern Cham language (Austronesian: Vietnam) are found to be instances of DC-marking. These phenomena include what appear on the surface to be topicalization, optional wh-movement, partitives, and inventory forms (e.g. bread, three loaves). I argue that these phenomena in Eastern Cham must be analyzed in terms of DC, not information structure or other previously proposed analyses for comparable constructions in other languages. As a result, multi-sentence discourses that control for discourse structure should be used as diagnostics for constructions that might be licensed by DC or something similar.This dissertation adds a new A'-feature, DC, to the typology of A'-features. DC-movement shares a variety of characteristics with A'-movement more broadly. It exhibits sensitivity to syntactic islands, weak crossover, and locality effects. As with other A'-features in some cases, there is a parallelism between CP and DP: a phrase can be DC-moved to the left edge of either a clause or a noun phrase. Similarly, wh-phrases can undergo secondary movement to the left edge of the noun phrase in some languages. Additionally, this dissertation examines the position of DC among other A'-features, following recent work on the hierarchy of A'-features (Aravind 2017, 2018). Despite its apparent optionality, DC-movement is argued to be obligatory, like wh-movement. DC is also found to be independent from other features such as wh; the same phrase can be marked as both DC and wh. Unlike wh, DC can only be checked once on a respective phrase in a derivation, and the movement of multiple DC-phrases to the edge of the same clause exhibits Path Containment Effects (Pesetsky 1982), not the tucking-in pattern observed in multiple wh-movement (Richards 1997).DC is argued to provide evidence for the need for a dynamic event semantics that allows the events introduced throughout the discourse to be tracked. This dissertation proposes that the DC-feature is introduced by a DC-particle on analogy with focus particles and the Q-particle on wh-phrases (Cable 2010). The DC-particle is shown to introduce a presupposition that checks the participants of two events in a discourse: the current event and an event in a prior sentence inferred by a subordinating discourse relation (i.e. explanation or elaboration).The dissertation proceeds as follows. Chapter 1 introduces DC, along with the ongoing debate on the existence of pragmatic features in syntax. Additionally, the concept of hierarchical discourse constraints (HDCs) is introduced as a heuristic to understand that position of DC in comparison with linear information states (LISs), exemplified by information structural notions like old information topic. Chapter 2 examines the basic case of DC-movement, topicalization, and demonstrates that DC must be an \={A}-feature. In Chapter 3, wh-phrases are shown to be able to be DC-marked in Eastern Cham. An analysis is proposed in which DC-particles and Q-particles can be present in the same DP. Chapter 4 turns to DC-marking inside a noun phrase and finds a CP/DP parallelism: phrases can be DC-moved to the left edge of a noun phrase. Despite these cases involving movement inside a noun phrase, the event semantic interpretation of DC is affirmed: DC is only computed between the larger events of which the noun phrase is a part. Chapter 5 concludes. Additionally, it analyzes clitic right-dislocation in Catalan as DC-movement with minor syntactic and semantic differences from DC-movement in Eastern Cham (cf. Lopez 2009). Contrastive topic is also examined and proposed to be a second hierarchical discourse constraint, different from DC.",ucb,,https://escholarship.org/uc/item/0349w4fh,,,eng,REGULAR,0,0
868,2304,Essays on Weather Indexed Insurance and Energy Use in Mexico,"Fuchs, Alan","de Janvry, Alain;Sadoulet, Elisabeth;",2011,"This dissertation consists of three chapters that analyze the effects of social development programs on productivity, risk management strategies, and energy consumption among the poorest population in Mexico. Weather shocks have important negative impacts on poor rural households' livelihood as they are not only closer to subsistence and more vulnerable but also depend on the weather for survival. Nonetheless, due to high administrative costs and information problems insurance markets tend to leave this part of the population unprotected. Similarly, poor rural households usually make use of cheap yet inefficient and potentially harmful sources of energy for cooking, lighting, and heating their homes. This situation does not only affect their health and daily activities, but also keeps them trapped in poverty. In the following chapters I discuss several ways in which government action can in fact improve this population's well-being. The first chapter entitled ""Drought and Retribution: Evidence from a large scale Rainfall-Indexed Insurance Program in Mexico"" studies the effects of the recently introduced rainfall-indexed insurance on farmers' productivity, risk management strategies, and per capita income and expenditures in Mexico. Weather shocks are a major source of income fluctuation and most of the world's poor lack insurance coverage against them. In addition, the absence of formal insurance contributes to poverty traps as investment decisions are conflicted with risk management decisions: risk-averse farmers tend to under-invest and concentrate in the production of lower yielding yet safer crops. Recently, weather-indexed insurance has gained increased attention as an effective tool providing small-scale farmers coverage against aggregate shocks. However, there is little empirical evidence about its effectiveness. According to the Ministry of Agriculture, 80 percent of agricultural catastrophic risk in Mexico stems from droughts. Therefore, in 2003 it implemented weather-indexed insurance as a pilot in five counties in the Mexican State of Guanajuato, and by 2008 it already covered almost 1.9 million hectares representing 15 percent of rain-fed agricultural land. The main identification strategy takes advantage of the variation across counties and across time in which the insurance was rolled-out. We find that insurance presence in treated counties has significant and positive effects on maize productivity. In fact, we find that insurance presence at the county level increases maize yields by more than 5 percent. Similarly, we find that insurance presence at the county level has had a positive effect on rural households' per capita expenditure and income of a magnitude close to 8 percent. However, we find no significant relation between insurance presence and the number of hectares destined to maize production.The second chapter entitled ""Voters Response to Natural Disasters Aid: Quasi-Experimental Evidence from Drought Relief Payment in Mexico"" estimates the effect of a government climatic contingency transfer allocated through the recently introduced rainfall indexed insurance on the 2006 Presidential election returns in Mexico. Using the discontinuity in payment based on rainfall accumulation measured on local weather stations that slightly deviate from a pre-established threshold, we show that voters reward the incumbent presidential party for delivering drought relief compensation. We find that receiving indemnity payments leads to a significant increase in average electoral support for the incumbent party of approximately 7.6 percentage points.  Our analysis suggests that the incumbent party is rewarded by disaster aid recipients and punished by non-recipients. This chapter provides evidence that voters evaluate government actions and respond to disaster spending contributing to the literature on retrospective voting.The third and final chapter entitled ""Conditional Cash Transfers schemes and Households' Energy Response in Mexico"" analyzes the relationship between income and energy use in poor households in Mexico using household expenditure surveys that were collected to evaluate the poverty alleviation program ""Oportunidades"". We argue that Oportunidades cash transfers provide an income shock that is exogenous to a household's energy demand, allowing us to estimate short-run and long-run income elasticities for energy use. Short-run estimates hold household's appliance stock constant and long-run estimates model the household's decision to acquire new appliances. As a general estimation strategy households' fixed-effects are included. We also use instrumental variable estimation and a matching difference-in-differences estimator to check for robustness and correct for pre-selection unbalances between treatment and control groups. Results suggest significant differences between long-run and short-run elasticities as households emerging from poverty become first-time purchasers of energy-using appliances. In particular, we find small and not significant effects of cash-transfers on short-run energy consumption expenditure, but find significant and important effects of cumulative conditional cash-transfers on appliance acquisition (i.e. refrigerators and gas stoves). This has important policy implication since poverty alleviation programs like Oportunidades conditional cash transfers program, although not evident in the short run, have significant effects on energy demand.",ucb,,https://escholarship.org/uc/item/0356c5mg,,,eng,REGULAR,0,0
869,2305,"The perception of shape, lighting, and material properties in images","O'Shea, James Patrick","Banks, Martin S;",2012,"Three scene properties determine the image of a 3D object: the material reflectance, the illumination, and the object's shape. Because all three properties determine the image, one cannot solve for any one property without knowing the other two. Nevertheless, people often are able to perceive these properties consistently and relatively accurately. We explore the relationship between these properties, the sources of image information the visual system can use to recover these properties, and the assumptions the visual system tends to make. We first conducted a shape perception experiment in which we investigate whether the visual system assumes the angle between the lighting direction and the viewing direction. Observer errors were minimized when the light was 20-30 degrees above the viewing direction, confirming the light-from-above prior. In a second study, we conducted two psychophysical experiments to determine how viewers use shape information to estimate the lighting direction from shaded images. We found that observers can accurately determine lighting direction when a host of shape cues specify the objects. When shading the is the only cue, observers always set lighting direction to be from above. We modeled the results in a Bayesian framework that include a prior distribution describing the assumed lighting direction. Finally, we explore how disparity and defocus information may be useful in material perception to distinguish glossy and matte surfaces. We describe the types of images needed to investigate this question, and introduce a method to render them.",ucb,,https://escholarship.org/uc/item/0358618m,,,eng,REGULAR,0,0
870,2306,Error Estimates for Least-Squares Problems,"Hallman, Eric","Gu, Ming;",2019,"In this thesis we consider error estimates for a family of iterative algorithms for solving the least-squares problem \min_x \|Ax-b\|_2 based on the Golub-Kahan-Lanczos bidiagonalization process. Given a lower bound on the smallest singular value of A, we show how to compute upper bounds on the Euclidean error \|x_k-x_*\|_2 as well as the backward error. In the case of the Euclidean error, we demonstrate that our bound is sharp given the information we use about the matrix A. We also present two new algorithms aimed at minimizing our error estimates. The first, LS-Craig, is constructed to minimize our estimate of \|x_k-x_*\|_2 with every iteration. The second, LSMB, minimizes an objective function closely related to our backward error estimate. We find that at each step the iterate x_k produced by LS-Craig is a convex combination of those produced by LSQR (which minimizes \|r_k\|_2 = \|b-Ax_k\|_2 over a Krylov subspace) and Craig's method. The iterates produced by LSMB, in turn, are a convex combination of those produced by LSQR and LSMR (which minimizes \|A\ts r_k\|_2 over the same subspace). Experiments on test cases from the University of Florida Sparse Matrix Collection show that in practice LS-Craig and LSMB perform at least as well as the existing algorithms, although never by more than a small margin. This suggests that LS-Craig could replace LSQR when stopping rules are based on the Euclidean error and LSMB could replace both LSQR and LSMR when stopping rules are based on the backward error. Finally, we extend the definition of the backward error to the case \min_X\|AX-B\|_F where B may have any number of columns. We derive a formula for the backward error and propose two estimates that experiments suggest are highly reliable. We prove that the estimates are reliable for a few special cases, although the general problem remains open.",ucb,,https://escholarship.org/uc/item/035866wh,,,eng,REGULAR,0,0
871,2307,Pressing Back: The Struggle for Control over China's Journalists,"Hassid, Jonathan","O'Brien, Kevin J;",2010,"Despite operating in one of the most tightly controlled media environments in the world, Chinese journalists sometimes take extraordinary risks in exposing wrongdoing by power holders.  Based on interviews with Chinese news workers and others over 14 months of fieldwork, and a computer-assisted content analysis of nearly 19,000 Chinese newspaper articles, my research examines the causes and consequences of this journalistic risk taking.  In doing so, I point to a new category of oppositional behavior situated between the poles of quiescence and outright resistance.  This behavior, which I term ""pushback,"" takes place when actors privileged by professional or traditional standing oppose the state or its policies in ways that they perceive to be within the boundaries of the permissible. These actors' oppositional acts, however, are made with the explicit or implicit goal of pushing long-term state policy in ways that the powerful might not currently find acceptable.  Although neither necessary nor sufficient to precipitate outright resistance, pushback can - under the right circumstances - be causal and predictive of later resistive acts. Even if pushback never ""progresses"" to explicit resistance, it can have important, long-term political effects of its own, including shifting power out of the hands of the party/state and toward journalists.Pushback, I argue, is spurred not by the macro-scale political and economic changes in the Chinese media, but by a type of professional orientation I term ""advocacy journalism."" Advocate journalists are those who view their role as advancing the development of the Chinese nation rather than either the Chinese Communist Party or Western-style press freedoms.  It is ultimately these advocate journalists, who tend to be younger and better educated than average, who push back against the Party/state's confining strictures.  Though circumstances vary, they also tend to move from pushback toward overt resistance when the Party/state effects a change that disrupts their everyday lives, when there is a clear target or targets for blame, and when there is a language of moral economy to rally around. It is ultimately these journalists who are pushing the boundaries of state power and potentially reshaping the Chinese political landscape.",ucb,,https://escholarship.org/uc/item/0531q579,,,eng,REGULAR,0,0
872,2308,Distinct Roles for Two Trypsin-like Proteases in Magnetosome Biomineralization,"Hershey, David Michael","Komeili, Arash;",2016,"The ability of living organisms to transform inorganic elements into insoluble crystalline structures is an underexplored theme in biology. A group of aquatic bacteria, called magnetotactic bacteria, produces chains of nanometer-sized magnetic crystals within their cells, allowing them to align in the earthâ€™s geomagnetic fields. Understanding the mechanism for this process has become increasingly important as the demand for customized nanoparticles in medical and industrial applications has grown. In particular, the protein factors required to transform soluble iron into magnetite (Fe3O4) are likely to contain novel mechanisms for manipulating insoluble inorganic compounds. This thesis describes the biochemical and genetic features of two such factors, MamO and MamE. A historical account is provided describing the discovery of magnetotactic bacteria, the development of Magnetospirillum magneticum AMB-1 as a model system and the identification of specific genes required to produce magnetite. This previous body of work led to the identification of MamO and MamE as predicted trypsin-like proteases that are required for biomineralization in AMB-1. Genetic, biochemical and structural studies reported here showed that the MamO protease domain is catalytically inactive and incapable of serine protease activity. Instead it has a novel di-histidine motif that participates in direct binding to transition metals. This motif is required for biomineralization in vivo, confirming that the MamO protein is a repurposed trypsin-like scaffold that promotes magnetite nucleation by binding directly to iron precursor atoms. Genomic and phylogenetic analysis of related serine proteases in other magnetotactic bacteria showed that the repurposing of trypsin-like proteases has occurred numerous times independently during the evolution of magnetosome formation. Also described is the observation that three biomineralization factors, MamE, MamO and MamP are proteolytically processed in AMB-1. MamE and MamO are both required for these proteolytic events, as are the predicted catalytic residues from MamE. However, consistent with its newly assigned pseudo-protease classification, the predicted MamO active site is dispensable. This suggested that MamE directly processes these targets in a manner that requires MamO. The proteolytic activity of MamE was reconstituted in vitro with a recombinant form of the protein. MamE cleaved a custom peptide substrate based on an in vivo cleavage site in MamO with positive cooperativity, and its auto-proteolytic activity could be stimulated by both substrates and peptides that bind to its regulatory domains. These enzymatic properties suggested that a switch-like regulatory mechanism modulated MamE-dependent proteolysis during biomineralization. This regulatory paradigm was confirmed by showing that both catalytically inactive and constitutively active alleles of mamE caused severe biomineralization defects in vivo. Although the genes required for biomineralization were known previously, the molecular mechanisms by which each protein promotes magnetite synthesis had not been explored. The results of these studies define biochemical functions for two of the four factors required for magnetite nucleation in AMB-1. Furthermore, describing the evolutionary repurposing of a trypsin scaffold along with the phylogenetic description of its evolutionary history add broad biological interest to magnetosome research.",ucb,,https://escholarship.org/uc/item/053898wf,,,eng,REGULAR,0,0
873,2309,The Effects of Hoja loca on Maize Organogenesis,"Sluis, Aaron Michael","Hake, Sarah C;",2016,"Plant organs initiate from meristems and grow into diverse forms. Initiation is followed by a morphological phase where organ shape is elaborated, and finally tissues differentiate into their mature state. This process involves a variety of mechanisms, including complex gene regulatory networks, intricate patterning of plant hormones, and dynamic tissue growth.  Hoja loca is a maize mutant that has defects in initiating and forming leaves and other lateral organs.  I aim to use the Hoja loca mutation to investigate processes involved in transitioning tissue from undifferentiated meristem to lateral organs.  Its unique phenotype suggests that it can greatly aid the understanding of plant organogenesis due to the specificity of mutant defects to organ initiation.In this dissertation, I investigate the effects of the Hoja loca mutation on shoot organogenesis in Zea mays. In Chapter 1, I describe the mutant phenotype.  In Chapter 2, I identify the mutant locus.  I use RNA-seq differential gene expression to look at global changes and identify genes that are likely to mediate the Hoja loca defect.  In Chapter 3, I investigate the localization of genes and metabolites that are important in organogenesis, namely the patterning of auxin. Together, these results suggest that Hoja loca causes an auxin-insensitivity in part of the auxin signaling response, likely at the site of organ initiation. This change is signaling leads to the misexpression of a beta glucosidase capable of activating cytokinin glucoside conjugates, and potentially the degradation of flavonols.  The balance of cytokinin and auxin is a critical regulator of tissue differentiation, and flavonols affect auxin transport.  Disrupting the localization of either of these compounds in the shoot could plausibly lead to the mutant phenotypes of skipped leaf initiation and altered leaf morphology.",ucb,,https://escholarship.org/uc/item/057699jg,,,eng,REGULAR,0,0
874,2310,Coherent Attributions with Co-occurring and Interacting Causes,"Jennings, Kyle Edward","Peng, Kaiping;",2010,"Many processes within social and personality psychology require individuals to attribute the cause of an effect for which there are multiple potential causes.  Whether people make these attributions correctly has been a topic of long-standing debate.  This work introduces a Bayesian identity that can determine the coherence (internal consistency) of people's attributions regarding the presence of a candidate causal factor, given the occurrence of an effect and the presence of another causal factor.  Letting U be the factor whose presence is uncertain, E be the effect that occurred, and C be the factor known to be present, then the attribution of interest, P(U|E,C) is equal to P(U)x[P(C|U)/P(C)]x[P(E|C,U)/P(E|C)], which are termed the prior probability, cause-cause co-occurrence, and relative effect likelihood, respectively.  Intuitively, they express how likely the uncertain factor's presence is in general, whether the two factors tend to occur together, and how much more likely the effect is when it is known that the uncertain factor is present, as compared to when the uncertain factor's presence is unknown.  This expression can be used to intuitively determine the coherent attribution in a particular scenario, or it can be applied quantitatively.  Studies are conducted that assess attribution coherence relative to people's reported assumptions and perceptions.  Two of the studies ask directly for the beliefs in the identity, one using a Likert scale to represent the log-ratio of terms in the model, the other asking for the individual probabilities.  People are generally coherent, with little difference cross-culturally (East Asian vs. European-American).  The approach can be made to directly match the inferences in trait and attitude attribution studies by using probability density functions over continuous variables, and letting the expected value of the posterior distribution be the normative attribution.  When applied to the attitude attribution paradigm, it is shown both via postdiction and prospectively that people's attitude attributions are potentially coherent according to the model.  Implications for discounting and augmenting, interactions between causes, the fundamental attribution error and correspondence bias, and cross-cultural attribution research are discussed, and recommendations are given for improving theory and research.",ucb,,https://escholarship.org/uc/item/057838qq,,,eng,REGULAR,0,0
875,2311,Electron and phonon transport in silicon nanostructures,"Lim, Jongwoo","Yang, Peidong;",2013,"Understanding electron and phonon transport in silicon nanostructures is essential for developing advanced electronic and thermoelectric systems. As opposed to electrons that contribute to conduction only near the Fermi surface, phonons show broadband spectrum when they transport heat in silicon. By controlling the dimension and morphology and accounting for characteristic length scale of electrons and phonons, we can engineer their transport properties. In this work, the transport of electrons and phonons in silicon nanostructures was investigated to improve future designs of thermoelectrics as well as nanoelectronics and phononics. 	Phonon scattering at boundaries in solid is considered diffusive or specular depending on the wavelength of phonons and the length scales of surface roughness. Thermal conductivity will be at minimum when the phonon scattering is purely diffusive, which is known as the Casimir limit. However, in rough silicon nanowires, the thermal conductivity was found to break the limit. Hence, the role of the roughness parameters (rms, correlation length and ï¡p, high frequency roughness factor) of etched VLS nanowires was systematically addressed for the suppression of thermal conductivity.  The surface roughness, characterized by amplitude (rms) and lateral length scale of roughness (correlation length), is demonstrated to dictate phonon transport in such nanowires down to 5W/mK. More interestingly, high frequency spectrum window of surface roughness close to dominant phonon wavelength (1 - 10 nm) at room temperature was chosen to correlate to thermal conductivity and it showed better the trend with the high frequency roughness factor. Electroless etched (EE) nanowires regardless of their unique irregular morphologies also further have been shown to follow the trend, which indicates sub-diffusive regime of phonon is mainly governed by high frequency surface roughness.  The measurement platforms where both electrical and thermal properties can be characterized for the same holy silicon (HS) ribbon were first developed. It was unclear how the morphology, the doping concentration, and the dimensions affect the transport properties of nanostructures mainly for thermoelectric applications. Hence we developed this platform for HS ribbons known for enhancing thermoelectric performance. Electron and phonon transport was independently controlled by tuning the neck size, which is the limiting distance between adjacent holes. Owing to accurate thermal conductivity characterization by the superior device design, the necking effect was quantified with a fixed pitch distance. The thermal conductivity in this structure was found below the incoherent phonon scattering regime due to the periodicity in nano-hole array, which possibly opens the new realm of coherent phonon scattering.  Finally, the first demonstration of ballistic phonon transport in the cross-plane direction of silicon has been shown with HS structures. We showed that long wavelength phonons can propagate ballistically up to a few hundred nanometers despite the lateral dimension of 20nm. The fundamental understanding in thermal conduction has direct relevance for heat management in silicon based electronics and thermoelectrics. Furthermore, phononic devices could potentially be realized via ballistic phonon components in silicon nanostructures.",ucb,,https://escholarship.org/uc/item/03b895tx,,,eng,REGULAR,0,0
876,2312,Studies in the History and Geography of California Languages,"Haynie, Hannah Jane","Garrett, Andrew;",2012,"This dissertation uses quantitative and geographic analysis techniques to examine the historical and geographical processes that have shaped California's linguistic diversity. Many questions in California historical linguistics have received diminishing attention in recent years, remaining unanswered despite their continued relevance. The studies included in this dissertation reinvigorate some of these lines of inquiry by introducing new analytical techniques that make effective use of computational advances and existing linguistic data. These studies represent three different scales of historical change -- and associated geographic patterns -- and demonstrate the broad applicability of new statistical and geographic methodologies in several areas of historical linguistics.The first of these studies (Chapter 2) focuses on the dialect scale, examining the network of internal diversity within the Eastern Miwok languages of the Central Sierra Nevada foothills. This study uses dialectometric measures of linguistic differentiation and geographic distance models to characterize the dialect geography of this language family and examine how human-environment relations have influenced its development. This study finds three primary linguistic divisions in the Eastern Miwok dialect network, corresponding to Plains Miwok, Southern Sierra Miwok and Northern/Central Sierra Miwok, as well as a number of smaller patterns of regional variation. It also identifies elevation, vegetation, and surface water as influences on the dialect network in the region and establishes the utility of cost distance modeling for studying historical linguistic contact networks in situations where our historical knowledge is limited.The second study (Chapter 3) evaluates the hypothesis that the small families and isolate languages of California form a few, deep genealogical ``stocks''. While attempts to validate two of these -- Hokan and Penutian -- have not met with widespread approval, the classifications themselves have been adopted widely. This study examines the statistical evidence for such deep, stock-level relationships among California languages by implementing a metric of recurrent sound correspondence and a Monte Carlo-style test for significance. The multilateral comparison involved in the clustering component of this method makes it particularly sensitive to the types of large clusters that might represent ""Hokan"" and ""Penutian"" groups. However, this test finds no evidence for such groupings and casts further doubt on the genealogical status of these categories.The scale of the final study (Chapter 4) is broader both temporally and geographically. Chapter 4 examines the idea that Northern California functions as a linguistic area. Uncertainty regarding the genealogical and contact-related influences on individual languages in the region and links between Northern California and other linguistic areas make it difficult to evaluate existing proposals about the region's areal status based only on the regional similarities such studies offer as evidence. This chapter uses measures of spatial autocorrelation to determine whether the spatial patterns exhibited by individual features and cumulative patterns in the region as a whole are likely to reflect a history of geographic trait diffusion. While there is good evidence for areal feature spread in Northern California, and particularly in the Northwestern California and Clear Lake areas, many of the features that occur in Northern California extend up the Pacific coast and suggest that Northern California may be better characterized as a peripheral part of the better-supported Northwest Coast linguistic area.",ucb,,https://escholarship.org/uc/item/03c2z9tr,,,eng,REGULAR,0,0
877,2313,Substantive Truth and Knowledge of Meaning,"Khatchirian, Arpy","Stroud, Barry;MacFarlane, John;",2014,"Deflationists have been hard at work convincing us that the concept of truth is far less interesting, and therefore far less mysterious, than long-lasting debates as to its true nature have made it seem. By denying that there is a substantive notion of truth expressed by our uses of the word `true,' deflationism promises to dissolve a number of explanatory hurdles.Donald Davidson rejected deflationism. He grew increasingly sceptical of attempts to assimilate our understanding of truth to any definition or schema, and came to see his own approach to meaning as crucially dependent on a substantive notion of truth. Exactly how, and where, does Davidson's approach to meaning depend on the availability of a substantive notion of truth? One of my goals is to answer this question. Another is to explain why it has proved so hard to answer it. A third goal is to bring to light what it would really be like to be a deflationist--in particular, what linguistic competence would look like if we took deflationism seriously.In Chapter 1, I clarify Davidson's proposal to use truth theories as meaning theories.  I explain this proposal as rooted in a certain conception of a meaning theory as an account of a speaker's understanding of her own sentences, and of this understanding as at least partly consisting in the speaker's knowledge of the truth-conditions of her sentences. In Chapter 2, I argue that deflationism does not allow us to describe competent speakers as knowing the truth-conditions of their sentences. If this is right, deflationism is incompatible with Davidson's conception of truth theories as theories of linguistic competence, but not for the reason usually given. That is, it is not because deflationism explains knowledge of truth-conditions as a trivial by-product of linguistic competence rather than being constitutive of it, since deflationism does not even allow us to ascribe such knowledge to speakers. In Chapter 3, I argue against the widespread assumption that Davidson's proposal concerning the form to be taken by a meaning theory is primarily an answer to the question: what knowledge would enable us to interpret any utterance in a given language? This assumption underlies recent attempts to exhibit Davidson's approach to meaning as hospitable to a deflationary account of truth.  However, as I explain, it undermines the distinction Davidson insists on between meaning theories and translation manuals, and thus cannot be right. Chapter 4 further argues that the methodology of radical interpretation does not, by itself, explain the need for a substantive notion of truth, contrary to what some (including Davidson) have suggested. Finally, in Chapter 5, I highlight some consequences of these results for both our understanding of Davidson's philosophy of language, and our understanding of deflationism.",ucb,,https://escholarship.org/uc/item/03f8n8t6,,,eng,REGULAR,0,0
878,2314,Exploring the Role of LHC Protein Structure and Function in the Evolution of NPQ Mechanisms in Eukaryotic Photosynthetic Organisms,"Erickson, Erika Erickson","Niyogi, Krishna K;",2016,"The initial step of photosynthesis occurs when light energy is absorbed by an array of pigments surrounding the photosynthetic reaction center. Chlorophyll and carotenoid molecules are coordinated by members of a family of intrinsic thylakoid membrane proteins known as Light-Harvesting Complex (LHC) proteins. LHCs are essential for stabilizing and tuning the spectroscopic characteristics of each individual pigment, as well as the pigment array, to allow for efficient energy transfer. Despite a high degree of sequence conservation within the protein family, each of the LHCs performs a different role in modulating the energy landscape of the thylakoid membrane. Two stress response LHC proteins, LHCSR and PSBS, are known to be essential for non-photochemical quenching in some photosynthetic eukaryotes. It has been shown that LHCSR is necessary for quenching in the green alga Chlamydomonas reinhardtii. Homologs of this stress-response protein are present in many photosynthetic eukaryotes, even those with chloroplasts that are of a red algal origin, but LHCSR is absent from terrestrial vascular plants. Likewise, PSBS is necessary for quenching in the plant Arabidopsis thaliana, but PSBS is only found in the green lineage of photosynthetic organisms. The molecular mechanism of both of these proteins is still unknown.This work describes the relationship between protein structure and quenching function in the stress response protein LHCSR. I show the distinct contributions to non-photochemical quenching from each of the two LHCSR protein isoforms, LHCSR1 and LHCSR3, in C. reinhardtii. I also contributed to the identification of three lumen-exposed residues in LHCSR3 that are required for quenching function. By using directed mutagenesis and expression in heterologous and native organisms, this shows that while LHCSR is necessary for nearly all non-photochemical quenching in its native cellular environment within C. reinhardtii, it is not sufficient to activate quenching in plants when expressed heterlogously. By altering some of the pigment-binding sites within LHCSR3, however, the heterologously expressed mutant LHCSR3 can activate additional quenching in a plant system.This also explores possible roles of the PSBS protein from C. reinhardtii, which has not previously been shown to have a function. Here, I present data showing that the algal PSBS protein can enhance non-photochemical quenching when expressed heterologously in plants. Since we do not see the same behavior in the alga, if PSBS is expressed in C. reinhardtii it most likely plays a different role or is not sufficient to induce quenching alone. Heterologous co-expression of both LHCSR and PSBS from C. reinhardtii results in enhanced quenching in the tobacco, Nicotiana benthamiana. The enhanced quenching phenotype is dependent upon PSBS being pH-sensitive. While no evidence has yet been obtained showing an interaction between PSBS and LHCSR in C. reinhardtii, this motivates further exploration.In order to better understand the evolution of photoprotective mechanisms employed by photosynthetic eukaryotes, I have also contributed to a genome annotation project aimed at establishing the marine heterokont alga, Nannochloropsis oceanica CCMP 1779, as a new model alga for studying photosynthesis. I identified three potential stress-response LHCs in the organismâ€™s genome based on sequence homology, whose functionality and expression characteristics are being investigated by other researchers.",ucb,,https://escholarship.org/uc/item/03g1f1pc,,,eng,REGULAR,0,0
879,2315,Targeted maximum likelihood estimation of treatment effects in randomized controlled trials and drug safety analysis,"Moore, Kelly","van der Laan, Mark J;",2009,"In most randomized controlled trials (RCTs), investigators typically rely on estimators of causal effects that do not exploit the information in the many baseline covariates that are routinely collected in addition to treatment and the outcome.   Ignoring these covariates can lead to a significant loss is estimation efficiency and thus power.  Statisticians have underscored the gain in efficiency that can be achieved from covariate adjustment in RCTs with a focus on problems involving linear models.  Despite recent theoretical advances, there has been a reluctance to adjust for covariates based on two primary reasons; 1) covariate-adjusted estimates based on non-linear regression models have been shown to be less precise than unadjusted methods, and, 2) concern over the opportunity to manipulate the model selection process for covariate adjustment in order to obtain favorable results.  This dissertation describes statistical approaches for covariate adjustment in RCTs using targeted maximum likelihood methodology for estimation of causal effects with binary and right-censored survival outcomes.Chapter 2 provides the targeted maximum likelihood approach to covariate adjustment in RCTs with binary outcomes, focusing on the estimation of the risk difference, relative risk and odds ratio.  In such trials, investigators generally rely on the unadjusted estimate as the literature indicates that covariate-adjusted estimates based on logistic regression models are less efficient. The crucial step that has been missing when adjusting for covariates is that one must integrate/average the adjusted estimate over those covariates in order to obtain the population-level effect.  Chapter 2 shows that covariate adjustmentin RCTs using logistic regression models can be mapped, by averaging over the covariate(s), to obtain a fully robust and efficient estimator of the marginal effect, which equals a targeted maximum likelihood estimator. Simulation studies are provided that demonstrate that this targeted maximum likelihood method increases efficiency and power over the unadjusted method, particularly for smaller sample sizes, even when the regression model is misspecified.Chapter 3 applies the methodology presented in Chapter 3 to a sampled RCT dataset with a binary outcome to further explore the origin of the gains in efficiency and provide a criterion for determining whether a gain in efficiency can be achieved with covariate adjustment over the unadjusted method.  This chapter demonstrates through simulation studies and the data analysis that not only is the relation between $R^2$ and efficiency gain important, but also the presence of empirical confounding. Based on the results of these studies, a complete strategy for analyzing these type of data is formalized that provides a robust method for covariate adjustment while protecting investigators from misuse of these methods for obtaining favorable inference.Chapters 4 and 5 focus on estimation of causal effects with right-censored survival outcomes.  Time-to-event outcomes are naturally subject to right-censoring due to early patient withdrawals.  In chapter 4, the targeted maximum likelihood methodology is applied to the estimation of treatment specific survival at a fixed end-point in time.  In chapter 5, the same methodology is applied to provide a competitor to the logrank test.  The proposed covariate adjusted estimators, under no or uninformative censoring, do not require any additional parametric modeling assumptions, and under informative censoring, are consistent under consistent estimation of the censoring mechanism or the conditional hazard for survival.  These targeted maximum likelihood estimators have two important advantages over the Kaplan-Meier and logrank approaches; 1) they exploit covariates to improve efficiency, and 2) they are consistent in the presence of informative censoring.  These properties are demonstrated through simulation studies.Chapter 6 concludes with a summary of the preceding chapters and a discussion of future research directions.",ucb,,https://escholarship.org/uc/item/03g5f6mc,,,eng,REGULAR,0,0
880,2316,"Developing the Country: ""Scientific Agriculture"" and the Roots of the Republican Party","Ron, Ariel","Einhorn, Robin L.;",2012,"This dissertation examines the emergence and political significance of the antebellum agricultural reform movement in order to investigate how economic change structured party realignment in the decade before the Civil War. It focuses attention on a critical yet almost ignored constituency of the period, northeastern farmers, showing why they would steadfastly support a Republican Party typically associated with manufacturers. Second, it uncovers the roots of one of our most powerful and enduring special interest groups--the agricultural lobby--demonstrating its powerful impact on federal policy as early as the antebellum period. It thus sheds new light on the causes of sectional conflict and on the course of American state development in the 1800s. At midcentury the rural Northeast faced a four-fold challenge: (1) depleted soils resulting from over-cropping; (2) western competition in grains; (3) steady out-migration; and (4) increasingly virulent pest infestations. Agricultural reformers responded by arguing for a modernized ""scientific agriculture"" that would reinvigorate the northeastern countryside. The new farming would be intensive, sustainable, and profitable, its practitioners both market and technology savvy. In order to offset western advantage in grains, reformers urged northeastern farmers to specialize in hay, wool and perishables for nearby urban centers. In order to increase production, they urged the adoption of commercial fertilizers, rational bookkeeping practices, and other innovations. I argue that as northeastern farmers shifted toward more capital intensive crop production for domestic markets, they forged an alliance with nascent American manufactures. Ideologically, this alliance was sustained by a vision of mutual reciprocity between town and country that promised rural modernization within a rubric of overall national growth. Practically, its substance was state aid for domestic economic development. Agricultural reformers lobbied vigorously for federal institutions such as land grant colleges and the Department of Agriculture while manufacturers demanded a protective tariff. Such claims on the federal government brought both groups into increasing conflict with southern slaveholders, who feared that any expansion in federal domestic functions portended danger for slavery. Consequently, agricultural reformers and manufacturers were drawn into the Republican Party's antislavery cause as a way to break southern power in Washington.Based on print and manuscript sources from across the Northeast, the dissertation integrates histories of party politics, commercial agriculture, education, the environment, and science and technology, to show how rural northeasterners organized themselves in order to demand that state and national governments help them prosper in a rapidly changing economy. These demands not only influenced the immediate course of American politics toward the Civil War, but helped define long-term processes of state formation by initiating a matrix of state and federal agencies that by the early twentieth century reached into virtually every rural county in the country.",ucb,,https://escholarship.org/uc/item/05j34575,,,eng,REGULAR,0,0
881,2317,Engineering Scalable Combinational Logic in Escherichia coli Using Zinc Finger Proteins,"Holtz, William Joseph","Keasling, Jay D;Maharbiz, Michel M;",2011,"Available to synthetic biologists are a wide range of genetic devices. Many of these devices are able to either sense or alter local conditions. The ability to sense a multitude of inputs combined with diverse outputs could enable engineered organisms that interact with their environment in new and complex ways. Currently the complexity of such systems has been limited by our ability to integrate several inputs into a desired output. Simple combinational logic functions, containing 1 to 3 logic gates, have been constructed in Escherichia coli, but more complex logic networks are needed to fully exploit the opportunities presented by these sensors and actuators. Use of genetic logic gates is constrained by the specific molecular interactions that are used to implement each gate. These interactions involve diffusible molecules that can move within the cytoplasm of the cell and therefore are not spatially separated from other gates. To make larger logic blocks, sets of gates that use unique molecular interactions with minimal crosstalk are required.Zinc finger proteins (ZFPs) can be used to predictably create a large number of unique protein-DNA interactions. These proteins can then be used to build transcriptional activators or repressors in E. coli, but these methods are not well defined.  Attempts at using ZFPs to make one-hybrid transcriptional activators have failed to give a fold activation of higher than 2. ZFP repressors based on steric hindrance of RNA polymerase performed better with fold repressions values of up to 300. The positional dependence of the ZFP operator site within the promoter was investigated, and both position and dissociation constant were found to play important roles in determining the level of repression. ZFP based repressors without cooperativity cannot be used to create logic gates. A new inverter topology using both ZFP based repressors and sRNA was designed. This topology uses a reference promoter to set the switching threshold of the gate. There are no cooperative interactions in this topology, but the maximum slope of the transfer function is similar to a Hill-equation with a coefficient of 10. The high slope and excellent transfer function of these gates make them robust to many types of parameter variation and noise.A set of 27 validated ZFP repressors and 27 promoters with ZFP operator sites were created and tested for non-orthogonal interactions. A sub-set of 5 repressor-promoter pairs were found to have a high degree of orthogonality where the cognate pairs resulted in more than 73% attenuation of the promoter and non-cognate pairs gave less than 19% attenuation.  The ZFPs and promoter used in this task were far from optimal and these attenuation values could readily be improved.The combination of these orthogonal repressor-promoter pairs and the new logic gate topology should enable more logic gates to be implemented in a single E. coli cell.",ucb,,https://escholarship.org/uc/item/05j372zt,,,eng,REGULAR,0,0
882,2318,"Fluid Hegemony: A Political Ecology of Water, Market Rule, and Insurgence at Bangalore's Frontier","Ranganathan, Malini","Roy, Ananya;",2010,"Since the turn of the millennium, the city of Bangalore (officially Bengaluru) has experimented with a series of neoliberal, market-oriented reforms to overhaul the institutional, pricing, and financial aspects of its urban services. The city's water, in particular, long considered a service that must be subsidized by the state, has been targeted under interventions that seek to commercialize, rationalize, and privatize delivery, while simultaneously deepening certain forms of regulatory oversight. Supported by a melee of international development actors and administered by state-level experts, these policy changes have been especially focused on reforming the city's outskirts, where unruly growth and conflicting governance arrangements have produced highly differentiated patterns of land tenure and water access. Analysis of why certain water policy imaginaries are ascendant today, how programs of reform are conceived of and by whom, and to what ends they proceed in this dynamic peri-urban frontier landscape is essential to an understanding of metropolitanization in the Global South more generally.   Yet, critical scholarship on Bangalore and elsewhere has largely neglected the articulations between new infrastructure policies and the local politics of peri-urban frontiers. This dissertation addresses this gap through a multi-sited ethnographic study of policy-making and practice affecting the governance of drinking water at Bangalore's peripheries. It investigates two case studies: (1) a program that sought to reengineer municipal management of citizen complaints, and (2) a project that aimed to extend piped water to residents financed, in part, through the debt market and upfront cash contributions from peripheral residents. The study draws on archival research, in-depth interviews, and participant observation to interrogate the underlying logic, material significance, and political contestation surrounding these projects in the peripheral localities of Bommanahalli, Byatarayanapura, and KR Puram.  This research argues, first, that reforms derive not only from the fiscal concerns of the current moment, but also from a preoccupation with disciplining the conduct of local government and citizens in line with market principles. Crucially, this is a regime of rule that is deeply inflected with the enduring legacies of state-led development. It is the concoction of the developmental approaches of yesterday and the pro-market approaches of today that marks the homegrown logic of neoliberalism in India. Second, for all the rhetoric purporting to have overcome the incompetencies of a previous era, today's neoliberal interventions are marked by profound contradictions and limitations. A narrow focus on financial criteria, for instance, has resulted in a disconnect between the promise and material reality of water several years after new market-based policies and programs were instituted. Third, the grounded workings of new water policies cannot be understood without grasping the politics of the peripheralized middle class--a sizeable cross-section of the middle class that is propertied, but that nonetheless does not enjoy the same degree of tenure security as its elite counterparts. Through collective organizing, members of this social grouping contest and ultimately compromise over cost recovery-focused water pricing policies in order to legitimate their property claims in this globalizing and increasingly exclusionary city. The research thus reveals that the outcomes of water policies are contingent on historical geographies of struggle and insurgent claims to space in the places where they unfold. Overall, this dissertation elucidates how neoliberal hegemony in the urban waterscape is fluid and mutable--prone to becoming imbued with a diverse set of interests and to taking shape in and through a terrain of citizenship politics.These findings, relevant for other rapidly urbanizing regions of the world, point to the need to rethink urban water praxis such that it valorizes diverse ways of knowing, anticipates the points of friction between water and spatial policies, and is sensitive to how and why institutionalized top-down schemes actually take root at the grassroots level.",ucb,,https://escholarship.org/uc/item/05m1k2fs,,,eng,REGULAR,0,0
883,2319,Synthesis and Magnetic Properties of Two-Coordinate Transition Metal Complexes,"Bunting, Philip Colin","Long, Jeffrey R;",2019,"Chapter 1 of this dissertation provides an introduction to the field of single-molecule magnetism through the lens of two-coordinate transition metal complexes. Single-molecule magnets are a class of materials which display magnetic properties, such as magnetic hysteresis, typically only observed for bulk materials. Given their small size, with many single-molecule magnets utilizing only one magnetic ion, these materials are also subject to quantum effects. The marriage of classical and quantum properties provides intriguing possibilities for the applications of these materials. From the perspective of basic research, they provide the challenge of controlling the electronic structure of these molecules down to the magnetic microstate level. Two-coordinate transition metal complexes provide excellent insight to the field as a whole, as it is possible to understand the relationships between molecular structure, electronic structure, and magnetic microstate structure and mixing. By understanding their electronic structure and magnetic anisotropy, it is also possible to gain insight into the mechanisms by which magnetic relaxation occurs.Chapter 2 and Chapter 3 are closely related. Calculations on a hypothetical complex which featured a linear Câ€“Coâ€“C moiety, Co(C(SiMe3)3)2, predicted a magnetic anisotropy near the physical limit (determined by the Co(II) spin-orbit coupling constant) arising from a non-Aufbau electronic structure, (dx2â€“y2, dxy)3(dxz, dyz)3(dz2)1. These calculations ignited a synthetic endeavor to isolate the first dialkyl Co(II) complex. Chapter 2 details the first success in this endeavor with the isolation of Co(C(SiMe2OPh)3)2. However, long-range CoÂ·Â·Â·O interactions led to a significantly bent Câ€“Coâ€“C axis, and thus Co(C(SiMe2OPh)3)2 behaved as an unremarkable single-molecule magnet. Other complexes of the type M(C(SiMe2OPh)3)2 (M = Cr, Mn, Fe, Zn) were synthesized in an effort to understand the deviation from linearity. Ultimately the bend in the in the Câ€“Coâ€“C axis arises from a compromise of several stabilizing forces. The ligand field stabilization energy is relatively weak and interligand non-covalent interactions are essential for the stability of the complexes; when those interligand interactions are not sufficiently strong, the metal moves closer to the nearby oxygen atom for additional stabilization. Though this metal-oxygen distance is longer than an actual metal-oxygen bond, the interaction is sufficient to both stabilize the molecule and destroy the magnetic anisotropy expected from the linear moiety.Chapter 3 details the end result of a search for a ligand which would support a linear Câ€“Coâ€“C moiety. By moving from phenyl to naphthyl substituents, the interligand non-covalent interactions were greatly enhanced. The complex, Co(C(SiMe2ONaph)3)2, exhibits a non-Aufbau ground stateâ€”an unprecedented electronic structure for a transition metal moleculeâ€”which arises from an extremely weak and high symmetry ligand field. The electronic structure is confirmed by dc magnetic susceptibility, ab initio calculations, and experimental charge density maps. Additionally, the electronic structure has the maximal orbital angular momentum for a transition metal complex, a property that was only recently observed in cobalt adatoms and is novel for a molecule. Due to the unquenched orbital angular momentum this molecule displays magnetic anisotropy that is near a physical limit for transition metals. The spin-reversal barrier, determined by a combination of variable-field far-IR spectroscopy and ac magnetic susceptibility, is the largest spin-reversal barrier (450 cmâˆ’1) for any transition metal containing molecule.Chapter 4 provides the beginning of a new synthetic endeavor for linear transition metal complexes. Magnetic anisotropy is typically limited by the spin-orbit coupling constant of the constituent magnetic ions. For mononuclear transition metal complexes, metal-ligand covalency nearly always diminishes magnetic anisotropy compared to the free-ion values. One possible exception to this trend is through the use of heavy ligands (i.e. ligands of the 4p, 5p, etc. rows), which have been shown to enhance magnetic anisotropy. Thus, complexes of the type [Fe(SiR3)2]0/âˆ’ were targeted. While no such complex was successfully synthesized, there are several promising leads in this direction. Specifically, the novel ligands [Si(carbazole)3]âˆ’ and [Si(2,7-dimethylcarbazole)3]âˆ’ provide several interesting new structures, including a new two-coordinate zinc complex, Zn(Si(carbazole)3)2.",ucb,,https://escholarship.org/uc/item/05r034mm,,,eng,REGULAR,0,0
884,2320,"Historical Biogeography of Sumatra and Western Archipelago, Indonesia: Insights from the flying lizards in the genus Draco (Iguania: Agamidae)","Lawalata, Shobi","McGuire, Jimmy A;",2011,"The island arc west of the island of Sumatra in Indonesia, here referred to as the Western Archipelago, is home to many endemic flora and fauna.  Despite their importance in the biogeographic theater of insular Southeast Asia, little scientific attention has been given to these islands, with the exception of the four islands that comprise the Mentawai group.  In this dissertation, I used the evolutionary history of the flying lizards in the genus Draco to elucidate the biogeographical history of Western Archipelago relative to its neighboring mega-island Sumatra.  In Chapter 1, I provide an updated checklist of the herpetofauna of the islands in the archipelago--a list that had not been revisited or updated in the last 20 years.  My visit to the islands of Western Archipelago proved to add considerably to our knowledge of the herpetofauna occurring in the area.  In Chapter 2, I present a revision of the molecular phylogeny of the genus Draco by incorporating sequence data from nuclear markers.  And finally, in Chapter 3 I looked at the phylogenetics and population genetics of the most widely distributed species of flying lizards in Sunda Shelf--Draco sumatranus the common flying lizards--to discern the historical process by which they colonized the islands of the Western Archipelago.  Using one mitochondrial locus and nine nuclear loci, I employed phylogenetic and coalescent-based population genetic methods to reconstruct the evolutionary history of Draco sumatranus.  My results suggest that the islands of Simeulue, Nias, Siberut, Sipora, North & South Pagai and Enggano are monophyletic, but the Batu and Banyak Islands themselves are more closely related to Northwest Sumatran populations.  This divergence is inferred to have occurred ~550,000 years ago.  These findings reject the hypothesis of independent overwater dispersal onto each island, and support the hypothesis that the Western Archipelago had been colonized via the Batu and Banyak Islands and was subsequently isolated by a vicariant event--most likely related to the Pleistocene changes in sea levels.  I also uncovered deep divergences of Sumatran D. sumatranus populations that cannot be adequately explained simply by the emergence of the Sunda Shelf basin during the last glacial maxima, or the modern-day geography of the island.  This hints at the cryptic diversity harbored within Sumatra, and merits a more rigorous study of the island's biogeography.",ucb,,https://escholarship.org/uc/item/05v6062g,,,eng,REGULAR,0,0
885,2321,Theoretical Investigation of Binary Eutectic Alloy Nanoscale Phase Diagrams,"Boswell-Koller, Cosima Nausikaa","Chrzan, Daryl C;",2012,"Recently, embedded binary eutectic alloy nanostructures (BEANs) have drawn some attention. A previously calculated equilibrium structure map predicts four possible nanocrystal alloy morphologies: phase-separated, bi-lobe, core-shell and inverse coreshellgoverned by two dimensionless interface energy parameters. The shape of the bilobe nanoparticles is obtained by nding the surface area of all interfaces that minimizes the overall energy, while also maintaining mechanical equilibrium at the triple point.Two representative alloy systems displaying eutectic phase diagrams and negligible solid solubility were chosen: GeSn and AuGe. GeSn samples were prepared by sequential implantation of Ge and Sn into SiO2. AuGe samples were prepared by implanting Ge within Au-doped silica lms. Transmission electron microscopy images revealed bi-lobe nanocrystals in both samples. Therefore, the interface energies in both systems must be such that the dimensionless parameters lie in the region of bi-lobe stability.Careful analysis of the bi-lobe structure leads to the determination of two dimensionless length scales, which describe the bi-lobe independent of the size of the nanoparticle. These two parameters, eta 1 and eta 2 can be used to calculate contours of equal eta 1 and eta 2 over the entire range of bi-lobe stability. Experimental measurement and comparison to predicted structures leads to determination of acting dimensionless interface energies. Experimentally available wetting data is then used to calculate the remaining interface energies in the system. gamma Ge(s)/SiO2 was found to be between 0.82-0.99 J/m2 . gamma Ge0.22Sn0.78(l)/SiO2 and gamma Au0.53Ge0.47(l)/SiO2 are determined to be 1.20 and 0.94 J/m2 , respectively.To investigate the possibility of size eects at the nanoscale, size dependent phase diagrams for the AuGe and GeSn system are determined. This is done by the theoretical approach rst outlined byWeissmueller et al., which takes into account the energy contribution of the various morphologies listed above. Results from this calculation are compared to those using the tangent line construction approach. The composition dependent surface energies of binary alloy liquids required in this calculation are determined using Butler's equation.",ucb,,https://escholarship.org/uc/item/05x231mf,,,eng,REGULAR,0,0
886,2322,"The Logic of Violence in Criminal War: Cartel-State Conflict in Mexico, Colombia, and Brazil","Lessing, Benjamin","Collier, David;Powell, Robert;",2012,"Why do drug cartels fight states? Episodes of armed conflict between drug cartels and states in Colombia, Brazil and Mexico have demonstrated that `criminal wars' can be just as destructive as civil wars. Yet insurgents in civil wars stand a reasonable chance of winning formal concessions of territory or outright victory. Why fight the state if, like drug cartels, you seek neither to topple nor secede from it?Equally puzzling are the divergent effects of state crackdowns. Mexico's militarized crackdown in 2006 was intended to quickly break up the cartels and curtail incipient inter-cartel and anti-state violence; five years later, splintered cartels are an order of magnitude more violent, with over 16,000 homicides and 600 of attacks on army troops in 2011 alone. Conversely, in Rio de Janeiro, a massive November 2010 invasion by state forces of a key urban zone that had been under cartel dominion for a generation failed to produce the grisly bloodbath that even the government's defenders predicted. Instead, it heralded what appears to be a decisive shift by cartels away from confrontation. Why do some crackdowns lead to violent blowback, while others successfully curtail cartel-state conflict?The key to both puzzles lies in a fundamental difference between cartel-state conflict and civil war. Cartels turn to anti-state violence, not, as in civil war, in hopes of conquering mutually prized territory or resources, but to influence state policy. Like many interest groups, cartels expend resources to influence policy, usually acting at the level of policy enforcement, through corruption, but sometimes also at the level of policy formation, through lobbying. Yet licit interest groups are not targeted for destruction by the state, and generally possess no means of physical coercion. Cartels always face some level of state repression, but fighting back usually provokes even greater repression. Often, this leads them to `hide' rather than `fight', using anonymity and bribes to minimize confrontation; under certain conditions, though, violence may seem the best pathway to policy influence. The decision to turn to violent forms of policy influence is thus highly sensitive to what the state is doing; shifts in state policy, especially crackdowns, can trigger sharp variation in cartel-state conflict.This study first distinguishes the logics of  violent corruption and  violent lobbying, as well as dynamics deriving from turf war among cartels, then identifies the conditions that make each logic operative. Violent corruption---epitomized by drug lord Pablo Escobar's infamous phrase ""plata o plomo?"" (bribe or bullet?)---is central; it occurs, in all three cases, prior to and with greater consistency than violent lobbying or other mechanisms. States face a dilemma: they cannot crack down on traffickers without inadvertently giving corrupt enforcers (police, judges, etc.) additional leverage to extract bribes. A formal model of bribe negotiation illustrates the cartel's choice: simply pay the larger bribe, or use the threat of violence to intimidate enforcers and reduce the equilibrium bribe demand. The central finding is that blanket crackdowns in a context of widespread corruption can  increase cartels' incentives to fight back, whereas more focused crackdowns that hinge on cartel behavior induce non-violent strategies. Conditionality of repression--the degree to which repressive force is applied in proportion to the amount of violence used by cartels--is thus a critical factor behind the divergent response of cartels to crackdowns across cases. A move toward conditional crackdowns occurred both in Colombia, after Escobar's demise and the fragmenting of the drug market, and in Rio de Janeiro, with its innovative `pacification' strategy. In both cases, cartels have shifted away from confrontation and toward non-violent `hiding' strategies. In Mexico, by contrast, the state has insisted on pursuing all cartels without distinction, leading to sharp increases in cartel-state violence.Other, less central logics help explain contrasting modalities of cartel violence. 'Violent lobbying', in the form of narco-terrorism and direct negotiation with state leaders, is dramatic and chilling, but only makes strategic sense when there is an open policy question that cartels can realistically hope to influence. Moreover, if the benefits of policy change are 'public' or non-excludable, violent lobbying is subject to the free-rider problem, and only likely to occur if cartels can cooperate. Thus violent lobbying has been intense in Colombia, where cartels were initially united and extradition remained an open policy question for a decade; occasional in Brazil where a dominant cartel uses it to influence carceral policy, and relatively rare in Mexico, where cartels are fragmented and the president's high-profile `ownership' of his crackdown creates overwhelming audience costs to policy change.Inter-cartel turf war is far more intense in Mexico than elsewhere, driving logics of reputation-building and false-flag attacks, and contributing to the prominence of `propagandistic' violence like mutilation and `narco-messages'. These turf-war dynamics are reinforced by the government's kingpin strategy and its splintering of the cartels. Moreover, fragmentation has a general-equilibrium effect on the maximum pressure the state can apply to any one cartel, given its unconditional approach. This further reduces the sanction cartels face for using violence, and drives the escalatory spiral presently gripping Mexico.The study concludes by asking why leaders do or do not adopt conditional strategies. Even when leaders would like to do so, they face both 'logistical constraints' arising from low capacity and fragmented security institutions, and 'acceptability constraints' deriving from the negative optics of `going easy' on less violent cartels (a necessary component  of conditional repression). Case evidence helps identify political circumstances that minimize these constraints. Coalitions or partisan hegemony can mitigate institutional fragmentation, while the 'Nixon-Goes-to-China' effect allows leaders perceived as hardliners to overcome acceptability constraints, particularly if they present conditionality as a tactical, operational imperative.",ucb,,https://escholarship.org/uc/item/03m9r44h,,,eng,REGULAR,0,0
887,2323,Keeping Home: Another Look at Domesticity in Antebellum America,"Chandler, Linda Lee","Snyder, Katherine;",2011,"In the eighteenth century and earlier, domesticity functions as the practice of housekeeping. During the American Revolution, republican motherhood set the stage for the concept of domesticity to expand.  By the mid-nineteenth century, domesticity becomes connected to womanhood as it develops into a virtue that defines true womanhood. Domesticity also operates as an ideology in the nineteenth century. Are we to assume that all three are identical?  Most critics would have us believe so, as they do not differentiate amongst these domesticities.  In fact, most critics take the definition of domesticity for granted, leaving the term undefined and assuming their audience knows what it means.  The criticism on domesticity to date has involved debates over the bifurcation of separate spheres, and whether domesticity operates as resistance to patriarchal authority or as conformity to the status quo.  My project does not take sides in these debates, but rather focuses on two under explored aspects of domesticity:  practice and virtue. What does it mean for a practice to become a virtue, how does it happen, what are the implications, and how are these two meanings related to the current critical notion of domesticity as an ideology?  My project responds to these questions.  With the reformulation of domesticity as a virtue, domesticity would seem to be no longer a task anyone can master, but an intrinsic quality that one may or may not possess.  As a virtue rather than a practice, domesticity becomes an issue of character, raising the questions: can it be developed and by anyone regardless of race, class, or gender?  In addition, as an issue of character, domesticity is no longer grounded in an external location, such as the home, but in the person.  What are the implications of this shift?  Using an array of texts, I examine the construction of domesticity as a virtue, the cultural significance of the transformation of domesticity from practice to virtue, and the implications for identity. My dissertation analyzes how representations of home, domestic practice, and family in a selection of texts from the long nineteenth century reconfigure current critical notions of domesticity.  By investigating the distinctions between practice and virtue, my project expands the critical dialogue on domesticity, and in the process, deepens contemporary discussions of identity and the value of home and family in antebellum American literature and culture.",ucb,,https://escholarship.org/uc/item/03p395v0,,,eng,REGULAR,0,0
888,2324,Minimum Probability Flow Learning: A New Method For Fitting Probabilistic Models,"Battaglino, Peter","Deweese, Michael R;",2014,"Fitting probabilistic models to data is often difficult, due to the general intractability of the partition function and its derivatives. In this dissertation we propose a new parameter estimation technique that does not require computing an intractable normalization factor or sampling from the equilibrium distribution of the model. This is achieved by establishing dynamics that would transform the observed data distribution into the model distribution, and then setting as the objective the minimization of the KL divergence between the data distribution and the distribution produced by running the dynamics for an infinitesimal time. Score matching, minimum velocity learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate parameter estimation in Ising models, deep belief networks and an independent component analysis model of natural scenes. In the Ising model case, current state of the art techniques are outperformed by at least an order of magnitude in learning time, with lower error in recovered coupling parameters.",ucb,,https://escholarship.org/uc/item/03p3t5x3,,,eng,REGULAR,0,0
889,2325,Geometric Algorithms for Cleanability in Manufacturing,"Yasui, Yusuke","McMains, Sara;",2011,"This thesis describes geometric algorithms to check the cleanability of a design during the manufacturing process. The automotive industry needs a computational tool to determine how to clean their products due to the trend of miniaturization and increased geometric complexity of mechanical parts. A newly emerging concept in a product design, Design-for-Cleanability, necessitates algorithms to help designers to design parts that are easy to clean during the manufacturing process. In this thesis, we consider cleaning using high-pressure water jets to clean off the surfaces of workpieces. Specifically, we solve the following two problems purely from a geometric perspective: predicting water trap regions of a workpiece and finding a rotation axis to drain a workpiece. Finding an orientation that minimizes the potential water trap regions and/or controls their locations when the workpiece is fixtured for water jet cleaning is important to increase the cleaning efficiency. Trapped water leads to stagnation areas, preventing efficient flow cleaning. Minimizing the potential water trap also reduces the draining time and effort after cleaning. We propose a new pool segmentation data structure and algorithm based on topological changes of 2D slices with respect to the gravity direction. Then, we can quickly predict potential water trap regions of a given geometry by analyzing our directed graph based on the segmented pools. Given a workpiece filled with water after cleaning, to minimize the subsequent drying time, our industrial partner first mounts workpieces on a slowly rotating carrier so that gravity can drain out as much water as possible. We propose an algorithm to find a rotation axis that drains the workpiece when the rotation axis is set parallel to the ground and the workpiece is rotated around the axis. Observing that all water traps contain a concave vertex, we solve our problem by constructing and analyzing a directed ""draining graph"" whose nodes correspond to concave vertices of the geometry and whose edges are set according to the transition of trapped water when we rotate the workpiece around the given axis. We first introduce an algorithm to test whether a given rotation axis can drain the workpiece. We then extend these concepts to design an algorithm to find the set of all rotation axes that drain the workpiece. If such a rotation axis does not exist, our algorithm will also detect that. To the best of our knowledge, our work is the first to tackle the draining problem and to give an algorithm for the problem.",ucb,,https://escholarship.org/uc/item/03t6c02q,,,eng,REGULAR,0,0
890,2326,Germanium-Source Tunnel Field Effect Transistors for Ultra-Low Power Digital Logic,"Kim, Sung Hwan","King Liu, Tsu-Jae;",2012,"Driven by  a strong demand for mobile and portable electronics, the chip market will undoubtedly impose ""low power"" as the key metric for microprocessor design. Although circuit and system level methods can be employed to reduce power, the fundamental limit in the overall energy  efficiency of a system is still rooted in the Metal-Oxide-Semiconductor Field Effect Transistor (MOSFET) operating principle and its immutable physics: an injection of thermally distributed carriers will not allow for switching characteristics better than 60 mV/dec at room temperature. This constraint ultimately defines the lowest energy consumed per digital operation attainable with current Complementary-Metal-Oxide-Semiconductor (CMOS) technology.  In this work, Tunnel Field Effect Transistor (TFET) based on Band-to-Band Tunneling (BTBT) will be proposed and investigated as an alternative logic switch which can achieve  steeper switching characteristics than the MOSFET to permit for lower threshold (V TH ) and supply voltage (V DD ) operation. It will be experimentally demonstrated that by employing Germanium (Ge) only in the source region of the device, a record high on to off current  ratio (I ON /I OFF ) can be obtained for 0.5 V operation. Technology Computer Aided Design  (TCAD) calibrated to the measured data will be used to perform design optimization study. The performance of the optimized Ge-source TFET will be benchmarked against CMOS technology to show greater than 10x improvement in the overall energy efficiency for frequency range up to 500 MHz. The fundamental challenges associated with TFET-based digital logic design will be addressed. In order to mitigate these constraints, a circuit-level solution based on n-channel TFET Pass-Transistor Logic (PTL) will be proposed and demonstrated through mixed-mode simulations. The accompanying design modifications required at the device level will be discussed.",ucb,,https://escholarship.org/uc/item/0616k5df,,,eng,REGULAR,0,0
891,2327,"Northern Plains Borders and the People In Between, 1860-1940","Hagen, Delia Lee","Klein, Kerwin L;Delay, Brian;",2015,"Northern Plains Borders and the People in Between is a transnational history of colonialism and mixed, mobile indigenous people in the borderlands of the northern Great Plains from 1860 to 1940. Based on archival documents from Canada and the United States, it focuses on social, spatial, political and legal developments.  It demonstrates that when American and Canadian militaries invaded, they relied on and targeted mixed indigenous communities.  Members of these communities were affiliated with tribes across the region, and moved often and far.  As they mixed and moved, they were involved in the many different conflicts that wracked the Northern Plains after 1860, and they physically linked period violence in Canada and the United States. Subsequently, both countries incorporated Plains inhabitants through Indian treaties and state status categories that created mutually-exclusive, spatialized legal classificationsâ€”American or Canadian, Sioux or Cree, MÃ©tis, Indian, citizen, or alien.  These classifications conveyed different rights, and status and rights were tightly tied to particular places, like homesteads, or nations or specific Indian reservations.  Oneâ€™s legal status thus had direct material implications, linking boundaries of race, place, tribe, and band to land.  On both sides of the international line, these social and spatial borders criminalized mixture and mobility. With the concurrent spread of allotment and tribal enrollment, many borderlands indigenes were left statelessâ€”they were excluded from every legal category through which Canada and the U.S. allocated status and rights.  This study shows how statelessness flowed through prior racial, tribal and spatial classificationsâ€”like enrolled member of the U.S. Turtle Mountain Chippewa Indians.  It wasnâ€™t just the international boundary that created indigenous statelessness, but the multi-faceted and layered boundary-making of settler colonialism.  For indigenous people, tribal membership boundaries, or enrollment, became the most significant aspect of allotment, both in terms of land loss and in terms of enduring community consequences. This dissertation concludes that statelessness originated not in overseas imperialism but in the earlier colonization of the continent. It also finds that the most critical implications of statelessness were material: stateless indigenes were not just landless, or homeless, but worseâ€”their mere presence was forbidden everywhere.  Legally, they had the right to occupy no place, no space.  In this context, people contested their statelessness, pursuing legal status, rights and property into the 20th century.  This study maps that ongoing political activity and associated mobility, revealing enduring indigenous geographies in a period when Indian people have been considered politically inactive, and reservation-bound.  It shows how, into the 1940s, indigenous mixture and movement entwined Canadian and American histories, making them not just parallel but inseparable. Ultimately, it engages discussions of space, power, violence, law and the state as they relate to histories of borderlands, frontiers, and the West, Native Americans, First Nations, immigration and race.",ucb,,https://escholarship.org/uc/item/0626r3kx,,,eng,REGULAR,0,0
892,2328,"The Practice and Politics of Children's Music Education in the German Democratic Republic, 1949-1976","Timberlake, Anicia","Taruskin, Richard;",2015,"This dissertation examines the politics of childrenâ€™s music education in the first decades of the German Democratic Republic. The East German state famously attempted to co-opt music education for propagandistic purposes by mandating songs with patriotic texts. However, as I show, most pedagogues believed that these songs were worthless as political education: children, they argued, learned not through the logic of texts, but through the immediacy of their bodies and their emotions. These educators believed music to be an especially effective site for childrenâ€™s political education, as music played to childrenâ€™s strongest suit: their unconscious mind and their emotions. Many pedagogues, composers, and musicologists thus adapted Weimar-era methods that used mostly non-texted music to instill what they held to be socialist values of collectivism, diligence, open-mindedness, and critical thought. I trace the fates of four of these pedagogical practicesâ€”solfÃ¨ge, the Orff Schulwerk, lessons in listening, and newly-composed â€œBrechtianâ€ childrenâ€™s operasâ€”demonstrating how educators sought to graft the new demands of the socialist society onto inherited German musical and pedagogical traditions. 	I argue that this marriage of old ideas and new aims was often fraught, as pedagogues and even state officials proved reluctant to interrogate fully the possible political uses of an art form which, in the Romantic tradition of Hoffmann and Hanslick, was held to be autonomous and stubbornly intractable to worldly purposes. Accordingly, music education in German socialism proceeded from a vision of aesthetic education that privileged the freedom of the individualâ€”an ideal that proved loyal less to the beliefs of Soviet communism than to the fundamental ideologies of German liberalism. More generally, the dissertation shows how even the most fervently socialist didactic ideals and practices often broke down on the issue of music. Childrenâ€™s music pedagogy represents, then, a moment in which developing human bodies and beliefs can be held up against the â€œreal existing socialismâ€ that the GDR claimed to have achieved: it exposes both the ideological underpinnings of German socialism and the halting practical steps that educators took to build it.",ucb,,https://escholarship.org/uc/item/062971m6,,,eng,REGULAR,0,0
893,2329,Bad Seeds: Inhuman Poetics in Nineteenth-Century America,"Osborne, Gillian","Otter, Samuel;",2014,"Plants sprout, vegetate, flower, and molder pervasively across nineteenth-century American literature and yet, like most roadside weeds today, are largely ignored. My dissertation demonstrates that, far from mere stylistic ornamentation, this profusion of vegetation was a means of imagining literature and humanness as inhuman: responsive to otherness outside of texts as well as at the core of a composing subject. Applied to aesthetic agents and objects, plant metaphor unsettled more rhetorical claims during the period for genius or formal convention as self-contained or individuated. The inhuman poetics I trace reveals ways in which poetry in nineteenth-century America was defined not only by genre and print conventions, but also by attempts to make literature responsive to what stands outside of texts: nature, history, and experience. I show how, by directing attention to literary texture and to the extra-literary, plant metaphors model ways of dialectically thinking through the relationship between humans and nature. Although, like many cultural forms in nineteenth-century America, this poetics drew on sources outside of the United States (particularly works of German and British Romanticism), attention to plant-life in America was necessarily localized. This common attention engaged many of the most canonical authors of the day. I undertake immersive readings of plant-life across the careers of Dickinson, Thoreau and Melville to offer new insight into some of their more under-studied works and to deepen understandings of what ""poetry"" meant to each. My work contextualizes these readings by demonstrating intersections between these authors and Romantic theory and biology, popular botany in ante-bellum New England, and sentimental poetry on friendship and flowers. Relating my findings to contemporary debates about poetics and crises in the humanities and in the environment, I demonstrate how historical particularity sheds light not only on the past but also on present attempts to theorize poetry's relationship to the social and ecological.",ucb,,https://escholarship.org/uc/item/0629g1j0,,,eng,REGULAR,0,0
894,2330,Wetlands as Best Management Practices to Mitigate Agricultural Nonpoint Source Pollution,"Karpuzcu, Mahmut Ekrem","Sedlak, David;Stringfellow, William;",2012,"Technological advances in modern agriculture and the application of nitrogen-based synthetic fertilizers and manure to agricultural crops have increased crop yields and food production for the world's growing population. However, a significant portion of the applied nitrogen is in excess of crop needs. This results in leaching of nitrate into the groundwater and eutrophication of surface water systems via surface runoff. Agricultural pesticides are also required to maintain high levels of crop production.  As a result of their inherent toxicity, they have adverse effects on the environment when they leave agricultural systems. Wetlands have been offered as Best Management Practices (BMPs) for the treatment of return flows from irrigated agriculture. To investigate nitrate removal kinetics in wetlands receiving agricultural drainage, field studies were conducted and nitrate removal efficiencies were determined in three agricultural wetland sites (Chapter 2). Microcosm studies were conducted to supplement field data and to provide insight into removal kinetics. The results suggest that wetlands constructed for purposes of wildlife habitat can remove nitrate from irrigation return flows, but efficiencies are typically low, with nitrate mass removal efficiencies ranging from 23% to 35% in wetlands examined in this study. Modified areal first-order removal rate constants (k) determined for field sites varied between 4.0 and 12.1 cm d-1. Microcosm studies were used to supplement field studies and to determine saturation kinetics which was practically impossible to measure in the field. The first order nitrate removal rate for the microcosm (12.97 cm d-1) was approximately equal to the observed k-value from Ramona Lake, the source of the sediments used in the microcosm, suggesting the value of microcosms for estimating nitrate removal rates in wetlands. Measurement of saturation kinetics in the microcosm system showed that the apparent half-saturation constant (Km) and maximum removal rate (Jmax) were 43.8 mg L-1 and 4.1 g m-2 d-1 respectively, for these sediments. Estimates of land requirements for wetlands in one agricultural watershed indicated that less than 3 % of the watershed area would need to be devoted to wetlands to achieve an effluent nitrate concentration of 0.5 mg L-1, the target value for limiting the growth of nuisance algae. Chlorpyrifos is the most widely used organophosphate insecticide in California's San Joaquin Valley and is widely used elsewhere. While several prior studies evaluated the effectiveness of different best management practices (BMPs) for chlorpyrifos mitigation, these studies have mostly focused on sorption of chlorpyrifos to wetland sediments and soils with removal efficiency assessed by measuring inlet and outlet concentrations. To assess the long-term performance of wetlands, it is also important to know the ultimate fate of chlorpyrifos in wetland sediments. Specifically, particle-associated pesticides stored in the sediments can be transported via runoff and other processes to surface water systems. Three different phosphoesterase enzymes; phosphomonoesterase, phosphodiesterase and phosphotriesterase, are involved in chlorpyrifos biotransformation pathway; however, the link between these enzymes and chlorpyrifos biotransformation rates had not been previously addressed. The research presented in this dissertation demonstrated that wetland sites showed temporal and spatial variation in observed chlorpyrifos biotransformation rates, with half-lives ranging from 1 to 35 days under aerobic conditions (Chapter 3). Chlorpyrifos transformation slowed significantly under anaerobic conditions, with a half-life of approximately 92 days. Biodegradation rates decreased significantly in sediments from the Hospital Creek site during 2011 due to flooded conditions that preceded sample collection. These results suggest that allowing a wet-dry cycle can enhance the transformation rates of an organophosphate insecticide in these systems by providing aerobic conditions in sediments. The dry phase would encompass the non-irrigation season in late fall and winter, and the wetland would be flooded again in spring and summer when the irrigation season begins. There was significant correlation between phosphotriesterase activity and the chlorpyrifos biotransformation rates, with this relationship varying among sites. Phosphotriesterase activities may be useful as an indicator of biodegradation potential with reference to the previously established site-specific correlations.  In addition, kinetic parameters obtained in the laboratory studies were used to model agricultural non-point source pollution in California's San Joaquin River (SJR) watershed using a previously developed water quality model WARMF (Watershed Analysis Risk Management Framework) (Chapter 4). The results of the nitrate simulations suggest that a wetland area about 1.8% of the agricultural land in the Orestimba Creek watershed could significantly reduce nitrate concentrations supporting our results from Chapter 2. The results of the chlorpyrifos simulations underlined the importance of the management strategies to enhance the biotransformation rates. The scenario using enhanced biotransformation rates was more effective at reducing chlorpyrifos concentrations to values below regulatory limits compared to the 30% chlorpyrifos use reduction scenario. From a management perspective of view, use reduction should be implemented together with other best management practices if possible, especially in impaired surface waters.  Given the difficulty with completely eliminating organophosphate pesticide use in agriculture under present conditions, management strategies for enhanced organophosphate pesticide removal are of crucial importance in efforts to keep the organophosphate pesticide concentrations within regulatory limits.",ucb,,https://escholarship.org/uc/item/0631d4nr,,,eng,REGULAR,0,0
895,2331,Cosmogenic Activation of TeO2 in the Neutrinoless Double-Beta Decay Experiment CUORE,"Wang, Barbara Sue","Norman, Eric B;",2014,"The Cryogenic Underground Observatory for Rare Events (CUORE) is an experiment that will search for neutrinoless double-beta (0&nu&beta&beta) decay of 130Te and other rare processes. Observing 0&nu&beta&beta decay would establish that neutrinos are massive Majorana fermions, demonstrate that lepton number is not conserved, and constrain the neutrino mass scale and hierarchy. The CUORE detector, currently being constructed underground at the Gran Sasso National Laboratory in Italy, is an array of 988 high-resolution TeO2 bolometers. Each bolometer is comprised of a thermistor and a TeO2 crystal that serves as both the source and the detector of 0&nu&beta&beta decay. The 0&nu&beta&beta decay signature for 130Te is a peak at the Q-value 2528 keV. Observation of 0&nu&beta&beta decay requires that the background at the peak be ultra-low. Background-source identification and characterization are therefore extremely important. One source of background that is poorly characterized is activation of the TeO2 crystals by sea-level cosmic-ray neutrons. This process, known as cosmogenic activation, produces long-lived radioisotopes that can obscure the 0&nu&beta&beta peak. Existing cross-section data is insufficient to estimate this background; therefore, an additional cross-section measurement has been performed in which a TeO2 target was irradiated with a neutron spectrum similar to that of cosmic-ray neutrons at sea-level. Analysis of the radionuclides produced reveals that 110mAg will dominate the cosmogenic activation background in CUORE. Estimations using the measured cross section for 110mAg indicate this source will be negligible compared with other contributions to the CUORE background.",ucb,,https://escholarship.org/uc/item/0631g3qm,,,eng,REGULAR,0,0
896,2332,"Return of the Indian: Bone Games, Transcription, and Other Gestures of Indigeneity","Minch, Mark Allen","Minh-ha, Trinh T.;",2014,"Native Americans are currently in what is being called a `renaissance.'  A term that in its application to Native Americans was originally understood to have been a matter of literary production, a re-birth on the level of imagination, its focus has shifted recently to the (re)production of peoples themselves through various projects of cultural and biopolitical revitalization.  Much of the material basis for this (second) re-birth lies in material culture, more particularly, the collections in museums, echoing the use of stories and rites from the archives in Native American literatures.  Rebeginning from and with the ruins: What are these materials collected during the heyday of colonial violence, understood by ethnographers and anthropologists at the time to be `salvage' of the remainders of peoples on the verge of disappearance?  In the somewhat rationally organized, but often baroquely cluttered, immensity of fragments, there is produced an odd relation to the post-apocalypse where the collections function as contested political sites of open and incomplete identity formation.  Because of the difficulty of materialization in its relation to the scripted imagination, this space of (re)production is both gestural and transcriptive, made emblematic by the relation between the collection and the archive.      	Return of the Indian explores the possible uses and misuses of these sensitive materials in their reappropriation by tribes for various revitalization projects, and tries to understand the current situation of Native American identity within such a politics and art of recovery.  Drawing on perspectives from philosophy; postcolonial theory and indigeneity; literary theory; art practice; and feminist, gender and sexuality studies, this dissertation seeks a sensitivity to the materials that respects opacity and the role of the interval in the delicate practice of making Indians.  Located between what Michael Taussig has called the ""rituals"" of University based knowledge-production and the politico-tribal praxis of reproduction of indigenous subjects using the University's materials and methodologies, it follows the path and praxis of an indigenous call for return.",ucb,,https://escholarship.org/uc/item/0644z3tz,,,eng,REGULAR,0,0
897,2333,Immunity to Toxoplasma gondii,"Grover, Harshita Satija","Shastri, Nilabh;Robey, Ellen;",2012,"AbstractImmunity to Toxoplasma gondiiby Harshita Satija GroverDoctor of Philosophy in Molecular and Cellular BiologyUniversity of California, BerkeleyProfessor Nilabh Shastri, Co-chairProfessor Ellen Robey, Co-chairToxoplasma gondii (T. gondii) is an obligate intracellular protozoan parasite, capable of infecting all warm-blooded animals, and can cause a severe disease in immunocompromised individuals. Protection to T. gondii is largely mediated by CD8 T cells, although CD4 T cells have also been shown to be important in immunity to T. gondii. However, the natural antigens recognized by these CD8 T cells, and how they mediate immune protection in vivo is poorly understood. Recently, it has been discovered that in mice with H-2d MHC molecules, there is an immunodominant CD8 T cell response to GRA6, a dense granule T. gondii protein. In the absence of the T cell response to GRA6, such as in ERAAP-/- animals, mice succumb to infection. On the contrary, C57BL/6 (H-2b) mice do not mount a response to GRA6. Studying antigen presentation dependent on MHC haplotype is important because the response to T. gondii in mice is controlled by genes in the H-2 region, wherein mice with the H-2b haplotype are more susceptible to infection than mice with H-2d MHC molecules. In addition, C57BL/6 (B6) mice are good models because a large number of genetic mutations in the innate and adaptive immune system have been established in this strain. To better understand T cell responses in the susceptible strain, we immunized B6 mice with T. gondii, and found that it leads to potent CD4 T cell but weak CD8 T cell response. To identify the CD4 T cell stimulating antigens, we generated a T. gondii-specific, lacZ inducible, CD4 T cell hybridoma and used it as a sensitive probe to screen a T. gondii-cDNA library. We isolated a cDNA encoding a putative secreted protein of unknown function that we named CD4Ag28m and identified the minimal peptide, AS15 that was presented by MHC class II molecules to the CD4 T cells.  Immunization of mice with the AS15 peptide provided protection to subsequent parasite challenge, resulting in a lower parasite burden and cyst load.Furthermore, in order to characterize the CD8 T cell response to T. gondii in B6 mice, we restimulated T cells from mice immunized with T. gondii, with MHC Class II-/- bone marrow dendritic cells (BMDCs) to allow for the proliferation of CD8 T cells. This allowed us to generate a CD8 T cell hybridoma that was T. gondii specific and MHC Class I H-2Db restricted. This hybridoma was used as a probe to screen the T. gondii cDNA library, which revealed that it recognizes ROP5, a rhoptry protein from T. gondii. We identified that YAL9 is the minimal antigenic peptide recognized by CD8 T cells. In chronically infected mice, response towards YAL9 in the brain and spleen is minor but detectable, and unlike the protective GRA6 response in H-2d mice, immunization of B6 mice with YAL9 does not provide protection in B6 mice challenged with lethal dose of T. gondii. Most importantly, we found that altering the expression and trafficking of ROP5 from rhoptries to dense granules in parasites enhances the immunogenicity of ROP5 in B6 mice and now protects the mice from lethal challenge with transgenic parasites after peptide immunization.Identification of the antigens in the H-2b mice will enable us to study antigenic specific T cell responses, including effector functions such as clonal expansion and memory response at different stages of infection. We can now generate T cell receptor transgenic mice and use the antigen specific T cells from these mice to visualize and measure the dynamics of T cell interaction with antigen presenting cells during infection. Most significantly, understanding the nature of how these antigens are presented to T cells will allow us to design new and improved vaccine candidates against T. gondii and other apicomplexan parasites.",ucb,,https://escholarship.org/uc/item/03t7540x,,,eng,REGULAR,0,0
898,2334,Speaker Diarization: Current Limitations and New Directions,"Knox, Mary Tai","Morgan, Nelson;",2013,"Speaker diarization is the problem of determining ""who spoke when"" in an audio recording when the number and identities of the speakers are unknown. Motivated by applications in automatic speech recognition and audio indexing, speaker diarization has been studied extensively over the past decade, and there are currently a wide variety of approaches - including both top-down and bottom-up unsupervised clustering methods. The contributions of this thesis are to provide a unified analysis of the current state-of-the-art, to understand where and why mistakes occur, and to identify directions for improvements.In the first part of the thesis, we analyze the behavior of six state-of-the-art diarization systems, all evaluated on the National Institute of Standards and Technology (NIST) Rich Transcription 2009 evaluation dataset. While performance is typically assessed in terms of a single number - the diarization error rate (DER) - we further characterize the errors based on speech segment durations and their proximity to speaker change points. It is shown that for all of the systems, performance degrades both as the segment duration decreases and as the proximity to the speaker change point increases. Although short segments are problematic, their overall impact on the DER is small since the majority of scored time occurs in segments greater than 2.5 seconds. By contrast, the amount of time near speaker change points is relatively high, and thus poor performance near these change points contributes significantly to the DER. For example, for the single distant microphone (SDM) and multiple distant microphone (MDM) conditions, over 33% and 40% of the errors occur within 0.5 seconds of a change point for all evaluated systems, respectively.In the next part of the thesis, we focus on the International Computer Science Institute (ICSI) speaker diarization system and explore the effects of various system modifications. This system contains many steps - including speech activity detection, initialization, speaker segmentation, and speaker clustering. Inspired by our previous analysis, we focus on modifi- cations that improve performance near speaker change points. We first implement an alter- native to the minimum duration constraint, which sets the shortest amount of speech time before a speaker change can occur. This modification results in a 12% relative improvement of the speaker error rate for the MDM condition, with the largest improvement occurring closest to the speaker change point, and a 3% relative improvement for the SDM condition. Next, we show how the difference between the largest and second largest log-likelihood scores provides valuable information for unsupervised clustering, namely it indicates which regions of the output are likely correct.Lastly, we explore the potential of applying speaker diarization methodologies to other applications. Specifically, we investigate the use of a diarization-based algorithm for the problem of duplication detection, where the goal is to determine whether a given query (e.g., a short audio clip) has been taken from a reference set (e.g., a large collection of copyrighted media). With minimal modifications of the ICSI diarization system, we are able to obtain moderate performance. However, our approach is not competitive with existing approaches designed specifically for the problem of duplication detection, and the extent to which diarization-based approaches are useful for this application remains an open question.",ucb,,https://escholarship.org/uc/item/03v5b9wd,,,eng,REGULAR,0,0
899,2335,"Supramolecular Host-Guest Interactions, Dynamics and Structure","Mugridge, Jeffrey Scott","Raymond, Kenneth N;",2010,"The work described in this thesis examines the noncovalent interactions, dynamics and structure of a self-assembled supramolecular host-guest system.  The highly-charged, water soluble supramolecular assembly is able to selectively bind cationic molecules to the host interior and exterior and mediate the physical properties and chemical reactivity of bound guests.  Herein, physical organic chemistry in the context of this supramolecular system is used to elucidate some of the fundamental host-guest interactions that underlie guest binding and reactivity. Chapter 1.  First, an overview of different noncovalent interactions and their importance to biological systems is presented.  Selected examples of synthetic supramolecular host-guest systems are then reviewed, focusing on examples that illustrate how chemists have used noncovalent interactions to construct complex host architectures and affect guest chemical reactivity.  Finally, the [Ga4L6]12- supramolecular assembly, which is the topic of Chapters 2 - 5, is introduced and previous work examining the host-guest chemistry of this system is briefly reviewed. Chapter 2.  Ortho-substituted benzylphosphonium guest molecules are used to quantitatively probe the steric effects of confinement within the [Ga4L6]12- host.  Encapsulated guest bond rotational barriers and tumbling rates are measured in different solvents and at elevated external pressures.  These studies reveal that despite the flexibility of the host assembly, guest molecules experience significant steric confinement on the host interior and their bond rotational barriers are increased by up to 6 kcal/mol.  Significant solvent and pressure effects on the bond rotational rates are also observed; these suggest that the host cavity is smaller or less flexible in organic than in aqueous solution, and that internal solvent pressure is responsible for the observed changes in ligand framework flexibility.  The apparently smaller cavity volumes in organic solvents are further supported by NOE distance measurements, which show shorter average host-guest distances in organic, as compared to aqueous, solutions. Chapter 3.  Building on the solvent effects observed in Chapter 2, some additional, qualitative solvent effects are first presented; these show that guest binding and exchange is also sensitive to bulk solvent and that N,N-dimethylformamide can act as weakly bound guest.  The second part of this chapter presents a quantitative study of the solvent effects on the thermodynamics and kinetics of guest binding and exchange in the [Ga4L6]12- host.  No correlation between the guest binding or exchange parameters across different solvents are observed, illustrating the complexity of host and guest solvation in these highly-charged supramolecular systems.  Chapter 4.  A brief literature review of isotope effects on guest binding and exchange in supramolecular host-guest systems is first presented.  The measurement of kinetic and equilibrium isotope effects on guest binding and exchange in the [Ga4L6]12- assembly are then described.  Protiated guests are found to be more strongly bound to both the host interior and exterior, with significantly larger equilibrium isotope effects observed for C-H/D bonds that can participate in cation-Ï€ interactions.  DFT-level computations reveal that the equilibrium isotope effects arise from changes in low frequency C-H/D vibrational motions upon guest association.  Kinetic isotope effects during the guest ejection process are also observed.  A general model to explain both equilibrium and kinetic isotope effects, based only on changes in C-H/D vibrational force constants and zero-point energies is presented.  The observation of significant isotope effects on both guest binding and exchange demonstrates the remarkable sensitivity of the [Ga4L6]12- host assembly to even the most subtle changes in guest architecture. Chapter 5.  Guest molecules encapsulated in [Ga4L6]12- experience dramatic changes in their 1H nuclear magnetic resonance (NMR) chemical shifts due to the unique magnetic environment of the host interior.  Gauge-independent atomic orbital NMR chemical shift calculations are carried out and compared with experiment to provide information about host-guest conformation and structure.",ucb,,https://escholarship.org/uc/item/03z205p1,,,eng,REGULAR,0,0
900,2336,"The Sound of Silence: Daqu (""big-suite"") and Medieval Chinese Performance","Sun, Xiaojing","Ashmore, Robert;West, Stephen H.;",2012,"My dissertation investigates Chinese medieval daqu(""big suite""), a performance consisting of a succession of musical sections that combines song lyrics with instrumental accompaniment, and includes solo or ensemble dance movements.  As one of the major components of court music, the lyrics of daqu provide a valuable window into the often submerged link between text and performance.  My dissertation focuses on how the performance texts be taken as a linguistic matrix in which the fossilized remains of performance are preserved and revealed to memory.  The first chapter examines the relationship between dance, dance lyrics and performance context.  The second chapter provides a general introduction on daqu, from that of the Wei and Jin to the Tang and Song.  The third chapter takes daqu compositions by Shi Hao (1106-1194) of the Southern Song as central texts, discussing several issues such as ""musical words (yueyu),"" medieval court performance, intertextualization and performance context, ""roaming transcendents (youxian)"" tradition and court performance.  In the appendix is an annotated translation on Shi Hao's seven daqu pieces.",ucb,,https://escholarship.org/uc/item/03z407vn,,,eng,REGULAR,0,0
901,2337,The Role of Students' Previous Understandings in Reasoning Across Contexts,"Casperson, Janet Marie","diSessa, Andrea A;",2011,"Learning is typically thought of as a change in a student's understanding within a single context.  The term context is used here to describe a domain or subdomain of knowledge.  Students' previous understandings of the context are known to play a considerable role in such learning.  Another important kind of learning is reasoning across contexts.  Through reasoning across contexts, students' understanding of one context influences understanding of another context.  This dissertation investigated the role of students' previous understandings in this reasoning process.Psychology research has employed one type of experimental design to investigate reasoning across contexts.  This has resulted in focus on a particular way of reasoning across contexts, called analogical transfer, in which participants' previous understandings play a minimal role.  In contrast, this dissertation employed a case study design for the purpose of investigating the role of participants' previous understandings in reasoning across contexts.Participants' previous understandings of contexts were discovered to play a considerable role in their reasoning across contexts.  Three categories of ways of reasoning across contexts in which students' previous understandings play a role were identified.  In the first of these, a participant's new understanding of a context is a combination of ideas used previously to understand the same context and ideas used to understand another context.  In the second, an idea from either the participant's previous understanding or from the participant's understanding of another context is modified to contribute to a new understanding.  In the third, an idea from the participant's previous understanding is brought into different focus through comparison with a corresponding idea from another context.  These three are termed combining, modifying, and refocusing interactions, respectively.Understanding of these different possible roles for students' previous understandings in reasoning across contexts can inform instruction in which a general concept is instructed through particular contexts.  An example of such a general concept is the concept of equilibration considered here.",ucb,,https://escholarship.org/uc/item/0418x2xd,,,eng,REGULAR,0,0
902,2338,The Contributions of Motility to the Behavior of Pseudomonas syringae: Phenotypes and Genetics,"Cho, Juliana","Lindow, Steven E;",2013,"Motility is considered to be a beneficial trait for microbes, as it solves many of the challenges these microorganisms face in their habitats, allowing them to find hosts, explore local environments, access protected niches and needed nutrients, and disperse to new locations. The quantitative relationship between motility and virulence, particularly of plant-associated microorganisms, remains unclear. This study has dissected the quantitative contributions of these factors in the behavior of the plant pathogenic bacterium Pseudomonas syringae within plant tissues as well as on leaf surfaces, where it commonly occurs as an epiphytic resident of leaves. P. syringae is the only known plant pathogen with pili from the widespread chaperone-usher fimbrial family as well as all subfamilies of type IV pili. These families of pili are important factors in bacterial virulence, and implicated in adhesion, invasion, and biofilm formation in mammalian systems. However, the role of pili are less well characterized in plant pathogens. As such, P. syringae is a useful model system for studying these processes in plant colonization. The availability of reproducible, quantitative variation in nearly isogenic P. syringae strains, coupled with readily generated mutants altered in expression of adhesive pili, made P. syringae an obvious choice for investigating the relative contributions of adhesive factors and motility to the behavior of plant pathogens.Nine nearly isogenic isolates of P. syringae pv. syringae strain B728a exhibited stable and reproducibly different swimming and swarming motility in culture. Swimming and swarming motility were positively correlated among the variants, suggesting that similar traits contributed to both processes. A greater rate of motility of variants was associated with their incitation of a higher incidence of lesion formation on bean leaves after topical application of bacteria. The size of lesions formed by variants after direct injection into bean pods, thus bypassing the need for invasion, was also positively correlated with both the relative rate of motility in culture and their ability to incite leaf lesions. The differences in motility among the variant strains was not associated with any difference in biosurfactant production, although mutants blocked in syringafactin production exhibited a deficiency in ability to move within the apoplast of bean pods. Re-sequencing of the variant strains using high-throughput (Illumina) sequencing followed by genomic analyses revealed five single nucleotide polymorphisms (SNPs) and two short simple repeat regions (SSRs) differing among the nine P. syringae variant strains. None of the SNPs or SSRs were associated with genes known to be directly involved in flagellar or fimbrial biogenesis and regulation. Site-directed knockout mutants in those loci altered in variant strains recapitulated the reduced swarming motility exhibited by a given strain. Phylogenetic reconstruction of the collection of strains based on variation in genome features was in agreement with historical relationships of the strains. Additional analyses of two P. syringae variant strains that were selected as having either higher or lower swarming motility than their parental strains revealed additional genetic elements involved with modulating swarming motility. A mutation leading to early truncation of ispA, encoding a protein involved in cell septation, was associated with the variant exhibiting reduced motility and thus reduced virulence to bean. While no genetic reason could be identified in the selected variant that exhibited enhanced motility, the fact that it exhibited traits specifically beneficial to motility in culture may explain why it was less virulent than expected based on its high degree of motility in vitro.Further investigation of factors affecting motility were explored through bioinformatic analyses of putative fimbrial biogenesis pathways. Five putative fimbrial types were identified which had structural similarities to either chaperone-usher (CU) fimbriae, type IVa fimbriae, type IVb  fimbriae, or the type IVb monophyletic sub-family known as Flp fimbriae. Additional analyses of strains deficient in the production of the major structural pilin of type IVa fimbriae (PilA), CU fimbriae (FimA), and Flp fimbriae (Flp) revealed that twitching motility was not a major contributor to P. syringae motility. However, flp mutants and to a lesser extent fimA mutants exhibited higher swimming and swarming motility, suggesting that adhesive features of these fimbriae suppressed motility. Consistent with such a role, over-expression of Flp pilin resulted in a significant suppression in motility of the wild-type P. syringae strain. Co-inoculation of wild-type and fimbrial mutants onto leaf surfaces revealed that Flp fimbriae are involved in surface attachment, as the flp mutant exhibited a decreased attachment to bean leaves when compared with the wild-type strain. Attachment by Flp fimbriae appears to be an early step in the colonization process, as the reduced attachment of the flp mutant was much less pronounced after a longer incubation period. fimA and pilA mutants exhibited similar abilities to attach to leaf surfaces as the wild-type strain. Topical application of cell suspensions of pilA mutants onto bean leaves resulted in the formation of fewer lesions than conferred by the wild-type strain.  When simulated rain was applied to leaves after inoculation, both the pilA and fimA mutant incited as much as 2-fold fewer lesions than the wild-type strain, suggesting that they adhered less tenaciously and thus were retained less well after the rain event. fimA and flp mutants but not a pilA mutant formed larger lesions than the wild-type strain when directly inoculated into plant tissue, suggesting that their decreased adhesiveness contributed to their ability to move within the plant after inoculation. Taken together, Flp and CU fimbriae appear to have significant roles in adhesion that affect their motility in planta, and type IVa fimbriae appear to be important in inciting disease on bean, presumably by aiding local movement to sites suitable for invasion of the leaf.",ucb,,https://escholarship.org/uc/item/069010p1,,,eng,REGULAR,0,0
903,2339,"Safe and Interactive Autonomy: Control, Learning, and Verification","Sadigh, Dorsa","Seshia, Sanjit;Sastry, Shankar;",2017,"The goal of my research is to enable safe and reliable integration of human-robot systems in our society by providing a unified framework for modeling and design of these systems. Today's society is rapidly advancing towards autonomous systems that interact and collaborate with humans, e.g., semiautonomous vehicles interacting with drivers and pedestrians, medical robots used in collaboration with doctors, or service robots interacting with their users in smart homes. The safety-critical nature of these systems require us to provide provably correct guarantees about their performance. In this dissertation, we develop a formalism for the design of algorithms and mathematical models that enable correct-by-construction control and verification of human-robot systems.We focus on two natural instances of this agenda. In the first part, we study interaction-aware control, where we use algorithmic HRI to be mindful of the effects of autonomous systems on humans' actions, and further leverage these effects for better safety, efficiency, coordination, and estimation. We further use active learning techniques to update and better learn human models, and study the accuracy and robustness of these models. In the second part, we address the problem of providing correctness guarantees, while taking into account the uncertainty arising from the environment or human models. Through this effort, we introduce Probabilistic Signal Temporal Logic (PrSTL), an expressive specification language that allows representing Bayesian graphical models as part of its predicates. Further, we  provide a solution for synthesizing controllers that satisfy temporal logic specifications in probabilistic and reactive settings, and discuss a diagnosis and repair algorithm for systematic transfer of control to the human in unrealizable settings.While the algorithms and techniques introduced can be applied to many human-robot systems, in this dissertation, we will mainly focus on the implications of my work for semiautonomous driving.",ucb,,https://escholarship.org/uc/item/06g4b5xs,,,eng,REGULAR,0,0
904,2340,Students' Understandings of Arithmetic Generalizations,"Haldar, Lina Chopra","Saxe, Geoffrey B;Schoenfeld, Alan;",2014,"This study examines fourth graders' understandings of arithmetic generalizations, the general properties of arithmetic that hold true for all numbers. Its focus is on three types of generalizations: (a) direction of change (e.g., addition of positive numbers increases the numerical value, while subtraction of positive numbers decreases the numerical value); (b) identity (e.g., the addition or subtraction of 0 to any number leaves its value unchanged); and (c) relationship between operations (e.g., addition and subtraction are inverse operations). Using a between subjects design, two interview studies were conducted to investigate the character of children's understandings and to understand how students' production of generalizations varied across different tasks (e.g., I am thinking about a number. If I multiply that number by 5 and then divide by 5, what will happen to my number?). Study 1 (n=24) focused on students' additive thinking in the context of addition and subtraction tasks, while Study 2 (n=24) focused on multiplicative thinking in the context of multiplication and division tasks. In both studies, qualitative analyses of the video data revealed four levels in student thinking, levels that show a spectrum of increasing generality with which students treat arithmetic operations. At Levels 1 and 2, students rely on specific instances and substitution of values. At Levels 3 and 4 (advanced generalizations), students do not rely on any examples and make generalizations about the arithmetic operations. Further quantitative analyses revealed that student thinking was not always consistent and that students' production of advanced generalizations was affected by generalization type and domain type. In both studies, the identity tasks prompted the most advanced generalizations, while the relationship between operations tasks elicited the least advanced generalizations from students. Similarly, children were more likely to produce an advanced generalization in the additive domain than in the multiplicative domain. Although the multiplicative tasks may have been more challenging for students, the character of students' thinking and the difficulty of the tasks in relation to one another were similar across both domains. These parallel findings across the studies indicate that additive and multiplicative generalizations may involve similar developmental progressions. This dissertation provides insight into a developmental model about children's construction of arithmetic generalizations. First, the four levels of generality describe qualitative shifts in student thinking and, second, these findings indicate that the development of students' understandings of arithmetic generalizations may be heterogeneous across a range of generalizations. This work can contribute to teachers' knowledge for teaching and inform the design of a developmentally appropriate instructional sequence.",ucb,,https://escholarship.org/uc/item/06g8g7wx,,,eng,REGULAR,0,0
905,2341,Novel Processing Schemes for Material Systems on Amorphous and Flexible Substrates,"Chen, Kevin Qiwen","Javey, Ali;",2017,"With the rise of the Internet of Things (IOT), demand for novel devices and sensors for a variety of applications has exploded, and as a result, there is a need for the development of new processing schemes and materials systems to accommodate the expanding needs of these applications. In particular, Chapter 2 explores the growth of III-V semiconductors with quality approaching that of epitaxial thin films directly onto amorphous substrates using a new growth mode known as template liquid phase (TLP) crystal growth. The fundamental theory and limitations of TLP growth are explored and the toolboxes necessary for enabling this method to be widely used such as in-situ doping and growth of ternary III-V compounds are demonstrated. Proof of concept demonstrations of multilayer growth for 3D integration, transistors and phototransistors for electronic and optoelectronic integration, and transfer onto plastic for flexible applications are shown.Chapter 3 extends the concept of using liquid metals to the field of flexible and deformable electronics. As an analogy to solid state electronics, where materials with varying electronic properties (metallic, semiconducting, insulating, etc) are connected together to form functional devices, the concept of â€œliquid state electronicsâ€ is introduced. Liquids, being able to conform to any shape, can enable electronic devices which can sustain extremely large amounts of deformation. By using microchannels to prevent intermixing, liquid-liquid heterojunctions composed of liquid metal (InGaSn) as the interconnect and ionic liquids as sensors are demonstrated. Chapter 4 focuses on the usage of solution processed carbon nanotubes for enabling high-performing flexible electronic devices. First, a method for n-type doping of carbon nanotubes for CMOS applications is introduce using fixed charge in silicon nitride films. An extension of this doping method to other materials systems, in particular 2D WSe2 is demonstrated as well. The usage of carbon nanotubes in printed electronics for enabling large scale, high throughput, flexible electronics manufacturing is explored, with >97% pixel yield in a 20Ã—20 active matrix backplane array being achieved. A proof of concept demonstration of tactile pressure mapping using the printed nanotube active matrix backplane is shown as a potential application of such devices.",ucb,,https://escholarship.org/uc/item/0427m2vs,,,eng,REGULAR,0,0
906,2342,Mechanisms of CRISPR-Cas Immune Adaptation,"Wright, Addison Von","Doudna, Jennifer A;",2018,"Prokaryotes have evolved a diverse array of strategies to prevent or mitigate infection by phage. Among these, CRISPR-Cas systems (clustered regularly interspaced short palindromic repeats - CRISPR-associated) are unique in that they adapt to infections by generating an immunological memory that allows the host cell to mount a robust defense against subsequent infections. These systems are characterized by the presence of a genomic feature called a CRISPR array, which is made up of an AT-rich leader sequence followed by a series of direct repeat sequences of 20-50 base pairs alternating with variable viral-derived spacer sequences of similar length. When a cell is infected by a phage, a small fragment of the phage genome can be captured and inserted into the CRISPR array as a new spacer through a process called acquisition. The CRISPR array can then be transcribed to generate crRNAs (CRISPR RNAs) that assemble with interference Cas proteins to surveil the cell for complementary nucleic acid sequences. If a match is found, the Cas proteins degrade the nucleic acid. While the interference proteins of CRISPR-Cas systems are highly diverse, acquisition is broadly conserved. The proteins Cas1 and Cas2 carry out the integration of new spacers at the CRISPR locus and are found in nearly all identified active CRISPR systems. This work examines the mechanisms of spacer acquisition with a focus on how Cas1 and Cas2 from different CRISPR systems recognize and maintain specificity for the CRISPR array.Cas1 and Cas2 function as a complex to capture fragments of foreign DNA, called protospacers prior to integration, and insert them at the leader-proximal repeat through an integrase-like mechanism that results in duplication of the repeat. We find that the Cas1-Cas2 from the Streptococcus pyogenes type II CRISPR system integrates with high specificity in vitro into both plasmid and short linear targets, and we identify sequence motifs in the leader and repeat required for integration. We present the first evidence of full-site integration in vitro and show that the sequence requirements for full-site integration are stricter than those for half-site integration. Our biochemical data suggest that full-site integration acts as a checkpoint to ensure specificity, while half-site integration occurs more promiscuously due to the limited potential for it to introduce mutations at off-target sites.Using x-ray crystallography and cryo-electron microscopy, we identify the structural basis of leader and repeat recognition by Cas1-Cas2 from the Escherichia coli type I system. Crystal structures of the proteins bound to substrates mimicking half-site and full-site products, supported with biochemical and bacterial genetic experiments, show that integration requires substantial distortion of the repeat DNA and that the repeat sequence is identified by its deformability. The EM structure of Cas1-Cas2 bound to an extended target and IHF, a host factor required for specificity, reveals that IHF bends the leader DNA 180ï‚° to bring an upstream recognition sequence into contact with Cas1 for additional sequence-specific recognition. These structures and assays show that Cas1-Cas2 rely on structural constraints to restrict full-site integration to the CRISPR array.",ucb,,https://escholarship.org/uc/item/0442858x,,,eng,REGULAR,0,0
907,2343,Mechatronic Considerations of Assistive Systems for Gait Rehabilitation,"Bae, Joonbum","Tomizuka, Masayoshi;",2011,"As the number of patients requiring gait rehabilitation treatments is increasing, assistive systems for gait rehabilitation are being actively investigated. Assistive systems enable more efficient rehabilitation by providing objective values for indicating the patient's status and assistive torque for practicing normal trajectories for rehabilitation. This thesis investigates several mechatronic technologies of assistive systems for gait rehabilitation, including (1) estimation and evaluation of the patient's status, (2) monitoring systems, (3) control of assistive systems, and (4) implementation of rehabilitation algorithms.Estimation and evaluation of a patient's status based on pertinent measurements is the first step toward determining appropriate rehabilitation intervention methods. This thesis introduces an algorithm that estimates gait phases using a hidden Markov model (HMM) based on ground reaction forces (GRFs) measured by force sensors embedded in shoes, called Smart Shoes. The GRFs and the center of the GRFs (CoGRF) are used for observing the patient's status, and gait abnormality is calculated based on deviations from healthy GRF levels. This information is supplied to the monitoring system, which is implemented as a mobile system and a tele-system using the Internet. Assistive torque is required for seriously impaired patients to achieve the desired motion or practice normal trajectories. Ideal force mode control is necessary for natural interactions between the assistive system and the patient. In this thesis, robust control algorithms for precise and safe generation of the desired torque are discussed. The proposed algorithms have been applied to the previously developed assistive systems such as a rotary series elastic actuator (RSEA), a compact rotary series elastic actuator (cRSEA), and a cable-driven assistive system. As a decision-making process for rehabilitation, both a power augmentation method and a rehabilitation method are discussed. For the power augmentation method, the joint torque of the lower extremities is estimated using a human model with seven links and four different ground contact conditions. For gait rehabilitation, a potential field around the desired trajectory and an iterative learning algorithm inspired by repetitive gait motions are proposed for determining the desired assistive torque. The proposed methods have been verified experimentally, including clinical tests using actual patients.",ucb,,https://escholarship.org/uc/item/0499n3fv,,,eng,REGULAR,0,0
908,2344,"Sound Bodies forÂ Sound Minds: Architectural Interventions to Ameliorate the Sedentary Life of Scholars on College Campuses, 1865-2016","DeClercq, Caitlin Price","Cranz, Galen;",2017,"Since the founding of the earliest colleges in the United States, the built campus environment has been designed and modified to foster a healthy student body, a vision long challenged by sedentary study. One building type has served as the focal point of administrative and pedagogical efforts to respond to the perils of sedentary classroom study: the campus gymnasium.  Often among the first buildings to populate a new campus, gymnasia housed special apparatuses for physical training and postural remediation, both of which were intended to move, strengthen, and train student bodies to withstand the rigors of scholarly life. Founded in 1861 as the first womenâ€™s college and with the explicit goal of educating student minds and bodies, Vassar College is an apt case study to understand how shifting ideas about the perils of sedentary behavior were translated to the specific context of college settings and responded to through a host of interventions over time. By analyzing Vassarâ€™s spaces of physical education, I demonstrate how gymnasiaâ€”from the earliest gyms of the 1860s and 1870s to todayâ€™s athletic facilitiesâ€”have both shaped and reflected changing ideas regarding the perils of sedentary study and methods for mitigating its effects on the student body.  In particular, I show how the shift from in loco parentis to laissez-faire and risk management paradigms of campus governance mirror a shift in public health approaches, from a focus on broad, environmental and policy interventions to approaches targeting individual behaviors and risk mitigation.  At the same timeâ€”and stemming from these changesâ€”physical education courses and programs moved from a compulsory part of the college curriculum and experience to a leisure-time pursuit.  The result of these shifts can be read in both campus interventions and the built environment as movement (physical education) was relegatedâ€”administratively and spatiallyâ€”to the periphery of campus.  Yet the decline of compulsory physical education does not signify the solution to the problem of sedentary behavior: in fact, the problem is largely unresolved today.We can also see in the example of Vassar College how the still body assumed in spaces of (mental) learningâ€”as opposed to the specialized spaces of bodily instruction epitomized by campus gymnasiaâ€”is increasingly problematic as we learn more about the perils of sedentary behavior. Indeed, given what we know today about the limits of individual, compensatory, and leisure-time based interventions to reduce sedentary behavior, it is clear that the solution to reducing the perils of prolonged sitting cannot be found in historical precedents.  What is needed instead are new interventions that address the built environment and experience outside of the gymnasium, namely classrooms.  Yet interventions must address the persistent problem of control in classrooms: control has long meant still bodies, so injecting movement requires a fundamental shift in culture, pedagogy, and the built environment. Thus, to respond to burgeoning findings regarding the deleterious physiological and cognitive impacts of prolonged sitting, I introduce design science as way to identify novel classroom interventions.  Ultimately, I propose a range of interventionsâ€”from modifications in individual behaviors to changes in objects and built environments, to deeper pedagogical and cultural changesâ€”as informed by research on educational environments and adjacent institutions of learning and work.",ucb,,https://escholarship.org/uc/item/06h8533m,,,eng,REGULAR,0,0
909,2345,The Descendants of Kambu: The Political Imagination of Angkorian Cambodia,"Lowman, Ian Nathaniel","Edwards, Penelope;",2011,"In the 9th century CE, a vast polity centered on the region of Angkor was taking shape in what is today Cambodia and Northeast Thailand. At this time the polity's inhabitants, the Khmers, began to see themselves as members of a community of territorial integrity and shared ethnic identity. This sense of belonging, enshrined in the polity's name, Kambujadesa (i.e., Cambodia) or ""the land of the descendants of Kambu,"" represents one of the most remarkable local cultural innovations in Southeast Asian history. However, the history and implications of early Cambodian identity have thus far been largely overlooked. In this study I use the evidence from the Old Khmer and Sanskrit inscriptions to argue that Angkorian Cambodia (9th-15th centuries CE) was at its conceptual core an ethnic polity or a ""nation""--an analytic category signifying, in Steven Grosby's words, an extensive ""territorial community of nativity."" The inscriptions of Cambodia's provincial elite suggest that the polity's autonomy and its people's common descent were widely disseminated ideals, celebrated in polity-wide myths and perpetuated in representations of the polity's foreign antagonists. I contend that this culture of territorial nativity contradicts the prevailing cosmological model of pre-modern politics in Southeast Asian studies, which assumes that polities before the 19th century were characterized by exaggerated royal claims to universal power and the absence of felt communities beyond extended family and religion. At the same time I seek to problematize standard historical accounts of the nation which fail to observe the affinity between territoriality and fictive kinship in select political cultures before the era of ideological nationalism.",ucb,,https://escholarship.org/uc/item/06j1b9tp,,,eng,REGULAR,0,0
910,2346,Experiments on Health and Education in Developing Economies,"Lu, Fangwen","Perloff, Jeffrey M.;",2011,"Health and education are two important issues in developing economies. Field and natural experiments provided me with great opportunities for identifying the effects of health insurance and incentive on doctors' prescribing behaviors and the peer influences among students.      The first chapter examines whether doctors write more expensive prescriptions for insured patients and if so, why. I conducted a randomized audit experiment using undercover visits to Chinese hospitals. The results show that prescriptions for insured patients are 43% more expensive than those for uninsured patients when doctors expect to obtain a proportion of their patients' drug expenditures. The differences in prescriptions are largely explained by a differential agency problem hypothesis that doctors act of self-interest and prescribe more unnecessary or expensive drugs to insured patients, rather than by a considerate doctor hypothesis that doctors consider the trade-off between drug efficacy and patients' ability to pay.       The second chapter studies peer effects in a natural experiment generated by an unusual change in college admission policy at a prestigious Chinese university. The change in admission policy brought a large number of low-scoring students into several academic departments which only admitted high-scoring students in usual years. Exploiting the large variations in peer characteristics and the strong interactions among peer groups, the analysis finds that specially admitted low-scoring students significantly reduced the performance of regular students in standardized English tests. This detrimental effect of specially admitted students is concentrated among students with English ability below average.       Most research on peer effects in pre-college education focuses on the class or school level and assumes that students are influenced by class- or school-level averages. The third chapter examines peer effects within small groups inside classrooms by exploiting an experiment with random seat assignments inside Chinese classrooms. We define peers as either deskmates or neighboring students who sit at desks directly in front of or behind the students. The results suggest different patterns of impacts from deskmate and other neighboring students: female deskmates improve test scores for both boys and girls while the proportion of females among other neighboring students has strong positive effects on girls but no impact on boys. Overall, the results suggest that organizing students into small groups of homogeneous gender inside a classroom - a small-scale version of single-sex education - can significantly improve test scores.",ucb,,https://escholarship.org/uc/item/06j4v5wz,,,eng,REGULAR,0,0
911,2347,Crafting the Institutional Self: Identity and Trajectory in Artistic Training and Creative Careers,"Rowe, Matthew","Swidler, Ann;",2018,"This dissertation is a study of identity processes in two social domains: higher education and professional careers. Each chapter presents a distinct form of social identity and shows how it serves either as a resource to guide social participation (in the case of careers) or is a product of social participation (in higher education). ""Institutional self"" is a shorthand term for a pragmatic definition of identity, an ongoing process through which individuals make sense of their own capacities and trajectories as economic agents, in relation to the cues systematically produced in different environments. Each chapter of the dissertation develops a distinct conceptual model related to institutional self-formation, using empirical cases that are of interest to sociologists: the tensions of work in cultural production; formative stages of boundary-spanning careers; and college-level vocational training. Each fills gaps in the sociological research in these areas.The data used throughout the dissertation come from 106 interviews conducted by the author with students, faculty, and graduates of one art school, Adams College of the Arts (43 students, eight faculty, and 55 graduates). The school is located in a metropolitan area on the West Coast of the United States, where interviews took place in 2012 and 2013. All participants are drawn from two academic departments at the school: Visual Design, centered on the discipline of graphic design, and Media Arts, a mix of several digital media applications. Subsequent qualitative analyses of interview transcripts were primarily inductive, involving several rounds of coding along with development of guiding questions that emerged from observed patterns in interviewees' personal accounts and detailed work histories. Each of the dissertation's three empirical chapters is presented as an independent research manuscript; introductory and concluding chapters frame the conceptual and empirical contributions of the project as a whole.Chapter 2, ""Boundary Work as Career Navigation in Design and Media,"" looks at how creative workers use rhetorics of creativity to justify preferences for a wide range of working arrangements. Interviewees pursue one of two distinct forms of boundary work: segmentation and integration. Segmentation involves reproducing the institutionalized opposition between artistry and commerce in the temporal and spatial arrangements of working lives. Integration breaks down the boundary, merging the opposing motivations. Each finds expression in a range of career-building practices, from maintaining separate creative projects, to becoming an entrepreneur, or leaving creative work altogether. In closing, the chapter questions the relevance of occupations as a place of sensemaking and belonging for skilled, contingent workers.Chapter 3, ""Self-Assessment and Self-Presentation in Disorderly Careers,"" looks at a different set of career navigation strategies, based on ongoing accounting of one's capacities in relation to the observed expectations of work roles and environments. The organization of American work has shifted fundamentally in the last few decades. Work in many skilled occupations now takes on patterns long found in creative fields: project-based work and ""portfolio careers"" that are disorderly, uncertain, and highly mobile. I find that young creatives continually evaluate their skills and personalities in market terms as they experience jobs in different contexts. These self-assessments lead to instrumental investment in ""human capital""-both emotional and technical capacities-and self-selection into work roles based on a sense of fit with a firm, project, or industry. The chapter illuminates the experience of boundary-spanning careers, reviving an underdeveloped stream of micro-sociological career theory.	Chapter 4, ""Crafting Identity: Two Approaches to Professionalization in Art School,"" turns to college education as a training ground in occupational identification and preparation for boundary-spanning careers. Professional training is the dominant contemporary form of higher education in America, having surpassed the arts and sciences in the number of undergraduate enrollees and graduates, yet sociologists know little about how students experience the professionalization process at the college level. I find that two departments providing artists with training in commercial art practices create distinct pedagogic cultures within the same school. One prepares students for industry-specific work roles to which students peg their future trajectories; the other cultivates general competencies that are applicable across industries, leaving students to identify likely work roles and career pathways. The analysis provides a conceptually nuanced model drawn from cultural and organizational sociology that is applicable across settings of higher education.	The dissertation concludes with a brief closing chapter that provides an overview of the contributions of each chapter and the project as a whole. It closes with questions for future research that are directly and indirectly informed by these findings that may be useful for sociologists of the arts and media, work and occupations, and culture.",ucb,,https://escholarship.org/uc/item/06k6c2sj,,,eng,REGULAR,0,0
912,2348,Essays in Macroeconomics and Trade,"Yatsynovich, Yury","Gorodnichenko, Yuriy;Rodriguez-Clare, Andres;",2015,"In the current thesis I investigate the impact of sectoral structure of the economy on some aspects of its short-run fluctuations and long-run trends.In the first chapter -- ""Cost Structure and Price Rigidity across Sectors"" -- I model a mechanism through which the structure of costs of producers can affect producers' decisions on the frequency of adjusting prices. First, I establish an empirical observation that sectors which are characterized by either a higher share of labor or more diversified structure of bundles of intermediate goods are characterized by more rigid prices. Then I build and solve a partial equilibrium model that describes optimal price-setting strategies of firms in different sectors. The model provides an explanation for heterogeneity in price rigidity across different sectors. The calibrated model can be used for predicting how changes in the production processes and in the structure of costs can affect the heterogeneity of price rigidity across sectors and, hence, the aggregate price rigidity in the economy. In the second chapter -- ""Technological Spillovers and Dynamics of Comparative Advantage"" -- I investigate the question of the evolution of sector productivity and comparative advantage under the presence of cross-sector technological spillovers. For that I develop a dynamic model of international trade with cross-sector spillovers. In addition to the standard effect of comparative advantage on labor allocation, the model accounts for the effects of labor allocation on the sector productivity and comparative advantage. The core mechanism is a combination of an idea-generating process within each sector and technological spillovers across sectors. I establish necessary and sufficient conditions for the existence and uniqueness of a balanced growth path and describe the conditions under which a welfare-improving industrial policy is possible. I calibrate the model using the US patent data to parametrize the strength of technological spillovers and use the model to describe the optimal industrial policy.",ucb,,https://escholarship.org/uc/item/06r0b65x,,,eng,REGULAR,0,0
913,2349,Moral Judgment and Historical Understanding,"Tsai, George Tsan","Sluga, Hans;Wallace, R. Jay;",2011,"Philosophers, historians, and social scientists often suppose that our moral judgments are insulated from our historical understanding, and vice versa.  That is, they generally assume that while our moral judgments appraise social and historical facts, they do not constrain our predictions and explanations of those facts; conversely, our historical accounts describe and explain social phenomena, including ethical phenomena, but they are separate from our evaluations of those phenomena.  I challenge both of these assumptions, in arguing that our historical understanding is wrapped up in certain inextricable ways with our moral outlook.  More specifically, I contend that some of our moral judgments presuppose assumptions about history and the social world; and also that our social and historical accounts can be informed by our views about morality.  In short, our moral and historical views are interdependent.These general claims are defended through an investigation of three cases of interaction between our moral judgments and historical understanding.  In the first case, I explore how assumptions about the causal dependence of present moral practices on past activities have implications for retrospective evaluations of those past activities.  More specifically, I grapple with the possibility and evaluative implications of there being large-scale past activities that are both unjust but also necessary causal conditions for the historical emergence of modern liberalism.  In the second case, I examine how the presupposition of universal validity in our moral outlook constrains our explanations of the historically distant.  More specifically, I consider whether, and how, liberal universalism, the widely-held view that liberal values are universally valid, can provide a plausible ""theory of error"" to account for the non-liberal tendencies of past societies.  In the final case, I reflect on how assumptions about the cultural or normative distance between other social worlds and our own inform our character assessments.  More specifically, I try to make sense of why our character assessment of someone who did something morally wrong is influenced by where we locate the wrongdoer in history.",ucb,,https://escholarship.org/uc/item/06t2x2rp,,,eng,REGULAR,0,0
914,2350,Transcriptional control of the recombination activating genes Rag1 and Rag2 in B lymphocytes and non-lymphoid cells,"Timblin, Greg Alan","Schlissel, Mark S;Tjian, Robert;",2014,"Expression of the recombination activating genes Rag1 and Rag2 (Rag) is essential for generation of a diverse B cell antigen receptor repertoire necessary for effective adaptive immune responses. The Rag genes are among the first lineage-specific genes expressed in lymphocyte progenitors, and their expression strongly correlates with commitment to the B lineage. Because V(D)J recombination involves generation of DNA breaks by the RAG proteins, Rag expression must be tightly controlled throughout B cell development to maintain genomic integrity. Using Abelson Murine Leukemia Virus-transformed B cells as a model system, we sought to identify factors that repress Rag transcription during pre-B cell differentiation, a period of B cell development marked by rapid proliferation during which the Rag genes are transiently repressed and then reactivated upon differentiation. A screen identified Ebf1 and c-Myb, two well-studied transcription factors in the context of early B cell development, as repressors of Rag transcription in these highly proliferative cells. As we found no evidence for direct Rag repression by Ebf1, we investigated how Ebf1 influences the expression of factors previously implicated in the regulation Rag transcription. We discovered that Ebf1 achieves Rag repression through both negative regulation of the Rag activator Foxo1, and positive regulation of the Rag repressor Gfi1b. In addition to influencing Foxo1 and Gfi1b expression in a similar manner, we found that c-Myb directly binds to the Erag enhancer in the Rag locus and antagonizes Foxo1 binding to this cis regulatory element. Together this work reveals previously unappreciated roles for Ebf1 and c-Myb in the transcriptional repression of the Rag genes during pre-B cell differentiation. In addition to screening for factors that regulate Rag expression during B cell development, we also performed a screen for factors capable of activating the Rag genes in non-lymphoid cells. Given the close connection between Rag expression and B lineage commitment in lymphocyte progenitors, we reasoned that if we were able to induce Rag transcription in non-lymphoid cells such as fibroblast, these cells might be activating a host of B cell-specific genes and undergoing direct reprogramming to the B lineage. While we identified factors that could readily induce Rag expression when overexpressed in primary fibroblasts, we could detect no evidence of a reprogramming event involving activation of additional B lineage genes. Together our work identified novel repressors of Rag transcription in developing B cells, and factors capable of activating Rag transcription in non-lymphoid cells that may inform future fibroblast-to-B cell reprogramming experiments.",ucb,,https://escholarship.org/uc/item/06t8c18t,,,eng,REGULAR,0,0
915,2351,Essays in Special Interest Politics,"Maheshri, Vikram","Roland, Gerard;",2010,"In the following three essays, I explore how organized political interests behave, interact with each other, and affect public policies. As special interest groups in the United States have proliferated both in number and size over the past several decades, policymakers have responded with a mixture of public consternation and private acceptance. American voters overwhelmingly disapprove of the activities of special interest groups, but observers have been unable to articulate convincingly the effects of these activities on legislative practices and social welfare. I fill in this gap.In my first essay, I consider the effects in Congress of competition between interest groups over public policy. I model the interactions of rival interest groups and legislators as a dual form of competition over both the substance of legislation -- that is, the legislative agenda -- and legislators' votes on legislation. This is, at it's heart, a model of congressional committee behavior. The unique prediction of this model is that interest groups may intentionally spend money to have legislation introduced that is known to have no prospect of being passed. Such interest groups are motivated by the inability to shape policy in a more favorable direction and the desire to protect a sufficiently palatable status quo. This result is attractive in light of the fact that roughly 90% of legislation fails passage. I then go on to provide empirical evidence in support of this prediction using an original, large and highly detailed dataset consisting of all pieces of legislation introduced into Congress over a twenty year period. I estimate that interest groups attempt to suppress roughly 56% of legislation that is introduced in the House of Representatives and 69% of legislation that is introduced into the Senate. Furthermore, I provide evidence that groups may suppress legislation by obfuscating the linguistic content of bills.In my second essay, I consider the effects of competition within interest groups on the direction of redistributive policies. Individuals and firms form interest groups as a means of pooling resources and overcoming free riding in order to shape favorable public policies. However, the objectives of interest groups and their constituents do not perfectly align. In particular, interest groups tend to be more farsighted than their constituents. This asymmetry of objectives generates an agency problem that may manifest itself in the persistence of inefficient public policies. That is, interest groups may be directed by their relatively shortsighted constitutents to oppose efficient policy reforms. In the long run, this agency problem is exacerbated by the responses of interest groups to free riding. I provide suggestive econometric evidence from the United States in support of this argument.In my third essay, I consider the implications of the very function of an interest groups: providing a means to aggregate the preferences of a motivated but potentially heterogeneous constituency. I begin by noting a very strong empirical regularity in lobbying activity in the United States. In nearly every industry, the distribution of lobbying expenditures follows a power law. This regularity is not predicted by standard models of interest group behavior. Instead, I provide a heuristic explanation for the empirical finding. If interest group expenditure decisions are made in response to periodic signals and are determined by a constituency that continuously updates its preferences according to a simple heuristic, then we should expect to see the distribution of lobbying expenditures follow a power law. I provide additional simulation evidence in support of this claim.",ucb,,https://escholarship.org/uc/item/070207sp,,,eng,REGULAR,0,0
916,2352,The European Union and the Constitutionalization of Democracy,"Phillips, Ryan Lee","Bevir, Mark M;",2015,"From the Treaty on European Union (1992) to the Treaty of Lisbon (2007), democracy became a central element in the constitutionalization of the European Union. Over these twenty-five years, democracy was embedded in the institutions and practices of the European Union in two primary respects. First, following the controversial the ratification of the TEU, a series of treaty reforms were introduced to address the EUâ€™s â€œdemocratic deficit.â€ These reforms culminated in the Lisbon Treaty. Second, in response to the political revolutions in Central and Eastern Europe the European Union made the transfer of democratic forms of rule to non-member states a key component of its membership policy. It is now the case that only democratic countries can be members in the democratic Union. This dissertation investigates the origins of some of the key ideas that led to these transformations.",ucb,,https://escholarship.org/uc/item/070574q8,,,eng,REGULAR,0,0
917,2353,Parental Feeding Practices and Childrenâ€™s Weight Status in Mexican American Families,"Penilla, Carlos","Ozer, Emily J;Deardorff, Julianna;",2018,"It is known that mothersâ€™ child-feeding behaviors are associated with their childrenâ€™s weight status, but this is only one familial factor. There is a dearth of research on the associations of both mothersâ€™ and fathersâ€™ child-feeding behaviors and their childrenâ€™s weight status in Mexican American families. In 2009-2010, 22% of Mexican American children aged 6 to 11 years had a body mass index (BMI) greater than or equal to the 95th percentile and were considered obese compared to 14% of non-Latino White children of similar ages. This disparity was also seen among children under age 6. In the same period, 16% of Mexican American children aged 2 to 5 years were considered obese compared to 9% of non-Latino White children. Obesity during these early years is associated with increased risk of obesity later in life. In Mexican families, where fathers often influence family decisions, it is important to understand how they may also influence decisions around child feeding. Parental child-feeding behaviors are a major focus of my research because they are modifiable risk factors in childrenâ€™s weight status, particularly when compared to other predictors, such as parental weight status, parental education level and ethnicity. Using the conceptual framework from Davison and Birchâ€™s (2001) ecological model, which identifies individual, family and sociocultural influences on childrenâ€™s weight status, this dissertation applies quantitative and qualitative methods to examine parental and sociocultural associations with child-feeding behaviors in Mexican American families.This dissertation research examines the associations of parental feeding behaviors and child weight status in Mexican American families, with a special focus on the role of fathers. I apply a three-pronged approach to the study of childhood obesity that includes a family, environmental, and nutrition policy component. At the family level, I demonstrate in my quantitative study (paper 1) that fathersâ€™ child-feeding practices, such as pressure to eat and use of food to control behavior are equally as significant as mothersâ€™ child-feeding practices in their associations with child weight status. For example, findings indicate that fathersâ€™ higher use of pressure to eat and use of food to control behavior were significantly related to childrenâ€™s lower weight status, after accounting for mothersâ€™ feeding practices and other covariates. At the environmental level, I demonstrate in my qualitative study (paper 2) that both mothers and fathers experience structural and environmental obstacles, such as a lack of social support among neighbors and dirty, under-policed streets in urban neighborhoods, which negatively influences their ability to leave the house and makes it difficult to feed their children healthful foods. Specifically, I examine how these obstacles in turn influence the development of overweight and obesity in children aged 2 to 5 years. I have integrated the results of my first two studies with the existing literature on obesity in Latino children to inform the third component of my dissertation, a health policy brief. In this brief, I ask the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) to take steps and develop procedures to encourage full access to their services by Latino fathers and encourage their participation and, by so doing, support WIC goals for the nutrition of low-income children and their families. Overall, my findings suggest that in order to effectively intervene in the development of childhood obesity, community stakeholders, scholars and policymakers need a better understanding of how structural and environmental obstacles, and parentsâ€™ resources, culture, gender and ethnicity intersect and impact child weight.",ucb,,https://escholarship.org/uc/item/0707m6m3,,,eng,REGULAR,0,0
918,2354,Eyes of the Heart: Illustration and the Visual Imagination in Modern Japanese Literature,"Bassoe, Pedro Thiago Ramos","O'Neill, Daniel;",2018,"My dissertation investigates the role of images in shaping literary production in Japan from the 1880â€™s to the 1930â€™s as writers negotiated shifting relationships of text and image in the literary and visual arts. Throughout the Edo period (1603-1868), works of fiction were liberally illustrated with woodblock printed images, which, especially towards the mid-19th century, had become an essential component of most popular literature in Japan. With the opening of Japanâ€™s borders in the Meiji period (1868-1912), writers who had grown up reading illustrated fiction were exposed to foreign works of literature that largely eschewed the use of illustration as a medium for storytelling, in turn leading them to reevaluate the role of image in their own literary tradition. As authors endeavored to produce a purely text-based form of fiction, modeled in part on the European novel, they began to reject the inclusion of images in their own work. This literary transformation, from a pictorial to logographic orientation, has previously been noted by scholars, but has often been mischaracterized as a sudden and total shift. In my dissertation, I show that, in fact, illustration remained a major component of literary publications in Japan well into the 20th century, as I argue that experimentation with verbal-visual form was a crucial element in the production of a modern literary idiom.I begin my dissertation by analyzing the work of Tsubouchi ShÅyÅ (1859-1935), who argued early on in his career that Japanese authors needed to replace illustration with descriptive language in order to develop a modern form of writing. I show that in his own fiction, however, ShÅyÅ continued to use illustration extensively, including images that he designed himself. Eventually, he came to see the traditional illustrated fiction of the Edo period not as an early stage of literary development to be overcome, but rather as a unique form of verbal-visual art that deserved to be treated as a national cultural heritage. In my second chapter, I explore Ozaki KÅyÅâ€™s (1867-1903) ambivalent relationship to illustration, which he vocally opposed in public statements, even while contributing personally to the visual design of his own work. According to contemporary artists, KÅyÅ was known for providing self-penned draft images with meticulous notes for his illustrators, while closely supervising every element of his workâ€™s visual expression. In his writing, KÅyÅ treated visual media as a metaphor for language, which he separated into two modes of representation: the photographic (unmediated) mode, which corresponds to literary realism, and the painterly (mediated) mode, which refers to early modern traditions of Japanese writing. The second half of my dissertation focuses on the work of Izumi KyÅka (1873-1939), a writer whose passion for Edo period picture-books (ehon or kusazÅshi) influenced his literary production throughout a nearly five-decade career. In his fiction, KyÅka created a complex visual matrix of symbolic imagery by combining references to art from the Edo period with extensive illustration and densely visual language. Evincing an attitude towards illustration that might best be described as reverent, KyÅka frequently wrote stories about popular images that transform into religious icons, while working closely with his favorite artists to produce spectral illusions that crossed the borders between text and image. His longest artistic collaboration was with Komura Settai (1887-1940), an artist whose romantic images of dark alleyways, faceless geisha, and Edo period architecture intersected with KyÅkaâ€™s literary depictions of urban space to produce a ghostly vision of modern Tokyo.",ucb,,https://escholarship.org/uc/item/0777d5sv,,,eng,REGULAR,0,0
919,2355,Metaphor in the Grammar of Argument Realization,"David, Oana A.",,2016,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/07j56079,,,eng,REGULAR,0,0
920,2356,Bidirectional Associations between Behavior Problems and Teacher-Child Relationship Quality in Chinese American Immigrant Children,"Ly, Jennifer","Zhou, Qing;",2013,"This study examined the prospective associations between behavior problems and teacher-child relationship quality (TCRQ) in a socio-economically diverse sample of Chinese American first- and second-grade children in immigrant families (N = 258). Externalizing and internalizing problems were assessed using parents' and teachers' ratings. Teachers completed a questionnaire measuring TCRQ dimensions of Warmth and Conflict and children completed a questionnaire measuring Closeness. Path analyses were conducted to examine the bidirectional associations between behavior problems and TCRQ, controlling for baseline levels and demographic characteristics. Results indicated that teacher-rated externalizing problems negatively predicted child-rated Closeness, teacher-rated internalizing problems negatively predicted teacher-rated Conflict, and parent-rated internalizing problems negatively predicted teacher-rated Warmth. Although teacher-rated TCRQ did not significantly predict teacher-rated behavior problems, teacher-rated Conflict positively predicted parent-rated externalizing problems. Unexpectedly, child-rated Closeness also positively predicted parent-rated externalizing problems. These findings highlight the importance of assessing TCRQ and children's behavior problems using multiple reporters. The present study also provides support for the transactional relationship between children's behavior problems and TCRQ. Recommendations for school-based interventions with Chinese American children in immigrant families are discussed.",ucb,,https://escholarship.org/uc/item/07p077tp,,,eng,REGULAR,0,0
921,2357,Learned Factorization Models to Explain Variability in Natural Image Sequences,"Culpepper, Benjamin Jackson","Olshausen, Bruno A;Malik, Jitendra;",2011,"Robust object recognition requires computational mechanisms that compensate for variability in the appearance of objects under natural viewing conditions. Yet, these have proven to be difficult to engineer. For this reason, the development of computational models that achieve invariance to the types of transformations that occur during natural viewing will both benefit our understanding of biological systems and help to achieve the goals of computer vision. This thesis develops a set of models that learn low dimensional representations of the transformations occurring in dynamic natural scenes. Good models of these transformations allow their effect to be compensated through an inference process, which jointly estimates a stable percept and a parsimonious description of its appearance.I propose a series of models based on the idea of factoring apart image sequences into two types of latent variables: a stable percept, and a low dimensional time-varying representation of its transformation. Such a two component model is a general mechanism for teasing apart the causes that conspire to produce a time-varying image. First, I show that when both components are represented by linear expansions, the resulting bilinear model can achieve some degree of image stabilization by utilizing the transformation model to explain the translation motions that occur in a small window of a movie. Yet, the recovered latent factors exhibit dependencies that motivate the investigation of a richer, exponential map as a second model for the dynamics of appearance. In addition to the translation motions captured by the linear appearance model, this richer model learns transformations that can compensate for rotations, expansions, and complex distortions in the data. Lastly, I propose a hierarchical model that describes images in terms of a hierarchy of grouped lower-level features; learning parameters in this hierarchy is enabled by a procedure that maintains uncertainty in the posterior distributions over the latent variables.The contribution of this work is a demonstration of an adaptive mechanism that can automatically learn transformations in a structured model, which enables sources of variability to be factored out by inverting it. This is an important step, because sources of variability are the main factor causing difficulties in artificial object recognition systems, and visual invariance is also closely related to the idea of generalization, an ability that is commonly equated with intelligence. Thus, to the extent that we are able to build seeing machines that can automatically compensate for category-level variability we will have achieved some part of the goal of artificial intelligence.",ucb,,https://escholarship.org/uc/item/07p3z33d,,,eng,REGULAR,0,0
922,2358,Exploiting Interference through Algebraic Structure,"Nazer, Bobak Anthony","Gastpar, Michael C;",2009,"In a network, interference between transmitters is usually viewed as highly undesirable and clever algorithms and protocols have been devised to avoid it. Collectively, these strategies transform the physical layer into a set of reliable bit pipes which can then be used seamlessly by higher layers in the protocol stack. Unfortunately, interference avoidance results in sharply decreasing rates as the number of users increases. In this thesis, we develop a new tool, computation coding, that allows receivers to reliably decode equations of transmitted messages by harnessing the interference structure of the channel. Applied to a wireless network, this enables relays to decode linear functions of the transmitted messages with coefficients dictated by the fading realization. Relays can then forward these equations towards the destinations which simply collect enough equations to solve for their desired messages.  Structured codes (such as lattices) ensure that these linear combinations can be decoded reliably at the relays, often at far higher rates than the messages individually. Through examples drawn from cooperative communication including cellular uplink, distributed MIMO and wireless network coding, we demonstrate that this compute-and-forward strategy can improve end-to-end throughput in a network. As a consequence, we will see that structured codes can play an important role in approaching the capacity of networks. We also show that our techniques can result in both energy and delay savings for distributed signal processing over a sensor network. Finally, by viewing interference as implicit computation, we provide a new perspective on the interference channel with time-varying fading. We describe a simple interference alignment scheme that enables each user to achieve at least  half its interference-free capacity at any signal-to-noise ratio.",ucb,,https://escholarship.org/uc/item/07q966w8,,,eng,REGULAR,0,0
923,2359,Spiritual Subjecthood and Institutional Legibility in Early Modern Spain and Spanish America,"Borowitz, Molly Elizabeth","del Valle, Ivonne;",2019,"My dissertation, â€œSpiritual Subjecthood and Institutional Legibility in Early Modern Spain and Spanish America,â€ argues that early modern subjects of the Spanish Empire respond intentionally and strategically to institutional interpellation, representing their subjection in ways that advance their own spiritual and political agendas. The project traces subjectsâ€™ efforts to shape, limit, and exploit their legibility to spiritual and political institutions through texts by Teresa of Ãvila, Ignatius of Loyola, MartÃ­n de Azpilcueta, fray LuÃ­s de Granada, and the Third Provincial Council of Lima. The concept of institutional legibility originates with James C. Scottâ€™s Seeing Like a State (Yale UP, 1998). Scott argues that institutions in early modern Europe develop â€œstate simplificationsâ€â€”shorthands by which they assimilate local variations in language, practice, or record-keeping to a state-wide standardâ€”in order to distill from their subject populations the information they need for operations like taxation and political control. State simplifications make local activities â€œlegibleâ€ to institutions by producing homogeneous data that reflects their interests. I invert Scottâ€™s definition of legibility, focusing on local subject formation rather than institutional data collection. Following Althusser, I contend that the attempt to render oneself legible is a response to interpellation; it entails self-subjection to the relevant institution. However, I argue with Butler that self-subjection occurs not once, but continually; it is a constant and evolving performance, a creative act as well as a compelled response to authority. The first chapter, â€œLimited Legibility in Teresa de JesÃºsâ€™ Las Moradas del castillo interior,â€ shows how Teresa promotes the suspect practice of mental prayer by establishing a shorthand for assessing the orthodoxy of practicing mystics while also obscuring the content of her own mystical encounters. She provides confessors with ready-made state simplifications to determine the provenance of a mysticâ€™s visions, but then sidesteps the obligation to describe the content of her own visions by allegorizing her relationship to God as a spiritual marriage, recasting their encounters as moments of spousal intimacy that good Christian wives should never discuss.The second chapter, â€œMaking mystic texts: Jesuit subject formation in the Spiritual Exercises,â€ argues that, through the Exercises, Ignatius of Loyola teaches Jesuits not only how to make their own mystical experiences, but also how to translate those experiences into mystic â€œtextsâ€â€”verbal accounts of their immediate encounters with divinityâ€”expressed in a standard, Society-wide language. The mutual legibility of Jesuit superiors and subordinates through the creation of mystic texts fosters the organizational cohesion and hierarchical stability that each individual Jesuitâ€™s power to make mystical experience puts at risk.The third chapter, â€œSacramental confession and institutional strategy in sixteenth-century Spain and Peru,â€ suggests that confession works to maintain imperial hegemony by teaching penitents to police their behavior for sins that undermine the Crownâ€™s projects of cultural homogenization and political compliance. Confessional manuals by MartÃ­n de Azpilcueta, fray LuÃ­s de Granada, and the Third Lima Council show how the sacrament controls the penitentâ€™s conscience through the inspiration of guilt, the obligation to self-examination, and the expansion of the categories of sin to encompass social, political, and economic activities that contravene imperial interests.  Constructing legibility as a process of subject formation illuminates the agency of institutional subjects. Though compulsory, legibility affords the early modern subject some room to negotiate with the institution, and can create opportunities for her to advance her individual agenda.",ucb,,https://escholarship.org/uc/item/07r1g3zh,,,eng,REGULAR,0,0
924,2360,Portable MRI and 129Xe Signal Amplification by Gas Extraction,"Graziani, Dominic Michael","Pines, Alexander;",2011,"Magnetic resonance has had an impact on nearly every branch of science from fundamental physics to neurobiology.  Despite its pervasiveness, it remains a relatively inaccessible technique due to the costs associated with the powerful and highly precise instrumentation needed for sensitive detection.  In this work, two techniques focused on improving the accessibility of magnetic resonance are presented.  First, a low cost and portable volume imaging system is described with the unique ability to scan the homogeneous region of the magnetic field through a sample of interest.  This system addresses the fundamental challenge of producing a compact imaging sensor while maintaining a sensitive region large enough to extract relevant information from the sample.  The design of a suitable set of single-sided gradient coils compatible with the adjustable imaging system is presented first, followed by imaging results obtained with this apparatus.  The second technique described in this work involves the use of xenon as a chemical sensor.  Xenon's chemical shift sensitivity to its environment make it and ideal probe of its surroundings.  Spin exchange optical pumping has made detection of dilute xenon solutions possible through hyperpolarization.  However, optical pumping requires high power circularly polarized laser light, limiting applications of xenon from widespread use.  An alternative method for signal amplification is presented, in which xenon is extracted from solution and compressed prior to detection.  A description of the method is followed by several applications as well as a detailed description of the apparatus involved in the gas extraction and compression technique.",ucb,,https://escholarship.org/uc/item/07w763dq,,,eng,REGULAR,0,0
925,2361,Metal-Organic Frameworks for Gas Storage and Separation,"Mason, Jarad Adam","Long, Jeffrey R.;",2015,"The work presented in this dissertation describes the design, synthesis, and characterization of metal-organic frameworks for applications in gas storage and gas separations, with a specific focus on natural gas and hydrogen storage for mobile applications and on post-combustion carbon dioxide capture from coal- or natural gas-fired power plants. A wide variety of techniques and spectroscopic methods are covered, including gas adsorption, x-ray diffraction, infrared and UV-vis-NIR spectroscopies, and calorimetry.  Chapter One provides a brief introduction to metal-organic frameworks as a new class of porous materials for gas adsorption-related applications. The potential of metal-organic frameworks for use in post-combustion carbon dioxide capture and natural gas storage is discussed, and the unique and promising properties of adsorbents with stepped adsorption isotherms for these applications are highlighted.In Chapter Two, two representative metal-organic frameworks, Zn4O(BTB)2 (BTB3- = 1,3,5-benzenetribenzoate; MOF-177) and Mg2(dobdc) (dobdc4- = 1,4-dioxido-2,5-benzenedicarboxylate; Mg-MOF-74, CPO-27-Mg), are evaluated in detail for their potential use in post-combustion CO2 capture via temperature swing adsorption (TSA). Low-pressure single-component CO2 and N2 adsorption isotherms were measured every 10 Â°C from 20 to 200 Â°C, allowing the performance of each material to be analyzed precisely. In order to gain a more complete understanding of the separation phenomena and the thermodynamics of CO2 adsorption, the isotherms were analyzed using a variety of methods. These results show that the presence of strong CO2 adsorption sites is essential for a metal-organic framework to be of utility in post-combustion CO2 capture via a TSA process, and present a methodology for the evaluation of new metal-organic frameworks via analysis of single-component gas adsorption isotherms.Chapter Three briefly discusses high-pressure adsorption measurements and reviews efforts to develop metal-organic frameworks with high methane storage capacities. To illustrate the most important properties for evaluating adsorbents for natural gas storage and for designing a next generation of improved materials, six metal-organic frameworks and an activated carbon, with a range of surface areas, pore structures, and surface chemistries representative of the most promising adsorbents for methane storage, are evaluated in detail. High-pressure methane adsorption isotherms are used to compare gravimetric and volumetric capacities, isosteric heats of adsorption, and usable storage capacities. Additionally, the relative importance of increasing volumetric capacity, rather than gravimetric capacity, for extending the driving range of natural gas vehicles is highlighted. Other important systems-level factors, such as thermal management, mechanical properties, and the effects of impurities, are also considered, and potential materials synthesis contributions to improving performance in a complete adsorbed natural gas system are discussed.Chapter Four discusses the design and validation of a high-throughput multicomponent adsorption instrument that can measure equilibrium adsorption isotherms for mixtures of gases at conditions that are representative of an actual flue gas from a power plant. This instrument is used to study 15 different metal-organic frameworks, zeolites, mesoporous silicas, and activated carbons representative of the broad range of solid adsorbents that have received attention for CO2 capture. While the multicomponent results provide many interesting fundamental insights, only adsorbents functionalized with alkylamines are shown to have any significant CO2 capacity in the presence of N2 and H2O at equilibrium partial pressures similar to those expected in a carbon capture process. Most significantly, the amine-appended metal organic framework mmen-Mg2(dobpdc) (mmen = N,Nâ€²-dimethylethylenediamine, dobpdc4â€“ = 4,4â€²-dioxido-3,3â€²-biphenyldicarboxylate) exhibits a record CO2 capacity of 4.2Â±0.2 mmol/g (16 wt %) at 0.1 bar and 40 Â°C in the presence of a high partial pressure of H2O.In Chapter Five, the flexible metal-organic frameworks M(bdp) (M = Fe, Co; bdp2â€“ = 1,4-benzene-dipyrazolate) are shown to exhibit methane adsorption isotherms that feature a sharp step, giving rise to unprecedented performance characteristics for ambient temperature methane storage. Adsorption measurements combined with in situ powder X-ray diffraction and microcalorimetry experiments performed on Co(bdp) demonstrate a new approach to designing adsorbents for gas storage, wherein a reversible phase transition is used to achieve a high deliverable capacity while providing intrinsic thermal management. Importantly, the energy of the phase transition, together with the adsorption and desorption step pressures, can be controlled through variations in the framework structure, such as replacing Co with Fe, or by application of mechanical pressure. This approach overcomes many of the challenges to developing adsorbents for natural gas storage discussed in Chapter Three and is also relevant to other gas storage applications.Chapter Six discusses the synthesis and characterization of a new Ti(III) metal-organic framework that is constructed from 1,4-benzenedicarboxylate bridged Ti3O(COO)6 clusters. While many metal-organic frameworks have been synthesized with exposed divalent metal cations, there are comparatively few examples of metal-organic frameworks with coordinatively unsaturated trivalent metal centers. Among other potential applications, frameworks with exposed trivalent metal cations are of particular interest for ambient temperature H2 storage. Additionally, there are also very few reported titanium-based metal-organic frameworks and none that contain all titanium(III). Through a combination of adsorption measurements, diffraction analysis, EPR, infrared, and UV-vis-NIR spectroscopies, and magnetic measurements, this framework is shown to contain five-coordinate Ti3+ cations that irreversibly bind O2 to form titanium(IV)-superoxo and -peroxo species.",ucb,,https://escholarship.org/uc/item/07w7s2d2,,,eng,REGULAR,0,0
926,2362,Essays on School Choice and the Returns to School Quality,"Ajayi, Kehinde Funmilola","Card, David;",2011,"This dissertation consists of three studies which collectively seek to examine: what the barriers are to receiving a high-quality education in a merit-based school choice setting; how policy reforms can address these barriers; and what benefits students gain from attending a high-quality school. Altogether, the papers focus on understanding the role of socio-economic background in explaining differences in education-related decisions and student outcomes.Chapter 2 examines whether school choice programs increase opportunities for educational mobility or reinforce initial disparities in schooling. I address this question in the context of the public education system in Ghana, which uses standardized tests and a nation-wide application process to allocate 150,000 elementary school students to 650 secondary schools. As has been found in other settings, students from lower-performing elementary schools in Ghana apply to less selective secondary schools than students with the same test scores from higher-performing elementary schools. I consider four potential explanations for this behavior: differences in decision-making quality, imperfect information about admission chances, costs and accessibility of schooling, and preferences for school quality. I use detailed data from three cohorts of applicants to evaluate the relative importance of these explanations. My analysis suggests that differences in application behavior are largely due to poor decision-making and incorrect beliefs about admission chances, rather than differences in preferences or the costs and accessibility of schools.Building on the theoretical framework outlined in the preceding analysis, Chapter 3 evaluates the impact of institutional reforms in school choice settings. Focusing again on the case of Ghana, I estimate the effects of a series of reforms in the application process that expanded the number of choices students could list and encouraged students to select a diversified portfolio of schools. I use a difference-in-differences approach to analyze the effect of each reform and find that both reforms decreased the difference in selectivity of schools chosen by students from high-performing and low-performing elementary schools, which suggests that application and admission rules play a significant role in explaining differences in application behavior. Moreover, these results are consistent with a setting in which imperfect information has a strong impact on students' choices and the effects can both be explained as a consequence of uncertainty in the Ghanaian choice system.Chapter 4 uses a unique dataset on Ghana's education system to examine the effect of school quality on student outcomes. My analysis draws on exogenous variation in student assignment due to the fact that admission of elementary school students into secondary school in 2005 was based on students' ranking of their three most preferred choices and their performance on a standardized test. I compare students on different sides of the cutoff for admission to their lowest-ranked choice and find that students who are not admitted to their chosen school are assigned to schools of lower-quality and are less likely to complete secondary school. Additionally, these students are more likely to transfer out of their initially-assigned schools. However, those who do complete secondary school do not perform any worse on the exit exam several years later. Thus, students' behavioral responses to their admission outcomes appear to moderate the effects of school quality on educational attainment in this context.",ucb,,https://escholarship.org/uc/item/07x1q3z6,,,eng,REGULAR,0,0
927,2363,"The Ecological Street Tree: Mainstreaming the Production of Street Tree-based Ecosystem Services in Northern California Cities, 1980-2008","Seamans, Georgia Norma Silvera","Hester, Randolph T;",2010,"This dissertation examined the role of municipal and nonprofit actors, scientific research, and local geography in the ecological characterization of street trees in the planning and policy arena of three northern California cities between 1980 and 2008.  During this time period, the discourse of ecosystem services such as carbon sequestration, stormwater runoff management, criteria air pollutant reduction, avoidance of energy use and energy savings and thus reduction in power plant emissions, and wildlife habitat provision has been applied to street trees.  Municipal agencies and nonprofit organizations have engaged in policies, programs, and activities that are increasingly characterizing street trees by the ecosystem functions they can provide; this is what I call the ecological street tree.  Since trees have been planted along city streets, they have provided ecosystem services.  Yet, over the last 30 years, two actions began occurring more systematically: (1) the ecosystem functions provided by street trees were incorporated into planning documents and activities and (2) researchers began publishing scientific evidence to support policy and advocacy claims about the environmental services provided by street trees.To measure and analyze the emergence of the ecological street tree, a multiple-case study of three Northern California cities was conducted.  One of the strengths of the case study, proposed Yin in 2003, is its ""methodological versatility,"" i.e. multiple methods and sources of evidence can be incorporated into an overall strategy.    Furthermore, the dissertation met Yin's criteria for using a case study strategy: (1) the study propositions were framed as ""how"" and ""why"" questions; (2) the phenomenon could not be manipulated by me; and (3) the ecological street tree phenomenon are contemporaneous.  The study relied on multiple sources of evidence such as municipal and nonprofit reports and plans, nonprofit newsletters, newspaper articles, and interview transcripts.  Computer-aided content analysis of nonprofit newsletters and document analysis of municipal and nonprofit reports and plans and newspaper articles were used to track the emergence of the ecological street tree.  Transcripts from face-to-face qualitative interviews were also analyzed.  Qualitative interviews were used in this project because I required in-depth information from the individuals involved in the conceptualization of the ecological street tree.  Furthermore, there are few studies on how and why this conceptualization has taken place.This dissertation asked six questions:1.	Has there been a rise in the ecological characterization of the street tree?2.	What is the role of the urban forest nonprofit? 3.	Has the concept of the ecological street tree been mainstreamed through the nonprofit's newsletter?4.	How is the production of research evidence implicated in mainstreaming the ecological street tree? 5.	If different services are salient to different cities, what factors account for this difference? 6.	What strategies are used by different actors, in different cities to capture tree-based ecosystem services?Based on a cross-case analysis of data from Sacramento, Palo Alto, and San Francisco, the main conclusions of this dissertation are as follows:*	Between 1980 and 2008, there was a rise in the ecological characterization of street trees in all three cities.*	The urban forest nonprofit has played a role in mainstreaming the ecological street tree, but this role varies in strength among the cities.  Also, the nonprofit has not acted alone.  Municipal agencies are part of the network of actors advancing the ecological value of street trees.*	The newsletter is not the nonprofit's primary mode of communicating the ecosystem benefits of street trees; it is one mode among a ""landscape of communications"".*	The production and dissemination of urban forest research was critical to legitimizing the ecological street tree.*	Different services were salient to different cities and contributing factors included climate, geography, infrastructure, culture, and the history of urban forestry development in each city.*	Different strategies such as policies and reports, program development and activities, funding streams, and research collaboration were used capture street tree-based ecosystem services.â€ƒ",ucb,,https://escholarship.org/uc/item/07x1t32x,,,eng,REGULAR,0,0
928,2364,Allocation of Space and the Costs of Multimodal Transport in Cities,"Gonzales, Eric Justin",,2011,Integration Of Locational Decisions with the Household Activity Pattern Problem and Its Applications in Transportation Sustainability,ucb,,https://escholarship.org/uc/item/07x7h9pg,,,eng,REGULAR,0,0
929,2365,Engineering Nanoporous Materials for Transformation Optics and Energy,"Gladden, Christopher Walker","Zhang, Xiang;",2012,"The focus of this work is on the design and fabrication of novel optical devices that exploit gradients in refractive index to bend and redirect light with a new level of control. Optical devices have now entered every facet of modern life, and enormous potential exists to harness light in new or more efficient ways. One such potential application is cloaking, which allows an object to be hidden from optical detection. Such an effect can be achieved even at visible frequencies with a passive device that simply uses a gradient in refractive index.When light experiences a gradient in index it is effectively pushed or pulled in the direction of the gradient. The theory of transformation optics allows for gradient index optical devices to be designed by performing coordinate transformations to bend and deform a virtual optical space. In this way we can push or pull on a virtual space such that it is deformed in the desired manner, and then calculate the gradients in refractive index required to push and pull light so that it behaves as if it is in the virtual space. These devices can achieve interesting optical effects, such as optical cloaking and perfect light concentration. The limit to the technique of transformation optics, however, is the fabrication of the designed devices. Transformed devices often require anisotropy, magnetic resonance, and large changes in refractive index in arbitrary profiles. To address this issue we develop additional theory to reduce these requirements, and several fabrication techniques are explored, beginning with traditional nanolithography and then moving to larger scale electro-chemical processes. The key to fabrication of transformation optics devices is the effective medium theory, which states that if the components of a composite material are deep sub-wavelength, the material gains the effective properties of the mixture of its components. We have focused on porous silicon-based materials where the pore size is deep sub-wavelength and pore density can be spatially controlled. This variable pore density allows us to create gradients in refractive index. Using these techniques we have demonstrated low index materials with exceptionally low surface roughness, cloaking of light in the visible spectrum, the fabrication of a gradient index concentrator, and even a lithium-ion battery.The first chapter of this dissertation describes the background of transformation optics and explores several examples of how the technique can be applied. The second and third chapters describe the design and nano-fabrication required to realize a cloaking device that operates at visible frequencies. This includes the fabrication of tunable low index optical substrates, the design process for a transformation optical cloak and experimental realization of the cloaking device. The second part of this dissertation focuses on applying these technological developments towards renewable energy by developing better fabrication processes that can create larger and more practical devices. This specifically includes the design of gradient index solar concentrators, large scale high throughput gradient index fabrication techniques, and porous silicon for lithium ion batteries.",ucb,,https://escholarship.org/uc/item/0832q4rh,,,eng,REGULAR,0,0
930,2366,The Role of microRNAs in Olfactory Regeneration,"Estrada, Jose Emilio","Ngai, John;",2012,"The mammalian main olfactory epithelium is the sensory tissue dedicated to detecting volatile chemical odorants that contribute to our social and natural environment. Natural neuronal turnover throughout adulthood and the ability to regenerate large amounts of damaged tissue are unique properties of the main olfactory epithelium in adult mammals. Independent stem cell populations  are activated to give rise to two distinct modes of neurogenesis:  transit amplifying globose basal cells maintain homeostatic neuronal turnover while horizontal basal cells enter the cell cycle and regenerate all cell types of the OE upon severe injury. Recent evidence has pointed toward the role of specific miRNAmRNA interactions in the regulation of stem cell self-renewal and  differentiationThe ability to observe these two modes of adult neurogenesis in vivo,homeostatic maintenance and regeneration after severe injury, makes the mainolfactory epithelium an excellent model in which to study the molecular regulationof stem cells as they enter the cell cycle and differentiate into mature neuronalprogeny. However, to fully understand the mechanisms regulating theseprocesses it is imperative to elucidate both the nature and extent of theseinteractions at the genome-wide level. Our work in profiling the horizontal basalcell transcriptome provides supporting evidence that a large portion of thetranscriptome may be regulated by miRNA mediated repression, which acts byreinforcing an undifferentiated state. Genetic ablation of miRNA biogenesismachinery shows that miRNA are necessary to repress aberrant cell cycle reentryand may promote the expression of the stem cell maintenance geneTransforming protein 63.1",ucb,,https://escholarship.org/uc/item/0849d524,,,eng,REGULAR,0,0
931,2367,"The Mana of the Tongan Everyday: Tongan Grief and Mourning, Patriarchal Violence, and Remembering Va","Niumeitolu, Fuifuilupe 'Alilia","Hilden, Patricia Penn;",2019,"AbstractThe Mana of the Tongan Everyday: Tongan Grief and Mourning, Patriarchal Violence, and Remembering VabyFuifuilupe NiumeitoluDoctor of Philosophy in Comparative Ethnic StudiesUniversity of California, BerkeleyProfessor Patricia Penn Hilden, ChairIn this dissertation, I contend that the Tongan economies and systems of va trace their roots to the Sacred, the Feminine, the heartbeat of Tonganness that consists of the natural world, the fonua (land) to the Moana (ocean) and encompasses all the worlds in between. In addition, I contend that the desecration of the Sacred was the quintessential goal of the colonial project in Tonga. The losses are systemically supplanted with the colonial institution, heteropatriarchy that is symbolized by the new white and male Christian God at the forefront of the new Tongan nation. I show that the systemic desecration of the Sacred was the aim of several historical â€œracialized projectsâ€ that relentlessly deployed a phenomenon that scholars term as â€œwhite terror,â€ which Frantz Fanon explains in his statement about European colonizers being â€œthe bringer of violence into the home and into the mind of the nativeâ€ (38). The European â€œracialized projectsâ€ began in the seventeenth century with the arrival of the first Europeans; they were Dutch explorers traveling on Tongan waters in an expedition searching for capitalistic and opportunistic gain. At the moment of contact with Tonganness, the Dutch explorers deployed â€œwhite terrorâ€ through the heinous use of firearms on the bodies of unarmed Tongan families riding on a tongiaki on their way to Samoa. This historical moment serves as a harbinger outlining the unrelenting violence of European and U.S. â€œracialized projectsâ€ on Tonganness. The colonial trajectories tracking the maneuvers of â€œracialized projectsâ€ deploying â€œwhite terrorâ€ on Tonganness continues to the historical voyages of the renowned British Captain James Cook and his paradigmatic naming of Tonga as the â€œfriendly islanders.â€ Yet this seemingly playful moniker masked a political strategy meant to erase the maneuvers and desires of British patriarchal domination and violence in the eighteenth century, to the arrival of the London Missionary Society (LMS) and the proliferation of the prodigious and unrelenting Christian missionizing project through the deployment of unyielding and layered forms of patriarchal violence or â€œwhite terror.â€ As a result, colonial invaders influenced a new Tongan nation that centered the colonial institution of heteropatriarchy, which became symbolized by both the new white and male deity at the forefront of the Tongan nation and the contemporaneous maneuvers of the U.S. Empireâ€™s military occupation of Tonga during WWII. The production of topographies of unrelenting â€œwhite terrorâ€ on Tonganness were indelibly marked by militarized violence that had been deployed, unrelentingly, on the bodies of Tongan women and girls. Consequently, this colonial legacy opened the door for U.S. institutions such as the Mormon Church to enter and take center stage in Tonga in the twentieth and twenty-first centuries. In addition, the heavy hand of the Mormon Church continues to perpetuate and proliferate the objectives of U.S. Empire on Tonganness in Tonga and in the production of Tongan communities here in the U.S. The systemic desecration of the Sacred, a â€œdichotomyâ€ that Gloria Anzaldua describes as â€œthe root of all violenceâ€ (59), was and continues to be a deliberate colonial strategy to subjugate Tonganness not just in the past, but to replicate it in the present moment through the normalization of violence against women within every ay Tongan lives and within the boundaries of Tongan families and intimate relationalities. Thus, I examine the colonial productions of Tongan intimate spatialities such as the colonial family production of the nineteenth-century Tongan Nationalist Family and the contemporaneous production of the Tongan Mormon Family that traces its genealogy to the maneuvers of U.S. Empire during WWII in Tonga. Furthermore, the goals of the colonial project are unyielding and without end. Its desires for domination extend to the future generation of Tongans, for as Frantz Fanon argues in his theory of â€œ perverted logic,â€ the desecration of the Sacred and the simultaneous severing of Tongan va to the Sacred are colonial strategies that stifle Tongan mana and self-determination. In fact, the aim of the desecration of the Sacredâ€” according to Fanonâ€”is the â€œtotalâ€ colonization of Tonganness.",ucb,,https://escholarship.org/uc/item/0862n739,,,eng,REGULAR,0,0
932,2368,Inequality and Society: Mechanisms and Methods for Understanding the Consequences of Rising Income Inequality,"Hastings, Orestes Patterson","Fligstein, Neil;",2017,"Income inequality has risen sharply in the United States over the past forty years, yet there remains substantial uncertainty about the consequences of income inequality on social life. This dissertation advances research on these consequences by focusing on mechanisms through which inequality may matter and on the methods by which the effects of income inequality are determined.I primarily draw on data from the 1973â€“2014 General Social Surveys linked to administrative data of the demographic and economic characteristics of each respondentâ€™s state. This includes state-level income inequality, for which I utilize a new annual series based on household income tax returns. I also conduct an online survey experiment that manipulates perceptions of state-level income inequality.First, numerous scholarly accounts posit that as income inequality rises, individuals will be less satisfied with their own finances as they feel increasingly deprived relative to othersâ€”driving individuals to try to spend more as they engage in positional competition and increasing their anxieties as position in the income distribution becomes ever more crucial. I find that higher state-level income inequality decreases financial satisfaction overall, and that this effect is especially pronounced for those in the middle of the income distribution. Counterfactual simulations suggest rising inequality explains a substantial portion of the over-time decline in financial satisfaction.Second, concerns about rising income inequality are frequently linked to discussions about opportunity and mobility, yet little research explores if and how this inequality affects peopleâ€™s economic optimism, something with far reaching implications for life satisfaction, public opinion, and real economic mobility. Both the survey analysis and the experiment show that higher income inequality decreases economic optimism. The survey shows that the rate of change in inequality moderates the effect of the level of inequality, and that household income further moderates the effects of the level and change in income inequality on economic optimism. There was no evidence of this moderation in the experiment. Key differences between the two methodological approaches are discussed.Third, although both popular and scholarly accounts have argued that income inequality reduces trust, some recent research has been more skeptical, noting these claims are more robust cross-sectionally than longitudinally. Furthermore, although multiple mechanisms have been proposed for why inequality could affect trust, these have rarely been tested explicitly. I find little evidence that states that have been more unequal over time have less trusting people. There is some evidence that the growth in income inequality is linked with a decrease in trust, but these effects are sensitive to how time is accounted for. While much of the inequality and trust research has focused on status anxiety and feelings of relative deprivation, this mechanism receives the weakest support, and mechanisms based on societal fractionalization and exploitation receive stronger support.Finally, social comparisons of income have far-reaching consequences for individual decision-making and public policy, yet there persists a significant gap between â€œtrueâ€ relative income and what Americans perceive. Although one compelling explanation is that reference groups affect what people perceive as â€œaverage,â€ there is little consensus about who people compare themselves with. Previous research has proposed reference groups based on both geographic proximity and on sociodemographic similarity, but few studies have considered multiple reference groups systematically or simultaneously. I find that the effect of reference group income depends on both egoist and fraternal comparisons: higher median incomes of large reference groups and those with weak status hierarchies increases perceived relative income, while higher median incomes of small reference groups and those with strong status hierarchies decreases perceived relative income. These results have important implications for how reference groups are used in research on neighborhood effects, residential segregation, and income inequality.",ucb,,https://escholarship.org/uc/item/0864v18g,,,eng,REGULAR,0,0
933,2369,Characterization of Expression and Function of Heparan Sulfate on Murine B cells,"Trujillo, Damian Luis","Coscoy, Laurent;",2012,"Heparan sulfate (HS) modulates many cellular processes including adhesion, motility, ligand-receptor interaction, and proliferation. Here I report that murine B cells express very little HS, and that upon either bacterial or viral infection, HS is rapidly upregulated at the surface of B cells. My thesis work first describes the molecular details of HS induction and regulation in B cells. I also describe the generation and characterization of mice that are unable to express heparan sulfate on B cells, which we used to investigate the role of HS expression on B cells in vivo. With this conditional knockout mouse, I show the importance of HS expression for the generation and/or maintenance of antibody producing cells during a viral infection.  Finally, I show data that suggests that HS expression on B cells enhances the antibody response to antigens containing a heparan sulfate-binding domain. The work presented here provides the first detailed evaluation of HS expression on B cells.   This work illustrates the importance of HS in the response to certain classes of antigens, as well as in the development of the B cell response.",ucb,,https://escholarship.org/uc/item/0871696v,,,eng,REGULAR,0,0
934,2370,"Choreotopias: Performance, State Violence, and the Near Past","Aldape MuÃ±oz, Juan Manuel","Kwan, SanSan;",2020,"Choreotopias: Performance, State Violence, and the Near Past uncovers the central role of dance in producing new social and political relationships in MÃ©xico since the 1980s. In the process it considers how performances have worked historically and aesthetically in violent contexts. The 1980s marked an inflection point in the social life of residents in MÃ©xico. A contested presidential election, a deadly earthquake, a receding economy, a changing middle-class, and gender and sexuality political movements created unprecedented conditions. This project uses elements of choreography as metaphoric possibilities for understanding political organization. It highlights under analyzed artists and it explores how corruption and misuse of power shape dance and performance makers and, in turn, how these artists respond to such conditions. I describe the collective communities that use artistic interventions in zones where the State actively works to restrict political organization. Choreotopias foregrounds three main sites where artists contest national projects that control bodies and their actions, even in death. I assess creative practices such as experimental street choreography in the 1980s, collaborations between feminist video artists and punk youth in the late-1980s and 1990s, and dance theatre processes where artists have used forensic science aesthetics since the 1990s. Artists are attuned to the embodied dimensions of history, representative governments, and the national imaginary. Across these visual and embodied forms of expression, artists reveal the limits of the State and, in some cases, offer visions of a better world. Performance studies links gender studies and nationalism studies to construct compelling arguments about the importance of video art, dance, and theater in creating collective communities under the threat of abusive power. This project combines a mix of feminism studies, political philosophy, queer of color critique, performance analysis, and nationalism studies to consider how artists use dance and the choreography of waste to refuse control societies. This interdisciplinary study stresses the importance of creative practices in offering new ways of political formation. Chapter One, ""From Heterotopias to Choreotopias: Dance and Desmadre in Mexico City,"" examines the punk-aesthetic choreographies created by Asaltodiario, a Mexico City theater and dance troupe formed in 1987. I assess the relationship between choreography and precarity in times of rapid urban development after a deadly earthquake sped up citywide renovation plans. Reassessing Michel Foucault's concept of heterotopias, I argue that choreographic practices offered an interconnectedness that had been ruptured by the structural collapse of the city and the State. I assess Asaltodiario's commitment to valuing the public street as a site on which to change urban life in Mexico City. The artists collaborated with homeless youth and drug addicts. Performances by Asaltodiario followed a then-recent trend in Mexican contemporary dance practices not seen in the cultural sector before the 1980s. I offer the concept I provisionally call choreotopias, or those temporary and spatially bounded ""dance floors"" that occur in unexpected places. Choreotopias are artistic-political dwelling spaces that rearrange social relationships and create temporary communities, anticipate new social imaginaries, and attend to the historic, present, and future lives of unwanted bodies. The subsequent chapters consider the notion of choreotopias in relationship to gender and death. Chapter Two, ""Mosh Pit Desires: Video Feminisms and Punk Performances,"" examines how disaffected women who are associated with counterculture movements at the end of the twentieth century navigate MÃ©xico's culture of repression. I evaluate the video art projects Nadie es inocente (1986) and Alma punk (1992), created by feminist artist Sarah Minter in collaboration with disaffected youth from the punk rock commons. I analyze Minter's approach to video art and its intersection with punk's multi-sensory and ""genre-punking"" aspects. The performers all played themselves in these documentary-fiction projects. The content and form of the materials exceeded morally and aesthetically conservative campaigns that attempted to control how women's bodies could move. The final chapter, ""Searching for the Missing: Performance, Forensics, and Democracy"" offers a compelling analysis of the relationship between performance and forensic practices through an evaluation of artists such as NAKA Dance Theater, Violeta Luna, and Lukas AvendaÃ±o. These artists have used visual representations of death, official reports from missing persons' cases, and medical-legal methods to develop their performances. I maintain that the artists use dance and performance investigative aesthetics for public criticism in violent contexts without encumbering the liabilities of journalismâ€”a discipline that became a deadly practice. This project will be of interest to scholars and students in gender studies, nationalism studies, urban studies, performance studies, and Latinx/Latin American Studies.",ucb,,https://escholarship.org/uc/item/08b9x7tp,,,eng,REGULAR,0,0
935,2371,Results and Techniques in Multiuser Information Theory,"Aminzadeh Gohari, Amin","Anantharam, Venkatachalam;",2010,"In this dissertation we develop new techniques and apply them to prove new results in multiuser information theory. In the first part of the dissertation, we introduce the ``potential function method,"" and apply it to prove converses for a series of multiterminal network capacity problems. In the second part of the dissertation, we introduce the ``perturbation method,"" and apply it to the general broadcast channel problem, a fundamental open problem in information theory. Furthermore, we address a number of computational issues associated with the general broadcast channel.The first part of the dissertation is devoted to the ``potential function method"" and its application to multiterminal networks. This method works by finding certain properties of expressions which will imply that they dominate the capacity region, and then proving a given bound by a verification argument. We show that this method can provide a unified framework for proving converses. We begin by considering the category of rate region problems without output feedback. The ``dynamic programming flavor"" of the technique and its use of one-step equations are brought up here. To demonstrate the use of technique in problems with feedback, we consider the problem of information-theoretically secure secret key agreement under the well-known emph{source model} and emph{channel model}. The concept of ``state"" and its evolution during the interactive communication by the parties are brought up here. The upper bounds we prove in this section are new and are strictly better than their corresponding previously best known upper bounds (our new lower bounds are relegated to an appendix in the end of the dissertation as they were not derived using the potential function method). Finally, we demonstrate the use of technique in a problem that involves transmission of dependent sources over strong interference channels. The new feature is that the notion of achievable rate regions is replaced by that of admissible sources. The result proved in this section is also new.The second part of the dissertation begins by discussing the ``perturbation method,"" and its application to the general broadcast channel problem. The perturbation method is based on an identity that relates the second derivative of the Shannon entropy of a discrete random variable (under a certain perturbation) to the corresponding Fisher information. We apply this tool to make Marton's inner bound for the general broadcast channel computable. Before this, the latter region was not computable (except in certain special cases) as no bounds on the cardinality of its auxiliary random variables existed. The main obstacle in proving cardinality bounds is the fact that the Carath'{e}odory theorem, the main known tool for proving cardinality bounds, does not yield a finite cardinality result. In order to go beyond the traditional Carath'{e}odory type arguments, we identify certain properties that the auxiliary random variables corresponding to the extreme points of the inner bound satisfy. These properties are then used to establish cardinality bounds on the auxiliary random variables of the inner bound, thereby proving the computability of the region. We continue the second part of the dissertation with several results on computing a number of regions associated with the general broadcast channels. For instance, we prove various results that help to restrict the search space for computing the sum-rate for Marton's inner bound.",ucb,,https://escholarship.org/uc/item/08c3v404,,,eng,REGULAR,0,0
936,2372,Essays in the Economics of Medical Malpractice Law,"Shurtz, Ity Shmuel","Saez, Emmanuel;",2011,"This dissertation explores the interaction between medical malpractice law and medical treatment. The first chapter addresses the question: How do malpractice lawsuits affect physician behavior?  In this chapter, I study the impact of malpractice claims against obstetricians, a specialty that is regarded as particularly subject to malpractice concerns, on their choice of whether to perform C-sections, a common procedure that is thought to be sensitive to physician incentives. I find that immediately after an adverse event (defined as an obstetrical procedure that ultimately leads to a malpractice claim), C-section rates jump discontinuously by 4%. The increase in C-section rates persists even 4.5 years after the adverse event. Several other findings provide support to the view that fear of litigation and damage to reputation explain the results, rather than a mere response to the negative outcome that brought about the malpractice claim. First, unsuccessful claims, which, at the time of the adverse event, are perceived as less harmful to physicians' reputation, do not lead to an increase in C-section rates. Second, the impact on C-section rates is larger for patients insured by a commercial insurance provider, for which reputational concerns are likely to be stronger, since they are less constrained in their choice of physicians. In addition, the impact is smaller for experienced physicians, but not for those with a prior history of litigation claims. I also find evidence of peer effects: following an adverse event, a physician's colleagues also have higher C-section rates. Overall, this chapter shows that following an adverse event physicians adopt more conservative and costly treatment strategies and that their response is likely to be related to fear of litigation and damage to reputation.The impacts of malpractice regulations and financial incentives for providers are typically studied independently. In the second chapter of this dissertation, I show that in order to make both positive and normative statements about medical malpractice liability, one must consider the legal and financial incentives faced by healthcare providers jointly. I develop a simple model of physician behavior to show that the effect of tort reforms on treatment decisions depends critically on physicians' financial incentives. When treatment is not profitable at the margin, liability reduction leads to a decrease in treatment levels; conversely when treatment is profitable, liability reduction leads to an increase in treatment levels. Motivated by this simple theoretical framework, I analyze the impact of a tort reform in Texas that reduced malpractice liability on C-section rates and common pediatric surgical procedures. Consistent with the theory, the data show that the rate of C-sections for commercially insured mothers, which are thought to be profitable, increase by about 2% relative to the rate of C-sections for mothers on Medicaid, which are considered to be unprofitable. Similarly, the reform increases the incidence of profitable pediatric procedures relative to unprofitable ones. These findings help explain why the existing literature on optimal medical malpractice law is inconclusive and underscore the importance of understanding the economic incentives at play when designing legal regulations.",ucb,,https://escholarship.org/uc/item/08c6b4qr,,,eng,REGULAR,0,0
937,2373,"Integrating Resource Access, Livelihoods and Human Health around Lake Victoria, Kenya","Fiorella, Kathryn Joan","Brashares, Justin S.;",2015,"While ecosystem and human health have been closely linked, the mechanisms through which natural and social systems interact to support human health are rarely understood. Yet the rapid transformation of Earthâ€™s natural systems makes appreciating the drivers and ramifications of environmental change for human health particularly critical. To illuminate these links, I analyze the effects of changing resource access on household livelihoods, food security, health and nutrition. Specifically, I use the case of Lake Victoria, Kenya, to examine ways individuals and households respond to and affect fish availability. I employ both qualitative and quantitative methodologies, including a longitudinal household survey, ecological monitoring of fish catch, and qualitative in-depth interviews. My analyses examine a bi-directional relationship between fish availability and the health of people who depend on fish resources. First, I analyze how participation in fishing livelihoods affects fish consumption and food security. Next, I examine the bounds of resource availability in time and space in affecting household fish consumption. Further, I assess the impact of illness in fishers on fishing methods, and thereby fishery sustainability. Finally, I analyze how fish declines affect womenâ€™s access to fish, and, in particular, the power dynamics of transactional fish-for-sex relationships and HIV risk. Through these analyses, I aim to illuminate both the extensive effects of environmental change and mechanisms that tie the health of people and their environment.",ucb,,https://escholarship.org/uc/item/08c8d8kw,,,eng,REGULAR,0,0
938,2374,Searching for the Decay of 229m Th,"Swanberg, Erik","Norman, Eric B;",2012,"The existence of a low energy excited state in 229<\super>Th was first postulated in 1975. The energy of the state was revised over the years from less than 100 eV in 1975, to 3.5 Â± 1.0 eV in 1994, to 7.8 Â± 0.5 eV in 2009, among others. Despite this progress, the decay of the state has never been observed, and the half life is unknown. While no measurement described in this dissertation was successful in these experiments either, many areas were searched and exclusion plots were produced. Measurements included looking for the internal conversion electron from the isomer, as well as any possible Î³ emission. 229m<\super>Th was produced via the Î± decay of 233<\super>U, and the recoiling 229<\super>Th was collected on a catcher.  Short time scales were probed using an Î± coincidence measurement that was sensitive from tens of nanoseconds to almost a millisecond. Intermediate time scales used a mechanical shutter that was sensitive from 2 milliseconds to a few seconds. Longer time scale measurements moved the catcher from the source to the detector, and were sensitive from a few seconds to a few days. The proper function of the system was confirmed by measuring 235m<\super>U, which generated new results for the variation in its half life based on catcher material.",ucb,,https://escholarship.org/uc/item/08d696rk,,,eng,REGULAR,0,0
939,2375,Phonetic Universals and Phonological Change,"Javkin, Hector",,1977,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/08f2z7np,,,eng,REGULAR,0,0
940,2376,"Bone Strength Multi-axial Behavior - Volume Fraction, Anisotropy and Microarchitecture","Sanyal, Arnav","Keaveny, Tony M;",2013,"Trabecular bone is a major load-bearing tissue in the musculoskeletal system and is subjected to various multi-axial loads in vivo. For example, in the vertebral body, the trabecular bone is primarily subjected to uniaxial loads, in the proximal femur, trabecular bone is subjected to biaxial loads i.e. loads oriented at two mutually perpendicular directions, in distal radius, the trabecular bone can subjected to shear loads due to off-axis loading during a traumatic event. Understanding the multi-axial strength and underlying tissue-level failure mechanisms of human trabecular bone is of great clinical and scientific importance since age-related osteoporotic fractures primarily occur at trabecular bone sites, such as the hip, spine and wrist. With the onset of osteoporosis, there is an increase in porosity and deterioration of the microarchitecture of trabecular bone, which results in increased fragility and fracture susceptibility of trabecular bone.Using high-resolution, micro-CT based nonlinear finite element models, we investigated the strength of trabecular bone under compression, shear, biaxial and multiaxial loading conditions. Under uniaxial loading, it was shown that the variation in both compressive and shear strength was primarily attributed to the volume fraction of the trabecular bone, but the observed scatter in the ratio of the shear and compressive strength was attributed to heterogeneity and anisotropy of the trabecular microarchitecture. At the tissue-level, it was shown that shear loading leads to predominantly tensile tissue failure unlike compression loading that makes trabecular bone much weaker under shear loading. Under biaxial loading, it was shown that the yield strength varied with both volume fraction and anisotropy, and most of the variation in biaxial strength could be primarily attributed to similar variation of the uniaxial strengths with minor variations due to trabecular microarchitecture. Based on these results, the complete multi-axial yield strength behavior of trabecular bone was investigated for over 200 multi-axial load cases. A new yield strength criterion was then formulated in the six-dimensional strain space to mathematically characterize the multiaxial failure criterion of human trabecular bone.The research presented in this dissertation has provided considerable insight into the variation of both apparent-level strength and the tissue-level failure mechanisms of trabecular bone under various loading conditions. The role of bone volume fraction, anisotropy and microarchitecture on the uniaxial and multi-axial strength has been outlined. A multi-axial failure criterion has been formulated which can be used to noninvasively predict the strength of whole bones of osteoporotic patients using clinical CT scans.",ucb,,https://escholarship.org/uc/item/08j3c24w,,,eng,REGULAR,0,0
941,2377,Structural and Biochemical Characterization of the XPC DNA Repair and Stem Cell Coactivator Complex,"Zhang, Elisa Tiannuo","Tjian, Robert;",2015,"The regulation of eukaryotic gene expression is critical for proper cell homeostasis and development and relies largely on appropriate initiation of transcription. Transcriptional regulators, such as sequence-specific transcription factors, coactivators, general transcription factors, and chromatin remodelers, are often expressed in a cell-type specific manner to drive cell fate decisions and developmental transitions. Recently, the XPC DNA repair factor was identified as the Stem Cell Coactivator (SCC) complex, a key transcriptional coactivator required to assist OCT4 and SOX2 in driving the expression of key pluripotency genes in embryonic stem cells. 	Chapter 1 provides an introduction to mechanisms of eukaryotic transcriptional regulation, eukaryotic DNA repair, and the involvement of the XPC/SCC complex in both of these capacities. 	In chapter 2, I describe the first structures of the human apo and DNA-bound XPC holo-complex, as solved by electron microscopy. Comparison of the apo and DNA-bound structures identified key regions that become locally disordered upon engagement with DNA. Using a combination of sequence homology, computational docking of a partial homolog crystal structure into the EM density, and mapped interaction domains, I present a predictive model illustrating regions of key contacts on the XPC/SCC complex.	In chapter 3, I describe the involvement of largely non-specific RNA but not DNA or heparin in mediating the interaction between SOX2 and SCC in a dose-dependent fashion, a novel mode of potentially RNA-mediated transcriptional regulation. I provide evidence that there are little or no sequence or structural requirements for the RNA I also show that while this interaction is RNA-dependent, direct contacts can be formed between SCC and SOX2, suggesting that RNA is stabilizing and possibly even multimerizing existing protein-protein contacts between SCC and SOX2. 	In summary, the findings described in this dissertation provide a structural and biochemical framework for understanding the molecular and cellular mechanisms of SCC-driven gene regulation.",ucb,,https://escholarship.org/uc/item/08k0g39d,,,eng,REGULAR,0,0
942,2378,Regulation of meiotic chromosome interactions in Caenorhabditis elegans,"Barber, Nicola Caroline","Dernburg, Abby;",2011,"Faithful segregation of homologous chromosomes during meiosis requires pairing, synapsis, and crossing-over. In the nematode Caenorhabditis elegans, homolog pairing and synapsis depend on pairing centers, special regions near one end of each chromosome that interact with the nuclear envelope and cytoplasmic microtubules. Pairing centers are required for nuclear reorganization at the onset of meiosis. Here, I show that pairing centers recruit the Polo-like kinase PLK-2 to induce nuclear envelope remodeling, chromosome pairing, and synapsis. Recruitment of PLK-2 is also required to mediate a cell cycle delay and selective apoptosis of nuclei containing unsynapsed chromosomes, establishing a molecular link between these two quality control mechanisms. I also show that the checkpoint kinase CHK-2 is recruited to the interface of pairing centers and nuclear envelope proteins, consistent with its role in promoting homolog interactions and nuclear reorganization. Interestingly, PLK-2 is later recruited to the axes of paired and synapsed chromosomes where it plays a downstream role in synaptonemal complex dynamics. This work reveals new functions for the conserved family of Polo-like kinases, and advances our understanding of how meiotic processes are properly coordinated to ensure transmission of genetic information from parents to progeny.",ucb,,https://escholarship.org/uc/item/08m7f723,,,eng,REGULAR,0,0
943,2379,"Narrative and (Meta)Physical Paradox in ""Grande Sertao: Veredas"" and ""Pedro Paramo""","Schneider, Caroline LeFeber","Slater, Candace;",2011,"The canonical Latin American novels Grande SertÃ£o: Veredas (JoÃ£o GuimarÃ£es Rosa, 1956) and Pedro PÃ¡ramo (Juan Rulfo, 1955) mirror each other across their linguistic and other divides due to their uses of literary and regional spaces.  Previous studies of both works often focus â€“ as do many studies of the works individually â€“ on a perceived dichotomy between local regional material, and innovative, even ""universal"" aesthetic technique. However, the relationship between setting and form in the works in fact has little to do with conflict.  Rather, concurrent analysis of the novels shows that, in both, their archetypal regional landscapes are the detailed foundations for the narrative construction of complex (meta)physical spaces that are central to the works: spaces at once spiritual and placed.  The true paradox of the novels, their apparent dichotomy which is nonetheless unity, lies in the confluence of physical and metaphysical realms in both the sertÃ£o of Riobaldo's journeys and the Jalisco of Juan Preciado's terrible heritage: one a land and paths (sertÃ£o and veredas) in which God and the devil may reside, and one a town in which wrought evil and spiritual corruption have resulted in a land-bound and interactive purgatory.The complexity of these (meta)physical realms is executed through the innovative narrative techniques of the works, in both the protagonists' communicated concerns and experiences, and in the forging of paradox, uncertainty, and textual lacunae in the narratives themselves. In many ways, the spaces are their narratives: the sertÃ£o and Riobaldo's paths through it are as the redemoinho of his refrain, a whirlwind of narrative; and the Comala of Juan Preciado's experience, and in consequence the reader's own, is a woven quilt like that of the sown and abandoned fields, their harvest the voices of the unredeemed dead. The works' textures, their forged openness, plus their further narrative bridges to the reader, create spaces of participation, even integration: literary spaces of a reader/author co-construction of telluric spaces of (meta)physical landscape.Setting and form are not at odds: it is due to the works' complex and creative narrative techniques that the sertÃ£o and Comala are built, in collaboration with the reader, into powerfully internalized geographies of both material and immaterial origin â€“ (meta)physical geographies to explore the works' concerns about identity, violence, redemption, and human relationship to land.",ucb,,https://escholarship.org/uc/item/08n1z3pd,,,eng,REGULAR,0,0
944,2380,Macroeconomic Lessons from the Great Recession: Evidence using Microeconomic Methods,"Chodorow-Reich, Gabriel Isaac","Romer, Christina;",2013,"This dissertation reflects two recent developments in the study of economics. The first concerns real world events. Beginning in 2007, the countries of Western Europe and the United States experienced a series of financial shocks and an economic downturn unprecedented in their experience since the Great Depression seventy years before. Dramatic changes in the economy raise new questions or renew old lines of inquiry. During the recession, many countries turned to discretionary fiscal stimulus packages. The size of these packages and the severity of the downturns make understanding their effectiveness of critical importance. The first chapter in this dissertation analyzes empirically the effects on employment of one particular component of the fiscal policy response in the United States. The Great Recession also renewed interest in the importance of credit to firms. The second chapter in this dissertation attempts to answer whether the withdrawal of credit following the bankruptcy filing of Lehman Brothers played a role in propagating the Great Recession.The second development consists of the application of unit-level datasets and applied microeconomic methods to answer questions of macroeconomic importance. This research program recognizes that cross-sectional variation across individual units often exceeds the time-series variation of an aggregate. Moreover, cross-sectional studies can hold constant many macroeconomic variables that might otherwise confound the identification of a causal effect. The studies of fiscal policy and firm credit in chapters 1 and 2 apply microeconomic methods. A full answer to these questions, however, requires a correspondence between the cross-sectional analysis and the general equilibrium outcomes. In general equilibrium, spending in one geographic area may affect employment in another. Likewise, a firm not directly affected by the withdrawal of bank credit may still change its employment in response to the adjustments of other firms. One approach to the problem of general equilibrium involves building a theoretical model that nests the cross-sectional analysis as well as the general equilibrium effects. The third chapter of this dissertation demonstrates this approach in the context of the effect of the supply of credit.",ucb,,https://escholarship.org/uc/item/08n7g6kd,,,eng,REGULAR,0,0
945,2381,"The good, the bad, and the ugly of top-down executive control","Lee, Taraz","D'Esposito, Mark;",2012,"Theories of working memory and attention postulate that the prefrontal cortex (PFC) provides top-down signals to other brain regions in order to keep behaviorally relevant sensory information activated and to suppress competing task-irrelevant information.  Although there are numerous studies that suggest the PFC is a source of top-down modulation of posterior brain regions, the vast majority of these studies offer only indirect evidence in support of this claim.  Additionally, while this executive control is usually thought of as being beneficial to the task at hand, there is reason to believe that there are certain circumstances in which this control is deleterious to performance. Here we provide direct evidence using transcranial magnetic stimulation and neuroimaging that 1) the PFC is a source of top-down control of early visual regions and 2) interfering with PFC function can sometimes lead to paradoxical improvements in task performance.",ucb,,https://escholarship.org/uc/item/08r0b4kh,,,eng,REGULAR,0,0
946,2382,Emancipatory Digital Archaeology,"Morgan, Colleen","Tringham, Ruth;",2012,"As archaeologists integrate digital media into all stages of archaeological methodology it is necessary to understand the implications of using this media to interpret the past. Using digital media is not a neutral or transparent act; to critically engage with digital media it is necessary to create an interdisciplinary space, drawing from the growing body of new media and visual studies, materiality, and anthropological and archaeological theory. This dissertation describes this interdisciplinary space in detail and investigates the following questions: what does it mean to employ digital media in the context of archaeology, how do digital technologies shape inquiry within archaeology, can new media theory change interpretation in archaeology, and can digital media serve as a mechanism for an emancipatory archaeology? To attend to these questions I address digital media created by archaeologists as digital archaeological artifacts, understood as active members of a network of interpretation in archaeology. To give structure to this understanding I assemble three object biographies that identify the digital archaeological artifact's context, the authorship of the artifact, the inclusion of multiple perspectives involved in its creation, and evaluate the openness or ability to share the artifact. The three object biographies that constitute the body of this work are a digital photograph taken of a teapot at Tall Dhiban in Jordan, a digital video of an unexpected excavator participating at Ã‡atalhÃ¶yÃ¼k in Turkey, and a 3D reconstruction of a Neolithic building excavated at Ã‡atalhÃ¶yÃ¼k within the virtual world of Second Life. In these object biographies I weave together narrative, imagery and rigorous, theoretically informed analyses to provide a reflexive investigation of digital archaeological artifacts. Drawing from this research, I advocate a critical making movement in archaeology that will enable archaeologists to use digital media in an activist, emancipatory role to highlight inequity, bring the voices of stakeholders into relief, de-center interpretations, and to make things and share them.",ucb,,https://escholarship.org/uc/item/08s0v1fb,,,eng,REGULAR,0,0
947,2383,"Posets, Polytopes and Positroids","Chavez, Anastasia Maria","Williams, Lauren K;",2017,"This dissertation explores questions about posets and polytopes through the lenses of positroids and geometry. The introduction and study of positroids, a special class of matroids, was pioneered by Postnikov in his study of the totally nonnegative Grassmannian and has subsequently been applied to various fields such as cluster algebras, physics, and free probability. Postnikov showed that positroids, the matroids realized by full rank $k\times n$ real matrices whose maximal minors are nonnegative, are in bijection with several combinatorial objects: Grassmann necklaces, decorated permutations, Le-diagrams and plabic graphs. In the first chapter, following work of Skandera and Reed, we define the unit interval positroid arising from a unit interval order poset via its associated antiadjacency matrix. We give a simple description of the decorated permutation representation of a unit interval positroid, and show it can be recovered from the Dyck path drawn on the associated antiadjacency matrix. We also describe the unit interval positroid cells in the totally nonnegative Grassmannian and their adjacencies. Finally, we provide a new description of the $f$-vector of posets.The second chapter concerns the $f$-vector of a $d$-dimensional polytope $P$, which stores the number of faces of each dimension. When $P$ is a simplicial polytope the Dehn--Sommerville relations condense the $f$-vector into the $g$-vector, which has length $\lceil{\frac{d+1}{2}}\rceil$. Thus, to determine the $f$-vector of $P$, we only need to know approximately half of its entries. This raises the question: Which $\left(\lceil{\frac{d+1}{2}}\rceil\right)$-subsets of the $f$-vector of a general simplicial polytope are sufficient to determine the whole $f$-vector? We prove that the answer is given by the bases of the Catalan matroid.In the final chapter, we explore the combinatorial structure of Knuth equivalence graphs $G_{\lambda}$. The vertices of $G_{\lambda}$ are the permutations whose insertion tableau is a fixed tableau of shape $\lambda$, and the edges are given by local Knuth moves on the permutations. The graph $G_{\lambda}$ is the $1$-skeleton of a cubical complex $C_{\lambda}$, and one can ask whether it is CAT(0); this is a desirable metric property that allows us to describe the combinatorial structure of $G_{\lambda}$ very explicitly. We prove that $C_{\lambda}$ is CAT(0) if and only if $\lambda$ is a hook.",ucb,,https://escholarship.org/uc/item/08t5c2m9,,,eng,REGULAR,0,0
948,2384,A Grammar of DiegueÃ±o: The Mesa Grande Dialect,"Langdon, Margaret",,1966,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/08t8n3db,,,eng,REGULAR,0,0
949,2385,Dynamics of Electron Relaxation Studied Using Time-Resolved Photoelectron Spectroscopy in Liquid Microjets,"Elkins, Madeline Hyde","Neumark, Daniel M.;",2015,"The solvated electron, an isolated electron in polar solution, is a species of fundamental interest to the physics of solvation and to the understanding of condensed phase reactions in the presence of ionizing radiation. In radiation chemistry and biology, the solvated electron acts as a powerful reductant and has been shown to act as a reagent in a wide range of processes from atmospheric chemistry to radiation-induced DNA damage. As the ""simplest'' quantum mechanical solute, the solvated electron serves as a fundamental probe of solute-solvent interaction and thus has been used as a model system for studying solvation processes.Here, time-resolved photoelectron spectroscopy is used to study the binding motifs and solvation dynamics of solvated electrons in various polar solvents. This thesis is separated into three parts: first, an introduction to the research and description of the apparatus, second, results from experiments on the relaxation dynamics of the solvated electron after photoexciation and after generation by charge-transfer-to-solvent and, third, preliminary designs and proposed experiments for a liquid jet photoelectron spectrometer with attosecond time resolution.",ucb,,https://escholarship.org/uc/item/08v7236d,,,eng,REGULAR,0,0
950,2386,Aircraft Design Optimization as a Geometric Program,"Hoburg, Warren Woodrow","Abbeel, Pieter;",2013,"Recent advances in convex optimization make it possible to solve certain classes of constrained optimization problems reliably and efficiently.These techniques offer significant advantages over general nonlinear optimization methods.In this thesis, conceptual-stage aircraft design problems are formulated as geometric programs (GPs), which are a specific type of convex optimization problem.Modern GP solvers are extremely fast, even on large problems, require no initial guesses or tuning of solver parameters, and guarantee globally optimal solutions.They also return optimal dual variables, which encode sensitivity information that is especially relevant in an aircraft design context.These benefits come at a price:all objective and constraint functions -- the mathematical models that describe aircraft design relations -- must be expressed within the restricted functional forms of GP.Perhaps surprisingly, this restricted set of functional forms appears again and again in prevailing physics-based models for aircraft systems.Moreover, for models that cannot be manipulated algebraically into the forms required by GP,one can use methods developed in this thesis to fit compact GP models that accurately approximate the original models.Each of these ideas is illustrated by way of concrete examples from aircraft design.",ucb,,https://escholarship.org/uc/item/0928939m,,,eng,REGULAR,0,0
951,2387,The Impact of the Central Asian Mountains on Downstream Storminess and Monsoon Onset,"Park, Hyo Seok","Chiang, John C. H;",2010,"In the first part of the thesis, the role of the Central Asian mountains on North Pacific storminess is examined using an atmospheric general circulation model by varying the height and area of the mountains. A series of model integrations shows that the presence of the Central Asian mountains suppresses North Pacific storminess by 20-30% during boreal winter. Their impact on storminess is found to be small during other seasons. Two main causes of the reduced storminess are diagnosed. First, the decrease in storminess appears to be associated with a weakening of downstream eddy development. The mountains disorganize the zonal coherency of wave packets and refract them more equatorward. As the zonal traveling distance of wave packets gets substantially shorter, downstream eddy development gets weaker, leading to the weakening of transient eddy kinetic energy and storminess. Second, the Central Asian mountains suppress the global baroclinic energy conversion. The decreased baroclinic energy conversion, particularly over the Eastern Eurasian continent decreases the number of eddy disturbances entering into the Western North Pacific. The `barotropic governor' does not appear to be a dominant factor in explaining the results. In the second part of the thesis, the impact of the Tibetan Plateau on the onset of the South Asian summer monsoon is examined using an atmospheric general circulation model. A series of model integrations shows that rainfall increases downstream of the mountains, whereas upstream of the mountains experiences anomalous subsidence. In particular, the Tibetan Plateau triggers low-level cross-equatorial flow and substantial rainfall over the Bay of Bengal during the pre-monsoon season (April-May). While the low-level cross-equatorial flow begins in May, the monsoon's onset over the Arabian Sea is suppressed until early June. The delayed onset over the Arabian Sea is probably because of the anomalous subsidence induced by the earlier moist convection over the Bay of Bengal. It is suggested that the earlier rainfall over the Bay of Bengal could be the response of stationary waves to the orographic forcing, such as low-level cyclonic motions downstream of the Tibetan Plateau. Because low-level westerlies exist over the Southern Tibetan Plateau during pre-monsoon season, the stationary wave response to the mountains is likely to occur.",ucb,,https://escholarship.org/uc/item/0937c3xj,,,eng,REGULAR,0,0
952,2388,Myth and Appropriation: Fryderyk Chopin in the Context of Russian and Polish Literature and Culture,"Lin, Tony Hsiu","Nesbet, Anne;Frick, David;",2014,"Fryderyk Chopin's fame today is too often taken for granted. Chopin lived in a time when Poland did not exist politically, and the history of his reception must take into consideration the role played by Poland's occupying powers. Prior to 1918, and arguably thereafter as well, Poles saw Chopin as central to their ""imagined community."" They endowed national meaning to Chopin and his music, but the tendency to glorify the composer was in a constant state of negotiation with the political circumstances of the time. This dissertation investigates the history of Chopin's reception by focusing on several events that would prove essential to preserving and propagating his legacy. Chapter 1 outlines the indispensable role some Russians played in memorializing Chopin, epitomized by Milii Balakirev's initiative to erect a monument in Chopin's birthplace Å»elazowa Wola in 1894. Despite their political tension, Russia and Poland came together in the common cause of venerating Chopin. Chapter 2 examines two instances of the Russian-Polish cooperation: Chopin centennial celebrations in 1910 and 1949. These celebrations featured speeches and commemorative concerts that later became the norm.  Chapter 3 considers the International Chopin Piano Competition, founded in independent Poland in 1927. As one of the earliest international musical contests of its kind and scale, the Chopin Competition effectively turned Chopin from a national into an international figure. Furthermore, the public nature of the competition led to the engagement of the entire society, involving spectators and the press. Besides the three main chapters, two Interludes survey the representations of Chopin and his music in Russian and Polish literature. In addition to literature, this dissertation analyzes works of visual art and music to consider the process of mythmaking and its implications.",ucb,,https://escholarship.org/uc/item/09b3q63z,,,eng,REGULAR,0,0
953,2389,A measurement of the 2 neutrino double beta decay rate of 130Te in the CUORICINO experiment,"Kogler, Laura Katherine","Freedman, Stuart J;",2011,"CUORICINO was a cryogenic bolometer experiment designed to search for neutrinolessdouble beta decay and other rare processes, including double beta decay with two neutrinos(2Î½Î²Î²). The experiment was located at Laboratori Nazionali del Gran Sasso and ran for a period of about 5 years, from 2003 to 2008. The detector consisted of an array of 62 TeO2 crystals arranged in a tower and operated at a temperature of âˆ¼10 mK. Events depositing energy in the detectors, such as radioactive decays or impinging particles, produced thermal pulses in the crystals which were read out using sensitive thermistors.The experiment included 4 enriched crystals, 2 enriched with 130Te and 2 with 128Te, in order to aid in the measurement of the 2Î½Î²Î² rate. The enriched crystals contained a total of âˆ¼350 g 130Te. The 128-enriched (130-depleted) crystals were used as background monitors, so that the shared backgrounds could be subtracted from the energy spectrum of the 130-enriched crystals. Residual backgrounds in the subtracted spectrum were ï¬t using spectra generated by Monte-Carlo simulations of natural radioactive contaminants located in and on the crystals. The 2Î½Î²Î² half-life was measured to be T1/2 = [9.81 Â± 0.96(stat) Â± 0.49(syst)]Ã—1020 y.",ucb,,https://escholarship.org/uc/item/09c0f2xc,,,eng,REGULAR,0,0
954,2390,"""Orgulloso de mi CaserÃ­o y de Quien Soy"": Race, Place, and Space in Puerto Rican ReggaetÃ³n","Rivera, Petra Raquel","Hintzen, Percy C;",2010,"My dissertation examines entanglements of race, place, gender, and class in Puerto Rican reggaetÃ³n.  Based on ethnographic and archival research in San Juan, Puerto Rico, and in New York, New York, I argue that Puerto Rican youth engage with an African diasporic space via their participation in the popular music reggaetÃ³n. By African diasporic space, I refer to the process by which local groups incorporate diasporic resources such as cultural practices or icons from other sites in the African diaspora into new expressions of blackness that respond to their localized experiences of racial exclusion. Participation in African diasporic space not only facilitates cultural exchange across different African diasporic sites, but it also exposes local communities in these sites to new understandings and expressions of blackness from other places.  As one manifestation of these processes in Puerto Rico, reggaetÃ³n refutes the hegemonic construction of Puerto Rican national identity as a ""racial democracy."" Similar to countries such as Brazil and Cuba, the discourse of racial democracy in Puerto Rico posits that Puerto Ricans are descendents of European, African, and indigenous ancestors. Yet this conception simultaneously holds that racial divisions in Puerto Rico are obsolete, a notion at serious odds with the reality of persistent white privilege on the island.  I argue that discourses of racial democracy are spatially constituted in a ""cultural topography.""  Different conceptions of blackness are ""emplaced"" within distinct locations throughout the island such that certain towns or neighborhoods are associated with particular ideas about blackness. These multiple ideas about blackness operate differently vis-Ã -vis racial democracy.  On one hand, representations of a ""folkloric"" and ""antiquated"" blackness have been emplaced in the northeastern town of LoÃ­za to symbolize the African component of racial democracy.  At the same time, an understanding of blackness as ""abject"" is emplaced in urban housing projects, or caserÃ­os, as the ""primitive"" counterpoint to the more ""modern"" whitened Puerto Rican nation sustained by racial democracy. The perceived connections between distinct conceptions of blackness and particular places throughout Puerto Rico often conflict and, in the process, expose some of the contradictions inherent to dominant discourses of racial democracy.  Specifically, mapping out the various emplacements of blackness in Puerto Rico calls attention to racial democracy discourses' simultaneous inclusion and exclusion of blackness into elitist depictions of Puerto Rican identity.  At the same time, possibilities exist within these contradictions to express other definitions of blackness that refute racial democracy discourses.  ReggaetÃ³n inserts one alternative understanding of blackness into Puerto Rican society. Multiple processes of migration and cultural exchange exposed Puerto Ricans in San Juan to musical practices from various sites in the African diaspora including hip hop, Jamaican dancehall, and Panamanian reggae en espaÃ±ol.  Puerto Rican youth combined these various musical influences in unique ways to produce reggaetÃ³n.  At the same time, however, participation in diasporic space also introduced Puerto Rican youth to new ways of imagining blackness. Through their own experiences with migration to the United States, Puerto Rican youth encountered new forms of racialization that, in turn, led to the recognition of comparable experiences of racial exclusion with other African diasporic communities.  ReggaetÃ³n developed in part through these various diasporic connections, expressing black identities that countered the local processes of racialization in Puerto Rican discourses of racial democracy.  My dissertation begins with the development of underground, the precursor to reggaetÃ³n, in the mid-1990s.  I discuss the specific ways in which engagement with diasporic space created the conditions of possibility for reggaetÃ³n to develop in Puerto Rico.  My chapters about the initial creation of underground and the success of artist Tego CalderÃ³n demonstrate that diasporic resources give new valence to understandings of blackness that transcend the boundaries of Puerto Rico's cultural topography. In addition, I show how these cultural expressions also transform the meanings of already widely recognized Afro-Puerto Rican signifiers. History has shown us that radical cultural politics are often met with tremendous resistance, and the constitutive forms of this resistance can tell a story about national and racial imperatives.  I analyze two government-sanctioned campaigns aimed at censoring reggaetÃ³n to consider the ways that diasporic spaces pose serious challenges to the authority of racial democracy.  In the 1990s, the Puerto Rican government confiscated underground recordings as part of a larger anti-crime initiative.  Several years later, in 2002, the Puerto Rican Senate launched an Anti-Pornography Campaign that targeted sexual imagery in reggaetÃ³n music videos.  I consider these initiatives as evidence of the perceived ""threat"" that reggaetÃ³n posed to hegemonic constructions of racial democracy.  An analysis of the multiple censorship campaigns against reggaetÃ³n reveals the pedagogical moves that the Puerto Rican government employed in an attempt to maintain the hegemony of elitist discourses of racial democracy.Situated in the intersecting fields of race theory, diaspora theory, and cultural studies, my dissertation employs interdisciplinary methodologies and a multi-faceted theoretical approach to consider the work of diaspora in Puerto Rican reggaetÃ³n.  My research illuminates the ways that reggaetÃ³n articulates understandings of blackness that challenge hegemonic constructions of race and national identity and, in the process, create possibilities for connecting blackness and Latinidad in new ways.",ucb,,https://escholarship.org/uc/item/09d1f6q0,,,eng,REGULAR,0,0
955,2391,Cognitive Underpinnings of Math Learning and Early Play Based Intervention,"Green, Chloe Teressa","Worrell, Frank C;Bunge, Silvia A;",2017,"For my graduate research presented in this dissertation, I employed cognitive development theory to evaluate key cognitive abilities that contribute to both typical and atypical mathematical learning in children and adolescence. I incorporated these findings into a novel play-based intervention for children at-risk for math learning disabilities (MLD). My dissertation work is represented in the following three papers.In the first paper, I synthesized literature identifying the common cognitive precursors to math learning disabilities. I analyzed how core numerical processing weaknesses (e.g. number sense) in early childhood, restrict the developmental plasticity of mathematical learning. Furthermore, I identified how common weaknesses in other domain-general cognitive abilities (e.g. working memory and processing speed) serve to further exacerbate mathematical learning weaknesses in MLD. Taken together, these findings inform theoretically grounded approaches used to identify children with MLD, and identified promising approaches to early intervention.In the second paper, I sought to characterize the cognitive factors that are most predictive of future math achievement in typically developing children and adolescents. I analyzed data from a longitudinal study of children between 6 and 21 years old who completed abattery of neuropsychological testing at 3 time points over the course of 5 years. I was specifically interested in the role of fluid reasoning (FR), or the ability to think logically to solve novel problems. Fluid reasoning has not been particularly well characterized in relation to math achievement. Structural equation modeling was employed to compare the relative contribution of spatial abilities, verbal reasoning, age, and FR in predicting future math achievement. This model accounted for nearly 90% of the variance in future math achievement. In this model, FR was the only significant predictor of future math achievement; age, vocabulary, and spatial skills were not significant predictors. The findings build on Cattellâ€™s conceptualization of FR as a scaffold for learning, showing that this domain-general ability supports the acquisition of rudimentary math skills as well as the ability to solve more complex mathematical problems.In the third paper, I pilot-tested a novel game-play intervention for children at risk for math learning disabilities. The intervention involved playing numeracy and cognitive speed games four days per week for 14 weeks. A single-case-study design was employed to evaluate response to intervention in 3 first- and second-grade students. The intervention took place during an after-school program. All three students demonstrated a significant improvement in weekly arithmetic fluency and marginal improvements in processing speed. However, there was variability during baseline testing in arithmetic fluency scores, limiting causal inference. This study provides preliminary evidence to suggest that game-based interventions that train basic numeracy and processing speed skills, may serve as an effective preventative approach that builds on childrenâ€™s intrinsic motivation to engage in playful learning.",ucb,,https://escholarship.org/uc/item/09f3z3n7,,,eng,REGULAR,0,0
956,2392,"Analyzing the Structure of Informal Transport: The Evening Commute Problem in Nairobi, Kenya","Chavis, Celeste","Daganzo, Carlos;",2012,"In many parts of the world, particularly in developing countries, informal privately-operated transportation plays an integral role in people's mobility. This study systematically analyzes the development and structure of informal transit systems as a function of the network, user, and modal characteristics for an evening commute problem along a linear corridor where passengers originate uniformly from a central business district and have destinations uniformly distributed along the corridor. The model jointly takes into account user mode choice and operator fare and frequency decisions. Three types of operators each with different objectives are analyzed: (1) informal transit with competing operators, (2) a welfare-maximizing government, and (3) a private monopoly or company. Policies, such as fare regulation and vehicle licensing schemes, are presented to help rationalize private and informal transit service using a government-operated service as the baseline. The use of continuum approximation tools allow for a 2-D graphical representation of the regulatory environments. Using Nairobi, Kenya as a case study, it was concluded that with proper regulation and oversight informal transit can perform similar to publicly provided transit.",ucb,,https://escholarship.org/uc/item/09h9f1jk,,,eng,REGULAR,0,0
957,2393,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,"Davidson, Joseph, Jr.",,1977,A Kashaya Grammar (Southwestern Pomo),ucb,,https://escholarship.org/uc/item/09j1q0rp,,,eng,REGULAR,0,0
958,2394,Gold(I)-Mediated Nucleophilic Additions to Allenes: from Enantioselective Catalysis to Supramolecular Chemistry,"Wang, Zhan","Toste, F D;",2012,"The ability of gold(I) complexes to activate carbon-carbon multiple bonds for the addition of nitrogen and oxygen nucleophiles has emerged as an attractive synthetic method in organic chemistry.  While many transformations catalyzed by gold(I) have been reported in recent years, few studies provide insight into the mechanism of gold(I) mediated processes.  The first half of this thesis will discuss the development of an enantioselective gold(I)-catalyzed hydroamination of allenes with hydroxylamine and hydrazine nucleophiles and mechanistic studies of this and related systems.  The second half of this thesis will cover our efforts to incorporate gold(I) catalysts into supramolecular host-guest complexes and will highlight potential advantages of this novel approach to gold(I) reactions.Chapter 1.  In this chapter, we report a novel enantioselective method for formation of isoxazolidines, pyrazolidines and tetrahydrooxazine heterocycles catalyzed by dinuclear bisphosphine gold(I) complexes.  Furthermore, we discuss our efforts toward the development of a dynamic kinetic resolution of chiral allenes via intermolecular asymmetric hydroamination.  Chapter 2. We perform detailed kinetic studies to elucidate the mechanism of two transformations: the gold(I)-catalyzed hydroamination of allenes with hydrazine nucleophiles and the intramolecular hydroamination of dienes in the presence of alcohol co-catalysts.  While the hydroamination of allenes proceeds via an outer sphere Â¥Ã°-acid mechanism, the related reaction with dienes likely proceeds via a Lewis-acid assisted-BrÂ©Âªnsted acid pathway.  Chapter 3.  This chapter will focus on our efforts to incorporate catalytically active gold(I) complexes into tetrahedral supramolecular guest of the form Ga4L6.  Using such a complex, we catalyze the intramolecular hydroalkoxylation of allenes.  We demonstrate that encapsulation of gold(I) increases the rate of reaction relative to Me3PAuBr and also leads to improved catalyst lifetime.  Chapter 4.  Herein we apply the supramolecular host-guest complex with gold(I) and Ga4L6 to tandem reactions with enzymes.  The encapsulated complex performs as well, if not better than the Â¡Â°freeÂ¡Â± gold(I) complex in chemoenzymatic processes with a variety of esterase and lipases.  In some cases the free gold(I) cations significantly reduce the rate of enzyme catalysis while the encapsulated complex has no effect on enzyme activity.",ucb,,https://escholarship.org/uc/item/09m0s499,,,eng,REGULAR,0,0
959,2395,Interaction of Higher-Order Laser Modes with Underdense Plasmas,"Djordjevic, Blagoje Zoran","Schroeder, Carl B;Bale, Stuart D;",2019,"Laser-plasma interactions have become a rapidly growing area of modern plasma physics and an important subfield of it is laser-plasma acceleration. Using high-intensity  lasers, one can drive a plasma structure with electric-field gradients three orders of magnitude higher than the gradients found in traditional, radio-frequency accelerators. This promises to enable great technological advances in medicine, spectroscopy, and experimental particle physics, as well as to open up new avenues of studying matter under extreme conditions. An important aspect of laser-plasma acceleration is how the transverse electromagnetic field of the laser affects and drives an accelerated particle via longitudinal waves in the plasma. To understand how the laser interacts with the plasma, it is necessary to understand that the transverse characteristics of the laser dictate its longitudinal propagation dynamics. The transverse radiation field of the laser pulse can be described in various ways and decomposed into bases of orthogonal modes. The presence of multiple higher-order modes, copropagating through the plasma, leads to mode beating. Likewise,  these modes propagate at different velocities through the plasma and are susceptible to nonlinear interactions with the plasma to varying degrees. The primary objective of this thesis is to understand how higher-order laser modes interact with the plasma and with one another. In this work, we discuss the detrimental consequences that mode beating may have on a laser-plasma accelerator and how higher-order modes can be filtered out using specially designed plasma structures. Also discussed is how higher-order mode content can be controlled and utilized to shape and control the wakefields. These ideas are extended to the concept of the plasma undulator as a plasma-based light source. Lastly, we discuss how nonlinear effects can excite higher-order mode content as path to understanding laser pulse break up into multiple filaments.",ucb,,https://escholarship.org/uc/item/09m5q2r5,,,eng,REGULAR,0,0
960,2396,Mechanisms of adaptive radiation in Encelia,"DiVittorio, Christopher Todd","Ackerly, David D;Fine, Paul V;",2014,"Adaptive radiations are thought to be one of the most important processes generating biological diversity on Earth. Although the existence of adaptive radiations is not in doubt, the exact mechanisms via which adaptation to different habitats translates into lineage splitting has been debated for over a century. Since the time of Darwin, biologists have invoked trade-offs during adaptation to environmental gradients as being the key to linking adaptation to species formation. However, identification of the causal gradients, trade-offs and extrinsic selective regimes involved in adaptive or ecological speciation requires detailed fieldwork and experimentation and cannot be inferred using genetic or observational data alone. This series of studies provides an empirical, experimental basis for the conclusion that adaptation to different habitats is driving divergence in Encelia (Asteraceae) a genus of perennial desert shrubs that has radiated extensively throughout the Mojave and Sonoran deserts. Encelia is an ideal system for studying the mechanisms of adaptation and speciation because all taxa are completely interfertile, and many are considered habitat specialists and form hybrid zones wherever their distributions abut. Reciprocal transplant field experiments between two taxa, Encelia palmeri and E. ventorum, showed that extremely strong postzygotic divergent natural selection is primarily responsible for preventing species fusion. A resource manipulation experiment between the same two species further showed that this was due to trade-offs caused by a gradient in water availability between dune and desert habitats. Patterns of seed germination, herbivory, and burial by sand were also important and showed interactions with taxa and habitat indicating that divergent selection at this site is complex and multifaceted.A second study involving the same two species asked whether a signature of postzygotic selection can be seen in the distribution of phenotypes through time and along a gradient of habitat disturbance. I found that novel phenotypes suggestive of recombination are produced at a high frequency but are not present in the adult population indicating a role for postzygotic natural selection in removing the products of recombination from the population. This conclusion was further strengthened by looking at disturbed versus undisturbed habitats. Disturbed habitats contained novel phenotypes suggestive of recombination that were absent in the undisturbed sites indicating that the hybrid swarms that frequently follow disturbance are likely caused by an alteration of postzygotic selective pressures. The higher resource availability of the disturbed sites suggests that a relaxation of selection is likely responsible. These results are elaborated on further by conducting a literature review of cases where hybridization, species fusion or hybrid swarm formation are associated with disturbances or changes in environmental forcing factors. I found that there are many cases in the literature that describe taxa maintained primarily if not solely by extrinsic postzygotic selection, although this appeared to be more true for plants than animals. Animals, in contrast, were isolated primarily by prezygotic barriers including allopatry and sexual selection. This discrepancy may help explain the disagreement between zoologists and botanists for the past century about species definitions; zoologists have typically favored prezygotic criteria while many botanists have pointed out that definitions based on reproductive compatibility fail to capture much of the variation observed among plants.Finally, I conducted a common garden experiment and an analysis of climatic niches with eight species in Encelia in order to determine whether there is evidence that selection is driving divergence among other taxa in the genus in addition to E. palmeri and E. ventorum. All taxa studied showed strong climatic differentiation according to temperature and precipitation, and trait divergence in the group was high with some taxa showing evidence of the evolution of key traits allowing colonization of high altitude and hyper-saline habitats. Despite this, trait variation did not reliably follow the predictions of leaf economic theory either within or among taxa. This may be due to the existence of multiple alternative ecologically equivalent strategies that may introduce noise into low-dimensional analyses of functional traits and climate. There were, however, exceptions to this pattern. Specific leaf area showed coherent variation within taxa but not among taxa, and ecotypes E. farinosa also varied in the directions predicted by leaf economic theory for nearly all of the traits examined.These and previous studies establish Encelia as a classic case of adaptive radiation and underscore the importance of empirical, field-based studies for disentangling the complex mechanisms driving adaptation and the formation of new species.",ucb,,https://escholarship.org/uc/item/09r205n5,,,eng,REGULAR,0,0
961,2397,Topics in Northern Pomo Grammar,"O'Connor, Mary",,1987,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/09r209d2,,,eng,REGULAR,0,0
962,2398,"Multiculturalism and the Imagined Community: Diversity, Policy, and National Identity in Public Opinion","Wright, Matthew Patrick","Citrin, Jack;",2010,"Developed democracies in Europe and elsewhere are experiencing an unprecedented influx of culturally diverse immigrants and asylum seekers into their national communities. For the study of political psychology, a critical issue is how the pattern of ethnic group relations affects process of identity formation and change and, derivatively, the pattern of public support for a range of public policies with implications for social inclusion and equality. While there has been extensive commentary on the issue of multiculturalism both in America and abroad, the specific question of how the perceived threat of heightened immigrant diversity on the normative content of national identity (that is, the question of ""who are we"") has only recently begun to receive systematic attention in the scholarly literature.Multiculturalism has both a purely demographic and a political meaning. The politics of diversity also refers to specific policies governments enact in order to either encourage or discourage cultural pluralism. The specific policies at issue typically refer to the representation and recognition of minority groups and may encompass affirmative action, language policies, border control, access to welfare state programs, and citizenship laws. Debate has raged for years among political philosophers of multiculturalism over the desirability of such policies. Some suggest that government policies devoted to ""cultural recognition"" and minority group representation ease political tensions in these increasingly diverse communities and promote national loyalty. Others suggest the reverse: government attempts to promote cultural recognition through multiculturalism policy harden barriers among groups, foster prejudice and hostility to immigration, and erode the overall sense of national attachment in a country. This debate, too, has only now begun to receive rigorous empirical scrutiny.The present study examines three main questions: first, how can we think about what the social boundaries of the national community might be, and why do they matter? Are narrower, more bounded notions of the nation in-group related to mass preferences onimmigration, immigrants, and cultural diversity more generally? More centrally, this study examines how immigrant diversity and policies of cultural recognition shape mainstream citizens' conceptions of normative national identity. Is it indeed the case that ethnic diversity and political multiculturalism undermine social harmony, by provoking - via cultural threat - the desire among mainstream citizens to adhere to a more ""ascriptive"" and exclusionary definition of who truly belongs on their soil? Finally, I go to the heart of the philosophical debates on cultural recognition, by asking whether immigrants' allegiance to the nation is in indeed undermined in ""multicultural"" nations. Are they less willing to participate in the political process? Do they have less faith in the political system and governing institutions? Are they less trusting and/or socially engaged?Merging aggregate level economic and demographic measures with cross-national public opinion data, I argue that mass publics do indeed seem to have reacted to increased levels of immigrant diversity by constraining their notion of who truly belongs to the national community along more ""ethnic"" lines. Furthermore, this backlash has been heightened in the countries that have more fully committed themselves to cultural recognition, versus those that have favored minority integration; this finding provides empirical support for many of the philosophical critiques of multiculturalism that have emerged vociferously in recent years. On the other hand, immigrants themselves appear to benefit from political multiculturalism, all else equal; they exhibit higher levels of satisfaction with politics and politicians in their adoptive nation, and perceive substantially less discrimination against them along ethnic, racial, linguistic, and religious lines.",ucb,,https://escholarship.org/uc/item/09v989kf,,,eng,REGULAR,0,0
963,2399,"The Actuality of Critical Theory in the Netherlands, 1931-1994","Barr Clingan, Nicolaas Peter","Jay, Martin E;",2012,"This dissertation reconstructs the intellectual and political reception of Critical Theory, as first developed in Germany by the ""Frankfurt School"" at the Institute of Social Research and subsequently reformulated by JÃ¼rgen Habermas, in the Netherlands from the mid to late twentieth century. Although some studies have acknowledged the role played by Critical Theory in reshaping particular academic disciplines in the Netherlands, while others have mentioned the popularity of figures such as Herbert Marcuse during the upheavals of the 1960s, this study shows how Critical Theory was appropriated more widely to challenge the technocratic directions taken by the project of vernieuwing (renewal or modernization) after World War II. During the sweeping transformations of Dutch society in the postwar period, the demands for greater democratization--of the universities, of the political parties under the system of ""pillarization,"" and of society more broadly--were frequently made using the intellectual resources of Critical Theory. In turn, the development of a progressive, ""posttraditional"" society in the Netherlands, which appeared to reach its apex in the 1970s, suggested to a number of intellectuals that Habermas's more sanguine ""theory of communicative action"" best conceptualized the democratic achievements of modern society and the continuing prospects for the ""rationalization of the lifeworld,"" through which injustices and social pathologies could be exposed to the scrutiny of critical reason. Critical Theory, then, had an ""actuality"" that went well beyond academia and had continuing ""relevance""--another meaning of the Dutch actualiteit or German AktualitÃ¤t--for understanding the past and future rationalization of society.There was, moreover, another sense in which Dutch thinkers interpreted the actuality of Critical Theory. In the transnational process of reception, ideas and theories are inevitably shaped by the contexts in which they are taken up. This study begins with the Dutch social democrat Andries Sternheim, who worked at the Institute's office in Geneva in the 1930s, and shows how tensions arose over the more speculative philosophical premises of Kritische Theorie, as formulated in director Max Horkheimer's key 1937 essay ""Traditional and Critical Theory."" These tensions prefigured the later emphases and inflections given to Critical Theory by its intellectual supporters (and detractors) in the Netherlands and reflected, I argue, a ""discourse of actuality"" with which Habermas's thought had greater resonance. Although some Dutch intellectuals gravitated towards the earlier arguments of Horkheimer and Theodor W. Adorno, which identified the roots of modern pathologies of social domination in the widespread expansion of ""enlightened thought"" into forms of ""instrumental reason,"" these claims were seen by many as overly pessimistic and speculative, particularly in comparison to Habermas's thought. Habermas argued that Horkheimer and Adorno's analysis of the ""dialectic of enlightenment"" had obscured a different form of rationality, made possible only by the rationalization of the lifeworld, namely ""communicative rationality,"" which had its basis not in the arguably metaphysical, ""emphatic"" concept of reason to which Horkheimer and Adorno appealed, but rather in the immanent practices of everyday, intersubjective communication. Although Habermas insisted that the telos of mutual understanding, toward which non-strategic communicative practices were oriented, remained a counterfactual ideal, many of his adherents went beyond Habermas in ascribing an empirical actuality to the idea of communicative rationality. Furthermore, even as Habermas's theory was challenged in the course of the ""modernism/postmodernism"" debates of the 1980s, Dutch scholars frequently interpreted ""poststructuralist"" thought in ways that broadened the concept of rationality, rather than pitting one side against the other, as the leading German and French antagonists often did. By putting the ideas of Critical Theory into historical and comparative relief, this reception history goes beyond strictly philosophical studies of the relative validity of the Frankfurt School and Habermas's competing forms of thought. The Dutch example offers a particularly revealing view into the ""actuality"" of what Habermas called ""the unfinished project of modernity,"" as well as its potential limitations. In concluding, however, I follow other scholars of early Critical Theory in arguing that Adorno's thought in particular may have its own pressing actuality, even as its philosophical premises are considered outdated in the wake of the ""linguistic turn."" Against the historical developments of the last decades of the twentieth century--not least in the Netherlands--we might yet have something to learn from Adorno's thought, even in its most apocalyptic and utopian exaggerations.",ucb,,https://escholarship.org/uc/item/0b12z2xk,,,eng,REGULAR,0,0
964,2400,Non-covalent Interactions in the Gas Phase: Infrared Spectroscopy and Nanocalorimetry of Ion-Biomolecule Complexes,"O'Brien, Jeremy Thomas","Williams, Evan;",2012,"In this dissertation, experiments investigating the effects of non-covalent interactions on the structure and reactivity of gas-phase, ion-containing clusters, including ion-amino acid and ion-water complexes, are presented and discussed.  Ions generated using electrospray ionization and analyzed using Fourier transform ion cyclotron resonance mass spectrometers at the University of California, Berkeley, and the FOM Institute for Plasma Physics Rijnhuizen in Nieuwegein, The Netherlands, are investigated using infrared photodissociation/infrared multiple photon dissociation (IRPD/IRMPD) spectroscopy, collision-induced dissociation, and electron capture dissociation (ECD).  Using ion nanocalorimetry, the effect of varying the potential which controls the kinetic energy of the thermally generated electrons used for ECD experiments is determined to be minimal under typical experimental conditions.  The results indicate that only a small population of electrons with near zero velocity relative to the trapped hydrated ions are captured and result in the observed ECD products.  IRMPD spectra of proton-bound heterodimers containing valine and basic amines indicate that the structure of the heterodimer changes with increasing basicity of the amine suggesting that the structure of an amino acid in a proton-bound dimer may be different than in isolation which breaks an assumption made in the determination of gas-phase basicities using the kinetic method.  IRMPD spectra of ion-amino acid complexes reported here reveal the effects of gas-phase acidity, ion polarity/charge state and ion size on the relative zwitterionic stability of the amino acids.  The coordination numbers (CN) of hydrated divalent transition metals are determined using IRPD spectroscopy and photodissociation kinetics measurements.  The CN of Cu2+ is lower than the other metal ions (CN = 4) due to Jahn-Teller effects brought about by the d9 electronic configuration of the ion.  IRPD spectra of SO42- with up to 80 water molecules attached indicate that the structure induced by the dianion dominates over the intrinsic water-water interactions for clusters with up to 43 water molecules, well beyond the first solvation shell.  The roles of ion charge state and cluster size in the structure of gas-phase ""nanodrops"" are investigated for the largest mass-selected ionic clusters for which IRPD spectra have been reported.  Effects of ion charge state are observed for nanodrops containing up to âˆ¼250 water molecules in contrast with recent reports that only the first solvation shell is affected by ions in aqueous solution.",ucb,,https://escholarship.org/uc/item/0b17x6vx,,,eng,REGULAR,0,0
965,2401,A polarization sensitive bolometer array for the South Pole Telescope and measurements of Cosmic Microwave Background secondary anisotropies,"George, Elizabeth Marie","Holzapfel, William L.;",2013,"Over the past several decades, measurements of the Cosmic Microwave Background (CMB) have been a major driving force in our understanding of cosmology. Measurements of the CMB on large angular scales places tight constraints on the parameters of the Lambda-CDM cosmological model. Measurements of the CMB at smaller angular scales constrains secondary anisotropies, such as the thermal and kinetic Sunyaev Zel'dovitch (tSZ and kSZ) effects, which constrain the structure of the universe at later times. The CMB is also polarized, and the polarization signal encodes information about both the inflationary era and late-time structure formation in our universe. In the first part of this dissertation, I introduce the CMB and discuss the measurements that have been made so far and what we can learn from them.The South Pole Telescope (SPT) is a 10 meter telescope that is dedicated to measuring the CMB down to small angular scales. So far, the SPT has housed two instruments, SPT-sz and SPT-pol.  SPT-sz was sensitive to temperature and completed a 2540 square degree survey of the southern sky from 2008-2011. SPT-pol is a polarization sensitive camera that was deployed in 2012, and has been conducting a polarization survey since that time. These instruments both use photon-noise limited superconducting Transition Edge Sensors (TESes) in large numbers to obtain their high sensitivities. TES design and fabrication in large format arrays was critical to the success of these instruments. The second part of this dissertation focuses on the instrumentation of the SPT. I present the theory behind TES bolometer design with a focus on detector stability and optimal performance. Finally, I discuss the design and performance of the two instruments, with a focus on the detector development for the SPT-pol instrument.In the final part of this dissertation, I use the full 2540 square degree SPT-sz survey to measure the power spectrum from 1850 < l < 11000 at 90, 150, and 220 GHz and constrain small scale CMB anisotropies. We use a multifrequency cross-spectrum analysis to fit the data using a model that includes the lensed primary CMB, secondary CMB anisotropies (tSZ and kSZ effects), and foregrounds. The foreground power consists of a poisson component from radio sources, poisson and clustered components from dusty point sources, galactic cirrus, and tSZ-CIB correlations. These data represent the most sensitive measurement of the small angular scale CMB power spectrum to date. The data constrain the amplitude of the tSZ and kSZ signals, from which we can learn about late time structure and the epoch of reionization.",ucb,,https://escholarship.org/uc/item/0b2428w1,,,eng,REGULAR,0,0
966,2402,Approximate svBRDF Capture From Uncalibrated Mobile Phone Video,"Albert, Rachel A.","O'Brien, James K;",2018,"I describe a new technique for obtaining a spatially varying BRDF (svBRDF) of a flat object using printed fiducial markers and a cell phone capable of continuous flash video. My homography-based video frame alignment method does not require the fiducial markers to be visible in every frame, thereby enabling me to capture larger areas at a closer distance and higher resolution than in previous work. Clusters of pixels in the resulting panorama that correspond to like materials are fit with a BRDF based on a recursive subdivision algorithm, utilizing all the light and view positions obtained from the video. I demonstrate the versatility of this method by capturing a variety of materials with both one- and two-camera input streams and rendering my results on 3D objects under complex illumination.",ucb,,https://escholarship.org/uc/item/0b89g7h3,,,eng,REGULAR,0,0
967,2403,Abandoned Channels as Refugia for Sustaining Pioneer Riparian Forests,"Hayden, Maya","Battles, John J;",2015,"In North America, cottonwood (Populus) and other members of the family Salicaceae are considered to exhibit the classic colonization-competition trade-off. Adaptations that make them highly successful colonists in disturbance-prone floodplain environments appear to reduce their ability to compete for resources in more benign environments. Because cottonwood recruitment dynamics are so tightly coupled to the natural disturbance regime, river regulation has led to widespread decline in seedling establishment along the active channel. However, pioneer trees that require disturbance events for regeneration use a variety of strategies for persisting during periods of relative stability. The use of spatial refugia, while critical for population recovery of many mobile organisms, is generally not considered an important strategy for trees. Episodic channel abandonment in meandering river systems, and the subsequent infill and terrestrialization of the abandoned channel, has been recently highlighted as critical for maintaining the population of a key pioneer riparian tree species, Fremont cottonwood (Populus fremontii). While controls on seedling establishment along the active channel are predominantly abiotic, temporal changes in abandoned channels result in a shift toward a more physically stable and more competitive environment. How a species with such strong colonization traits can establish in abandoned channels, and for how long, are the main questions addressed in my dissertation.In a controlled community mesocosm experiment (Chapter 1), I used field-informed gradients of substrate texture and herbaceous cover to test interacting effects of soil moisture and interspecific competition on first year cottonwood seedling survival. I found that primary controls on cottonwood seedling establishment switched from abiotic to biotic drivers as a result of the biogeomorphic development of abandoned channels. Like on the active channel, seedlings were strongly moisture limited in conditions immediately following channel abandonment, but competition became a more important determinant of survival in conditions representative of an older abandoned channel. However, I also found that cottonwood seedlings were better competitors than anticipated, and were able to survive in the more physically benign and competitive conditions as well as the more physically stressful conditions to which they are classically adapted. This suggests that abandoned channels provide conditions favorable for cottonwood establishment for a broad window of time. While my focus was on understanding mechanisms controlling seedling establishment within abandoned channels, my results clarify interactions between abiotic and biotic controls that are more broadly applicable within a meandering river corridor. My results also add to evidence that species lie along a competition-colonization continuum, and have implications for incorporating secondary recruitment locations into management and restoration of pioneer riparian forests.The well-known interspecific trade-off of high-light growth in early colonizing species, versus low-light survival in species that are better resource competitors, leads to the logical conclusion that pioneer trees such as cottonwood are shade intolerant. Empirical evidence also finds seedlings rarely establish in vegetated areas. So how are cottonwoods able to establish in the more densely vegetated (i.e., shadier) environment of abandoned channels? I examined this question using a shade-cloth mesocosm experiment (Chapter 2), in which I considered interactive gradients of moisture and light. In dry ecosystems, sunlight can be considered both a resource and a stress, with shading reducing evaporative demand by maintaining a cooler understory microclimate. I found that shading resulted in reduced vapor pressure deficits and higher soil moistures, and a strong positive effect on first year cottonwood seedling survival in the Mediterranean climate of California. However, seedlings that survived to the end of the experiment showed decreased final biomass and root growth in shade. These results suggest that cottonwoods are much more plastic in their shade tolerance than has been previously assumed. The positive effect of shade on survival was observed regardless of soil moisture availability, whereas the positive effect of sun on growth was much stronger in wetter conditions, suggesting that soil moisture is the dominant limiting resource for seedling growth. I conclude that their high moisture requirement, along with their plasticity in shade tolerance, is what allows cottonwoods to successful establish in abandoned channels. I suggest that lack of understory recruitment along the active channel may have more to do with the fact that vegetated areas are typically higher and drier as a result of biogeomorphic feedbacks.Based on current theory and previous empirical evidence, it was considered likely that cottonwood establishment would be limited to the period immediately following channel abandonment, when the abandoned channel retains elements of physical dynamism to which the species is well adapted. However, my experimental evidence of better competitive ability and more plastic shade-tolerance, particularly under conditions of high soil moisture, suggested that cottonwood establishment in abandoned channels may be supported for a much longer window of time. On the Sacramento River, California, I used a chronosequence (space-for-time) approach to understand patterns of cottonwood establishment as a function of time since abandonment and biogeomorphic stage (Chapter 3). I examined patterns in overstory community composition and cottonwood diameter size distribution as indicators of past establishment dynamics. I used tree ring analysis to sample the age structure and determine establishment timing. I addressed the main sources of error in the use of tree ring analysis for determining establishment age by cross-dating tree cores, and developing and applying correction-factors for cores potentially missing the earliest years of growth. I also quantified the uncertainty around my correction-factors using a Monte Carlo simulation approach, and propagated the uncertainty through my analyses. My results support a recruitment window that begins at channel abandonment, and consistently lasts ~ 20 years, regardless of site age. Thus, while channel abandonment is episodic, a cohort of trees always successfully establishes, and cohorts can continue to successfully establish for a period of approximately two decades. The duration of the recruitment window extends in time the spatial refuge provided by abandoned channels, and thus helps ensure continued persistence of the cottonwood population.",ucb,,https://escholarship.org/uc/item/0b99j4hx,,,eng,REGULAR,0,0
968,2404,Group Antenatal Care in Rwanda: Examining Successes and Challenges in Implementation,"Singh, Kalee","Prata, Ndola;",2020,"Increasing access to quality maternal health services for women in low and middle income countries (LMICs) is crucial in improving maternal and child health outcomes. Antenatal care (ANC) serves as an opportunity to provide clinical care and may be associated with an increased odds of delivering in a health institution.1â€“3 To address the utilization and quality of ANC, in 2016 the World Health Organization (WHO) provided updated recommendations for health systems interventions. Group antenatal care (group ANC) provided by qualified health-care professionals was highlighted as a promising intervention warranting implementation in the context of research.4 Traditional ANC models are centered around one-on-one visits between a pregnant woman and her health care provider. The visit focuses on the womanâ€™s physical health, with the provider communicating clinical information and self-care recommendations. In contrast, group ANC uses an integrated approach, combining a physical assessment with group education and support. The Preterm Birth Initiative (PTBi) â€“ Rwanda conducted a cluster randomized controlled trial of group ANC to assess the impact of this care model on preterm birth. This dissertation assesses Community Health Workersâ€™ (CHWs) experiences in piloting group ANC, the impact of introducing ultrasound in the context of individual ANC, and successes and challenges in implementing the program with fidelity to the program design.Using focus group data, paper 1 explores the successes and challenges CHWs experienced in implementing the group ANC pilot program. Successes included effective training, collaboration between nurses and CHWs and perceived complementarity of group ANC with CHWâ€™s existing community based responsibilities. Challenges included a need for additional training on various clinical topics, financial barriers to CHW engagement and problems in scheduling.There is limited research on how the introduction of a technological intervention, such as ultrasound, can affect the delivery of other critical services when working in low resource settings.  Paper 2 utilizes patient data from 8,910 women from 18 health centers to measure the association between the provision of ultrasound and the number of ANC screening factors assessed. There was no significant association between the provision of ultrasound and number of ANC risk factors assessed. Given the several limitations in conducting this study, we recommend further studies to assess the impact of ultrasound on ANC services.Through the analysis of qualitative Activity Reports and quantitative Model Fidelity Assessments, paper 3 examines whether the program was implemented with fidelity. Results found implementing group ANC with model fidelity required: group ANC scheduling, preparing the room for group ANC sessions, provider capacity to co-facilitate group ANC, and facilitator knowledge and skills regarding group ANC content and process. Duration of program implementation was associated with improvements in implementation fidelity. Results illustrate the importance of monitoring implementation fidelity through the use of quantitative and qualitative tools and providing consistent training and mentorship throughout program implementation.",ucb,,https://escholarship.org/uc/item/0bb114x3,,,eng,REGULAR,0,0
969,2405,Expression of Ligands for the NKG2D Activating Receptor are Linked to Proliferative Signals,"Jung, Heiyoun","Raulet, David H;",2011,"NKG2D is a stimulatory receptor expressed by natural killer cells and subsets of T cells. The receptor recognizes a set of self-encoded cell surface proteins that are usually not displayed on the surface of healthy cells but are often induced in transformed and infected cells. NKG2D engagement activates or enhances the cell killing function and cytokine production programs of NK cells and certain T cells. Emerging evidence suggests that different ligands are to some extent regulated by distinct signals associated with disease states, thus enabling the immune system to respond to a broad range of disease-associated stimuli via a single activating receptor.  The research presented in this thesis demonstrated that at least one of the murine NKG2D ligands, RAE-1 Îµ (gene: Raet1e), is transcriptionally activated by signals associated with cell proliferation. Primary cultured fibroblasts from normal tissue, which did not express RAE1 in vivo, were induced to express large amounts of cell surface RAE-1 Îµ upon culture in vitro. RAE-1 Îµ induction was associated with increased Raet1e transcription. Inhibitor and other experiments showed that RAE-1 Îµ induction was dependent on sustained cell proliferation induced by growth factors, but was not dependent on a variety of other pathways, including the DNA damage response, oxidative stress or serum components other than growth factors.  In vivo, correlative, evidence showed that RAE-1 Îµ was also displayed on rapidly proliferating brain cells in early embryos, but was extinguished at later stages of brain development as cell proliferation slowed.In line with these findings, analysis showed that the Raet1e promoter was more active in proliferating cells than quiescent cells. In silico analysis of the Raet1e promoter for potential binding sites for transcription factors associated with cell cycle regulation revealed multiple putative binding sites for E2F family members. Chromatin immunoprecipitation studies demonstrated that the Raet1e promoter was bound in vivo by E2F family members in proliferating cells. Overexpression of activating E2F family members induced endogenous Raet1e transcripts in nonproliferating cells and transactivated a Raet1e promoter reporter plasmid. Transactivation was strongly inhibited if two putative E2F sites in the promoter were mutated.   Collectively, the data show that transcription factors that regulate cell proliferation regulate the transcriptional activation of cell surface ligands that target transformed and infected cells for destruction by NK cells and T cells. E2F family members are often mutated in cancer and proliferative signals often accompany viral infections. On the other hand, many healthy cells undergo proliferation without induction of RAE-1 on the cell surface, so it appears that proliferation by itself is not always sufficient to induce RAE-1 expression at the cell surface. Other stress pathways activated during tumorigenesis or in infected cells are likely to work together with the proliferative signal documented here to ensure that cell surface expression of NKG2D ligands is restricted to unhealthy cells.",ucb,,https://escholarship.org/uc/item/0bf4c1f4,,,eng,REGULAR,0,0
970,2406,An improved all-speed projection algorithm for low Mach number flows,"Chaplin, Christopher Michael","Colella, Phillip;Papadopoulos, Panayiotis;",2018,"The objective of this work is to design and implement a high fidelity method for simulatinglow Mach number compressible flows ranging from inviscid gas dynamics to compressibleNavier-Stokes with reactions. In order to achieve this objective, we introduce three goals.The first is that âˆ†t only be constrained by the advective CFL condition. The seconds goalis that we attain accurate treatment of compressible effects such as bulk compression andexpansion due to thermo-chemical processes and long wavelength acoustics. In particular,we want to capture well-known low Mach number limiting behavior. The third and finalgoal is that we want to use easy solvers.",ucb,,https://escholarship.org/uc/item/0bf8314r,,,eng,REGULAR,0,0
971,2407,Rationality and Expected Utility,"Gee, Max","Kolodny, Niko;Buchak, Lara;",2015,"We commonly make a distinction between what we simply tend to do and whatwe would have done had we undergone an ideal reasoning process - or, in other words,what we would have done if we were perfectly rational. Formal decision theories, likeExpected Utility Theory or Risk-Weighted Expected Utility Theory, have been usedto model the considerations that govern rational behavior.But questions arise when we try to articulate what this kind of modeling amountsto. Firstly, it is not clear how the components of the formal model correspond toreal-world psychological or physical facts that ground judgments about what weought to do. Secondly, there is a great deal of debate surrounding what an accuratemodel of rationality would look like. Theorists disagree about how much flexibility a rational agent has in weighing the risk of a loss against the value of potential gains, for example.The goal of this project is to provide an interpretation of Expected Utility Theorywhereby it explicates or represents the pressure that fundamentally governs howhuman agents ought to behave. That means both articulating how the componentsof the formal model correspond to real-world facts, and defending Expected UtilityTheory against alternative formal models of rationality.",ucb,,https://escholarship.org/uc/item/0bg2x07c,,,eng,REGULAR,0,0
972,2408,Controller Design and Implementation for a Powered Prosthetic Knee,"Rosa, Matthew David","Kazerooni, Homayoon;",2012,"Powered prosthetic knees offer many improvements over passive devices; however, the added actuation is difficult to control due to the lack of input from the user. A means of controlling a powered prosthetic knee is proposed by predicting the type of swing behavior the knee must perform based on the position of the foot relative to the person during toe-off. The software to accomplish this is implemented via a finite state machine which activates specific knee angle reference generators for each state. The reference generators serve as the input of a nonlinear feedback controller to ensure accurate positioning of the knee joint.",ucb,,https://escholarship.org/uc/item/0bh2s1f3,,,eng,REGULAR,0,0
973,2409,Threat modeling and circumvention of Internet censorship,"Fifield, David","Tygar, Justin D;",2017,"Research on Internet censorship is hampered by poor models of censor behavior. Censor models guide the development of circumvention systems, so it is important to get them right. A censor model should be understood not just as a set of capabilitiesâ€”such as the ability to monitor network trafficâ€”but as a set of priorities constrained by resource limitations.My research addresses the twin themes of modeling and circumvention. With a grounding in empirical research, I build up an abstract model of the circumvention problem and examine how to adapt it to concrete censorship challenges. I describe the results of experiments on censors that probe their strengths and weaknesses; specifically, on the subject of active probing to discover proxy servers, and on delays in their reaction to changes in circumvention. I present two circumvention designs: domain fronting, which derives its resistance to blocking from the censor's reluctance to block other useful services; and Snowflake, based on quickly changing peer-to-peer proxy servers. I hope to change the perception that the circumvention problem is a cat-and-mouse game that affords only incremental and temporary advancements. Rather, let us state the assumptions about censor behavior atop which we build circumvention designs, and let those assumptions be based on an informed understanding of censor behavior.",ucb,,https://escholarship.org/uc/item/0bh5c50p,,,eng,REGULAR,0,0
974,2410,Large-scale sparse regression models under weak assumptions,"Raskutti, Garvesh","Yu, Bin;Wainwright, Martin J;",2012,"Many modern problems in science and other areas involve extraction of useful information from so-called 'big data.' However, many classical statistical techniques are not equipped to meet with the challenges posed by big data problems.  Furthermore, existing statistical methods often result in intractable algorithms. Consequently the last $15-20$ years has seen a flurry of research on adapting existing methods and developing new methods that overcome some of the statistical and computational challenges posed by problems involving big data. Regression is one of the oldest statistical techniques. For many modern regression problems involving big datasets, the number of predictors or covariates $\pdim$ is large compared the number of samples $n$, causing significant computational and statistical challenges. To overcome these challenges, many researchers have proposed imposing sparsity on the vector of regression co-efficients $\beta \in \mathbb{R}^{\pdim}$. Furthermore, researchers have proposed using $\ell_1$-based convex penalties for estimating $\beta$ under the sparsity assumption since they yield implementable algorithms with desirable performance guarantees. While there was already an established body of work on developing procedures for sparse regression models, most existing results rely on very restrictive model assumptions. These assumptions are often not satisfied for many scientific problems. In this thesis, we relax $3$ restrictive model assumptions that are commonly imposed in the literature for estimating sparse regression models. The $3$ assumptions are: (1) Strict sparsity, that is the vector of regression co-efficients $\beta$ contains only a small number of non-zeros; (2) The covariates or predictors are independent; (3) Response depends linearly on covariates. Given that these $3$ model assumptions are often not satisfied for many practical settings, it is important to understand whether existing theoretical results exhibit robustness to these assumptions. In Chapter 2, we impose a weaker notion of sparsity known as $\ell_q$-ball sparsity on $\beta$ which ensures the vector of regression co-efficients lies in an $\ell_q$ ball, but need not have any non-zeros. We prove that under the weaker $\ell_q$-ball sparsity assumption, it is possible to develop estimators with desirable mean-squared error behavior, even in the regime where $\pdim \gg n$.The weakest known condition under which the Lasso achieves optimal mean-squared error rate is the restricted eigenvalue condition~\cite{vandeGeer07,BicRitTsy08, Negahban09}. Existing results prove that in cases when the covariates are independent, the restricted eigenvalue condition is satisfied. However, the setting when predictors or covariates are correlated are also of interest and there was considerably less work dealing with this case. In Chapter 3, we prove that the restricted eigenvalue condition is satisfied for various correlated Gaussian designs, including time series models, spiked covariance models and others.Finally, in Chapter 4 we analyze sparse additive models, a non-parametric analog of sparse linear models, in which each component function lies in an ellipsoid or more formally a Reproducing kernel Hilbert space $\Hil$. Hence we weaken the assumption that our response depends on the covariate via a linear function. A new $\ell_1$-based polynomial-time method is developed and we prove that this method has desirable mean-squared error performance, even when $\pdim \gg n$. Furthermore, we prove lower bounds on the mean-squared error for estimating sparse additive models that match the upper bounds for our method. Hence our algorithm is optimal in terms of mean-squared error rate.",ucb,,https://escholarship.org/uc/item/0bm0j7z0,,,eng,REGULAR,0,0
975,2411,Super-hot (T > 30 MK) Thermal Plasma in Solar Flares,"Caspi, Amir","Lin, Robert P;",2010,"The Sun offers a convenient nearby laboratory to study the physical processes of particle acceleration and impulsive energy release in magnetized plasmas that occur throughout the universe, from planetary magnetospheres to black hole accretion disks.  Solar flares are the most powerful explosions in the solar system, releasing up to 1032-1033 ergs over only 100-1,000 seconds.  These events can accelerate electrons up to hundreds of MeV and can heat plasma to tens of MK, exceeding ~40 MK in the most intense flares.  The accelerated electrons and the hot plasma each contain tens of percent of the total flare energy, indicating an intimate link between particle acceleration, plasma heating, and flare energy release.X-ray emission is the most direct signature of these processes; accelerated electrons emit hard X-ray bremsstrahlung as they collide with the ambient atmosphere, while hot plasma emits soft X-rays from both bremsstrahlung and excitation lines of highly-ionized atoms.  The Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) observes this emission from ~3 keV to ~17 MeV with unprecedented spectral, spatial, and temporal resolution, providing the most precise measurements of the X-ray flare spectrum and enabling the most accurate characterization of the X-ray-emitting hot and accelerated electron populations.RHESSI observations show that ""super-hot"" temperatures exceeding ~30 MK are common in large flares but are achieved almost exclusively by X-class events and appear to be strictly associated with coronal magnetic field strengths exceeding ~170 Gauss; these results suggest a direct link between the magnetic field and heating of super-hot plasma, and that super-hot flares may require a minimum threshold of field strength and overall flare intensity.Imaging and spectroscopic observations of the 2002 July 23 X4.8 event show that the super-hot plasma is both spectrally and spatially distinct from the usual ~10-20 MK plasma observed in nearly all flares, and is located above rather than at the top of the loop containing the cooler plasma.  It exists with high density even during the pre-impulsive phase, which is dominated by coronal non-thermal emission with negligible footpoints, suggesting that particle acceleration and plasma heating are intrinsically related but that, rather than the traditional picture of chromospheric evaporation, the origins of super-hot plasma may be the compression and subsequent thermalization of ambient material accelerated in the reconnection region above the flare loop, a physically-plausible process not detectable with current instruments but potentially observable with future telescopes.  Explaining the origins of super-hot plasma would thus ultimately help to understand the mechanisms of particle acceleration and impulsive energy release in solar flares.",ucb,,https://escholarship.org/uc/item/0bq4h2mh,,,eng,REGULAR,0,0
976,2412,Leveraging Similar Regions to Improve Genome Data Processing,"Curtis, Kristal Lyn","Patterson, David A;Fox, Armando;",2015,"Though DNA sequencing has improved dramatically over the past decade, variant calling, which is the process of reconstructing a patientâ€™s genome from the reads that the sequencers produce, remains a difficult problem, largely due to the genomeâ€™s redundant structure. In this thesis, we describe SiRen, our algorithm for characterizing the genomeâ€™s structure in a way that makes sense from the perspective of the reads themselves. We use the term similar regions to refer to the areas of redundancy that we have identified. We then confirm that the similar regions are characterized by low variant calling accuracy. We show that the structure of the similar regions provides a platform for repairing alignment errors, thus leading to significantly improved variant calling accuracy.",ucb,,https://escholarship.org/uc/item/0bq516zw,,,eng,REGULAR,0,0
977,2413,"The Voyager in Question: Histories of Travel, 1930-2010","Yoshioka-Maxwell, Olivia Chidori","Guerlac, Suzanne;Britto, Karl A.;",2014,"This dissertation is situated within a field of cultural studies that seeks to develop a broader understanding of the political stakes of twentieth- and twenty-first century debates over who can travel. This line of inquiry responds to the persistent tendency in travel writing over the course of the century to exclude certain subjects in movement from the role of the traveler. The question, ""What is travel?"" is often treated in both literary texts and criticism as a philosophical or abstract question, stripped of its historical and political implications. I ask, what is the relationship between this effort to restrict the identity of the traveler and France's imperialist history? How and why are non-European subjects denied the status of traveler, and how does the debate over the traveler / tourist binary, which has received more critical attention, relate to the reification of colonial and postcolonial subjects in the role of the ""sedentary native""? Taking these questions as a point of departure, this dissertation explores how the theoretical opposition between the dynamic traveler and the passive travelee is constructed, undermined, and directly challenged in texts belonging to a wide variety of genres, from the historical novel to the Oulipian literary exercise to autofiction.	The first part of the dissertation considers representations of travel at the ""apogee"" of the French Empire in the early 1930s. In Chapter One, I analyze how the 1931 Colonial Exposition, in framing a visit to the fair as a voyage around the world, both reifies the identity of the European visitor to the fair (indigenous to the metropole) as a traveler and reinforces the notion that the colonial subjects imported to perform the role of the ""natives"" at the fair (many of whom had traveled thousands of miles to reach Paris) could never occupy the role of the traveler. Chapter Two moves across genres to AndrÃ© Malraux's adventure novel La voie royale (1930). I show how this modernist text--while continuing to exclude colonial subjects from the role of the traveler--nevertheless challenges the association of Europeans with dynamism and progress so central to the rhetoric of the mission civilisatrice. The second part of the dissertation analyzes texts from the last quarter of the century that reconfigure ideas of travel. In Chapter Three, I demonstrate how Maryse CondÃ©'s two-volume historical novel SÃ©gou (1984-85) challenges the myth of African ahistoricity that emerges from both colonial historiographies and certain Negritude discourses through its narration of histories of travel in West Africa and throughout the Diaspora. In contrast to the representation of West African cultural spaces at the Exposition through an essentialized, monolithic architecture, the figure of the city in SÃ©gou is a site of cultural exchange and hybridity. Chapter Four turns to two works published during the mid-1970s: a watershed moment for the history of tourism and immigration in the twentieth century. I juxtapose Georges Perec's Tentative d'Ã©puisement d'un lieu parisien (1975)--an account of several days spent sitting in the Place Saint-Sulpice--with Rachid Boudjedra's Topographie idÃ©ale pour une agression caractÃ©risÃ©e (1975), which narrates an unnamed emigrant's struggle to navigate the subterranean labyrinth of the Paris metro. I explore how Perec's encounters with tourists circulating through the Place Saint-Sulpice can shed light on the ways in which native Parisians respond to the presence of Boudjedra's protagonist in the metropolitan capital. In both texts, interactions between Parisians and travelers to Paris are shaped by the natives' anxiety over the perceived globalization of mobility in the 1970s. In Chapter Five, I examine a pair of texts representing urban itineraries in the French capital at the turn of the century: Bessora's 53 cm (1999) and Patrick Modiano's Dora Bruder (1999). By reimagining Paris as a site of travel, the location of multiple histories and cultures, the texts read in this last chapter fundamentally undermine the oppositions between ""here"" and ""elsewhere,"" home and abroad, traveler and native, and more broadly speaking, between travel and dwelling, which have defined colonial and neocolonial ideas of travel throughout the twentieth century. Finally, in the Afterword, I suggest how an approach to travel literature structured around close attention to historical context can inform contemporary debates over the disciplinary boundaries of French / Francophone Studies.",ucb,,https://escholarship.org/uc/item/0bq7p5cc,,,eng,REGULAR,0,0
978,2414,"Water Quality and Quantity in Intermittent and Continuous Piped Water Supplies in Hubli-Dharwad, India","Kumpel, Emily Katherine","Nelson, Kara L;",2013,"In at least 45 low- and middle-income countries, piped water systems deliver water for limited durations. Few data are available of the impact of intermittent water supply (IWS) on the water quality and quantity delivered to households. This thesis examines the impact of intermittently supplied piped water on the quality and quantity of water delivered to residential taps in Hubli-Dharwad, India, when compared to continuous piped water supply. A framework for understanding the pathways through which IWS can impact water quality is first developed. The extent to which contamination occurs in Hubli-Dharwad is quantified by comparing microbial water quality throughout the distribution system in an intermittent system and a continuous system in the same city. The mechanisms affecting water quality in the IWS network in Hubli-Dharwad are identified by measuring changes in water quality over time using continuous measurements from pressure and physico-chemical sensors paired with grab samples tested for indicator bacteria. In the final chapter, a new method of measuring household water consumption in an IWS when supply durations are limited and few metered data are available is developed. This thesis showed that the intermittent supply was frequently subject to contamination in the distribution system and that households with intermittent supply consumed limited quantities of water. While these results demonstrated that converting to a continuous water supply can improve water quality when compared to intermittent supply, this conversion may not be possible in the near future for resource-constrained towns and cities. This thesis contributes to knowledge of the mechanisms causing contamination and constricting water access in IWS systems, which can help improve systems to ensure that people with piped water receive water that is reliable, safe, and sufficient.",ucb,,https://escholarship.org/uc/item/0br51246,,,eng,REGULAR,0,0
979,2415,Utilizing Chemoproteomic Platforms to Elucidate Toxicological Mechanisms,"Ford, Breanna","Nomura, Daniel;",2019,"A large number of pharmaceuticals, endogenous metabolites, and environmental chemicals act through covalent mechanisms with protein targets. Yet, their specific interactions with the proteome still remain poorly defined for most of these reactive chemicals. Deciphering direct protein targets of reactive small-molecules is critical in understanding their biological action, off-target effects, and potential toxicological liabilities, as well as for the development of safer and more selective chemical agents. Chemoproteomic technologies have arisen as a powerful strategy that enables the assessment of proteome-wide interactions of these irreversible agents directly in complex biological systems. In Chapter one, we review several chemoproteomic strategies that have facilitated our understanding of specific protein interactions of irreversibly-acting pharmaceuticals, endogenous metabolites, and environmental electrophiles to reveal novel pharmacological, biological, and toxicological mechanisms.The usefulness of chemoproteomic platforms in assessing the toxicity of environmental chemicals is further demonstrated in Chapter two; here, we utilize the chemoproteomic strategy termed Isotopic tandem orthogonal proteolysis-enabled activity-based protein profiling (isoTOP-ABPP) in identifying the direct protein binding, possible targets, mechanism of toxicity and target selectivity of the herbicide glyphosate. Glyphosate, the active ingredient in the commercial herbicide formulation RoundupÂ®, is one of the most widely used pesticides in agriculture and home garden use. Whether glyphosate causes any mammalian toxicity remains highly controversial. While many studies have associated glyphosate with numerous adverse health effects, the mechanisms underlying glyphosate toxicity in mammals remain poorly understood. In chapter two, we used activity-based protein profiling to map the reactivity of glyphosate metabolites in vivo in mice. We show that glyphosate at high doses can be metabolized in vivo to reactive metabolites such as glyoxylate and react with several cysteines across many different protein targets in mouse liver. We show that glyoxylate inhibits several fatty acid oxidation enzymes and high-dose glyphosate treatment in mice increases the levels of several lipid metabolites, including triglycerides and cholesteryl esters, likely resulting from diversion of fatty acids away from oxidation and towards other lipid pathways. Our study underscores the utility of using chemoproteomic platforms to identify novel toxicological mechanisms of environmental chemicals such as glyphosate.Understanding the many ways in which environmental chemicals, such as pesticides, or pharmaceuticals compounds interact with complex biological systems is absolutely necessary in determining the mechanism of toxicity, secondary targets, off-targets, selectivity, and therefore potential for toxicity of exogenous compounds. In this dissertation we demonstrate the versatility of chemoproteomics technologies, such as IsoTOP-ABPP, to aid in elucidating the direct targets and mechanism of action of environmental chemicals, such as the herbicide glyphosate. Furthermore, it is an additional aim of this research to demonstrate the potential of chemoproteomic platforms to be useful as part of the covalent compound discovery and development process towards developing safer chemicals.",ucb,,https://escholarship.org/uc/item/0bs9118x,,,eng,REGULAR,0,0
980,2416,Fast and accurate quantification and differential analysis of transcriptomes,"Pimentel, Harold Joseph","Pachter, Lior;",2016,"As access to DNA sequencing has become ubiquitous to scientists, the use of sequencers has expanded from determining the genomes of individuals to performing molecular probing assays. These assays have turned DNA sequencers into molecule counting machines and can be used to measure biological activities such as gene expression (RNA-Seq), DNA accessibility (ATAC-Seq) and many others.Each new assay poses new analytical challenges, and the main focus presented here is in analyzing RNA-Seq data.One of the main challenges in RNA-Seq is that sequenced fragments are often ambiguous, meaning they are compatible with multiple splice forms or genomic locations.In order to estimate gene abundances effectively, these ambiguous fragments should be used in a comprehensive model in order to not bias results.Analysis has come a long way from ignoring ambiguous mappings, to maximum likelihood models, and even streaming models.Advancements in these models have greatly improved the accuracy of estimating gene and transcript abundances.In parallel, methods for determining true expression differences between experimental conditions, termed differential expression, have been developed.Historically, these methods have mostly ignored advancements in gene expression estimation but have made much progress in between-sample variance estimation when sample sizes are small -- a common practice in this field.Herein, we present advancements to both abundance estimation and differential expression analysis.We show dramatic improvements to the speed of abundance estimation while maintaining accuracy.Furthermore, we bridge these two fields by developing a differential expression model incorporating the uncertainty introduced by abundance estimation.We show that this model outperforms existing techniques at both the transcript and gene level.Additionally, we show that these methods can be used to address other biological questions such as the discovery of novel retained introns and estimation of their abundances.An extension to the differential expression model is proposed to identify differences in retained intron levels while incorporating abundance estimation uncertainty.",ucb,,https://escholarship.org/uc/item/0bx9t7c5,,,eng,REGULAR,0,0
981,2417,Investigating the Design and Development of Multitouch Applications,"Kin, Kenrick","Agrawala, Maneesh;",2012,"Multitouch is a ubiquitous input technique, used primarily in mobile devices such as phones and tablets. Larger multitouch displays have been mostly limited to tabletop research projects, but hardware manufacturers are also integrating multitouch into desktop workstations. Multitouch input has several key differences from mouse and keyboard input that make it a promising input technique. While the mouse is an indirect and primarily unimanual input device, multitouch often supports direct-touch input and encourages bimanual interaction. Multitouch also supports the use of all ten fingers as input, providing many more degrees of freedom of input than the 2D mouse cursor.Building multitouch applications first requires understanding these differences. We present a pair of user studies that contribute to the understanding of the benefits of direct-touch and bimanual input afforded by multitouch input. We then discuss how we leverage these benefits to create multitouch gestures for a professional content-creation application run on a large multitouch display. The differences between multitouch and mouse input also greatly affect the needs of an application developer. We lastly present a declarative multitouch framework that helps developers build and manage gestures that require the coordination of multiple fingers.In our first study, users select multiple targets with a mouse and with multitouch using one finger, two fingers (one from each hand), and any number of fingers. We find that the fastest multitouch interaction is about twice as fast as the mouse for selection. The direct-touch nature of multitouch accounts for 83% of the reduction in selection time. Bimanual interaction, using at least one finger on each hand, accounts for the remaining reduction. To further investigate bimanual interaction for making directional motions, we examine two-handed marking menus, bimanual techniques in which users make directional strokes to select menu items. We find that bimanually coordinating directional strokes is more difficult than making single strokes. But, with training, making strokes bimanually outperforms making strokes serially by 10-15%.Our user studies demonstrate that users benefit from multitouch input. However, little work has been done to determine how to design multitouch applications that leverage these benefits for professional content-creation tasks. We investigate using multitouch input for a professional-level task at Pixar Animation Studios. We work with a professional set construction artist to design and develop Eden, a multitouch application for building virtual organic sets for computer-animated films. The experience of the artist suggests that Eden outperforms Maya, a mouse and keyboard system currently used by set construction artists. We present a set of design guidelines that enabled us to create a gesture set that is both easy for the artist to remember and easy for the artist to perform.Eden demonstrates the viability of multitouch applications for improving real user workflows. However, multitouch applications are challenging to implement. Despite the differences between multitouch and mouse input, current multitouch frameworks follow the event-handling pattern of mouse-based frameworks. Tracking a single mouse cursor is relatively straightforward as mouse events are broken sequentially into the order in which they must occur: down, move, and up. For multitouch however, developers must meticulously track the proper sequence of touch events from multiple temporally overlapping touch streams using disparate event-handling callbacks. In addition, managing gesture sets can be tedious, as multiple gestures often begin with the same touch event sequence leading to gesture conflicts in which the user input is ambiguous. Thus, developers must perform extensive runtime testing to detect conflicts and then resolve them.We simplify multitouch gesture creation and management with Proton, a framework that allows developers to declaratively specify a gesture as a regular expression of customizable touch event symbols. Proton provides automatic gesture matching and the static analysis of gesture conflicts. We also introduce gesture tablature, a graphical gesture notation that concisely describes the sequencing of multiple interleaved touch events over time. We demonstrate the expressiveness of Proton with four proof-of-concept applications. Finally, we present a user study that indicates that users can read and interpret gesture tablature over four times faster than event-handling pseudocode.Multitouch applications require new design principles and tools for development. This dissertation addresses the challenges of designing gestures and interfaces that benefit from multiple parallel touch input and presents tools to help developers build and recognize these new multitouch gestures. This work serves to facilitate a wider adoption of multitouch interfaces. We conclude with several research directions for continuing the investigation of multitouch input.",ucb,,https://escholarship.org/uc/item/0c0871cf,,,eng,REGULAR,0,0
982,2418,"Multifunctional Riverscapes: Stream restoration, Capability Brown's water features, and artificial whitewater","Podolak, Kristen","Kondolf, G. Mathias;",2012,"Society is investing in river restoration and urban river revitalization as a solution for sustainable development.  Many of these river projects adopt a multifunctional planning and design approach that strives to meld ecological, aesthetic, and recreational functions.  However our understanding of how to accomplish multifunctionality and how the different functions work together is incomplete.  Numerous ecologically justified river restoration projects may actually be driven by aesthetic and recreational preferences that are largely unexamined.  At the same time river projects originally designed for aesthetics or recreation are now attempting to integrate habitat and environmental considerations to make the rivers more sustainable.  Through in-depth study of a variety of constructed river landscapes - including dense historical river bend designs, artificial whitewater, and urban stream restoration this dissertation analyzes how aesthetic, ecological, and recreational functions intersect and potentially conflict. To explore how aesthetic and biophysical processes work together in riverscapes, I explored the relationship between one ideal of beauty, an s-curve illustrated by William Hogarth in the 18th century and two sets of river designs: 18th century river designs in England and late 20th century river restoration designs in North America.  I used two quantifiable variables, sinuosity and symmetry, to compare the ideal curve of beauty to the designed river curves.  Hogarth's s-curve and river restoration meanders had symmetrical curves.  Symmetry in restoration designs represents a theoretical condition and is counter to how most natural rivers meander.  A second aesthetic-ecological study examined whether 18th century English landscape design represents design with nature.  By tracing the persistence of Capability Brown's river designs over the past two centuries, the results show Brown's designs required maintenance and are not self-perpetuating as expected of a design based on natural processes.To evaluate the intersection of recreation and ecological functions, I conducted a case study of three urban river projects, a historical study of artificial whitewater designs, and an observational study of summertime whitewater park use.  By comparing the ecological and social impacts of three urban river projects (Cheonggyecheon in Seoul, South Korea, the South Platte Greenway in Denver, United States, and the Isar River in Munich, Germany), one emerged as moving towards multifunctional planning and design.  The Isar River project was unique because the planners and designers used a dynamic guiding image, gave the river room to roam, and allowed some dynamic biophysical processes to occur.  The selection of the guiding image for the Isar restoration was fortuitously a publicly valued stream reach for its aesthetics and existing recreational use.  The South Platte Greenway, which contains a whitewater park, illustrates a riverscape made primarily for recreation.  The history of artificial whitewater designs evolved since the 1970s to a point in 2000 when the Sydney Olympic Whitewater Course was disconnected from a stream to create a fair playing field for competitors where all of the whitewater variables could be controlled.  Meanwhile, instream whitewater parks began to include habitat and fish passage considerations in the engineered wave structures. Observations of whitewater park use and surveys of park user's perceptions of the parks revealed that kayakers represent only a small fraction of park users and overall use evinced no clear relationship to streamflow but varied with air and water temperature. Summer streamflow provisions for whitewater parks potentially limits the diversity of instream users and the ecological function.  While whitewater park users value clean water as the most important characteristic, all interviewed park users wanted the park to have a natural appearance, but they did not mind seeing concrete in the river. Understanding the patterns of recreation and perceptions of rivers in relation to biophysical processes such as streamflow or channel pattern is fundamental to achieving sustainability.  The forms that people prefer--perhaps because they are beautiful--and a local-level understanding of recreational use need to be considered alongside the physical and ecological patterns and processes of rivers, the domain of landscape ecology and river science.  Combining ground level research and perception studies with environmentally based landscape planning can create multifunctional landscapes.  For previously impacted rivers in developed areas, multifunctional riverscape planning and design offers a sustainable development solution.",ucb,,https://escholarship.org/uc/item/0c18207s,,,eng,REGULAR,0,0
983,2419,How Foundationsâ€™ Field-Building Helped the Reproductive Health Movement Change the International Population and Development Paradigm,"ELKIND, PERRIN","Swidler, Ann;Voss, Kim;",2015,"How Foundationsâ€™ Field-Building Helped the Reproductive Health Movement Change the International Population and Development Paradigmby Perrin Liana ElkindDoctor of Philosophy in SociologyUniversity of California, BerkeleyProfessor Ann Swidler, ChairScholars have demonstrated that foundation grants channel social movements by encouraging professionalization and favoring moderate tactics, but they have overlooked critical mechanisms of foundation influence. Advancing Tim Bartleyâ€™s (2007) field-building framework, I identify new mechanismsâ€”including grants and activities other than grantmakingâ€”through which five foundations helped channel the international Reproductive Health movement between 1990 and 2005, shaping its composition, trajectory, and outcomes. The first of its kind, this study combines an analysis of an original data set including 8,103 grants made by five major philanthropic foundations from 1990-2005, interviews with foundation staff and leadership, and archival data, with an historical narrative of the population field and the Reproductive Health movement. I explain foundationsâ€™ roles in the Reproductive Health movementâ€™s successful campaign targeting the 1994 United Nations International Conference on Population and Development (ICPD). There the movement transformed the population fieldâ€™s frame from Family Planningâ€”reducing fertility through increasing access to contraceptivesâ€”to Reproductive Healthâ€”meeting womenâ€™s broader reproductive health needs and advancing gender equality.Unlike scholars who focus on movement organizations that receive grants, I analyze the grants themselves, including those to both movement and non-movement actors. Through examining the grantsâ€™ purposes and the movementâ€™s trajectory, I find that foundationsâ€™ field-building mechanisms included grants for research; communications; capacity-building, technical assistance, and training; networks/conference; and policy work. Grants to non-movement actors indirectly contributed to the movementâ€™s success by supporting the movementâ€™s strategy or shaping its context. In addition to their material resources, foundations apply unique human and symbolic resources toward field-building. Mechanisms other than grantmaking that foundations used included brokerage, advocacy, and coordination. The foundationsâ€™ field-building work helped to certify movement actors and frames and to diffuse frames. Foundationsâ€™ operations and programs were influenced by the historical eras in which the foundations were established and by the foundersâ€™ involvement. Staff and board membersâ€™ professional and personal networks were also influential, as was the presence of movement actors on staff. Status pressures within the foundation, the philanthropic sector, and the program area further shaped the foundationsâ€™ work. From the 1950s through the 1980s, the Ford and Rockefeller foundations helped establish the population field and the frame that the Reproductive Health movement later challenged at ICPD. Ford, MacArthur, and Rockefeller aided the ICPD campaign, including by intervening to afford the movement critical access to the United Nations. Following ICPD, these three foundations plus Packard and Hewlett helped institutionalize the Reproductive Health frame. Two of the funders actively promoted the frame; three resisted the movement but also inadvertently helped advance its frame. Major funders of movements are themselves movement actors. Foundations were not the most important actors in the Reproductive Health movement field but their support at critical junctures was instrumental to the movementâ€™s success. Understanding the funder-movement relationship requires close examination of how foundations strategically use their material, human, and symbolic resources to build a movement field.",ucb,,https://escholarship.org/uc/item/0c25c6hw,,,eng,REGULAR,0,0
984,2420,"String Theory, Chern-Simons Theory and the Fractional Quantum Hall Effect","Moore, Nathan Paul","Ganor, Ori;",2014,"In this thesis we explore two interesting relationships between string theory and quantum field theory.  Firstly, we develop an equivalence between two Hilbert spaces: (i) the space of states of U(1)^n Chern-Simons theory with a certain class of tridiagonal matrices of coupling constants (with corners) on T^2; and (ii) the space of ground states of strings on an associated mapping torus with T^2 fiber. The equivalence is deduced by studying the space of ground states of SL(2,Z)-twisted circle compactifications of U(1) gauge theory, connected with a Janus configuration, and further compactified on T^2.  The equality of dimensions of the two Hilbert spaces (i) and (ii) is equivalent to a known identity on determinants of tridiagonal matrices with corners.  The equivalence of operator algebras acting on the two Hilbert spaces follows from a relation between the Smith normal form of the Chern-Simons coupling constant matrix and the isometry group of the mapping torus, as well as the torsion part of its first homology group. Secondly, the Fractional Quantum Hall Effect appears as part of the low-energy description of the Coulomb branch of the A_1 (2,0)-theory formulated on (S^1xR^2)/Z_k, where the generator of Z_k acts as a combination of translation on S^1 and rotation by 2&pi/k on R^2. At low-energy the configuration is described in terms of a 4+1D Super-Yang-Mills theory on a cone (R^2/Z_k) with additional 2+1D degrees of freedom at the tip of the cone. Fractionally charged quasi-particles have a natural description in terms of BPS strings of the (2,0)-theory. We analyze the large k limit, where a smooth cigar-geometry provides an alternative description. In this framework a W-boson can be modeled as a bound state of k quasi-particles. The W-boson becomes a Q-ball, and it can be described by a soliton solution of BPS monopole equations on a certain auxiliary curved space. We show that axisymmetric solutions of these equations correspond to singular maps from AdS_3 to AdS_2, and we present some numerical results.",ucb,,https://escholarship.org/uc/item/0c27m3gk,,,eng,REGULAR,0,0
985,2421,Phenomenologies of Egalitarianism in Free Improvisation: A Virtual Performer Meets its Critics,"Banerji, Ritwik","Brinner, Benjamin E.;",2018,"This dissertation offers a descriptive, anthropological account of diverse phenomenologies of egalitarian ethics through an experimental ethnography of music and social interaction among performers of free improvisation. Based on several years of ethnographic fieldwork with improvisers in Berlin, Chicago, and San Francisco as a saxophonist and participant in these scenes, I show how performers of free improvisation refrain from instructing or criticizing their peers out of respect for their creative liberty and to maintain their experience of equal status. Consequently, I argue that canonical modes of musical ethnography tend to confirm the utopian conception of free improvisation, widely promoted in discourses on this form of musicking over the past half century, as a practice in which performers are liberated from aesthetic constraints and interpersonal hierarchies.	This project highlights the shortcomings of typical modes of musical and anthropological fieldwork through an experimental fusion of ethnography and arts-technology research in which I have asked improvisers to critique the interactive and performance capabilities of a virtual performer, known as â€œMaxine.â€ Designed based on my experiences improvising with, watching, and listening to others, Maxine is simultaneously a virtual performer but also a form of interactive, algorithmic ethnography. By asking improvisers to play with this system and compare it to human performers, the fieldwork presented in this dissertation illustrates how improvisers espouse a wide variety of notions of how egalitarian experience is realized in how each player listens and responds to others in the course of performance. Crucially, it demonstrates that what is revealed in an encounter with a nonhuman social interactant like Maxine is precisely what often escapes canonical approaches to ethnographic fieldwork. Broadly speaking, while some improvisers regard continual displays of attentiveness from their partners as essential to the experience and performance of an egalitarian ethos in musical interaction, others regard these behaviors as injurious to the ideal of a nonhierarchical sociality in sound. 	In the process of offering this experiential account of egalitarianism, this dissertation illustrates the many ways in which the design of artificial social interactants implicitly poses a variety of provocative hypotheses about the nature of listening and musical cognition as it takes place between improvisers, many of which are yet to be tested through methodologies such as the one practiced in this project as well as analysis of recorded performances of free improvisation. Furthermore, I also argue that the methodology developed in this project suggests productive new avenues for phenomenologically-oriented ethnography in that encounters with an artificial social interactant like Maxine elicit commentary on real-time sociality as an experience  which participants normally refrain from discussing, both within and far beyond free improvisation. By eliciting explicit articulations of ethical normativities, as conceived of by improvisers interacting with Maxine, this project also responds to recent work in the anthropology of ethics by suggesting that much of human moral experience may take the form of latent moral critiques, in which subjects experience an unfulfilled desire to express criticism of the behavior of their interlocutors but nonetheless refrain from doing so for a variety of reasons. Lastly, I argue that subjecting artificial social interactants to the evaluation of the human beings they are modeled upon constitutes a radically new, vibrant form of critical ethnography, one which is implicitly performed throughout the field of human computer interaction even if its practitioners hardly theorize it as such.",ucb,,https://escholarship.org/uc/item/0c28589b,,,eng,REGULAR,0,0
986,2422,Essays in Energy and Environmental Economics,"Hausman, Catherine Helena","Auffhammer, Maximilian;Borenstein, Severin;",2013,"Energy production is associated with a number of significant environmental externalities. For instance, coal-fired power plants emit both local pollutants (such as particulate matter and sulphur dioxide) and the global pollutant carbon dioxide. This creates a need for government intervention: left to their own devices, energy producers will do more environmental damage than is socially optimal. The choices faced by policy-makers in regulating the energy industry are, however, rarely clear. Government regulators must trade off the externalities caused by different types of energy production. While nuclear power generation does not emit carbon dioxide, there is the risk of significant environmental damage in the event of a nuclear meltdown. While many proponents of biofuels hoped that replacing fossil fuels with biofuels would decrease carbon dioxide emissions, land-use changes associated with biofuels production can cause environmental damage. These trade-offs motivate the chapters of this dissertation.In the first chapter, I study changes to nuclear power safety following major regulatory changes in electricity markets. Following electricity market restructuring, approximately half of all commercial U.S. nuclear power reactors were sold by price-regulated public utilities to independent power producers. At the time of the sales, some policy-makers raised concerns that these corporations would ignore safety. Others claimed that the sales would bring improved reactor management, with positive effects on safety. Using data on various safety measures and a difference-in-difference estimation strategy, I find that safety improved following ownership transfers and the removal of price regulations. Generation increased, and this does not appear to have come at the cost of public safety. This paper contributes to several strands of the energy literature. First, it fits in with the literature on electricity deregulation. While this literature has considered a broad set of outcomes, my paper is the first to look closely at safety, an outcome of particular interest for nuclear energy. In line with that, it also contributes to the literature on nuclear safety, which has been of particular interest given accidents at Three Mile Island, Chernobyl, and Fukushima. Finally, my work is germane to the literature on the consequences of deregulation for outcomes beyond private efficiency gains. While there is now some consensus that deregulation can lead to the alignment of private costs and thus to efficiency gains, less is known about the effect on external costs. Papers in this literature are necessarily industry-specific: the interaction of private cost reductions with changes to quality or changes to external costs is highly context-dependent. However, this paper provides intuition for the mechanisms at work, some of which are generalizable beyond the nuclear power industry.In the second and third chapters, I study land-use changes relating to biofuels production. Transportation in the U.S. accounts for a significant portion of greenhouse gas emissions. Motor gasoline, excluding ethanol, accounts for around 20 percent of U.S. greenhouse gas emissions, or over 1 billion metric tons each year. Biofuels have been promoted as an alternative to petroleum products that bypasses some of the fundamental problems with the oil market: supporters claim that it is renewable (whereas conventional oil is exhaustible), produced in the U.S. (as opposed to regimes in some cases unfriendly to the U.S.), and carbon-friendly. As the acreage devoted to biofuels crop production expands, however, it can compete with cropland used for food or with natural ecosystems.In the second chapter, joint with Maximilian Auffhammer and Peter Berck, I examine price impacts of biofuels production.  The last ten years have seen tremendous expansion in biofuels production, particularly in corn ethanol in the United States, at the same time that commodity prices (e.g., corn) have experienced significant spikes. While supporters claim that biofuels are renewable and carbon-friendly, concerns have been raised about their impacts on land use and food prices. This paper analyzes how U.S. crop prices have responded to shocks in acreage supply; these shocks can be thought of as a shock to the residual supply of corn for food. Using a structural vector auto-regression framework, we examine shocks to a crop's own acreage and to total cropland. This allows us to estimate the effect of dedicating cropland or non-crop farmlands to biofuels feedstock production. A negative shock in own acreage leads to an increase in price for soybeans and corn. Our calculations show that increased corn ethanol production during the boom production year 2006/2007 explains approximately 27 percent of the experienced corn price rise. In the final chapter, I study land-use change in Brazil arising from biofuels production. Scientists and economists are increasingly worried that biofuels production is leading to deforestation, and hence loss of habitats and increased carbon dioxide  emissions. I estimate land use changes in response to shocks in sugarcane (a biofuels feedstock) and soybean (thought to be affected by United States corn ethanol production) prices in Brazil at a national and regional level. Using county-level data from 1973 to 2005, I consider a dynamic panel data model of input demand for land, conditioning on price changes of other commodities. Unlike the existing literature, I apply a dynamic panel data estimator that is unbiased (unlike OLS with fixed effects) and more precise than GMM. The short-run price elasticity of sugarcane acreage in Brazil is estimated to be approximately zero, whereas the elasticity of soybean acreage is 0.9 when both spot and futures prices change. The regional estimates for soybeans show considerable variation, and are highest in areas of ecological importance, such as the cerrado. Sugarcane estimates are more homogeneous. These results should be taken into account in impact assessments of biofuels.",ucb,,https://escholarship.org/uc/item/0c65t1z0,,,eng,REGULAR,0,0
987,2423,Empire of the Imagination: The Power of Public Fictions in Ovid's 'Reader Response' to Augustan Rome,"Pandey, Nandini B.","McCarthy, Kathleen;",2011,"The idea of an `Augustan discourse' represents a valuable step forward from the twentieth-century belief that Augustus ruled through patronage and propaganda, insofar as it better accommodates the polyvocality of the literature of his age as well as the delicacy of the princeps' political position between republic and empire.  I seek to expand on this approach by drawing literary works into more thoroughgoing dialogue with contemporary `texts' in other media, including coins and architecture, and by treating all these as examples of reader responses to Augustus that both construct and reflect public interpretations of the emperor.  This work focuses in particular on Ovid's readings of the visual iconography of the principate, arguing that these influenced both ancient and modern historians' conception of Augustus as the master architect of his own public image.My project is inspired by poets' creation of a sense of professional rivalry between themselves and the princeps, particularly Ovid's portrayal of Augustus as a fellow manipulator of fictions.  However, individual chapters deconstruct this idea by examining how specific `pro-Augustan' icons cannot be regarded as a tool of propaganda, but rather, exist only within individual representations that often embed critical, evolving, and dialogic perspectives on the emperor.  The first chapter analyzes historical evidence for the appearance and interpretation of a comet over Caesar's funeral games in 44 BCE, as well as representations of this sidus Iulium in Roman coins and the poems of Vergil, Horace, Propertius, and Ovid.  I argue that the imagistic metamorphosis of the sidus from a star into a comet over the course of Augustus' reign reflects the growth of an ahistorical sense that the young Octavian took a proactive role in deifying Caesar, and a larger tendency to retroject Augustus' mature power onto his early career.  My second chapter interweaves an analysis of the archaeological remains of Augustus' temple complex on the Palatine with close readings of Horace, Propertius, and Ovid's literary responses to its architectonics; I argue that these poets' reappropriations of public space for private purposes, particularly Ovid's critique of the Palatine iconography and urban topography, have encouraged modern scholars to overread triumphalist intentions into the Augustan building program.  In my last chapter, I compare visual and verbal representations of the triumph ceremony, culminating with Ovid's use of the subject to explore how ritual may be extended through time and space, how writing may be employed to serve empire, and how readers may intervene in a text's creation of meaning.Building on this latter idea, a brief conclusion explores how Ovid's exile poems treat Augustus himself as a text - that is, as a publicly circulating representation of power that was potentially unrepresentative of reality, subject to audience interpretation in defiance of authorial intention, and beholden to the imaginative participation of reader-subjects throughout the empire.  Ovid also gives Augustan readers the tools by which to take interpretive control over texts and to examine their own complicity in constructing Augustan power.  This parallels my broader theme that modern scholarly interpretations of the period cannot be disentangled from these subjective reader responses to Augustan Rome, and thus become part of a succession of imaginative rereadings and reinterpretations of the figure of Augustus.",ucb,,https://escholarship.org/uc/item/0c67c4nf,,,eng,REGULAR,0,0
988,2424,"On Betti Tables, Monomial Ideals, and Unit Groups","Chen, Yi-Chang Justin","Eisenbud, David;",2018,"This thesis explores two topics in commutative algebra. The first topicis Betti tables, particularly of monomial ideals, and how these relate toBetti tables of arbitrary graded ideals. We systematically study theconcept of mono, the largest monomial subideal of a given ideal, andfor an Artinian ideal I, deduce relations in the last column of the Betti tables of I and mono(I). We then apply this philosophy towards a conjecture of Postnikov-Shapiro, concerning Betti tables of certain ideals generated by powers of linear forms: by studying monomial subideals of the so-called power ideal, we deduce special cases of this conjecture.The second topic concerns the group of units of a ring. Motivated bythe question of when a surjection of rings induces a surjection on unitgroups, we give a general sufficient condition for induced surjectivity to hold, and introduce a new class of rings, called semi-fields, in the process.As units are precisely the elements which avoid all the maximal ideals, we then investigate infinite prime avoidance in general, and in this direction, produce an example of a ring that is not a semi-field, for whichsurjectivity on unit groups still holds.",ucb,,https://escholarship.org/uc/item/0c68r97p,,,eng,REGULAR,0,0
989,2425,Seismic Performance of Reinforced Concrete Bridges Allowed to Uplift During Multi-Directional Excitation,"Espinoza, Andres Oscar","Mahin, Stephen A;",2011,"AbstractSeismic Performance of Reinforced Concrete Bridges Allowed to Uplift DuringMulti-Directional ExcitationbyAndres Oscar EspinozaDoctor of Philosophy in Engineering - Civil and Environmental EngineeringUniversity of California, BerkeleyProfessor Stephen A. Mahin, ChairThe behavior of bridges subjected to recent moderate and large earthquakes has led to bridge design detailed for better seismic performance, particularly through wider bridge foundations to handle larger expected design forces. Foundation uplift, which is not employed in conventional bridge design, has been identified as an important mechanism, in conjunction with structural yielding and soil-structure interaction that may dissipate energy during earthquakes. Preventing uplift through wider foundations looks past the technical and economical feasibility of allowing foundation uplift during seismic events. The research presented in this thesis is part of a larger experimental and analytical investigation to develop and validate design methods for bridge piers on shallow foundations allowed to uplift during seismic events.Several analytical and some experimental studies have been performed to assess rocking and or uplift of shallow foundation systems, however they have evaluated systems with a limited range of footing dimensions and seismic excitations. As such, there is an uncertainty in the information needed to base a performance evaluation and develop design methods. The purpose of this study is to investigate, through experimental and analytical studies, the seismic performance of uplifting bridge piers on shallow foundations when considering different ground motions and footing dimensions. As well as to identify key differences in performance evaluation criteria for conventional and uplifting bridge pier systems.The experimental study dynamically tested a single reinforced concrete bridge column specimen with three adjustable footing configurations grouped by footing dimension, and tested for various combinations of one, two, and three components of seismic excitation.  Groups one and two evaluated uplifting systems where the column was limited to elastic loading levels while group three considered inelastic column loading levels. All test groups remained stable and exhibited some rocking and or uplift during testing. Analytical models were developed and validated using the experimental testing results to predict local and global footing and column response. Reliable estimates of forces and displacements during elastic and inelastic response were achieved. To assess the seismic performance of a range of bridge pier systems allowed to uplift a parametric investigation using the validated analytical models was performed in which the column was modeled per conventional design criteria to ensure adequate strength and flexural ductility. The parameters varied include footing width, ground motion excitation, and elastic or inelastic column response. Response of the uplifting bridge pier systems was found to be sensitive to the structural periods, magnitude of excitation, and footing width.",ucb,,https://escholarship.org/uc/item/0cg6x3m4,,,eng,REGULAR,0,0
990,2426,Using lidar in wildfire ecology of the California Sierra-Nevada forests,"Jakubowski, Marek K.","Kelly, Maggi;",2012,"California's fire suppression policy has dramatically changed Sierra Nevada forests over the last century. Forests are becoming more dense and homogenous, leading to fire regime changes that increase the potential of stand-replacing wildfires over large, continuous areas. To mitigate this problem on public lands, the US Forest Service has proposed to implement strategically placed forest fuel reduction treatments. These treatments have been proved effective in modeled and simulated environments, but their efficacy and impact in real forests is not known. The research described in this dissertation is part of a large multidisciplinary project, known as the Sierra Nevada Adaptive Management Project (SNAMP), that aims to evaluate strategically placed landscape area treatments (SPLATs) in two forests of the Sierra Nevada mountains. Specifically, in this thesis, I investigate the feasibility of using an airborne light detection and ranging (lidar) system to gain accurate information about forest structure to inform wildfire behavior models, forest management, and habitat mapping.First, I investigate the use of lidar data in predicting metrics at the landscape level, specifically to derive surface fuel models and continuous canopy metrics at the plot scale. My results in Chapter 2 indicate that using lidar to predict specific fuel models for FARSITE wildfire behavior model is challenging. However, the prediction of more general fuel models and continuous canopy metrics is feasible and reliable, especially for metrics near the top of the canopy.It is also possible to derive canopy parameters at the individual tree level. In Chapter 3, I compare the ability of two processing methods--object-based image analysis (OBIA) and 3D segmentation of the lidar point cloud--to detect and delineate individual trees. I find that while both methods delineate dominant trees and accurately predict their heights, the lidar-derived polygons more closely resemble the shape of realistic individual tree crowns.Acquiring remotely sensed data at high resolution and over large areas can be expensive, especially in the case of lidar. In Chapter 4, I investigate the ability of lidar data to reliably predict forest canopy metrics at the plot level as the data resolution declines. I show that canopy metrics can be predicted at a reasonable accuracy with data resolutions as low as one pulse per squared meter. These findings will be useful to land managers making cost benefit decisions when acquiring new lidar data.Collectively, the results of this dissertation suggest that remote sensing, and in particular lidar, can reliably and cost-effectively provide forest information across scales--from the individual tree level to the landscape level. These results will be useful for the fire and forest management community in general, as well as being key to the goals of the SNAMP program.",ucb,,https://escholarship.org/uc/item/0cj0x9hm,,,eng,REGULAR,0,0
991,2427,Molecular Sensing and Signaling in Leukocyte-Endothelial Adhesion: From Integrin Activation to Orai Gating,"Haydari, Zainab","Mofrad, Mohammad RK;",2020,"Leukocyte-endothelial adhesion and firm adhesion are chief steps in leukocyte recruitment cascade and immune system response to inflammation. Irregularities in these processes have been related to several human diseases including arthritis, cancer, and atherosclerosis. Given the fundamental importance of integrin activation and Ca2+ signaling in adhesion assembly and strengthening, understanding the molecular mechanisms of kindlin-mediated integrin activation and STIM-mediated Orai gating have potentially important implications in designing new therapeutic interventions. This study employs molecular dynamics and bioinformatic techniques to unravel detailed molecular interactions in these two activation processes. First, we analyzed the effect of kindlin cooperation with talin for integrin activation. Integrins are transmembrane proteins that mediate the signaling between the cytoplasm and extracellular matrix to promote cell adhesion. In order to initiate signaling, integrins must be activated from a low-affinity state to a high-affinity conformation for binding to extracellular molecules. Talin and kindlin are two cytoplasmic proteins that bind to integrin and modulate its affinity for extracellular ligands. Although the molecular details of talin-mediated integrin activation are known, the mechanism of kindlin involvement in this process remains elusive. Here, we carried out, for the first time, a comprehensive molecular dynamics study of integrin activation mediated through simultaneous interaction of talin and kindlin with its b subdomain. We showed that kindlin modifies the molecular mechanisms of inside-out activation by enhancing talin interaction with the membrane proximal region of integrin and decreasing the crossing angle between transmembrane helices of integrin, which eventually results in parallelization of integrin dimer.Second, we investigated the molecular details of STIM-Orai interaction and its downstream consequences on the Orai channel gating. The immune response is triggered by a decline in Ca2+ concentration within the endoplasmic reticulum (ER) that is followed by the opening of the calcium release-activated channels (CRAC), thus increasing the intracellular Ca2+ concentration. Stromal interaction molecule (STIM) has been identified as a Ca+2 sensor in the ER activating the CRAC channel via physical interaction with Orai, the pore unit of the CRAC channel. In this study, we showed that the forces from STIM binding allosterically propagate through Orai helices, which lead to opening of the basic region of the pore. Moreover, we demonstrated important interactions that maintain the open structure of the Orai channel. Overall, this study provides valuable insights into the molecular mechanisms that contribute to the activation of mechanosensitive proteins involved in leukocyte adhesion and firm adhesion.",ucb,,https://escholarship.org/uc/item/0cj5892q,,,eng,REGULAR,0,0
992,2428,From actin to Zap70: the outcomes of the interaction of the actin cytoskeleton with T-cell receptors,"Smoligovets, Alexander Alexandrovich","Groves, Jay T;Isacoff, Ehud;",2012,"The actin cytoskeleton has for decades been known to be important in the process of T-cell recognition of antigen. Upon receiving stimulus from T-cell receptors (TCRs), the T-cell cytoskeleton adopts a pattern of radially symmetric, centripetal flow that imposes multi-micrometer scale organization on the protein complement of a T-cell membrane. Many of the details of the interactions between actin and cell surface receptors remain unknown; of particular interest are the connections to TCRs themselves. The actin cytoskeleton appears to modulate TCR-mediated signaling, and thus T-cell behaviors, on a number of levels. On the level of single TCR molecules and oligomers, the presence of actin affects TCR-antigen recognition by altering binding kinetics through mechanisms that are still unclear. On a larger spatial and longer temporal level, actin drives the formation and centripetal movement of small TCR assemblies whose radial distribution correlates with their signaling state. Little is known about the local organization and dynamics of the actin cytoskeleton around these assemblies. A more complete understanding of the process of T-cell activation depends on the ability to characterize the involvement of actin in detail.This thesis describes how the T-cell actin cytoskeleton locally reacts to TCR assemblies, how it modulates the kinetics of the TCR-antigen interaction, and what role myosin IIA plays in organization of and signaling by TCRs. Direct imaging of actin reveals that the cytoskeletal network transiently slows and compresses at sites of mobility-limited T-cell receptors. Actin disruption during direct observation of antigen peptide binding to TCRs shows that the presence of a functional cytoskeleton increases the rate of antigen unbinding from these receptors. Tracking and disruption of labeled myosin IIA indicates that the motor protein helps but is not required for proper establishment of T-cell membrane organization, but it is necessary for appropriate calcium influx into the T-cell cytoplasm during activation. For all of these observations and others, new experimental and analytical methods are developed or applied.",ucb,,https://escholarship.org/uc/item/0cn4f9zp,,,eng,REGULAR,0,0
993,2429,Towards Precision Standard Model Calculations,"Girard, Michael","Bauer, Christian;Nomura, Yasunori;",2018,"For many years the consistency of the Standard Model(SM) of particle physics has been unflinching. The structure and content of the Standard Model has remained whole in the face of shockingly many experimental tests. Predicting the existence of new particles and yielding incredibly accurate calculations of fundamental measurements have been the hallmark of the Standard Model. Currently, experimentalists are testing several Standard Model quantities to such precision that 5th order terms are needed in theoretical calculations to match this precision. Never-the-less, the predictions of the Standard Model have kept pace with experiment and have yet to be contradicted.   Unfortunately, this incredible stretch of experimental validation comes at a cost. Several very large problems still are without solutions and finding them in the particle content of the Standard Model alone is unlikely. The huge range of scales seen in nature, a dark matter candidate, and the cosmic baryon asymmetry still loom as major flaws in the Standard Model. In this manuscript we aim to increase the accuracy of several Standard Model predictions to push the experimental limits of the Standard Model. We start with a brief introduction to the Standard Model and its sector we will be mainly concerned with, namely QCD. In Chapter 2, we will consider the Standard Model as an effective field theory for a new physics model that resides at a mass scale much higher than the electro-weak scale. By interpreting the SM as an effective theory it allows us to extend the Standard Model Lagrangian with new, higher-dimensional operators that depend on the scale of new physics. We then calculate the next-to-leading order corrections to these operators in QCD and combine these calculations with parton showers via the \texttt{POWHEG} method. In the final section of Chapter 2 we present a discussion of the bounds on the scale of new physics that can be drawn from comparing our results to Large-Hadron-Collider(LHC) searches. In the third chapter of this manuscript, we will briefly introduce the importance of W+charm events at the LHC followed by a discussion of NLO QCD calculations and heavy meson fragmentation resummation. Next, we give a general method for combining a fixed order, massive calculation with a massless calculation with large logarithms resummed. Numerical results of resummation and an implementation of the afore mention method are discussed followed by future outlook.",ucb,,https://escholarship.org/uc/item/0cp4d788,,,eng,REGULAR,0,0
994,2430,"The Gascon Ã‰nonciatif System: Past, Present, and Future, A Study of Language Contact, Change, Endangerment, and Maintenance","Marcus, Nicole",,2010,A Contrastive Study of the Grammatical Structures of Aymara and Cuzco Kechua,ucb,,https://escholarship.org/uc/item/0cq5t3v1,,,eng,REGULAR,0,0
995,2431,"Defining, Measuring, and Evaluating Path Walkability, and Testing Its Impacts on Transit Usersâ€™ Mode Choice and Walking Distance to the Station","Park, Sungjin",,2008,"The major purpose of this research is to test the effects of street-level urban design attributes on travel behavior. There are two goals: (1) operationalizing path walkability, which includes developing a walkability measurement instrument and quantifying path walkability, and (2) testing the effect of path walkability on transit usersâ€™ access mode choice and walking distance to the station.A case study was conducted in the station area of Mountain View, California. In 2005, three different surveys were done. A station user survey was conducted by distributing self-administered, mail-back questionnaires to the entering transit users at the gates of the station. The user survey collected access mode choices, trip origins, and socio-economic data from 249 transit users who provided their routes. A walker perception survey was conducted with 68 transit users who walked to the station. This on-board survey asked them to score their walking routes. Based on the routes identified by both surveys, this research selected 270 street segments. For each segment, 30 street elements were measured by using a two-page survey instrument. The surveyed street data produced more than 40 path walkability indicators.The first part of this dissertation conducted a factor analysis with the path walkability indicators derived from the 249 surveyed routes, and found four path walkability factors: â€œsidewalk amenities,â€ â€œtraffic impacts,â€ â€œstreet scale and enclosure,â€ and â€œlandscaping elements.â€ With the four factor scores as new variables, a pair of logit analyses was conducted. All four path walkability variables significantly influence transit usersâ€™ mode choice decision â€“ good walkability increases the transit usersâ€™ chance of walking over driving to the station. The second part created a composite walkability index based on the walker perception survey result. The walkability index was also tested in mode choice models, which confirmed that good path walkability increases the chance of walking. The third part conducted a regression analysis of transit usersâ€™ walking distance, and found that a travelerâ€™s walking distance increased by more than 300 feet for every 0.5 increase in the composite walkability score. This research also found a donut-shaped critical walking zone, where walkability mattered more.",ucb,,https://escholarship.org/uc/item/0ct7c30p,,,eng,REGULAR,0,0
996,2432,"Kiezdeutsch, Kiezenglish: English in German Multilingual/-ethnic Speech Communities","Preseau, Lindsay","Rauch, Irmengard;",2018,"This study characterizes the role of English in multilingual/-ethnic speech communities in urban Germany. Drawing on both existing corpus data and new data from a multimodal fieldwork study, this dissertation demonstrates that English plays a far greater role in these speech communities than has previously been recognized. I argue that this is especially true with respect to the Kiezdeutsch speech community, exploring how the social meanings and formal linguistic features of English are reflected in Kiezdeutsch (and vice-versa). Furthermore, I explore the role of English in contact between the current wave of refugees, many of whom speak better English than German upon arrival in Germany, and pre-existing multilingual speech communities. As the older, largely Turkish-speaking population of Germans with migrant backgrounds absorbs the new refugees, many of whom speak unrelated languages and non-mutually intelligible dialects of Arabic, English has become a lingua franca both for daily communication and as a shared second language in German classrooms. Drawing on my fieldwork data collected in refugee language classrooms, I argue that understanding and navigating this new contact situation will require taking seriously the role of English as a Lingua Franca (ELF) in urban Germany. Furthermore, I argue that this contact between new migrant communities and established speakers of Kiezdeutsch necessitates rethinking assumptions about Kiezdeutsch as a â€œnative dialectâ€ of German. Following an overview of the study in Chapter 1, Chapter 2 summarizes the existing literature on ethnolects in Germany. I give a historical overview of the field and, ultimately, illustrate possible reasons that English has largely been ignored in this work. Chapter 3 demonstrates that English does indeed play a role in the existing data by analyzing the influence of English on data from the Kiezdeutsch corpus (KiDKo). Chapters 4 and 5 present new data from my ethnographic-linguistic fieldwork study. Chapter 4 characterizes the influence of English on the repertoires of post-migrant youth associated with the Kiezdeutsch speech community. Chapter 5, on the other hand, examines the role of English among refugee youth, arguing that English constitutes an important point of connection between post-migrant and migrant speech communities. Chapters 4 and 5, taken together, suggest the emergence of a new German-English hybrid repertoire which I term â€œKiezenglish.â€ Finally, Chapter 6 concludes the study by offering implications of the increasing presence of English for descriptive linguistic study of ethnolects such as Kiezdeutsch, as well as for language acquisition, pedagogy and language policy.",ucb,,https://escholarship.org/uc/item/0cv4s57d,,,eng,REGULAR,0,0
997,2433,Three-Dimensional Microwave Imaging for Indoor Environments,"Scott, Simon","Wawrzynek, John;",2017,"Microwave imaging involves the use of antenna arrays, operating at microwave and millimeter-wave frequencies, for capturing  images of real-world objects. Typically, one or more antennas in the array illuminate the scene with a radio-frequency (RF) signal. Part of this signal reflects back to the other antennas, which record both the amplitude and phase of the reflected signal. These reflected RF signals are then processed to form an image of the scene.This work focuses on using planar antenna arrays, operating between 17 and 26 GHz, to capture three-dimensional images of people and other objects inside a room. Such an imaging system enables applications such as indoor positioning and tracking, health monitoring and hand gesture recognition.Microwave imaging techniques based on beamforming cannot be used for indoor imaging, as most objects lie within the array near-field. Therefore, the range-migration algorithm (RMA) is used instead, as it compensates for the curvature of the reflected wavefronts, hence enabling near-field imaging. It is also based on fast-Fourier transforms and is therefore computationally efficient. A number of novel RMA variants were developed to support a wider variety of antenna array configurations, as well as to generate 3-D velocity maps of objects moving around a room.The choice of antenna array configuration, microwave transceiver components and transmit power has a significant effect on both the energy consumed by the imaging system and the quality of the resulting images. A generic microwave imaging testbed was therefore built to characterize the effect of these antenna array parameters on image quality in the 20 GHz band. All variants of the RMA were compared and found to produce good quality three-dimensional images with transmit power levels as low as 1 uW. With an array size of 80x80 antennas, most of the imaging algorithms were able to image objects at 0.5 m range with 12.5 mm resolution, although some were only able to achieve 20 mm resolution. Increasing the size of the antenna array further results in a proportional improvement in image resolution and image SNR, until the resolution reaches the half-wavelength limit.While microwave imaging is not a new technology, it has seen little commercial success due to the cost and power consumption of the large number of antennas and radio transceivers required to build such a system. The cost and power consumption can be reduced by using low-power and low-cost components in both the transmit and receive RF chains, even if these components have poor noise figures. Alternatively, the cost and power consumption can be reduced by decreasing the number of antennas in the array, while keeping the aperture constant. This reduction in antenna count is achieved by randomly depopulating the array, resulting in a sparse antenna array. A novel compressive sensing algorithm, coupled with the wavelet transform, is used to process the samples collected by the sparse array and form a 3-D image of the scene. This algorithm works well for antenna arrays that are up to 96% sparse, equating to a 25 times reduction in the number of required antennas.For microwave imaging to be useful, it needs to capture images of the scene in real time. The architecture of a system capable of capturing real-time 3-D microwave images is therefore designed. The system consists of a modular antenna array, constructed by plugging RF daughtercards into a carrier board. Each daughtercard is a self-contained radio system, containing an antenna, RF transceiver  baseband signal chain, and analog-to-digital converters. A small number of daughtercards have been built, and proven to be suitable for real-time microwave imaging. By arranging these daughtercards in different ways, any antenna array pattern can be built. This architecture allows real-time microwave imaging systems to be rapidly prototyped, while still being able to generate images at video frame rates.",ucb,,https://escholarship.org/uc/item/0cw3c1d4,,,eng,REGULAR,0,0
998,2434,Essays on Structural Estimation in the European Car Market,"Noton Norambuena, Carlos Esteban","Obstfeld, Maurice;",2010,"This dissertation consists of two essays of structural estimation in the European car market, where the first chapter focuses on the demand side and the second chapter focuses on the supply side.In the first chapter, I incorporate home bias in a structural demand, consistent with the strong dominant position of domestic car manufacturers in Europe. I use the Berry, Levinsohn, and Pakes (1995) methodology which considers heterogeneous consumers, controls for price endogeneity, and does not impose decision nests. My estimates suggest that home bias is a key determinant of the market shares differences and is well captured by a fixed effect. Contrary to previous finding, domestic producers do not face a less sensitive demand in their domestic markets than foreign competitors, which is a crucial distinction for pricing behavior.In the second chapter, I estimate the underlying cost structure of the car manufacturers to rationalize two key features: incomplete degree of exchange rate pass-through and gradual price adjustments. Using the methodology developed by Bajari, Benkard, and Levin (2007) to estimate dynamic games, I identify structural cost parameters including destination-currency cost components and price adjustment costs. The main results are: i) the destination-currency cost component should be about 20-30% of total costs in order to rationalize the observed incomplete degree of exchange rate pass-through; ii) relatively small price adjustment costs can rationalize the observed inertia in car prices; iii) there are heterogeneous price adjustment costs at producer-market level suggesting a new dimension of pricing to market behavior; and iv) in this particular dynamic environment, there is a positive relationship between price elasticities and markups.",ucb,,https://escholarship.org/uc/item/0cz1q52v,,,eng,REGULAR,0,0
999,2435,Essays in Political Economics,"Wang, Zenan","Miguel, Edward;",2020,"This dissertation consists of three chapters. The first two chapters study the local governance in China. Guided by theoretical insights originated from the political economics literature, I exploit natural experiments to provide empirical evidence on how local governance decisions are affected by bureaucratic incentives and incoordination between local governments. The third chapter is a descriptive study examining the broader social science research community on researchers' attitudes and behaviors towards open science practices.In Chapter 1 (coauthored with Shaoda Wang), we investigate a question central to the long-standing debates on federalism and decentralization: how does decentralized decision-making distort the governmentsâ€™ incentives to internalize border spillovers, and what are the associated economic and welfare consequences? We attempt to answer these questions by exploiting the ``township merger program'' in China, where thousands of pairs of neighboring townships were required to merge over the last two decades. Collecting novel firm-level geocoded emission and production panel datasets, and exploiting more than 3000 cases of township mergers between 2002 and 2008, we find evidence that local governments are internalizing spillovers on the merging borders. Empirical results show that when a polluting firm suddenly ``moves'' from the border to the center of the town, it receives lower government subsidies, faces higher de facto tax rates, and at the same time reduces pollutant emissions and invests more in emission abatement equipment. Utilizing another transaction-level dataset containing the universe of land auctions in China, we observe that both land prices and new developments of residential buildings increase near the merging borders with polluting firms, indicating that household welfare increases with the internalization of border pollution.Chapter 2 puts focus on bureaucrats running the local governments and try to understand whether and how bureaucrats respond to non-pecuniary incentives besides career concern. Specifically, I investigate whether appointing bureaucrats in the place where they originate would improve or impair their performance. By exploiting exogenous variations in city leadership vacancy and the turnovers in the personnel decision-making body, I find that Chinese municipal leadersâ€™ biographical background indeed plays an important role in their governance decisions. Natives, who grew up in the city they serve, would implement policies that lead to a 7\% reduction in total tax revenue. Estimates from firm-level data also show a significant drop in tax payment from firms during natives' tenure despite increases in outputs and profits. But further examination suggests that only firms in the home counties of native leaders benefit from the tax breaks. With respect to budgetary policies, native officials exhibit a pro-social tendency, allocating a higher share of municipal budget to education and health care, and a lower share to infrastructure. However, despite the changes in budget composition, real outcomes of public goods deteriorate under the native city leadership. Taken together, my results suggest that social proximity hampers bureaucrat performance and facilitates local favoritism.Chapter 3, joint work with Garret Christensen, Elizabeth Levy Paluck, Nicholas Swanson, David Birke, Edward Miguel, and Rebecca Littman, offers a textured description of the current state of social science regarding research transparency and open science practices. Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy---including controversies over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80\% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the pace of change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.",ucb,,https://escholarship.org/uc/item/0d40m7n0,,,eng,REGULAR,0,0
1000,2436,Decision Making Under Radical Uncertainty: A Multiple Case Comparison of Companies Creating New Technologies Featuring New Forms of Personal Data,"Brooks, Andrew Lee","Cheshire, Coye;",2017,"How do companies creating new technologies featuring new forms of personal data respond to the radical uncertainty of designing data ecosystems and accompanying business models with that sensitive data?This dissertation answers this question with an empirical, inductive, multiple case comparison of the development of pioneering fitness technologies by Suunto, Garmin, and Adidas. Cases draw on interviews with key product team members who created these technologies and related public artifacts. Teams responded to this uncertainty in different ways, leading to varying data ecosystem designs that created value in varying ways. Evidence strongly supports that teams' responses are primarily explained by their interpretation of the relative importance of user needs, followed by their interpretations of the relative importance of information economics concepts and their own expertise, and lastly regulations regarding the collection and use of personal data.These findings pose implications for the creation of future technologies featuring new forms of personal data. Teams' responses will evolve as user needs evolve, and will offer insights into the dynamics and terms of teams' social license for their technologies. New regulations may impact how these ecosystem designs create value.",ucb,,https://escholarship.org/uc/item/0d53t854,,,eng,REGULAR,0,0
